{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ac7e6c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b1d0881f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'exmaple.txt', 'pages': 1, 'author': 'Krish Naik', 'date_created': '2025-01-01'}, page_content='this is the main text content I am using to create RAG')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(\n",
    "    page_content=\"this is the main text content I am using to create RAG\",\n",
    "    metadata={\n",
    "        \"source\":\"exmaple.txt\",\n",
    "        \"pages\":1,\n",
    "        \"author\":\"Krish Naik\",\n",
    "        \"date_created\":\"2025-01-01\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "19c08bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs (\"../data/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b53e8e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"✅ Sample text files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fae4e232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "### TextLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader(\"../data/text_files/python_intro.txt\",encoding=\"utf-8\")\n",
    "document=loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83e704ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    '),\n",
       " Document(metadata={'source': '..\\\\data\\\\text_files\\\\python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Directory Loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob=\"**/*.txt\", ## Pattern to match files  \n",
    "    loader_cls= TextLoader, ##loader class to use\n",
    "    loader_kwargs={'encoding': 'utf-8'},\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "documents=dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "be75700e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 0}, page_content='GOVERNMENT OF INDIA\\nBUDGET 2023-2024\\nSPEECH\\nOF\\nNIRMALA SITHARAMAN\\nMINISTER OF FINANCE\\nFebruary 1,  2023'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 1}, page_content=''),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 2}, page_content='CONTENTS \\nPART-A \\nPage No.\\n\\uf0b7 \\nIntroduction \\n1\\n\\uf0b7 \\nAchievements since 2014: Leaving no one behind \\n2\\n\\uf0b7 \\nVision for Amrit Kaal – an empowered and inclusive economy \\n3\\n\\uf0b7 \\nPriorities of this Budget \\n5\\ni. Inclusive Development  \\nii. Reaching the Last Mile \\niii. Infrastructure and Investment \\niv. Unleashing the Potential \\nv. Green Growth \\nvi. Youth Power  \\nvii. Financial Sector \\n \\n \\n \\n \\n \\n \\n \\n \\n\\uf0b7 \\nFiscal Management \\n24\\nPART B\\nIndirect Taxes\\n27\\n\\uf0b7 \\nGreen Mobility \\n\\uf0b7 \\nElectronics  \\n\\uf0b7 \\nElectrical  \\n\\uf0b7 \\nChemicals and Petrochemicals  \\n\\uf0b7 \\nMarine products \\n\\uf0b7 \\nLab Grown Diamonds \\n\\uf0b7 \\nPrecious Metals \\n\\uf0b7 \\nMetals \\n\\uf0b7 \\nCompounded Rubber \\n\\uf0b7 \\nCigarettes \\nDirect Taxes\\n30\\n\\uf0b7 \\nMSMEs and Professionals  \\n\\uf0b7 \\nCooperation \\n\\uf0b7 \\nStart-Ups \\n\\uf0b7 \\nAppeals \\n\\uf0b7 \\nBetter targeting of tax concessions \\n\\uf0b7 \\nRationalisation \\n\\uf0b7 \\nOthers \\n\\uf0b7 \\nPersonal Income Tax \\nAnnexures\\n35\\n\\uf0b7 \\nAnnexure to Part B of the Budget Speech 2023-24 \\ni. Amendments relating to Direct Taxes \\nii. Amendments relating to Indirect Taxes'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 3}, page_content=''),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 4}, page_content='Budget 2023-2024 \\n \\nSpeech of \\nNirmala Sitharaman \\nMinister of Finance \\nFebruary 1, 2023 \\nHon’ble Speaker,  \\n \\nI present the Budget for 2023-24. This is the first Budget in Amrit \\nKaal. \\nIntroduction \\n1. \\nThis Budget hopes to build on the foundation laid in the previous \\nBudget, and the blueprint drawn for India@100. We envision a prosperous \\nand inclusive India, in which the fruits of development reach all regions and \\ncitizens, especially our youth, women, farmers, OBCs, Scheduled Castes and \\nScheduled Tribes.  \\n2. \\nIn the 75th year of our Independence, the world has recognised the \\nIndian economy as a ‘bright star’. Our current year’s economic growth is \\nestimated to be at 7 per cent. It is notable that this is the highest among all \\nthe major economies. This is in spite of the massive slowdown globally \\ncaused by Covid-19 and a war. The Indian economy is therefore on the right \\ntrack, and despite a time of challenges, heading towards a bright future.  \\n3. \\nToday as Indians stands with their head held high, and the world \\nappreciates India’s achievements and successes, we are sure that elders \\nwho had fought for India’s independence, will with joy, bless us our \\nendeavors going forward. \\nResilience amidst multiple crises \\n4. \\nOur focus on wide-ranging reforms and sound policies, implemented \\nthrough Sabka Prayas resulting in Jan Bhagidari and targeted support to \\nthose in need, helped us perform well in trying times. India’s rising global'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 5}, page_content='2 \\n \\n \\n \\nprofile is because of several accomplishments: unique world class digital \\npublic infrastructure, e.g., Aadhaar, Co-Win and UPI; Covid vaccination drive \\nin unparalleled scale and speed; proactive role in frontier areas such as \\nachieving the climate related goals, mission LiFE, and National Hydrogen \\nMission.  \\n5. \\nDuring the Covid-19 pandemic, we ensured that no one goes to bed \\nhungry, with a scheme to supply free food grains to over 80 crore persons \\nfor 28 months. Continuing our commitment to ensure food and nutritional \\nsecurity, we are implementing, from 1st January 2023, a scheme to supply \\nfree food grain to all Antyodaya and priority households for the next one \\nyear, under PM Garib Kalyan Anna Yojana (PMGKAY). The entire \\nexpenditure of about ` 2 lakh crore will be borne by the Central \\nGovernment. \\nG20 Presidency: Steering the global agenda through challenges \\n6. \\nIn these times of global challenges, the G20 Presidency gives us a \\nunique opportunity to strengthen India’s role in the world economic order. \\nWith the theme of ‘Vasudhaiva Kutumbakam’, we are steering an \\nambitious, people-centric agenda to address global challenges, and to \\nfacilitate sustainable economic development.  \\nAchievements since 2014: Leaving no one behind \\n7. \\nThe government’s efforts since 2014 have ensured for all citizens a \\nbetter quality of living and a life of dignity. The per capita income has more \\nthan doubled to ` 1.97 lakh.   \\n8. \\nIn these nine years, the Indian economy has increased in size from \\nbeing 10th to 5th largest in the world. We have significantly improved our \\nposition as a well-governed and innovative country with a conducive \\nenvironment for business as reflected in several global indices. We have \\nmade significant progress in many Sustainable Development Goals.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 6}, page_content='3 \\n \\n \\n \\n9. \\nThe economy has become a lot more formalised as reflected in the \\nEPFO membership more than doubling to 27 crore, and 7,400 crore digital \\npayments of ` 126 lakh crore through UPI in 2022.    \\n10. \\nThe \\nefficient \\nimplementation \\nof \\nmany \\nschemes, \\nwith \\nuniversalisation of targeted benefits, has resulted in inclusive development. \\nSome of the schemes are: \\ni. \\n11.7 crore household toilets under Swachh Bharat Mission,  \\nii. \\n9.6 crore LPG connections under Ujjawala,  \\niii. \\n220 crore Covid vaccination of 102 crore persons,    \\niv. \\n47.8 crore PM Jan Dhan bank accounts, \\nv. \\nInsurance cover for 44.6 crore persons under PM Suraksha \\nBima and PM Jeevan Jyoti Yojana, and \\nvi. \\nCash transfer of ` 2.2 lakh crore to over 11.4 crore farmers \\nunder PM Kisan Samman Nidhi. \\nVision for Amrit Kaal – an empowered and inclusive economy \\n11. \\nOur vision for the Amrit Kaal includes technology-driven and \\nknowledge-based economy with strong public finances, and a robust \\nfinancial sector. To achieve this, Jan Bhagidari through Sabka Saath Sabka \\nPrayas is essential.   \\n12. \\nThe economic agenda for achieving this vision focuses on three \\nthings: first, facilitating ample opportunities for citizens, especially the \\nyouth, to fulfil their aspirations; second, providing strong impetus to growth \\nand job creation; and third, strengthening macro-economic stability.    \\n13. \\nTo service these focus areas in our journey to India@100, we believe \\nthat the following four opportunities can be transformative during Amrit \\nKaal.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 7}, page_content='4 \\n \\n \\n \\n1) Economic Empowerment of Women: Deendayal Antyodaya Yojana \\nNational Rural Livelihood Mission has achieved remarkable success \\nby mobilizing rural women into 81 lakh Self Help Groups. We will \\nenable these groups to reach the next stage of economic \\nempowerment through formation of large producer enterprises or \\ncollectives with each having several thousand members and \\nmanaged professionally. They will be helped with supply of raw \\nmaterials and for better design, quality, branding and marketing of \\ntheir products. Through supporting policies, they will be enabled to \\nscale up their operations to serve the large consumer markets, as \\nhas been the case with several start-ups growing into ‘Unicorns’. \\n2) PM VIshwakarma KAushal Samman (PM VIKAS): For centuries, \\ntraditional artisans and craftspeople, who work with their hands \\nusing tools, have brought renown for India. They are generally \\nreferred to as Vishwakarma. The art and handicraft created by them \\nrepresents the true spirit of Atmanirbhar Bharat. For the first time, a \\npackage of assistance for them has been conceptualized. The new \\nscheme will enable them to improve the quality, scale and reach of \\ntheir products, integrating them with the MSME value chain. The \\ncomponents of the scheme will include not only financial support \\nbut also access to advanced skill training, knowledge of modern \\ndigital techniques and efficient green technologies, brand \\npromotion, linkage with local and global markets, digital payments, \\nand social security. This will greatly benefit the Scheduled Castes, \\nScheduled Tribes, OBCs, women and people belonging to the weaker \\nsections.  \\n3) Tourism: The country offers immense attraction for domestic as well \\nas foreign tourists. There is a large potential to be tapped in tourism. \\nThe sector holds huge opportunities for jobs and entrepreneurship \\nfor youth in particular.  Promotion of tourism will be taken up on \\nmission mode, with active participation of states, convergence of \\ngovernment programmes and public-private partnerships.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 8}, page_content='5 \\n \\n \\n \\n4) Green Growth: We are implementing many programmes for green \\nfuel, green energy, green farming, green mobility, green buildings, \\nand green equipment, and policies for efficient use of energy across \\nvarious economic sectors. These green growth efforts help in \\nreducing carbon intensity of the economy and provides for large-\\nscale green job opportunities.  \\nPriorities of this Budget \\n14. \\nThe Budget adopts the following seven priorities. They complement \\neach other and act as the ‘Saptarishi’ guiding us through the Amrit Kaal. \\n1) Inclusive Development  \\n2) Reaching the Last Mile \\n3) Infrastructure and Investment \\n4) Unleashing the Potential \\n5) Green Growth \\n6) Youth Power  \\n7) Financial Sector \\nPriority 1: Inclusive Development  \\n15. \\nThe Government’s philosophy of Sabka Saath Sabka Vikas has \\nfacilitated inclusive development covering in specific, farmers, women, \\nyouth, OBCs, Scheduled Castes, Scheduled Tribes, divyangjan and \\neconomically weaker sections, and overall priority for the underprivileged \\n(vanchiton ko variyata). There has also been a sustained focus on Jammu & \\nKashmir, Ladakh and the North-East. This Budget builds on those efforts.  \\nAgriculture and Cooperation   \\nDigital Public Infrastructure for Agriculture \\n16. \\nDigital public infrastructure for agriculture will be built as an open \\nsource, open standard and inter operable public good. This will enable'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 9}, page_content=\"6 \\n \\n \\n \\ninclusive, farmer-centric solutions through relevant information services for \\ncrop planning and health, improved access to farm inputs, credit, and \\ninsurance, help for crop estimation, market intelligence, and support for \\ngrowth of agri-tech industry and start-ups.  \\nAgriculture Accelerator Fund \\n17. \\nAn Agriculture Accelerator Fund will be set-up to encourage agri-\\nstartups by young entrepreneurs in rural areas. The Fund will aim at \\nbringing innovative and affordable solutions for challenges faced by \\nfarmers. It will also bring in modern technologies to transform agricultural \\npractices, increase productivity and profitability. \\nEnhancing productivity of cotton crop  \\n18. \\nTo enhance the productivity of extra-long staple cotton, we will \\nadopt a cluster-based and value chain approach through Public Private \\nPartnerships (PPP). This will mean collaboration between farmers, state and \\nindustry for input supplies, extension services, and market linkages. \\nAtmanirbhar Horticulture Clean Plant Program  \\n19. \\nWe will launch an Atmanirbhar Clean Plant Program to boost \\navailability of disease-free, quality planting material for high value \\nhorticultural crops at an outlay of ` 2,200 crore. \\nGlobal Hub for Millets: ‘Shree Anna’ \\n20. \\n“India is at the forefront of popularizing Millets, whose consumption \\nfurthers nutrition, food security and welfare of farmers,” said Hon’ble Prime \\nMinister. \\n21. \\n We are the largest producer and second largest exporter of ‘Shree \\nAnna’ in the world. We grow several types of 'Shree Anna' such as jowar, \\nragi, bajra, kuttu, ramdana, kangni, kutki, kodo, cheena, and sama. These \\nhave a number of health benefits, and have been an integral part of our \\nfood for centuries. I acknowledge with pride the huge service done by small\"),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 10}, page_content=\"7 \\n \\n \\n \\nfarmers in contributing to the health of fellow citizens by growing these \\n‘Shree Anna’.  \\n22. \\nNow to make India a global hub for 'Shree Anna', the Indian Institute \\nof Millet Research, Hyderabad will be supported as the Centre of Excellence \\nfor sharing best practices, research and technologies at the international \\nlevel.    \\nAgriculture Credit  \\n23. \\nThe \\nagriculture \\ncredit \\ntarget \\nwill \\nbe \\nincreased  \\nto ` 20 lakh crore with focus on animal husbandry, dairy and fisheries.  \\nFisheries \\n24. \\nWe will launch a new sub-scheme of PM Matsya Sampada Yojana \\nwith targeted investment of ` 6,000 crore to further enable activities of \\nfishermen, fish vendors, and micro & small enterprises, improve value chain \\nefficiencies, and expand the market. \\nCooperation \\n25. \\nFor farmers, especially small and marginal farmers, and other \\nmarginalised sections, the government is promoting cooperative-based \\neconomic development model. A new Ministry of Cooperation was formed \\nwith a mandate to realise the vision of ‘Sahakar Se Samriddhi’. To realise \\nthis vision, the government has already initiated computerisation of 63,000 \\nPrimary Agricultural Credit Societies (PACS) with an investment of ` 2,516 \\ncrore. In consultation with all stakeholders and states, model bye-laws for \\nPACS were formulated enabling them to become multipurpose PACS. A \\nnational cooperative database is being prepared for country-wide mapping \\nof cooperative societies.  \\n26. \\nWith this backdrop, we will implement a plan to set up massive \\ndecentralised storage capacity. This will help farmers store their produce \\nand realize remunerative prices through sale at appropriate times. The \\ngovernment will also facilitate setting up of a large number of multipurpose\"),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 11}, page_content='8 \\n \\n \\n \\ncooperative societies, primary fishery societies and dairy cooperative \\nsocieties in uncovered panchayats and villages in the next 5 years.  \\nHealth, Education and Skilling \\nNursing Colleges   \\n27. \\nOne hundred and fifty-seven new nursing colleges will be \\nestablished in co-location with the existing 157 medical colleges established \\nsince 2014. \\nSickle Cell Anaemia Elimination Mission \\n28. \\nA Mission to eliminate Sickle Cell Anaemia by 2047 will be launched. \\nIt will entail awareness creation, universal screening of 7 crore people in the \\nage group of 0-40 years in affected tribal areas, and counselling through \\ncollaborative efforts of central ministries and state governments.  \\nMedical Research  \\n29. \\nFacilities in select ICMR Labs will be made available for research by \\npublic and private medical college faculty and private sector R&D teams for \\nencouraging collaborative research and innovation. \\nPharma Innovation  \\n30. \\nA new programme to promote research and innovation in \\npharmaceuticals will be taken up through centers of excellence. We shall \\nalso encourage industry to invest in research and development in specific \\npriority areas.  \\nMultidisciplinary courses for medical devices \\n31. \\nDedicated multidisciplinary courses for medical devices will be \\nsupported in existing institutions to ensure availability of skilled manpower \\nfor futuristic medical technologies, high-end manufacturing and research.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 12}, page_content='9 \\n \\n \\n \\nTeachers’ Training \\n32. \\nTeachers’ training will be re-envisioned through innovative \\npedagogy, curriculum transaction, continuous professional development, \\ndipstick surveys, and ICT implementation. The District Institutes of \\nEducation and Training will be developed as vibrant institutes of excellence \\nfor this purpose.   \\nNational Digital Library for Children and Adolescents  \\n33. \\nA National Digital Library for children and adolescents will be set-up \\nfor facilitating availability of quality books across geographies, languages, \\ngenres and levels, and device agnostic accessibility. States will be \\nencouraged to set up physical libraries for them at panchayat and ward \\nlevels and provide infrastructure for accessing the National Digital Library \\nresources. \\n34. \\nAdditionally, to build a culture of reading, and to make up for \\npandemic-time learning loss, the National Book Trust, Children’s Book Trust \\nand other sources will be encouraged to provide and replenish non-\\ncurricular titles in regional languages and English to these physical libraries. \\nCollaboration with NGOs that work in literacy will also be a part of this \\ninitiative. To inculcate financial literacy, financial sector regulators and \\norganizations will be encouraged to provide age-appropriate reading \\nmaterial to these libraries.  \\nPriority 2: Reaching the Last Mile \\n35. \\nPrime Minister Vajpayee’s government had formed the Ministry of \\nTribal Affairs and the Department of Development of North-Eastern Region. \\nTo provide a sharper focus to the objective of ‘reaching the last mile’, our \\ngovernment has formed the ministries of AYUSH, Fisheries, Animal \\nHusbandry and Dairying, Skill Development, Jal Shakti and Cooperation.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 13}, page_content='10 \\n \\n \\n \\nAspirational Districts and Blocks Programme \\n36. \\nBuilding on the success of the Aspirational Districts Programme, the \\nGovernment has recently launched the Aspirational Blocks Programme \\ncovering 500 blocks for saturation of essential government services across \\nmultiple domains such as health, nutrition, education, agriculture, water \\nresources, financial inclusion, skill development, and basic infrastructure. \\nPradhan Mantri PVTG Development Mission \\n37. \\nTo improve socio-economic conditions of the particularly vulnerable \\ntribal groups (PVTGs), Pradhan Mantri PVTG Development Mission will be \\nlaunched. This will saturate PVTG families and habitations with basic \\nfacilities such as safe housing, clean drinking water and sanitation, \\nimproved access to education, health and nutrition, road and telecom \\nconnectivity, and sustainable livelihood opportunities. An amount  \\nof ` 15,000 crore will be made available to implement the Mission in the \\nnext three years under the Development Action Plan for the Scheduled \\nTribes.  \\nEklavya Model Residential Schools \\n38. \\nIn the next three years, centre will recruit 38,800 teachers and \\nsupport staff for the 740 Eklavya Model Residential Schools, serving 3.5 lakh \\ntribal students. \\nWater for Drought Prone Region \\n39. \\nIn the drought prone central region of Karnataka, central assistance \\nof ` 5,300 crore will be given to Upper Bhadra Project to provide \\nsustainable micro irrigation and filling up of surface tanks for drinking \\nwater.  \\nPM Awas Yojana \\n40. \\nThe \\noutlay \\nfor \\nPM \\nAwas \\nYojana \\nis \\nbeing \\nenhanced \\n by 66 per cent to over ` 79,000 crore.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 14}, page_content='11 \\n \\n \\n \\nBharat Shared Repository of Inscriptions (Bharat SHRI) \\n41. \\n‘Bharat Shared Repository of Inscriptions’ will be set up in a digital \\nepigraphy museum, with digitization of one lakh ancient inscriptions in the \\nfirst stage.   \\nSupport for poor prisoners \\n42. \\nFor poor persons who are in prisons and unable to afford the \\npenalty or the bail amount, required financial support will be provided.   \\n \\nPriority 3: Infrastructure & Investment \\n43. \\nInvestments in Infrastructure and productive capacity have a large \\nmultiplier impact on growth and employment. After the subdued period of \\nthe pandemic, private investments are growing again. The Budget takes the \\nlead once again to ramp up the virtuous cycle of investment and job \\ncreation.    \\n \\nCapital Investment as driver of growth and jobs \\n44. \\nCapital investment outlay is being increased steeply for the third \\nyear in a row by 33 per cent to ` 10 lakh crore, which would be 3.3 per cent \\nof GDP. This will be almost three times the outlay in 2019-20.   \\n45. \\nThis substantial increase in recent years is central to the \\ngovernment’s efforts to enhance growth potential and job creation, crowd-\\nin private investments, and provide a cushion against global headwinds. \\nEffective Capital Expenditure  \\n46. \\nThe direct capital investment by the Centre is complemented by the \\nprovision made for creation of capital assets through Grants-in-Aid to \\nStates. The ‘Effective Capital Expenditure’ of the Centre is budgeted at  \\n` 13.7 lakh crore, which will be 4.5 per cent of GDP.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 15}, page_content='12 \\n \\n \\n \\nSupport to State Governments for Capital Investment \\n47. \\nI have decided to continue the 50-year interest free loan to state \\ngovernments for one more year to spur investment in infrastructure and to \\nincentivize them for complementary policy actions, with a significantly \\nenhanced outlay of ` 1.3 lakh crore.   \\nEnhancing opportunities for private investment in Infrastructure \\n48. \\nThe newly established Infrastructure Finance Secretariat will assist \\nall stakeholders for more private investment in infrastructure, including \\nrailways, roads, urban infrastructure and power, which are predominantly \\ndependent on public resources.  \\nHarmonized Master List of Infrastructure \\n49. \\nThe Harmonized Master List of Infrastructure will be reviewed by an \\nexpert committee for recommending the classification and financing \\nframework suitable for Amrit Kaal. \\nRailways \\n50. \\nA capital outlay of ` 2.40 lakh crore has been provided for the \\nRailways. This highest ever outlay is about 9 times the outlay made in 2013-\\n14.  \\nLogistics \\n51. \\nOne hundred critical transport infrastructure projects, for last and \\nfirst mile connectivity for ports, coal, steel, fertilizer, and food grains sectors \\nhave been identified. They will be taken up on priority with investment of \\n` 75,000 crore, including ` 15,000 crore from private sources. \\nRegional Connectivity \\n52. \\nFifty additional airports, heliports, water aerodromes and advance \\nlanding grounds will be revived for improving regional air connectivity.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 16}, page_content='13 \\n \\n \\n \\nSustainable Cities of Tomorrow \\n53. \\nStates and cities will be encouraged to undertake urban planning \\nreforms and actions to transform our cities into ‘sustainable cities of \\ntomorrow’. This means efficient use of land resources, adequate resources \\nfor \\nurban \\ninfrastructure, \\ntransit-oriented \\ndevelopment, \\nenhanced \\navailability and affordability of urban land, and opportunities for all.  \\nMaking Cities ready for Municipal Bonds \\n54. \\nThrough property tax governance reforms and ring-fencing user \\ncharges on urban infrastructure, cities will be incentivized to improve their \\ncredit worthiness for municipal bonds.   \\nUrban Infrastructure Development Fund  \\n55. \\nLike the RIDF, an Urban Infrastructure Development Fund (UIDF) will \\nbe established through use of priority sector lending shortfall. This will be \\nmanaged by the National Housing Bank, and will be used by public agencies \\nto create urban infrastructure in Tier 2 and Tier 3 cities. States will be \\nencouraged to leverage resources from the grants of the 15th Finance \\nCommission, as well as existing schemes, to adopt appropriate user charges \\nwhile \\naccessing \\nthe \\nUIDF. \\nWe \\nexpect \\nto \\nmake  \\navailable ` 10,000 crore per annum for this purpose. \\nUrban Sanitation \\n56. \\nAll cities and towns will be enabled for 100 per cent mechanical \\ndesludging of septic tanks and sewers to transition from manhole to \\nmachine-hole mode. Enhanced focus will be provided for scientific \\nmanagement of dry and wet waste. \\nPriority 4: Unleashing the Potential \\n57. \\n“Good Governance is the key to a nation’s progress. Our government \\nis committed to providing a transparent and accountable administration \\nwhich works for the betterment and welfare of the common citizen,” said \\nHon’ble Prime Minister.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 17}, page_content='14 \\n \\n \\n \\nMission Karmayogi \\n58. \\nUnder Mission Karmayogi, Centre, States and Union Territories are \\nmaking and implementing capacity-building plans for civil servants. The \\ngovernment has also launched an integrated online training platform, iGOT \\nKarmayogi, to provide continuous learning opportunities for lakhs of \\ngovernment employees to upgrade their skills and facilitate people-centric \\napproach.   \\n59. \\nFor \\nenhancing \\nease \\nof \\ndoing \\nbusiness, \\nmore \\nthan  \\n39,000 \\ncompliances \\nhave \\nbeen \\nreduced \\nand \\nmore \\nthan  \\n3,400 legal provisions have been decriminalized. For furthering the trust-\\nbased governance, we have introduced the Jan Vishwas Bill to amend 42 \\nCentral Acts. This Budget proposes a series of measures to unleash the \\npotential of our economy.  \\nCentres of Excellence for Artificial Intelligence \\n60. \\nFor realizing the vision of “Make AI in India and Make AI work for \\nIndia”, three centres of excellence for Artificial Intelligence will be set-up in \\ntop educational institutions. Leading industry players will partner in \\nconducting interdisciplinary research, develop cutting-edge applications and \\nscalable problem solutions in the areas of agriculture, health, and \\nsustainable cities. This will galvanize an effective AI ecosystem and nurture \\nquality human resources in the field. \\nNational Data Governance Policy \\n61. \\nTo unleash innovation and research by start-ups and academia, a \\nNational Data Governance Policy will be brought out. This will enable access \\nto anonymized data. \\nSimplification of Know Your Customer (KYC) process  \\n62. \\nThe KYC process will be simplified adopting a ‘risk-based’ instead of \\n‘one size fits all’ approach. The financial sector regulators will also be'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 18}, page_content='15 \\n \\n \\n \\nencouraged to have a KYC system fully amenable to meet the needs of \\nDigital India. \\nOne stop solution for identity and address updating \\n63. \\nA one stop solution for reconciliation and updating of identity and \\naddress of individuals maintained by various government agencies, \\nregulators and regulated entities will be established using DigiLocker service \\nand Aadhaar as foundational identity.    \\nCommon Business Identifier   \\n64. \\nFor the business establishments required to have a Permanent \\nAccount Number (PAN), the PAN will be used as the common identifier for \\nall digital systems of specified government agencies. This will bring ease of \\ndoing business; and it will be facilitated through a legal mandate. \\nUnified Filing Process \\n65. \\nFor obviating the need for separate submission of same information \\nto different government agencies, a system of ‘Unified Filing Process’ will be \\nset-up. Such filing of information or return in simplified forms on a common \\nportal, will be shared with other agencies as per filer’s choice.  \\nVivad se Vishwas I – Relief for MSMEs  \\n66. \\nIn cases of failure by MSMEs to execute contracts during the Covid \\nperiod, 95 per cent of the forfeited amount relating to bid or performance \\nsecurity, will be returned to them by government and government \\nundertakings.  This will provide relief to MSMEs.  \\nVivad se Vishwas II – Settling Contractual Disputes  \\n67. \\nTo settle contractual disputes of government and government \\nundertakings, wherein arbitral award is under challenge in a court, a \\nvoluntary settlement scheme with standardized terms will be introduced. \\nThis will be done by offering graded settlement terms depending on \\npendency level of the dispute.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 19}, page_content='16 \\n \\n \\n \\nState Support Mission  \\n68. \\nThe State Support Mission of NITI Aayog will be continued for three \\nyears for our collective efforts towards national priorities. \\nResult Based Financing  \\n69. \\nTo better allocate scarce resources for competing development \\nneeds, the financing of select schemes will be changed, on a pilot basis, \\nfrom ‘input-based’ to ‘result-based’. \\nE-Courts  \\n70. \\nFor \\nefficient \\nadministration \\nof \\njustice, \\nPhase-3 \\nof \\nthe \\n E-Courts \\nproject \\nwill \\nbe \\nlaunched \\nwith \\nan \\noutlay  \\nof ` 7,000 crore.  \\nFintech Services  \\n71. \\nFintech services in India have been facilitated by our digital public \\ninfrastructure including Aadhaar, PM Jan Dhan Yojana, Video KYC, India \\nStack and UPI. To enable more Fintech innovative services, the scope of \\ndocuments available in DigiLocker for individuals will be expanded.  \\nEntity DigiLocker  \\n72. \\nAn Entity DigiLocker will be set up for use by MSMEs, large business \\nand charitable trusts. This will be towards storing and sharing documents \\nonline securely, whenever needed, with various authorities, regulators, \\nbanks and other business entities.  \\n5G Services \\n73. \\nOne \\nhundred \\nlabs \\nfor \\ndeveloping \\napplications \\nusing  \\n5G services will be set up in engineering institutions to realise a new range \\nof opportunities, business models, and employment potential. The labs will \\ncover, among others, applications such as smart classrooms, precision \\nfarming, intelligent transport systems, and health care applications.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 20}, page_content='17 \\n \\n \\n \\nLab Grown Diamonds \\n74. \\nLab Grown Diamonds (LGD) is a technology-and innovation-driven \\nemerging sector with high employment potential. These environment-\\nfriendly diamonds which have optically and chemically the same properties \\nas natural diamonds. To encourage indigenous production of LGD seeds and \\nmachines and to reduce import dependency, a research and development \\ngrant will be provided to one of the IITs for five years.   \\n75. \\nTo reduce the cost of production, a proposal to review the custom \\nduty rate on LGD seeds will be indicated in Part B of the speech.   \\nPriority 5: Green Growth \\n76. \\nHon’ble Prime Minister has given a vision for “LiFE”, or Lifestyle for \\nEnvironment, to spur a movement of environmentally conscious lifestyle. \\nIndia is moving forward firmly for the ‘panchamrit’ and net-zero carbon \\nemission by 2070 to usher in green industrial and economic transition. This \\nBudget builds on our focus on green growth.    \\nGreen Hydrogen Mission \\n77. \\nThe recently launched National Green Hydrogen Mission, with an \\noutlay of ` 19,700 crores, will facilitate transition of the economy to low \\ncarbon intensity, reduce dependence on fossil fuel imports, and make the \\ncountry assume technology and market leadership in this sunrise sector. \\nOur target is to reach an annual production of 5 MMT by 2030.  \\nEnergy Transition \\n78. \\nThis Budget provides ` 35,000 crore for priority capital investments \\ntowards energy transition and net zero objectives, and energy security by \\nMinistry of Petroleum & Natural Gas.  \\nEnergy Storage Projects \\n79. \\nTo steer the economy on the sustainable development path, Battery \\nEnergy Storage Systems with capacity of 4,000 MWH will be supported with'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 21}, page_content='18 \\n \\n \\n \\nViability Gap Funding. A detailed framework for Pumped Storage Projects \\nwill also be formulated.  \\nRenewable Energy Evacuation \\n80. \\nThe Inter-state transmission system for evacuation and grid \\nintegration of 13 GW renewable energy from Ladakh will be constructed \\nwith investment of ` 20,700 crore including central support of ` 8,300 crore. \\nGreen Credit Programme \\n81. \\nFor encouraging behavioural change, a Green Credit Programme will \\nbe notified under the Environment (Protection) Act. This will incentivize \\nenvironmentally sustainable and responsive actions by companies, \\nindividuals and local bodies, and help mobilize additional resources for such \\nactivities.  \\nPM-PRANAM \\n82. \\n“PM Programme for Restoration, Awareness, Nourishment and \\nAmelioration of Mother Earth” will be launched to incentivize States and \\nUnion Territories to promote alternative fertilizers and balanced use of \\nchemical fertilizers. \\nGOBARdhan scheme \\n83. \\n500 new ‘waste to wealth’ plants under GOBARdhan (Galvanizing \\nOrganic Bio-Agro Resources Dhan) scheme will be established for promoting \\ncircular economy. These will include 200 compressed biogas (CBG) plants, \\nincluding 75 plants in urban areas, and 300 community or cluster-based \\nplants at total investment of ` 10,000 crore. I will refer to this in Part B. In \\ndue course, a 5 per cent CBG mandate will be introduced for all \\norganizations marketing natural and bio gas. For collection of bio-mass and \\ndistribution of bio-manure, appropriate fiscal support will be provided.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 22}, page_content='19 \\n \\n \\n \\nBhartiya Prakritik Kheti Bio-Input Resource Centres   \\n84. \\nOver the next 3 years, we will facilitate 1 crore farmers to adopt \\nnatural farming. For this, 10,000 Bio-Input Resource Centres will be set-up, \\ncreating a national-level distributed micro-fertilizer and pesticide \\nmanufacturing network.  \\nMISHTI \\n85. \\nBuilding on India’s success in afforestation, ‘Mangrove Initiative for \\nShoreline Habitats & Tangible Incomes’, MISHTI, will be taken up for \\nmangrove plantation along the coastline and on salt pan lands, wherever \\nfeasible, through convergence between MGNREGS, CAMPA Fund and other \\nsources. \\nAmrit Dharohar \\n86. \\nWetlands are vital ecosystems which sustain biological diversity. In \\nhis latest Mann Ki Baat, the Prime Minister said, “Now the total number of \\nRamsar sites in our country has increased to 75. Whereas, before 2014, \\nthere were only 26…” Local communities have always been at the forefront \\nof conservation efforts. The government will promote their unique \\nconservation values through Amrit Dharohar, a scheme that will be \\nimplemented over the next three years to encourage optimal use of \\nwetlands, \\nand \\nenhance \\nbio-diversity, \\ncarbon \\nstock,  \\neco-tourism opportunities and income generation for local communities.  \\nCoastal Shipping \\n87. \\nCoastal shipping will be promoted as the energy efficient and lower \\ncost mode of transport, both for passengers and freight, through PPP mode \\nwith viability gap funding.   \\nVehicle Replacement \\n88. \\nReplacing old polluting vehicles is an important part of greening our \\neconomy. In furtherance of the vehicle scrapping policy mentioned in \\nBudget 2021-22, I have allocated adequate funds to scrap old vehicles of'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 23}, page_content='20 \\n \\n \\n \\nthe Central Government. States will also be supported in replacing old \\nvehicles and ambulances.  \\nPriority 6: Youth Power \\n \\n89. \\nTo empower our youth and help the ‘Amrit Peedhi’ realize their \\ndreams, we have formulated the National Education Policy, focused on \\nskilling, adopted economic policies that facilitate job creation at scale, and \\nhave supported business opportunities.   \\nPradhan Mantri Kaushal Vikas Yojana 4.0 \\n90. \\nPradhan Mantri Kaushal Vikas Yojana 4.0 will be launched to skill \\nlakhs of youth within the next three years.  On-job training, industry \\npartnership, and alignment of courses with needs of industry will be \\nemphasized. The scheme will also cover new age courses for Industry 4.0 \\nlike coding, AI, robotics, mechatronics, IOT, 3D printing, drones, and soft \\nskills. To skill youth for international opportunities, 30 Skill India \\nInternational Centres will be set up across different States.  \\n \\nSkill India Digital Platform \\n91. \\nThe digital ecosystem for skilling will be further expanded with the \\nlaunch of a unified Skill India Digital platform for: \\n\\uf0b7 enabling demand-based formal skilling,  \\n\\uf0b7 linking with employers including MSMEs, and \\n\\uf0b7 facilitating access to entrepreneurship schemes.  \\nNational Apprenticeship Promotion Scheme \\n92. \\nTo provide stipend support to 47 lakh youth in three years, Direct \\nBenefit Transfer under a pan-India National Apprenticeship Promotion \\nScheme will be rolled out.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 24}, page_content='21 \\n \\n \\n \\nTourism \\n93. \\nWith \\nan \\nintegrated \\nand \\ninnovative \\napproach, \\nat  \\nleast 50 destinations will be selected through challenge mode. In addition to \\naspects such as physical connectivity, virtual connectivity, tourist guides, \\nhigh standards for food streets and tourists’ security, all the relevant \\naspects would be made available on an App to enhance tourist experience. \\nEvery destination would be developed as a complete package. The focus of \\ndevelopment of tourism would be on domestic as well as foreign tourists.  \\n94. \\nSector specific skilling and entrepreneurship development will be \\ndovetailed to achieve the objectives of the ‘Dekho Apna Desh’ initiative. \\nThis was launched as an appeal by the Prime Minister to the middle class to \\nprefer domestic tourism over international tourism. For integrated \\ndevelopment of theme-based tourist circuits, the ‘Swadesh Darshan \\nScheme’ was also launched. Under the Vibrant Villages Programme, tourism \\ninfrastructure and amenities will also be facilitated in border villages.  \\nUnity Mall \\n95. \\nStates will be encouraged to set up a Unity Mall in their state capital \\nor most prominent tourism centre or the financial capital for promotion and \\nsale of their own ODOPs (one district, one product), GI products and other \\nhandicraft products, and for providing space for such products of all other \\nStates.   \\nPriority 7: Financial Sector \\n96. \\nOur reforms in the financial sector and innovative use of technology \\nhave led to financial inclusion at scale, better and faster service delivery, \\nease of access to credit and participation in financial markets. This Budget \\nproposes to further these measures.    \\nCredit Guarantee for MSMEs \\n97. \\nLast year, I proposed revamping of the credit guarantee scheme for \\nMSMEs. I am happy to announce that the revamped scheme will take effect'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 25}, page_content='22 \\n \\n \\n \\nfrom 1st April 2023 through infusion of ` 9,000 crore in the corpus. This will \\nenable additional collateral-free guaranteed credit of ` 2 lakh crore. \\nFurther, the cost of the credit will be reduced by about 1 per cent.     \\nNational Financial Information Registry \\n98. \\nA national financial information registry will be set up to serve as the \\ncentral repository of financial and ancillary information. This will facilitate \\nefficient flow of credit, promote financial inclusion, and foster financial \\nstability. A new legislative framework will govern this credit public \\ninfrastructure, and it will be designed in consultation with the RBI. \\nFinancial Sector Regulations  \\n99. \\nTo meet the needs of Amrit Kaal and to facilitate optimum \\nregulation in the financial sector, public consultation, as necessary and \\nfeasible, will be brought to the process of regulation-making and issuing \\nsubsidiary directions. \\n100. \\nTo simplify, ease and reduce cost of compliance, financial sector \\nregulators will be requested to carry out a comprehensive review of existing \\nregulations. For this, they will consider suggestions from public and \\nregulated entities. Time limits to decide the applications under various \\nregulations will also be laid down. \\nGIFT IFSC  \\n101. \\nTo enhance business activities in GIFT IFSC, the following measures \\nwill be taken: \\n\\uf0b7 \\nDelegating powers under the SEZ Act to IFSCA to avoid dual \\nregulation, \\n\\uf0b7 \\nSetting up a single window IT system for registration and \\napproval from IFSCA, SEZ authorities, GSTN, RBI, SEBI and \\nIRDAI,'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 26}, page_content='23 \\n \\n \\n \\n\\uf0b7 \\nPermitting acquisition financing by IFSC Banking Units of \\nforeign banks,  \\n\\uf0b7 \\nEstablishing \\na \\nsubsidiary \\nof \\nEXIM \\nBank \\nfor \\ntrade  \\nre-financing, \\n\\uf0b7 \\nAmending IFSCA Act for statutory provisions for arbitration, \\nancillary services, and avoiding dual regulation under SEZ Act, \\nand \\n\\uf0b7 \\nRecognizing offshore derivative instruments as valid contracts.  \\n \\nData Embassy \\n102. \\nFor countries looking for digital continuity solutions, we will \\nfacilitate setting up of their Data Embassies in GIFT IFSC.  \\nImproving Governance and Investor Protection in Banking Sector \\n103. \\nTo improve bank governance and enhance investors’ protection, \\ncertain amendments to the Banking Regulation Act, the Banking Companies \\nAct and the Reserve Bank of India Act are proposed. \\n \\nCapacity Building in Securities Market \\n104. \\nTo build capacity of functionaries and professionals in the securities \\nmarket, SEBI will be empowered to develop, regulate, maintain and enforce \\nnorms and standards for education in the National Institute of Securities \\nMarkets and to recognize award of degrees, diplomas and certificates.  \\nCentral Data Processing Centre  \\n105. \\nA Central Processing Centre will be setup for faster response to \\ncompanies through centralized handling of various forms filed with field \\noffices under the Companies Act.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 27}, page_content='24 \\n \\n \\n \\nReclaiming of shares and dividends  \\n106. \\nFor investors to reclaim unclaimed shares and unpaid dividends \\nfrom the Investor Education and Protection Fund Authority with ease, an \\nintegrated IT portal will be established. \\nDigital Payments  \\n107. \\nDigital payments continue to find wide acceptance. In 2022, they \\nshow \\nincrease \\nof \\n76 \\nper \\ncent \\nin \\ntransactions  \\nand 91 per cent in value. Fiscal support for this digital public infrastructure \\nwill continue in 2023-24.  \\nAzadi Ka Amrit Mahotsav Mahila Samman Bachat Patra  \\n108. \\nFor commemorating Azadi Ka Amrit Mahotsav, a one-time new small \\nsavings scheme, Mahila Samman Savings Certificate, will be made available \\nfor a two-year period up to March 2025. This will offer deposit facility upto \\n` 2 lakh in the name of women or girls for a tenor of 2 years at fixed \\ninterest rate of 7.5 per cent with partial withdrawal option.  \\nSenior Citizens \\n109. \\nThe maximum deposit limit for Senior Citizen Savings Scheme will be \\nenhanced from ` 15 lakh to ` 30 lakh. \\n110. \\n The maximum deposit limit for Monthly Income Account Scheme \\nwill be enhanced from ` 4.5 lakh to ` 9 lakh for single account and from ` 9 \\nlakh to ` 15 lakh for joint account. \\nFiscal Management \\nFifty-year interest free loan to States \\n111. \\nThe entire fifty-year loan to states has to be spent on capital \\nexpenditure within 2023-24. Most of this will be at the discretion of states, \\nbut a part will be conditional on states increasing their actual capital'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 28}, page_content='25 \\n \\n \\n \\nexpenditure. Parts of the outlay will also be linked to, or allocated for, the \\nfollowing purposes: \\n\\uf0b7 Scrapping old government vehicles, \\n\\uf0b7 Urban planning reforms and actions, \\n\\uf0b7 Financing reforms in urban local bodies to make them \\ncreditworthy for municipal bonds, \\n\\uf0b7 Housing for police personnel above or as part of police stations,  \\n\\uf0b7 Constructing Unity Malls, \\n\\uf0b7 Children and adolescents’ libraries and digital infrastructure, \\nand \\n\\uf0b7 State share of capital expenditure of central schemes. \\nFiscal Deficit of States \\n112. \\nStates will be allowed a fiscal deficit of 3.5 per cent of GSDP of which \\n0.5 per cent will be tied to power sector reforms.  \\nRevised Estimates 2022-23 \\n113. \\nThe Revised Estimate of the total receipts other than borrowings is  \\n` \\n24.3 \\nlakh \\ncrore, \\nof \\nwhich \\nthe \\nnet \\ntax \\nreceipts  \\nare ` 20.9 lakh crore. The Revised Estimate of the total expenditure is  \\n` 41.9 lakh crore, of which the capital expenditure is about ` 7.3 lakh crore.      \\n114. \\nThe Revised Estimate of the fiscal deficit is 6.4 per cent of GDP, \\nadhering to the Budget Estimate.        \\nBudget Estimates 2023-24 \\n115. \\nComing to 2023-24, the total receipts other than borrowings and the \\ntotal expenditure are estimated at ` 27.2 lakh crore and ` 45 lakh crore \\nrespectively. The net tax receipts are estimated at ` 23.3 lakh crore.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 29}, page_content='26 \\n \\n \\n \\n116. \\nThe fiscal deficit is estimated to be 5.9 per cent of GDP. In my \\nBudget Speech for 2021-22, I had announced that we plan to continue the \\npath of fiscal consolidation, reaching a fiscal deficit below 4.5 per cent by \\n2025-26 with a fairly steady decline over the period. We have adhered to \\nthis path, and I reiterate my intention to bring the fiscal deficit below 4.5 \\nper cent of GDP by 2025-26.  \\n117. \\n To finance the fiscal deficit in 2023-24, the net market borrowings \\nfrom dated securities are estimated at ` 11.8 lakh crore. The balance \\nfinancing is expected to come from small savings and other sources. The \\ngross market borrowings are estimated at ` 15.4 lakh crore. \\nI will, now, move to Part B.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 30}, page_content='27 \\n \\n \\n \\nPART B \\nIndirect Taxes \\n118. \\nMy indirect tax proposals aim to promote exports, boost domestic \\nmanufacturing, enhance domestic value addition, encourage green energy \\nand mobility.  \\n119. \\nA simplified tax structure with fewer tax rates helps in reducing \\ncompliance burden and improving tax administration. I propose to reduce \\nthe number of basic customs duty rates on goods, other than textiles and \\nagriculture, from 21 to 13. As a result, there are minor changes in the basic \\ncustom duties, cesses and surcharges on some items including toys, \\nbicycles, automobiles and naphtha. \\n \\nGreen Mobility \\n120. \\nTo avoid cascading of taxes on blended compressed natural gas, I \\npropose to exempt excise duty on GST-paid compressed bio gas contained \\nin it. To further provide impetus to green mobility, customs duty exemption \\nis being extended to import of capital goods and machinery required for \\nmanufacture of lithium-ion cells for batteries used in electric vehicles. \\nElectronics  \\n121. \\nAs a result of various initiatives of the Government, including the \\nPhased Manufacturing programme, mobile phone production in India has \\nincreased from 5.8 crore units valued at about ` 18,900 crore in 2014-15 to \\n31 crore units valued at over ` 2,75,000 crore in the last financial year. To \\nfurther deepen domestic value addition in manufacture of mobile phones, I \\npropose to provide relief in customs duty on import of certain parts and \\ninputs like camera lens and continue the concessional duty on lithium-ion \\ncells for batteries for another year.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 31}, page_content='28 \\n \\n \\n \\n122. \\nSimilarly, to promote value addition in manufacture of televisions, I \\npropose to reduce the basic customs duty on parts of open cells of TV \\npanels to 2.5 per cent.  \\nElectrical  \\n123. \\nTo rectify inversion of duty structure and encourage manufacturing \\nof electric kitchen chimneys, the basic customs duty on electric kitchen \\nchimney is being increased from 7.5 per cent to 15 per cent and that on \\nheat coils for these is proposed to be reduced from 20 per cent to 15 per \\ncent. \\nChemicals and Petrochemicals  \\n124. \\nDenatured \\nethyl \\nalcohol \\nis \\nused \\nin \\nchemical \\nindustry. \\n I propose to exempt basic customs duty on it. This will also support the \\nEthanol Blending Programme and facilitate our endeavour for energy \\ntransition. Basic customs duty is also being reduced on acid grade fluorspar \\nfrom 5 per cent to 2.5 per cent to make the domestic fluorochemicals \\nindustry competitive. Further, the basic customs duty on crude glycerin for \\nuse in manufacture of epicholorhydrin is proposed to be reduced from 7.5 \\nper cent to 2.5 per cent. \\nMarine products \\n125. \\nIn the last financial year, marine products recorded the highest \\nexport growth benefitting farmers in the coastal states of the country. To \\nfurther enhance the export competitiveness of marine products, \\nparticularly shrimps, duty is being reduced on key inputs for domestic \\nmanufacture of shrimp feed. \\nLab Grown Diamonds \\n126. \\nIndia is a global leader in cutting and polishing of natural diamonds, \\ncontributing about three-fourths of the global turnover by value. With the \\ndepletion in deposits of natural diamonds, the industry is moving towards \\nLab Grown Diamonds (LGDs) and it holds huge promise. To seize this'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 32}, page_content='29 \\n \\n \\n \\nopportunity, I propose to reduce basic customs duty on seeds used in their \\nmanufacture.  \\n \\nPrecious Metals \\n127. \\nCustoms Duties on dore and bars of gold and platinum were \\nincreased earlier this fiscal. I now propose to increase the duties on articles \\nmade therefrom to enhance the duty differential. I also propose to increase \\nthe import duty on silver dore, bars and articles to align them with that on \\ngold and platinum. \\nMetals \\n128. \\nTo facilitate availability of raw materials for the steel sector, \\nexemption from Basic Customs Duty on raw materials for manufacture of \\nCRGO Steel, ferrous scrap and nickel cathode is being continued. \\n129. \\nSimilarly, the concessional BCD of 2.5 per cent on copper scrap is \\nalso being continued to ensure the availability of raw materials for \\nsecondary copper producers who are mainly in the MSME sector. \\nCompounded Rubber \\n130. \\nThe basic customs duty rate on compounded rubber is being \\nincreased from 10 per cent to ‘25 per cent or ` 30/kg whichever is lower’, at \\npar with that on natural rubber other than latex, to curb circumvention of \\nduty.  \\nCigarettes \\n131. \\nNational Calamity Contingent Duty (NCCD) on specified cigarettes \\nwas last revised three years ago. This is proposed to be revised upwards by \\nabout 16 per cent.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 33}, page_content='30 \\n \\n \\n \\n \\nDirect Taxes \\n132. \\nI now come to my direct tax proposals. These proposals aim to \\nmaintain continuity and stability of taxation, further simplify and rationalise \\nvarious provisions to reduce the compliance burden, promote the \\nentrepreneurial spirit and provide tax relief to citizens. \\n133. \\nIt has been the constant endeavour of the Income Tax Department \\nto improve Tax Payers Services by making compliance easy and smooth. Our \\ntax payers’ portal received a maximum of 72 lakh returns in a day; \\nprocessed more than 6.5 crore returns this year; average processing period \\nreduced from 93 days in financial year 13-14 to 16 days now;  \\nand 45 per cent of the returns were processed within 24 hours. We intend \\nto further improve this, roll out a next-generation Common IT Return Form \\nfor tax payer convenience, and also plan to strengthen the grievance \\nredressal mechanism.  \\nMSMEs and Professionals  \\n134. \\nMSMEs are growth engines of our economy.  Micro enterprises with \\nturnover up to ` 2 crore and certain professionals with turnover of up to  \\n` 50 lakh can avail the benefit of presumptive taxation. I propose to provide \\nenhanced limits of ` 3 crore and ` 75 lakh respectively, to the tax payers \\nwhose cash receipts are no more than 5 per cent. Moreover, to support \\nMSMEs in timely receipt of payments, I propose to allow deduction for \\nexpenditure incurred on payments made to them only when payment is \\nactually made.  \\nCooperation \\n135. \\nCooperation is a value to be cherished. In realizing our Prime \\nMinister’s goal of “Sahkar se Samriddhi”, and his resolve to “connect the \\nspirit of cooperation with the spirit of Amrit Kaal”, in addition to the \\nmeasures proposed in Part A, I have a slew of proposals for the co-operative \\nsector.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 34}, page_content='31 \\n \\n \\n \\n136. \\nFirst, new co-operatives that commence manufacturing activities till \\n31.3.2024 shall get the benefit of a lower tax rate of 15 per cent, as is \\npresently available to new manufacturing companies. \\n137. \\nSecondly, I propose to provide an opportunity to sugar co-operatives \\nto claim payments made to sugarcane farmers for the period prior to \\nassessment year 2016-17 as expenditure. This is expected to provide them \\nwith a relief of almost ` 10,000 crore.  \\n138. \\nThirdly, I am providing a higher limit of ` 2 lakh per member for cash \\ndeposits to and loans in cash by Primary Agricultural Co-operative Societies \\n(PACS) and Primary Co-operative Agriculture and Rural Development Banks \\n(PCARDBs).  \\n139. \\nSimilarly, a higher limit of ` 3 crore for TDS on cash withdrawal is \\nbeing provided to co-operative societies. \\nStart-Ups \\n140. \\nEntrepreneurship is vital for a country’s economic development. We \\nhave taken a number of measures for start-ups and they have borne results. \\nIndia is now the third largest ecosystem for start-ups globally, and ranks \\nsecond in innovation quality among middle-income countries. I propose to \\nextend the date of incorporation for income tax benefits to start-ups from \\n31.03.23 to 31.3.24. I further propose to provide the benefit of carry \\nforward of losses on change of shareholding of start-ups from seven years \\nof incorporation to ten years. \\nAppeals \\n141. \\nTo reduce the pendency of appeals at Commissioner level, I propose \\nto deploy about 100 Joint Commissioners for disposal of small appeals. We \\nshall also be more selective in taking up cases for scrutiny of returns already \\nreceived this year.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 35}, page_content='32 \\n \\n \\n \\nBetter targeting of tax concessions \\n142. \\nFor better targeting of tax concessions and exemptions, \\n I propose to cap deduction from capital gains on investment in residential \\nhouse under sections 54 and 54F to ` 10 crore. Another proposal with \\nsimilar intent is to limit income tax exemption from proceeds of insurance \\npolicies with very high value. \\nRationalisation \\n143. \\nThere are a number of proposals relating to rationalisation and \\nsimplification. Income of authorities, boards and commissions set up by \\nstatutes of the Union or State for the purpose of housing, development of \\ncities, towns and villages, and regulating, or regulating and developing an \\nactivity or matter, is proposed to be exempted from income tax. Other \\nmajor measures in this direction are: \\n\\uf0b7 Removing the minimum threshold of ` 10,000/- for TDS and \\nclarifying taxability relating to online gaming; \\n\\uf0b7 Not treating conversion of gold into electronic gold receipt and vice \\nversa as capital gain;  \\n\\uf0b7 Reducing the TDS rate from 30 per cent to 20 per cent on taxable \\nportion of EPF withdrawal in non-PAN cases; and \\n\\uf0b7 Taxation on income from Market Linked Debentures. \\nOthers \\n144. \\nOther major proposals in the Finance Bill relate to the following: \\n\\uf0b7 Extension of period of tax benefits to funds relocating to IFSC, GIFT \\nCity till 31.03.2025; \\n\\uf0b7 Decriminalisation under section 276A of the Income Tax Act; \\n\\uf0b7 Allowing carry forward of losses on strategic disinvestment including \\nthat of IDBI Bank; and \\n\\uf0b7 Providing EEE status to Agniveer Fund.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 36}, page_content='33 \\n \\n \\n \\nPersonal Income Tax \\n145. \\nNow, I come to what everyone is waiting for -- personal income tax. I \\nhave five major announcements to make in this regard. These primarily \\nbenefit our hard-working middle class. \\n146. \\nThe first one concerns rebate. Currently, those with income up to  \\n` 5 lakh do not pay any income tax in both old and new tax regimes. I \\npropose to increase the rebate limit to ` 7 lakh in the new tax regime. Thus, \\npersons in the new tax regime, with income up to ` 7 lakh will not have to \\npay any tax.  \\n147. \\nThe \\nsecond \\nproposal \\nrelates \\nto \\nmiddle-class \\nindividuals. \\n I had introduced, in the year 2020, the new personal income tax regime \\nwith six income slabs starting from ` 2.5 lakh. I propose to change the tax \\nstructure in this regime by reducing the number of slabs to five and \\nincreasing the tax exemption limit to ` 3 lakh. The new tax rates are: \\n` 0-3 lakh \\nNil \\n` 3-6 lakh \\n5 per cent \\n` 6-9 lakh \\n10 per cent \\n` 9-12 lakh \\n15 per cent \\n` 12-15 lakh \\n20 per cent \\nAbove ` 15 lakh \\n30 per cent \\n \\n148. \\nThis will provide major relief to all tax payers in the new regime. An \\nindividual with an annual income of ` 9 lakh will be required to pay only  \\n` 45,000/-. This is only 5 per cent of his or her income. It is a reduction of 25 \\nper cent on what he or she is required to pay now, ie, ` 60,000/-. Similarly, \\nan individual with an income of ` 15 lakh would be required to pay only  \\n` 1.5 lakh or 10 per cent of his or her income, a reduction of 20 per cent \\nfrom the existing liability of ` 1,87,500/.  \\n149. \\nMy third proposal is for the salaried class and the pensioners \\nincluding family pensioners, for whom I propose to extend the benefit of'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 37}, page_content='34 \\n \\n \\n \\nstandard deduction to the new tax regime. Each salaried person with an \\nincome of ` 15.5 lakh or more will thus stand to benefit by ` 52,500. \\n150. \\nMy fourth announcement in personal income tax is regarding the \\nhighest tax rate which in our country is 42.74 per cent. This is among the \\nhighest in the world. I propose to reduce the highest surcharge rate from 37 \\nper cent to 25 per cent in the new tax regime. This would result in reduction \\nof the maximum tax rate to 39 per cent. \\n151. \\nLastly, the limit of ` 3 lakh for tax exemption on leave encashment \\non retirement of non-government salaried employees was last fixed in the \\nyear 2002, when the highest basic pay in the government was ` 30,000/- \\npm. In line with the increase in government salaries, I am proposing to \\nincrease this limit to ` 25 lakh. \\n152. \\nWe are also making the new income tax regime as the default tax \\nregime. However, citizens will continue to have the option to avail the \\nbenefit of the old tax regime. \\n153. \\nApart from these, I am also making some other changes as given in \\nthe annexure. \\n154. \\nAs a result of these proposals, revenue of about ` 38,000 crore –  \\n` 37,000 crore in direct taxes and ` 1,000 crore in indirect taxes – will be \\nforgone while revenue of about ` 3,000 crore will be additionally mobilized. \\nThus, the total revenue forgone is about ` 35,000 crore annually. \\n155. \\nMr. Speaker Sir, with these words, I commend the Budget to this \\naugust House. \\n*****'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 38}, page_content='35 \\n \\n \\n \\nAnnexure to Part B of the Budget Speech 2023-24 \\nAmendments relating to Direct Taxes \\nA. PROVIDING TAX RELIEF UNDER NEW PERSONAL TAX REGIME \\nA.1     The new tax regime for Individual and HUF, introduced by the \\nFinance Act 2020, is now proposed to be the default regime.  \\nA.2      This regime would also become the default regime for AOP (other \\nthan co-operative), BOI and AJP.  \\nA.3      Any individual, HUF, AOP (other than co-operative), BOI or AJP not \\nwilling to be taxed under this new regime can opt to be taxed \\nunder the old regime. For those person having income under the \\nhead “profit and gains of business or profession” and having opted \\nfor old regime can revoke that option only once and after that \\nthey will continue to be taxed under the new regime. For those \\nnot having income under the head “profit and gains of business or \\nprofession”, option for old regime may be exercised in each year. \\nA.4      Substantial relief is proposed under the new regime with new slabs \\nand tax rates as under: \\nTotal Income (`) \\nRate (per cent) \\nUpto 3,00,000 \\nNil \\nFrom 3,00,001 to 6,00,000 \\n5 \\nFrom 6,00,001 to 9,00,000 \\n10 \\nFrom 9,00,001 to 12,00,000 \\n15 \\nFrom 12,00,001 to 15,00,000 \\n20 \\nAbove 15,00,000 \\n30 \\n \\nA.5      Resident individual with total income up to ` 5,00,000 do not pay \\nany tax due to rebate under both old and new regime. It is \\nproposed to increase the rebate for the resident individual under \\nthe new regime so that they do not pay tax if their total income is \\nup to ` 7,00,000. \\nA.6     Standard deduction of ` 50,000 to salaried individual, and'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 39}, page_content='36 \\n \\n \\n \\ndeduction from family pension up to ` 15,000, is currently allowed \\nonly under the old regime. It is proposed to allow these two \\ndeductions under the new regime also. \\n A.7      Surcharge on income-tax under both old regime and new regime is \\n10 per cent if income is above ` 50 lakh and up to ` 1 crore, 15 per \\ncent if income is above  `1 crore and up to ` 2 crore, 25 per cent if \\nincome is above ` 2 crore and up to ` 5 crore, and 37 per cent if \\nincome is above ` 5 crore. It is proposed that the for those \\nindividuals, HUF, AOP (other than co-operative), BOI and AJP \\nunder the new regime, surcharge would be same except that the \\nsurcharge rate of 37 per cent will not apply. Highest surcharge \\nshall \\nbe \\n25 \\nper \\ncent \\nfor \\nincome \\nabove \\n` 2 crore. This would reduce the maximum rate from about 42.7 \\nper cent to about 39 per cent. No change in surcharge is proposed \\nfor those who opt to be under the old regime. \\nA.8      Encashment of earned leave up to 10 months of average salary, at \\nthe time of retirement in case of an employee (other than an \\nemployee of the Central Government or State Government), is \\nexempt under sub-clause (ii) of clause (10AA) of section 10 of the \\nIncome-tax Act (“the Act”) to the extent notified. The maximum \\namount which can be exempted is ` 3 lakh at present. It is \\nproposed to issue notification to extend this limit to ` 25 lakh.  \\nB. SOCIO-ECONOMIC WELFARE MEASURES \\nB.1\\nPromoting timely payments to Micro and Small Enterprises\\nIn order to promote timely payments to micro and small \\nenterprises, it is proposed to include payments made to such \\nenterprises within the ambit of section 43B of the Act. Thus, \\ndeduction for such payments would be allowed only when actually \\npaid. It will be allowed on accrual basis only if the payment is \\nwithin the time mandated under the Micro, Small and Medium \\nEnterprises Development Act. \\nB.2 \\nAgnipath Scheme, 2022 \\nThe payment received from the Agniveer Corpus Fund by the \\nAgniveers enrolled in Agnipath Scheme, 2022 is proposed to be \\nexempt from taxes. Deduction in the computation of total income \\nis proposed to be allowed to the Agniveer on the contribution'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 40}, page_content='37 \\n \\n \\n \\nmade by him or the Central Government to his Seva Nidhi \\naccount. \\nB.3 \\nRelief to sugar co-operatives from past demand \\nIt is proposed that for sugar co-operatives, for years prior to A.Y. \\n2016-17, if any deduction claimed for expenditure made on \\npurchase of sugar has been disallowed, an application may be \\nmade to the Assessing Officer, who shall recompute the income of \\nthe relevant previous year after allowing such deduction up to the \\nprice fixed or approved by the Government for such previous year.\\nB.4 \\nIncreasing threshold limit for Co-operatives to withdraw cash \\nwithout TDS \\nIt is proposed to enable co-operatives to withdraw cash up to ` 3 \\ncrore in a year without being subjected to TDS on such \\nwithdrawal.  \\nB.5 \\nPenalty for cash loan/transactions against primary co-operatives \\nIt is proposed to  amend section 269SS of the Act to provide that \\nwhere a deposit is accepted by a primary agricultural credit \\nsociety or a primary co-operative agricultural and rural \\ndevelopment bank from its member or a loan is taken from a \\nprimary agricultural credit society or a primary co-operative \\nagricultural and rural development bank by its member in cash, no \\npenal consequence would arise, if the amount of such loan or \\ndeposit in cash is less than  ` 2 lakh. Further, section 269T of the \\nAct is proposed to be amended to provide that where a deposit is \\nrepaid by a primary agricultural credit society or a primary co-\\noperative agricultural and rural development bank to its member \\nor such loan is repaid to a primary agricultural credit society or a \\nprimary co-operative agricultural and rural development bank by \\nits member in cash, no penal consequence shall arise, if the \\namount of such loan or deposit in cash is less than ` 2 lakh. \\nB.6 \\nRelief to start-ups in carrying forward and setting off of losses \\nThe condition of continuity of at least 51 per cent shareholding for \\nsetting off of carried forward losses is relaxed for an eligible start \\nup if all the shareholders of the company continue to hold those \\nshares. At present this relaxation applies for losses incurred during \\nthe period of 7 years from incorporation of such start-up. It is'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 41}, page_content='38 \\n \\n \\n \\nproposed to increase this period to 10 years.\\nB.7 \\nExtension of date of incorporation for eligible start up for \\nexemption  \\nCertain start-ups are eligible for some tax benefit if they are \\nincorporated before 1st April, 2023. The period of incorporation of \\nsuch eligible start-ups is proposed to be extended by one year to \\nbefore 1st April, 2024.  \\nB.8 \\nGold to Electronic Gold Receipt \\nThe conversion of physical gold to Electronic Gold Receipt and vice \\nversa is proposed not to be treated as a transfer and not to attract \\nany capital gains. This would promote investments in electronic \\nequivalent of gold. \\nB.9 \\nIncentives to IFSC \\nRelocation of funds to IFSC has certain tax exemptions, if the \\nrelocation is before 31.03.2023. This date is proposed to be \\nextended to 31.03.2025. Further, any distributed income from the \\noffshore derivative instruments entered into with an offshore \\nbanking unit is also proposed to be exempted subject to certain \\nconditions. \\nB.10 \\nExemption to development authorities etc. \\nIt is proposed to provide exemption to any income arising to a \\nbody or authority or board or trust or commission, (not being a \\ncompany) which  has been established or constituted by or under \\na Central or State Act with the purposes of satisfying the need for \\nhousing or for planning, development or improvement of cities, \\ntowns and villages or for regulating any activity or matter, \\nirrespective of whether it is carrying out commercial activity. \\nB.11 \\nFacilitating certain strategic disinvestments \\nTo facilitate certain strategic disinvestments, it is proposed to \\nallow carry forward of accumulated losses and unabsorbed \\ndepreciation allowance in the case of amalgamation of one or \\nmore banking company with any other banking institution or a \\ncompany subsequent to a strategic disinvestment, if such \\namalgamation \\ntakes \\nplace \\nwithin \\n5 \\nyears \\nof \\nstrategic \\ndisinvestment. It is also proposed to modify the definition of \\n‘strategic disinvestment’.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 42}, page_content='39 \\n \\n \\n \\nB.12\\n15 per cent concessional tax to promote new manufacturing co-\\noperative society \\nIn order to promote the growth of manufacturing in co-operative \\nsector, a new co-operative society formed on or after 01.04.2023, \\nwhich commences manufacturing or production by 31.03.2024 \\nand do not avail of any specified incentive or deduction, is \\nproposed to be allowed an option to pay tax at a concessional rate \\nof 15 per cent similar to what is available to new manufacturing \\ncompanies. \\nC. EASE OF COMPLIANCE \\nC.1\\nEase in claiming deduction on amortization of preliminary \\nexpenditure \\nAt present for claiming amortization of certain preliminary \\nexpenses, the activity is to be carried out either by the assessee or \\nby a concern approved by the Board. In order to ease the process \\nof claiming amortization of these expenses it is proposed to \\nremove the condition of activity in connection with these \\nexpenses to be carried out by a concern approved by the Board. \\nFormat for reporting of such expenses by the assessee shall be \\nprescribed. \\nC.2 \\nIncreasing threshold limits for presumptive taxation schemes \\nIn order to ease compliance and to promote non-cash \\ntransactions, it is proposed to increase the threshold limits for \\npresumptive scheme of taxation for eligible businesses from ` 2 \\ncrore to ` 3 crore and for specified professions from ` 50 lakh to \\n` 75 lakh. The increased limit will apply only in case the amount or \\naggregate of the amounts received during the year, in cash, does \\nnot exceed five per cent of the total gross receipts/turnover. \\nC.3 \\nExtending the scope for deduction of tax at source at lower or nil \\nrate \\nIt is proposed to allow a taxpayer to obtain certificate of \\ndeduction of tax at source to lower or nil rate on sums on which \\ntax is required to be deducted under section 194LBA of the Act by \\nBusiness Trusts.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 43}, page_content='40 \\n \\n \\n \\nD. WIDENING & DEEPENING OF TAX BASE AND ANTI AVOIDANCE \\nD.1\\nIt is proposed to extend the deemed income accrual provision \\nrelating to sums of money exceeding fifty thousand rupees, \\nreceived from residents without consideration to a not ordinarily \\nresident with effect from 1st April, 2023. \\nD.2 \\nIt is proposed to omit the provision to allow tax exemption to \\nnews agencies set up in India solely for collection and distribution \\nof news from the financial year 2023-24.  \\nD.3 \\nIt is proposed to tax distributed income by business trusts in the \\nhands of a unit holder (other than dividend, interest or rent which \\nis already taxable) on which tax is currently avoided both in the \\nhands of unit holder as well as in the hands of business trust.   \\nD.4 \\nIt is proposed to withdraw the exemption from TDS currently \\navailable on interest payment on listed debentures. \\nD.5 \\nWith respect to presumptive schemes for non-residents, it is \\nproposed to disallow carried forward and set off of loss computed \\nas per books of account with presumptive income. \\nD.6 \\nFor online games, it is proposed to provide for TDS and taxability \\non net winnings at the time of withdrawal or at the end of the \\nfinancial year. Moreover, TDS would be without the threshold of \\n` 10,000. For lottery, crossword puzzles games, etc threshold limit \\n` 10,000 for TDS shall continue but shall apply to aggregate \\nwinnings during a financial year. \\nD.7     The rate of TCS for foreign remittances for education and for \\nmedical treatment is proposed to continue to be 5 per cent for \\nremittances in excess of ` 7 lakh. Similarly, the rate of TCS on \\nforeign remittances for the purpose of education through loan \\nfrom financial institutions is proposed to continue to be 0.5 per \\ncent in excess of `7 lakh. However, for foreign remittances for \\nother purposes under LRS and purchase of overseas tour program, \\nit is proposed to increase the rates of TCS from 5 per cent to 20 \\nper cent. \\nD.8 \\nTax on capital gains can be avoided by investing proceeds of such \\ngains in residential property. This is proposed to be capped at ` 10 \\ncrore.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 44}, page_content='41 \\n \\n \\n \\nD.9\\nThe income from market linked debentures is proposed to be \\ntaxed as short-term capital gains at the applicable rates. \\nD.10 \\nIt is proposed to provide for some provisions to minimise risk to \\nrevenue due to undervaluation of inventory. \\nD.11 \\nIt is proposed to provide that where aggregate of premium for life \\ninsurance policies (other than ULIP) issued on or after 1st April, \\n2023 is above ` 5 lakh, income from only those policies with \\naggregate premium up to ` 5 lakh shall be exempt. This will not \\naffect the tax exemption provided to the amount received on the \\ndeath of person insured. It will also not affect insurance policies \\nissued till 31st March, 2023. \\nD.12 \\nIt is proposed to amend provisions for computing capital gains in \\ncase of joint development of property to include the amount \\nreceived through cheque etc. as consideration.  \\nD.13 \\nWhile interest paid on borrowed capital for acquiring or improving \\na property can, subject to certain conditions, be claimed as \\ndeduction from income, it can also be included in the cost of \\nacquisition or improvement on transfer, thereby reducing capital \\ngains. It is proposed to provide that the cost of acquisition or \\nimprovement shall not include the amount of interest claimed \\nearlier as deduction. \\nD.14 \\nThere are certain assets like intangible assets or rights for which \\nno consideration has been paid for acquisition and the transfer of \\nwhich may result in generation of income. Their cost of acquisition \\nis proposed to be defined to be NIL. \\nE. IMPROVING COMPLIANCE AND TAX ADMINISTRATION \\nE.1\\nWith respect to rectification of orders by the Interim Board of \\nSettlement, it is proposed to provide that where the time-limit for \\namending an order by it or for making an application to it expires \\non or after 01.02.2021 but before 01.02.2022, such time-limit shall \\nstand extended to 30.09.2023. \\nE.2 \\nTo expedite the disposal of certain appeals pending with \\nCommissioner (Appeals), it is proposed to introduce a new \\nauthority in the rank of Joint Commissioner/ Additional \\nCommissioner [JCIT(Appeals)], for appeals against certain orders'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 45}, page_content='42 \\n \\n \\n \\npassed by or with the approval of an authority below the rank of \\nJoint \\nCommissioner. \\nCertain \\nrelated \\nand \\nconsequential \\namendments are also proposed in this regard.  \\nE.3 \\nIt is proposed to reduce the minimum time period required to be \\nprovided by the transfer pricing officer to assessee for production \\nof documents and information from 30 days to 10 days. \\nE.4 \\nIt is proposed to provide for appeal against penalty orders passed \\nby Commissioner (Appeals) under certain sections of the Act \\nbefore the Appellate Tribunal. It is also proposed to provide that \\nan order under section 263 of the Act passed by the Principal \\nChief Commissioner or Chief Commissioner and any rectification \\norder for the same shall also be appealable before the Appellate \\nTribunal. Further, it is proposed to enable filing of memorandum \\nof cross-objections in all classes of cases against which appeal can \\nbe made to the Appellate Tribunal. \\nE.5 \\nIt is proposed to amend section 132 of the Act, dealing with \\nsearch and seizure, to allow the authorised officer to take \\nassistance of specific domain experts like digital forensic \\nprofessionals, valuers and services of other professionals like \\nlocksmiths, carpenters etc. during the course of search and also to \\naid in accurate estimation of undisclosed income held in the form \\nof property by the assessee.  \\nE.6 \\nSection 170A of the Act, inserted vide Finance Act, 2022 is \\nproposed to be substituted to clarify that a modified return shall \\nbe furnished by an entity to whom the order of the business \\nreorganisation applies, and to introduce provisions for assessment \\nor reassessment in cases where such modified return is furnished. \\nE.7 \\nIt is proposed that an order of assessment may be passed within a \\nperiod of 12 months from the end of the relevant assessment year \\nor the financial year in which updated return is filed, as the case \\nmay be. It is also proposed that in cases where search under \\nsection 132 of the Act or requisition under section 132A of the Act \\nhas been made, the period of limitation of pending assessments \\nshall be extended by twelve months.  \\nE.8 \\nIt is proposed to make amendments to empower the Central \\nGovernment to make modifications in the already notified'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 46}, page_content='43 \\n \\n \\n \\nschemes regarding e-Verification, Dispute Resolution, Advance \\nRulings, Appeal and Penalty, at any time to enable better \\nimplementation of such schemes. \\nE.9 \\nIt is proposed to limit the time for furnishing of a return for \\nreassessment. Further, it is also proposed to  provide that in cases \\nwhere search related information is available after 15th March of \\nany financial year, an additional period of fifteen days shall be \\nallowed for issuance of notice, for assessment/reassessments etc, \\nunder section 148 of the Act. It is also proposed to clarify that the \\nspecified authority for granting approval shall be Principal Chief \\nCommissioner or Principal Director General or Chief Commissioner \\nor Director General. \\nE.10 \\nIt is proposed to provide a penalty of ` 5,000 if there is any \\ninaccuracy in the statement of financial transactions submitted by \\na prescribed reporting financial institution due to false or \\ninaccurate information submitted by the account holder. \\nE.11 \\nIt is proposed to amend section 271C and section 276B of the Act \\nto provide for penalty and prosecution where default in TDS \\nrelates to transaction in kind. \\nE.12.   It is proposed to amend the time period for filing of appeal against \\nthe order of the Adjudicating authority under Benami Act within a \\nperiod of 45 days from the date when such order is received by \\nthe Initiating Officer or the aggrieved person. The definition of \\n‘High Court’ is also proposed to be modified to allow \\ndetermination of jurisdiction for filing appeal in the case of non-\\nresidents. \\nF. RATIONALISATION \\nF.1\\nThe restriction on interest deductibility on interest payment to \\noverseas associated enterprise does not apply to those in the \\nbusiness of banking and insurance. It is proposed to extend this \\nbenefit to non-banking financial companies, as may be notified. \\nF.2 \\nTDS on payment of certain income to a non-resident is currently at \\nthe rate of 20 per cent, but the tax rate in treaties may be lower. It \\nis proposed to allow the benefit of tax treaty at the time of TDS on \\nsuch income under section 196A of the Act.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 47}, page_content='44 \\n \\n \\n \\nF.3\\nAt present the TDS rate on withdrawal of taxable component from \\nEmployees’ Provident Fund Scheme in non-PAN cases is 30 per \\ncent. It is proposed to reduce it to 20 per cent, as in other non-\\nPAN cases. \\nF.4 \\nSometimes, tax for income of an earlier year is deducted later, \\nwhile tax thereon has already been paid in the earlier year. \\nAmendment is proposed to facilitate such taxpayers to claim \\ncredit of this TDS in the earlier year.  \\nF.5 \\nHigher TDS/TCS rate applies, if the recipient is a non-filer i.e. who \\nhas not furnished his return of income of preceding previous year \\nand has aggregate of TDS and TCS of ` 50,000 or more. It is \\nproposed to exclude a person who is not required to furnish the \\nreturn of income for such previous year and who is notified by the \\nCentral Government in the Official Gazette in this behalf. \\nF.6 \\nIt is proposed to clarify that the amount of advance tax paid is \\nreduced only once for computing the interest payable u/s 234B in \\nthe case of an updated return. \\nF.7 \\nIt is proposed to extend taxability of the consideration (share \\napplication money/ share premium) for shares exceeding the face \\nvalue of such shares to all investors including non-residents. \\nF.8 \\nIt is proposed to enable prescription of a uniform methodology for \\ncomputing the value of perquisite with respect to accommodation \\nprovided by employers to their employees. \\nF.9 \\nIt is proposed to provide a time limit for an SEZ unit to bring the \\nproceeds from exports of goods or services into India. The filing of \\nincome-tax return is also proposed to be made mandatory for \\nclaiming deduction on export income. \\nF.10 \\nDue to changes in classification of non-banking financial \\ncompanies by the Reserve Bank of India, it is proposed to make \\nnecessary amendments to align such classifications in the Act with \\nthe same. \\nF.11 \\nIt is proposed to clarify that for taxability under section 28 of the \\nAct as well for tax deduction at source under section 194R of the \\nAct, the benefit could also be in cash. \\nF.12 \\nIt is proposed to make amendments relating to exemption'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 48}, page_content='45 \\n \\n \\n \\nprovided to charitable trusts and institution to\\n\\uf0b7 provide clarity on tax treatment on replenishment of corpus \\nand on repayment of loans/borrowings; \\n\\uf0b7 treat only 85 per cent of donation made to another trust as \\napplication; \\n\\uf0b7 omit the redundant provisions related to rolling back of \\nexemption; \\n\\uf0b7 combine provisional and regular registration in some cases; \\n\\uf0b7 modify the scope of specified violation; \\n\\uf0b7 provide for payment of tax on assets if a trust does not apply \\nfor exemption after getting provisional exemption and for re-\\nexemption after expiry of exemption; \\n\\uf0b7 align of time for furnishing of certain forms; \\n\\uf0b7 clarify that the time provided for furnishing return of income \\nfor claiming exemption shall not include the time provided for \\nfurnishing updated return. \\nF.13 \\nIt is proposed to omit certain name-based funds from section 80G \\nof the Act, which provides for deduction of donation to such funds \\nfrom the income of the donor. \\nF.14 \\nIt is proposed to provide that where refund is due to a person, \\nsuch refund shall be set off against existing demand, and if \\nproceedings for assessment or reassessment are pending in such \\ncase, the refund due will be withheld by the Assessing Officer till \\nthe date of assessment or reassessment. \\nG. OTHERS \\nG.1\\nIt is proposed to omit section 88 and some of the clauses of \\nsection 10 of the Act which are no longer in force. \\nG.2 \\nIt is proposed to extend tax exemption to Specified Undertaking of \\nUnit Trust of India (SUUTI) till 30th September, 2023. It is also \\nproposed to enable the Central Government to notify the date of \\nvacation of office of administrator of SUUTI. \\nG.3 \\nIt is proposed to decriminalize certain acts of omission of \\nliquidators under section 276A of the Act with effect from 1st \\nApril, 2023.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 49}, page_content='46 \\n \\n \\n \\nAnnexure to Part B of the Budget Speech 2023-24 \\nAmendments relating to Indirect Taxes \\n \\nA. LEGISLATIVE CHANGES IN CUSTOMS LAWS \\nA.1       Amendments in the Customs Act, 1962 \\nSection 25 (4A) is being amended to  exclude certain categories of \\nconditional customs duty exemptions from the validity period of \\ntwo years, such as, notifications issued in relation to multilateral \\nor bilateral trade agreements; obligations under international \\nagreements, treaties, conventions including with respect to UN \\nagencies, diplomats, international organizations; privileges of \\nconstitutional authorities; schemes under Foreign Trade Policy; \\nCentral Government schemes having a validity of more than two \\nyears; re-imports, temporary imports, goods imported as gifts or \\npersonal baggage; any other duties of Customs under any other \\nlaw in force including  IGST levied under section 3(7) of Customs \\nTariff Act, 1975, other than duty of customs levied under section \\n12 of the Customs Act 1962. \\nSection 127C is being amended to specify a time limit of nine \\nmonths from date of filing application for passing final order by \\nSettlement Commission.  \\nA.2  Amendments in the provisions relating to Anti-Dumping Duty \\n(ADD), Countervailing Duty (CVD), and Safeguard Measures \\nSections 9, 9A, 9C of the Customs Tariff Act are being amended to \\nclarify the intent and scope of these provisions. They are also \\nbeing validated retrospectively with effect from 1st January 1995. \\nA.3      Amendments in the First Schedule to the Customs Tariff Act, 1975 \\nThe First Schedule to the Customs Tariff Act, 1975 is being \\namended to increase the rates on certain tariff items with effect \\nfrom 02.02.2023 and also modify the rates on certain other tariff \\nitems as part of rate rationalisation with effect from date of \\nassent. \\nThe First Schedule to the Customs Tariff Act is being proposed to \\nbe amended in accordance with HSN 2022 amendments.  \\nNew tariff lines are also proposed to be created, which will help in \\nbetter identification of millet-based products, mozzarella cheese, \\nmedicinal plants and their parts, certain pesticides, telecom'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 50}, page_content='47 \\n \\n \\n \\nproducts, synthetic diamonds, cotton, fertilizer grade urea etc. \\nThis will also help in trade facilitation by better identification of \\nthe above items, getting clarity on availing concessional import \\nduty through various notifications and thus reducing dwell time.  \\nThese changes shall come into effect from 01.05.2023. \\nA.4     Amendment in the Second Schedule to the Customs Tariff Act, \\n1975 \\nThe Second Schedule (Export Tariff) is being amended to align the \\nentries under heading 1202 with that of the First Schedule (Import \\nTariff) . \\nB. LEGISLATIVE CHANGES IN GST LAWS \\nB.1 Decriminalisation \\nSection 132 and section 138 of CGST Act are being amended, inter \\nalia, to - \\n\\uf0b7 \\nraise the minimum threshold of tax amount for launching \\nprosecution under GST from ` one crore to ` two crore, \\nexcept for the offence of issuance of invoices without supply \\nof goods or services or both; \\n\\uf0b7 \\nreduce the compounding amount from the present range of \\n50 per cent  to 150 per cent of tax amount to the range of 25 \\nper cent to 100 per cent; \\n\\uf0b7 \\ndecriminalize certain offences specified under clause (g), (j) \\nand (k) of sub-section (1) of section 132 of CGST Act, 2017, \\nviz.- \\no obstruction or preventing any officer in discharge of his \\nduties;  \\no deliberate tempering of material evidence; \\no failure to supply the information. \\nB.2        Facilitate e-commerce for micro enterprises \\nAmendments are being made in section 10 and section 122 of the \\nCGST Act to enable unregistered suppliers and composition \\ntaxpayers to make intra-state supply of goods through E-\\nCommerce Operators (ECOs), subject to certain conditions.  \\nB.3        Amendment to Schedule III of CGST Act, 2017 \\nParas 7, 8 (a) and 8 (b) were inserted in Schedule III of CGST Act, \\n2017 with effect from 01.02.2019 to keep certain transactions/ \\nactivities, such as supplies of goods from a place outside the \\ntaxable territory to another place outside the taxable territory, \\nhigh sea sales and supply of warehoused goods before their home'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 51}, page_content='48 \\n \\n \\n \\nclearance, outside the purview of GST. In order to remove the \\ndoubts and ambiguities regarding taxability of such transactions/ \\nactivities during the period 01.07.2017 to 31.01.2019, provisions \\nare being incorporated to make the said paras effective from \\n01.07.2017. However, no refund of tax paid shall be available in \\ncases where any tax has already been paid in respect of such \\ntransactions/ activities during the period 01.07.2017 to \\n31.01.2019. \\nB.4        Return filing under GST  \\nSections 37, 39, 44 and 52 of CGST Act, 2017 are being amended \\nto restrict filing of returns/ statements to a maximum period of \\nthree years from the due date of filing of the relevant return / \\nstatement.  \\nB.5        Input Tax Credit for expenditure related to CSR \\nSection 17(5) of CGST Act is being amended to provide that input \\ntax credit shall not be available in respect of goods or services or \\nboth received by a taxable person, which are used or intended to \\nbe used for activities relating to his obligations under corporate \\nsocial responsibility referred to in section 135 of the Companies \\nAct, 2013. \\nB.6        Sharing of information \\nA new section 158A in CGST Act is being inserted to enable sharing \\nof the information furnished by the registered person in his return \\nor application of registration or statement of outward supplies, or \\nthe details uploaded by him for generation of electronic invoice or \\nE-way bill or any other details on the common portal, with other \\nsystems in a manner to be prescribed \\nB.7        Amendments in section 2 clause (16) of IGST Act, 2017 \\nClause (16) of section 2 of IGST Act is amended to revise the \\ndefinition of “non-taxable online recipient” by removing the \\ncondition of receipt of online information and database access or \\nretrieval services for purposes other than commerce, industry or \\nany other business or profession so as to provide for taxability of \\nOIDAR service provided by any person located in non-taxable \\nterritory to an unregistered person receiving the said services and \\nlocated in the taxable territory. Further, it also seeks to clarify that \\nthe persons registered solely in terms of clause (vi) of Section 24 \\nof CGST Act shall be treated as unregistered person for the \\npurpose of the said clause.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 52}, page_content='49 \\n \\n \\n \\nB.8        Online information and database access or retrieval services \\nClause (17) of section 2 of IGST Act is being amended to revise the \\ndefinition of “online information and database access or retrieval \\nservices” to remove the condition of rendering of the said supply \\nbeing essentially automated and involving minimal human \\nintervention.  \\nB.9        Place of supply in certain cases \\nProviso to sub-section (8) of section 12 of the IGST Act is being \\nomitted so as to specify the place of supply, irrespective of \\ndestination of the goods, in cases where the supplier of services \\nand recipient of services are located in India.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 53}, page_content='50 \\n \\n \\n \\n \\nC. \\nCUSTOMS DUTY RATE CHANGES \\n \\nC.1. Reduction in basic customs duty to reduce input costs, deepen value \\naddition, to promote export competitiveness, correct inverted duty \\nstructure so as to boost domestic manufacturing etc [with effect \\nfrom 02.02.2023] \\nS. \\nNo. Commodity \\nFrom \\n(per cent) \\nTo \\n(per cent) \\nI. \\nAgricultural Products \\n1. \\nPecan Nuts \\n100  \\n30 \\n2. \\nFish meal for manufacture of aquatic \\nfeed \\n15 \\n5 \\n3. \\nKrill meal for manufacture of aquatic \\nfeed \\n15 \\n5 \\n4. \\nFish lipid oil for manufacture of aquatic \\nfeed \\n30 \\n15 \\n5. \\nAlgal Prime (flour) for manufacture of \\naquatic feed \\n30 \\n15 \\n6. \\nMineral and Vitamin Premixes for \\nmanufacture of aquatic feed \\n15 \\n5 \\n7 \\nCrude glycerin for use in manufacture \\nof Epichlorohydrin \\n7.5 \\n2.5 \\n8 \\nDenatured ethyl alcohol for use in \\nmanufacture of industrial chemicals. \\n5 \\nNil \\nII. \\nMinerals \\n1 \\nAcid grade fluorspar (containing by \\nweight more than 97 per cent of \\ncalcium fluoride) \\n5 \\n2.5 \\nIII. \\nGems and Jewellery Sector \\n1. \\nSeeds for use in manufacturing of \\nrough lab-grown diamonds \\n5 \\nNil'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 54}, page_content='51 \\n \\n \\n \\nIV. \\nCapital Goods \\n1. \\nSpecified capital goods/machinery for \\nmanufacture of lithium-ion cell for use \\nin battery of electrically operated \\nvehicle (EVs) \\nAs \\napplicable \\nNil  \\n(up to \\n31.03.2024)\\nV. \\nIT and Electronics  \\n \\n1. \\nSpecified \\nchemicals/items \\nfor \\nmanufacture of Pre-calcined Ferrite \\nPowder \\n7.5 \\nNil \\n(up to \\n31.03.2024)\\n2. \\nPalladium Tetra Amine Sulphate for \\nmanufacture of parts of connectors \\n7.5 \\nNil \\n(up to \\n31.03.2024)\\n3. \\nCamera lens and its inputs/parts for \\nuse in manufacture of camera module \\nof cellular mobile phone \\n2.5 \\nNil \\n4. \\nSpecified parts for manufacture of \\nopen cell of TV panel \\n5 \\n2.5 \\nVI. \\nElectronic Appliances \\n1. \\nHeat coil for manufacture of electric \\nkitchen chimneys \\n20 \\n15 \\nVII. \\nOthers \\n1. \\nWarm blood horse imported by sports \\nperson of outstanding eminence for \\ntraining purpose \\n30 \\nNil \\n2. \\nVehicles, \\nspecified \\nautomobile \\nparts/components, sub-systems and \\ntyres when imported by notified \\ntesting agencies, for the purpose of \\ntesting and/ or certification, subject to \\nconditions. \\nAs \\napplicable \\nNil'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 55}, page_content='52 \\n \\n \\n \\nC.2. \\nIncrease in Customs duty [with effect from 02.02.2023]  \\nS. No.\\nCommodity \\n \\nRate of duties \\nFrom \\n(per cent) \\nTo \\n(per cent) \\nI. \\nChemicals \\n1. \\nStyrene \\n2 \\n(+0.2 SWS) \\n2.5 \\n(+0.25 \\nSWS) \\n2. \\nVinyl chloride monomer \\n2 \\n(+0.2 SWS) \\n2.5 \\n(+0.25 \\nSWS) \\nII \\nPetrochemical \\n1 \\nNaphtha \\n1 \\n(+ 0.1 SWS) \\n2.5 \\n(+0.25  SWS)\\nIII. \\nPrecious Metals \\n1. \\nSilver (including silver plated with gold \\nor platinum), unwrought or in semi-\\nmanufactured forms, or in powder \\nform \\n7.5 \\n(+ 2.5 \\nAIDC+ 0.75 \\nSWS) \\n10 \\n(+ 5 AIDC+ \\nNil SWS) \\n2. \\nSilver dore \\n6.1 \\n(+ 2.5 \\nAIDC+ 0.61  \\nSWS) \\n10 \\n(+ 4.35 \\nAIDC+ Nil \\nSWS) \\nIV. \\nGems and Jewellery Sector \\n1. \\nArticles of Precious Metals such as \\ngold/silver/platinum \\n20 \\n(+Nil AIDC \\n+2 SWS)  \\n25 \\n(+Nil AIDC \\n+Nil SWS) \\n2. \\nImitation Jewellery \\n20 or ` \\n400/kg., \\nwhichever is \\nhigher \\n \\n(+Nil AIDC +2 \\nor ` 40 per \\nKg SWS) \\n \\n25 or ` \\n600/kg., \\nwhichever is \\nhigher \\n(+Nil AIDC \\n+Nil SWS)'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 56}, page_content='53 \\n \\n \\n \\nS. No.\\nCommodity \\n \\nRate of duties \\nFrom \\n(per cent) \\nTo \\n(per cent) \\nV. \\nAutomobiles \\n1 \\nVehicle (including electric vehicles) in \\nSemi-Knocked Down (SKD) form . \\n30 \\n(+3 SWS) \\n35 \\n(+Nil SWS) \\n2 \\nVehicle in Completely Built Unit (CBU) \\nform, other than with CIF more than \\nUSD 40,000 or with engine capacity \\nmore than 3000 cc for petrol-run \\nvehicle and more than 2500 cc for \\ndiesel-run vehicles, or with both \\n60 \\n(+6  SWS) \\n70 \\n(+Nil SWS) \\n3 \\nElectrically \\noperated \\nVehicle \\nin \\nCompletely Built Unit (CBU) form, \\nother than with CIF value more than \\nUSD 40,000 \\n60 \\n(+ 6 SWS) \\n70 \\n(+Nil SWS) \\nVI. \\nOthers \\n1. \\nBicycles \\n30 \\n \\n(+ Nil AIDC \\n+3 SWS) \\n35 \\n \\n(+ Nil AIDC \\n+Nil SWS) \\n2. \\nToys and parts of toys (other than \\nparts of electronic toys) \\n60 \\n \\n(+Nil AIDC+ \\n6 SWS) \\n70 \\n \\n(+Nil AIDC+ \\nNil SWS) \\n3. \\nCompounded Rubber \\n10 \\n \\n \\n25 or ` \\n30/kg., \\nwhichever is \\nlower \\n4. \\nElectric Kitchen Chimney \\n7.5 \\n \\n15 \\n \\n* AIDC -Agriculture Infrastructure Development Cess; SWS – Social Welfare \\nSurcharge'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'file_path': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-02-01T08:28:21+05:30', 'trapped': '', 'modDate': \"D:20230201082821+05'30'\", 'creationDate': \"D:20230201052804+05'30'\", 'page': 57}, page_content='54 \\n \\n \\n \\nD. \\nCHANGES IN CENTRAL EXCISE \\nD.1. \\nNCCD Duty rate  on Cigarettes [with effect from 02.02.2023] \\n \\n \\nDescription of goods \\nRate of excise duty \\nFrom \\n(` per 1000 \\nsticks) \\nTo \\n(` per 1000 \\nsticks) \\nOther than filter cigarettes, of length not \\nexceeding 65 mm \\n200 \\n230 \\nOther than filter cigarettes, of length exceeding \\n65 mm but not exceeding 70 mm \\n250 \\n290 \\nFilter cigarettes of length not exceeding 65 mm \\n440 \\n510 \\nFilter cigarettes of length exceeding 65 mm but \\nnot exceeding 70 mm \\n440 \\n510 \\nFilter cigarettes of length exceeding 70 mm but \\nnot exceeding 75 mm \\n545 \\n630 \\nOther cigarettes \\n735 \\n850 \\nCigarettes of tobacco substitutes \\n600 \\n690 \\n \\n \\nD.2. \\nOther changes in Central Excise [with effect from 02.02.2023] \\nIn order to promote green fuel, central excise duty exemption is being \\nprovided to blended Compressed Natural Gas from so much of the amount \\nas is equal to the GST paid on Bio Gas/Compressed Bio Gas contained in the \\nblended CNG. \\nE. \\nOTHERS \\nThere are few other changes of minor nature. For details of the budget \\nproposals, the Explanatory Memorandum and other relevant budget \\ndocuments may be referred to. \\n*****'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'subject': '', 'keywords': '', 'moddate': '2025-01-17T00:36:33+05:30', 'trapped': '', 'modDate': \"D:20250117003633+05'30'\", 'creationDate': \"D:20250117003633+05'30'\", 'page': 0}, page_content=\"Conversational Text Extraction with Large \\nLanguage Models Using Retrieval-Augmented \\nSystems \\nSoham Roy1, Mitul Goswami1, Nisharg Nargund1, Suneeta Mohanty1 and Prasant Kumar Pattnaik1 \\nSchool of Computer Engineering, Kalinga Institute of Industrial Technology, Patia, Bhubaneswar, 751024, India1, \\nAbstract— This study introduces a system leveraging Large \\nLanguage Models (LLMs) to extract text and enhance user \\ninteraction with PDF documents via a conversational interface. \\nUtilizing Retrieval-Augmented Generation (RAG), the system \\nprovides informative responses to user inquiries while \\nhighlighting relevant passages within the PDF. Upon user \\nupload, the system processes the PDF, employing sentence \\nembeddings to create a document-specific vector store. This \\nvector store enables efficient retrieval of pertinent sections in \\nresponse to user queries. The LLM then engages in a \\nconversational exchange, using the retrieved information to \\nextract text and generate comprehensive, contextually aware \\nanswers. While our approach demonstrates competitive \\nROUGE values compared to existing state-of-the-art techniques \\nfor text extraction and summarization, we acknowledge that \\nfurther qualitative evaluation is necessary to fully assess its \\neffectiveness in real-world applications.  The proposed system \\ngives competitive ROUGE values as compared to existing state-\\nof-the-art techniques for text extraction and summarization, \\nthus offering a valuable tool for researchers, students, and \\nanyone seeking to efficiently extract knowledge and gain \\ninsights from documents through an intuitive question-\\nanswering interface. \\nKeywords—Large \\nLanguage \\nModel \\n(LLM), \\nRetrieval \\nAugmented Generation, Embeddings, Text Extraction, ROUGE \\nI. INTRODUCTION  \\nThe ever-growing volume of digital documents, particularly \\nPDFs, presents a significant challenge: efficiently extracting \\nknowledge from their text-heavy content. Over the years, \\nvarious tools and techniques have been developed to address \\nthis issue, from basic keyword search functionalities to more \\nadvanced text mining and natural language processing (NLP) \\nalgorithms [1]. Despite these advancements, many solutions \\nstill fall short of providing contextually relevant information \\nquickly and accurately. The evolution of artificial \\nintelligence and machine learning, particularly in the form of \\nlarge language models, has revolutionized this process, \\nenabling more sophisticated and efficient extraction of \\nknowledge from vast repositories of digital documents [2]. \\n \\nLarge language models (LLMs) have undergone significant \\nevolution, transforming the landscape of natural language \\nprocessing (NLP) and information retrieval. While the \\nintegration of Retrieval-Augmented Generation (RAG) with \\nLLMs is noteworthy, it is essential to recognize that similar \\nframeworks have been explored in existing literature. This \\npaper aims to build upon these studies by providing a tailored \\napplication for document interaction. The advent of machine \\nlearning, particularly deep learning, marked a significant leap \\nforward, with models like Word2Vec and GloVe introducing \\nword embeddings that captured semantic relationships \\nbetween words [3]. Furthermore, Transformers utilize self-\\nattention mechanisms to process and understand text in \\nparallel, rather than sequentially, enabling them to capture \\nlong-range dependencies and contextual information more \\neffectively. BERT, with its bidirectional approach, improved \\nthe understanding of context within a text, while GPT, with \\nits autoregressive nature, excelled in text generation [4][5]. \\nHowever, while the use of these models has become \\nwidespread, the integration of retrieval augmented generation \\nfor targeted PDF interaction remains under-explored. This \\nwork focuses on addressing this niche, aiming to bridge this \\ngap by combining large language models with document \\nretrieval in the conversational interface, providing a more \\ntailored application in the domain of document interaction. \\nThese advancements in LLMs have significantly enhanced \\ntext extraction and data retrieval capabilities. This capability \\nis particularly useful for handling the ever-growing volume \\nof digital documents, enabling efficient knowledge extraction \\nand insight generation [6].  \\n \\nBuilding on the advancements in LLMs, Retrieval-\\nAugmented Generation (RAG) systems enhance the \\ncapability of these models by integrating a retrieval \\nmechanism. RAG combines information retrieval and \\ngenerative processes to produce highly accurate and \\ncontextually relevant responses [7]. In a RAG framework, the \\nsystem first retrieves relevant passages from a large corpus of \\ndocuments based on the user's query [8][9]. This combination \\nallows the model to generate responses that are both informed \\nby a broad understanding of language and enriched with \\nprecise, relevant details from the retrieved content. This \\napproach significantly improves the model's ability to handle \\ncomplex queries and extract pertinent information from large \\ndatasets, making it an invaluable tool for efficient and \\naccurate knowledge extraction. In this study, the authors \\nintroduce a novel approach  for text extraction that leverages \\nan LLM system to enhance user interaction with documents \\nvia a conversational interface.  \\nII. RELATED WORK \\nRecent advancements in document understanding and \\ninformation extraction have been driven by deep learning \\ntechniques. Traditional keyword matching and rule-based \\nmethods often struggle with complex documents, while deep \\nlearning models provide more robust solutions capable of \\naccurately handling intricate structures. For instance, M. Li \\net al. introduced the “BiomedRAG” model, which supervises \\nretrieval in the biomedical domain through varied chunk \\ndatabase creation, enhancing prediction accuracy [10]. \\nSimilarly, M. D. Cyril Zakka et al. developed the “Almanac”\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'subject': '', 'keywords': '', 'moddate': '2025-01-17T00:36:33+05:30', 'trapped': '', 'modDate': \"D:20250117003633+05'30'\", 'creationDate': \"D:20250117003633+05'30'\", 'page': 1}, page_content='framework, which retrieves medical guidelines and treatment \\nadvice, \\noutperforming \\ntypical \\nLLMs \\nin \\nfactuality, \\ncompleteness, user preference, and safety [11]. Additionally, \\nK. Yang et al. introduced \"LeanDojo,\" a RAG-based LLM \\nthat streamlines theorem proving with comprehensive \\ntoolkits and data [12]. P. Lewis et al. explored a fine-tuning \\nrecipe for RAG models, leveraging pre-trained parametric \\nand \\nnon-parametric \\nmemory \\nto \\nimprove \\nlanguage \\ndevelopment [13]. \\n \\nZ. Feng et al. proposed an iterative retrieval-generation \\ncollaborative framework that not only allows for the use of \\nboth parametric and non-parametric knowledge but also aids \\nin the discovery of the correct reasoning path via retrieval-\\ngeneration interactions, which is critical for tasks that require \\nmulti-step reasoning [14]. J. Miao et al. demonstrated the \\ndevelopment of a specialized ChatGPT model connected with \\nan RAG system that is intended to meet the KDIGO 2023 \\ncriteria for chronic kidney disease [15]. In a different domain, \\nH. Li et al. demonstrated the efficacy of leveraging attention \\nprocesses in neural networks to focus on key areas of material \\nfor better question answering in language models [16]. \\nSimilarly, Y. Zhang et al. suggested a unique Multi-Modal \\nKnowledge-aware \\nHierarchical \\nAttention \\nNetwork \\n(MKHAN) to efficiently leverage a multi-modal knowledge \\ngraph (MKG) for explainable medical question answering \\n[17]. However, these approaches are often tailored to specific \\nuse cases, lacking the generalizability required for broader \\ndocument interaction tasks. \\nOur work builds upon these advancements by presenting a \\nRAG-inspired system for the interactive exploration of user-\\nuploaded PDF documents. We employ advanced sentence \\nembeddings to ensure efficient retrieval of relevant content. \\nBy integrating this context into the response generation \\nprocess of the large language model (LLM), we create a more \\ntailored and contextually rich user experience. This approach \\nallows users to engage in focused conversations that explore \\nthe specific content and nuances of the uploaded PDFs, \\nthereby enhancing the effectiveness and relevance of \\ninformation retrieval and dialogue within the system. \\nIII. RETRIEVAL AUGMENTED GENERATION  \\nThe Retrieval-Augmented Generation (RAG) architecture \\nrepresents a sophisticated integration of information retrieval \\n(IR) and generative modeling techniques, designed to \\nenhance the precision and relevance of generated responses \\nin natural language processing tasks. The RAG process \\ncommences with a robust retrieval component that sifts \\nthrough a vast corpus of documents to pinpoint relevant \\npassages aligned with the user\\'s query. Traditional IR \\ntechniques like TF-IDF and BM25 evaluate the statistical \\nrelevance of terms across documents, prioritizing those that \\nclosely match the query [18]. Fig. 1 demonstrates the detailed \\nRAG architecture. Advanced methods such as neural \\nretrievers, exemplified by Dense Passage Retrieval (DPR), \\nemploy deep learning models to encode documents into dense \\nembeddings, capturing semantic relationships and enhancing \\ncontextual understanding. \\n \\n \\n \\nFig. 1. RAG Architecture \\n \\nOnce relevant passages are identified, they undergo encoding \\ninto document embeddings. These embeddings encapsulate \\nthe semantic meaning and context of the retrieved text, \\nemploying techniques like sentence embeddings from models \\nsuch as the Universal Sentence Encoder or BERT [19]. These \\nembeddings serve as enriched inputs to the subsequent stage \\nof the RAG architecture. Integration with a generative model, \\ntypically an LLM such as GPT, marks the next critical phase. \\nThe generative model utilizes the contextual information \\nembedded in the document embeddings to produce responses \\nthat are not only grammatically accurate and fluent but also \\ncontextually aligned with the user\\'s query [20]. By integrating \\ndetailed context from the embeddings, the generative model \\nensures that its responses are informed by both the broad \\nlinguistic knowledge it has learned during training and the \\nspecific details extracted from the retrieved passages. To \\nfurther refine performance, the RAG architecture often \\ninvolves fine-tuning the generative model on task-specific \\ndatasets [21].  \\nIV. PROPOSED METHODOLOGY \\nA. Data Chunking \\nThe integration of the PyPDF2 library enables efficient text \\nextraction and management of PDF documents within the \\nmodel. Initially, a PdfReader object is created to represent the \\nentire PDF, facilitating seamless interaction with its content \\n[22]. \\n \\n \\nFig. 2. Workflow of the Proposed Model \\n \\nTo extract the raw text, the model iteratively traverses each \\npage using a loop, employing the extract_text() method. The \\nextracted texts are then consolidated into a single string \\nvariable, pdf_text, which captures the entire textual content \\nof the PDF. Given the potentially large size of pdf_text, the \\nmodel implements text chunking to improve computational'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'subject': '', 'keywords': '', 'moddate': '2025-01-17T00:36:33+05:30', 'trapped': '', 'modDate': \"D:20250117003633+05'30'\", 'creationDate': \"D:20250117003633+05'30'\", 'page': 2}, page_content='efficiency. Equation (1) outlines the mathematical process for \\ntext chunking, where n represents the number of desired \\nchunks, chunk_size indicates the size of each chunk, and  \\nchunk_overlap defines the overlap between consecutive \\nchunks. \\n𝑖𝑛= (𝑛−1) × (𝑐ℎ𝑢𝑛𝑘_𝑠𝑖𝑧𝑒 −𝑐ℎ𝑢𝑛𝑘_𝑜𝑣𝑒𝑟𝑙𝑎𝑝)           (1) \\n \\nEquation (1) calculates the starting index for each chunk \\nbased on its position n, chunk size, and overlap, ensuring that \\neach chunk overlaps with the previous ones by a specified \\nnumber of characters. To determine the ending index for each \\nchunk, Equation (2) provides a formula where jn indicates the \\nending index of chunk n. \\n𝑗𝑛= 𝑖𝑛+ 𝑐ℎ𝑢𝑛𝑘_𝑠𝑖𝑧𝑒                                                     (2) \\n \\nThis approach allows for the systematic division of large text \\ninto smaller segments, facilitating easier processing and \\nanalysis in natural language processing tasks such as \\ninformation retrieval, text summarization, and machine \\ntranslations. Each chunk is associated with metadata to enrich \\nthe context and facilitate easier retrieval of specific text \\nsegments. Metadata is organized as a list of dictionaries, with \\neach dictionary corresponding to a chunk in the text list. \\nTypically, metadata includes a key-value pair where the key \\ndenotes the origin or source identifier of the chunk within the \\nPDF signifies the page number, and \"pl\" denotes paragraph \\nlevel). This approach allows for precise tracking and retrieval \\nof information within the PDF document, enhancing the \\nmodel’s capability to handle and manipulate textual content \\neffectively in various applications and user interactions. \\nB. Vector Embeddings For Efficient Retrieval \\nIn preparing text for efficient retrieval, the model utilizes \\nsentence embedding techniques to convert text chunks into \\nnumerical representations. This step is crucial for enabling \\nfast and accurate retrieval of semantically similar sentences \\nor passages from a document. Sentence embedding \\ntechniques are designed to map sentences from their original \\nhigh-dimensional textual space into a lower-dimensional \\nvector space. This transformation allows for efficient \\ncomparison and retrieval of sentences based on their semantic \\ncontent. A widely used approach for generating these \\nembeddings is employing pre-trained sentence transformers. \\nIn the model, the specific embedding used is “sentence-\\ntransformers/all-MiniLM-L6-v2”. These models are trained \\non extensive text corpora and have learned to encapsulate the \\nsemantic essence of sentences within vector representations \\n[23]. The sentence embedding function is defined in Equation \\n(3) where S is a sentence composed of a sequence of words, \\nW is the embedding matrix for the vocabulary, and  f is the \\nsentence embedding function that maps a sentence S to a \\nvector 𝑠∈𝑅𝑑. \\n \\n𝑠= 𝑓(𝑠)                                                                            (3) \\n \\nThis function f often involves multiple steps, including word \\nembeddings, contextual embeddings using transformer \\nmodels, and aggregation methods. Each word 𝜔𝑖 in the \\nsentence S is mapped to a vector 𝑤i using an embedding \\nmatrix W in Equation (4) where 𝑊[𝜔𝑖] \\n \\n𝑤𝑖= 𝑊[𝜔𝑖]                                                                       (4)                                                                                                                             \\n \\nTo obtain a single fixed-dimensional vector representing the \\nentire sentence, an aggregation method using mean pooling \\nis applied to the contextual embeddings. Equation (5) \\ncomputes the average of the contextual embeddings of all \\nwords in the sentence, resulting in the final sentence \\nembedding  \\n \\n𝑆= \\n1\\n𝑛∑ℎ\\n𝑛\\n1\\n                                                                         (5) \\n \\nThe vector S in Equation (5) is the sentence embedding, \\nwhich captures the meaning of the sentence in a way that \\nallows for efficient comparison and retrieval in natural \\nlanguage \\nprocessing \\ntasks. \\nThe \\nmodel \\nuses \\nHuggingFaceEmbedings \\nclass \\nfrom \\nlangchain-\\ncommunity.embeddings module to work with the pre-trained \\nsentence transformer model. To load the model, the model \\nname is specified along with any extra configuration options. \\nThe embeddings object generates vector representations for \\neach text chunk using the compute_embeddings method, \\nwhich takes a list of chunks as input and outputs \\ncorresponding embedding vectors that capture their meanings \\nin numerical form. These vectors are then combined with \\nmetadata, which includes information about the source of \\neach chunk within the PDF document. This integration results \\nin a final list of document representations optimized for \\nefficient retrieval. Consequently, the model can quickly and \\naccurately locate relevant passages in response to user \\nqueries, leveraging the meanings captured in the embeddings \\nalong with contextual details from the metadata. \\nC. Building The Conversational Retrieval Chain(CRC) \\n \\n This comprehensive approach involves several key \\ncomponents that synergize to deliver a seamless user \\nexperience. \\n \\n• \\nLarge Language Model  \\n \\nAt the heart of the CRC is the LLM, which generates \\nresponses to user queries. The model utilizes the Groq LLM \\n(llm_groq), integrated through the langchain_groq library. \\nThis pre-trained LLM leverages its extensive knowledge \\nbase, derived from vast amounts of text data, to understand \\nand answer user questions accurately. The LLM\\'s capability \\nto generate coherent and contextually appropriate responses \\nmakes it a crucial component of the CRC. \\n \\n• \\nRetriever \\n \\nThe retriever component is responsible for fetching relevant \\ninformation from the document based on the user\\'s query. \\nThe \\nmodel \\nemploys \\nthe \\nfaiss \\nlibrary \\nfrom \\nlangchain_community.vectorstores to create a vector store \\nusing the document embeddings generated earlier. These \\nembeddings \\ntransform \\ntext \\nchunks \\ninto \\nnumerical \\nrepresentations that capture their semantic content. The \\nvector store allows for efficient retrieval of document \\nsections (chunks) that are semantically similar to the user\\'s \\nquery. The as_retriever method of the vector store object is'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'subject': '', 'keywords': '', 'moddate': '2025-01-17T00:36:33+05:30', 'trapped': '', 'modDate': \"D:20250117003633+05'30'\", 'creationDate': \"D:20250117003633+05'30'\", 'page': 3}, page_content='used to create a retriever object that integrates into the CRC, \\nenabling precise and relevant information retrieval. \\n \\n• \\nMemory \\n \\nMemory \\nmanagement \\nis \\nessential \\nfor \\npreserving \\nconversational \\ncontext. \\nThe \\nmodel \\nemploys \\nthe \\nConversationBufferMemory class from langchain.memory to \\nstore past user queries and LLM responses. This history is \\ncrucial for the LLM to reference prior interactions when \\ngenerating current responses. The memory is set up with \\nkeys: memory_key=\"chat_history\" for conversation history \\nand output_key=\"answer\" for the LLM\\'s responses. This \\nconfiguration facilitates more coherent and contextually \\naware interactions. \\n \\n• \\nChain Configuration \\n \\n To \\nintegrate \\nthese \\ncomponents, \\nthe \\nConversationalRetrievalChain.from_llm method is employed \\nwith specific parameters. The LLM parameter is configured \\nto utilize thr Groq LLM object(llm_groq). The chain_type is \\ndesignated as “stuff”, indicating a focus on retrieving factual \\ncontent from the document. The retriever parameter is linked \\nto the retriever object generated from the vector store,  \\nensuring efficient retrieval of relevant document section.  The \\nmemory parameter is set to conversation buffer memory \\nobject. Further, return_source_documents is True instructing \\nchains to return chunks along with responses. This ensures \\nthe accurate answers enriched with relevant context from the \\ndocuments.  \\nD. User Interaction And Model Response \\nThe model enables a natural and interactive conversation \\nbetween users and their uploaded PDF documents. The \\nprocess starts when users input their questions through a text \\nfield integrated into the Streamlit interface, ensuring that \\ninitiating queries is straightforward and accessible. Users \\ntype their questions into the provided input field \\n(st.text_input), and upon submission, the system promptly \\ncaptures the query and processes it using the retrieval chain. \\nThe chain.invoke method efficiently directs the query to \\nsubsequent stages of the workflow. \\nAt the core of the model\\'s functionality is the Conversational \\nRetrieval Chain. To generate contextually rich responses, the \\nchain first accesses the conversation history via the \\nConversationBufferMemory [25], which retains past user \\nqueries and responses, ensuring that the current interaction \\nbenefits from previous exchanges. \\nSubsequently, the system utilizes a retriever that operates on \\na pre-constructed document vector store, comprising \\nembeddings of text chunks extracted from the PDF. The \\nretriever searches for document sections that are semantically \\nsimilar to the user\\'s query, using cosine similarity to evaluate \\nhow closely related two vectors are within the embedding \\nspace. Equation (6) illustrates the concept of cosine \\nsimilarity. \\n \\ncos(𝑢, 𝑣) =\\n𝑢.𝑣\\n(‖𝑢‖|‖𝑣‖)                                                      (6) \\n \\nCosine similarity scores range from -1 (completely \\ndissimilar) to 1 (identical). The model retrieves the document \\nsections with the highest cosine similarity scores, indicating \\ntheir relevance to the user\\'s query. These retrieved sections, \\nalong with the conversation context, are then fed into the \\nGroq LLM. By leveraging its pre-trained knowledge and the \\nspecific context from the retrieved text, the Groq LLM \\ngenerates comprehensive and accurate responses to user \\nquestions. \\n \\nWhen relevant document sections are retrieved, the model \\nenhances responses by referencing these sources. It assigns \\nunique identifiers to each retrieved section and may include \\nthese references in the response text. This method not only \\nensures transparency but also allows users to verify the \\ninformation\\'s origin. To improve usability, Streamlit \\nexpanders (st.expander) are used, enabling users to easily \\nview the content of the retrieved document sections. By \\nclicking on the corresponding source names, users can \\nexpand and read the specific excerpts that informed the \\nLLM\\'s response. This interactive feature allows users to \\nexplore the document content more deeply, enhancing their \\nunderstanding and engagement with the material. \\nV. EXPERIMENTATIONS AND RESULTS \\nTo assess the model\\'s capability to navigate and summarize \\ncomplex academic materials, we employed ROUGE (Recall-\\nOriented Understudy for Gisting Evaluation) scores, a widely \\naccepted metric for evaluating the quality of automatically \\ngenerated summaries against human-written references. \\nHowever, relying solely on ROUGE metrics may not \\nadequately reflect the system\\'s interactive and conversational \\naspects. Therefore, future studies will include qualitative \\nevaluations to examine user interaction quality and the \\nmodel\\'s effectiveness from an end-user perspective, ensuring \\na comprehensive assessment of its performance. \\nThe evaluation utilized a carefully curated dataset comprising \\ntop-cited research articles, known for their dense information \\ncontent, technical language, and intricate methodologies. \\nThese articles posed significant challenges, making them \\nwell-suited for rigorously testing the model\\'s summarization \\ncapabilities. The article abstracts served as input reference \\nsummaries for calculating the ROUGE scores for each \\ndocument. This analysis provided valuable insights into the \\nmodel\\'s proficiency in accurately capturing and summarizing \\ncritical findings from highly technical and detailed research \\nliterature. ROUGE measures the overlap of n-grams between \\nthe generated text and the reference text, mathematically \\nrepresented in Equation (6), where the maximum number of \\nn-grams co-occurring in both the candidate and reference \\nsummaries is considered. \\n \\n𝑅𝑂𝑈𝐺𝐸= \\n∑\\n∑\\n𝐶𝑜𝑢𝑛𝑡_𝑚𝑎𝑡𝑐ℎ(𝑔𝑟𝑎𝑚𝑛)\\n𝑔𝑟𝑎𝑚𝑛∈𝑆\\n𝑆∈{𝑆𝑢𝑚𝑚𝑎𝑟𝑖𝑒𝑠}\\n∑\\n∑\\n𝐶𝑜𝑢𝑛𝑡(𝑔𝑟𝑎𝑚𝑛)\\n𝑔𝑟𝑎𝑚𝑛∈𝑆\\n𝑆∈{𝑆𝑢𝑚𝑚𝑎𝑟𝑖𝑒𝑠}\\n       (6) \\n \\nThe study specifically used ROUGE-1 (unigrams) and \\nROUGE-2 (bigrams) in the evaluation. ROUGE-L measures \\nthe longest common subsequence (LCS) between the \\ncandidate and reference summaries. It\\'s calculated using \\nEquations (7), and (8) followed by Equation (9), where X is \\nthe reference summary of length m, Y is the candidate'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'subject': '', 'keywords': '', 'moddate': '2025-01-17T00:36:33+05:30', 'trapped': '', 'modDate': \"D:20250117003633+05'30'\", 'creationDate': \"D:20250117003633+05'30'\", 'page': 4}, page_content=\"summary of length n, and β is typically set to favor recall (β \\n>1). \\n \\n𝑅𝑂𝑈𝐺𝐸−𝐿𝑅𝑒𝑐𝑎𝑙𝑙= \\n𝐿𝐶𝑆(𝑋,𝑌)\\n𝑚\\n                                           (7) \\n𝑅𝑂𝑈𝐺𝐸−𝐿𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛= \\n𝐿𝐶𝑆(𝑋,𝑌)\\n𝑛\\n                                       (8) \\n𝑅𝑂𝑈𝐺𝐸−𝐿𝐹= \\n(1+𝛽2)∙𝑅∙𝑃\\n𝑅+𝛽2∙𝑃                                               (9) \\n \\nTo evaluate the model performance, the authors tested it with \\na custom dataset of various research papers sourced from top \\nresearch databases and analyzed the ROUGE scores of the \\ngenerated answers. The relatively moderate ROUGE scores \\ncan be attributed to the model’s focus on condensing \\nextensive content into concise responses. This indicates the \\nmodel’s tendency to prioritize brevity and specificity over \\nword-for-word overlap. The average representative scores \\nobtained from evaluating upon the dataset, are given in Table. \\nI. \\nTABLE I.  \\nPERFORMANCE METRICS OF THE MODEL \\nPerformance Metric \\nScore Values (Average) \\nROUGE – 1 \\n0.4604 \\nROUGE – 2 \\n0.3576 \\nROUGE - L \\n0.4283 \\n \\nThe scores indicate that approximately 46% of individual \\nwords (ROUGE-1) and around 35% of bigram phrases \\n(ROUGE-2) in the generated responses matched those found \\nin the original documents. The ROUGE-L score, which lies \\nbetween ROUGE-1 and ROUGE-2, demonstrates some \\npreservation of word order while accommodating gaps and \\nrephrasing. The relatively low ROUGE scores highlight the \\nsystem's capability to distill information into concise answers \\ninstead of replicating large text segments. Moreover, the \\ncomplexity and dense information structure of the input \\nresearch articles creates a high bar for any model aiming to \\nbalance conciseness with informativeness. Good summaries \\noften rephrase ideas, leading to lower word-for-word matches \\nbut potentially better conveyance of key concepts. \\nFurthermore, the system focuses on providing specific \\nanswers to questions, naturally leading to lower overlap with \\nthe full text of the documents. Moreover, the significant \\nlength difference between focused answers and entire articles \\nfurther contributes to the lower ROUGE scores. Table. II \\ncompares the model performance with other SOTA \\napproaches. \\nTABLE II.  \\nCOMPARISON OF PROPOSED MODEL PERFORMANCE \\nMETRICS \\nModel \\nROUGE - 1 \\nROUGE - 2 \\nROUGE - L \\nRAG-PDF \\n (Our Model) \\n0.4604 \\n0.3576 \\n0.4283 \\nML + RL \\nROUGE + Novel, \\nWith LM  [26] \\n \\n0.4019 \\n \\n0.1738 \\n \\n0.3752 \\nCOSUM [27] \\n0.4908 \\n0.2379 \\n0.2834 \\nLatent Semantic \\nAnalysis [28] \\n0.4621 \\n0.2618 \\n0.3479 \\nEdgeSumm [29] \\n0.5379 \\n0.2858 \\n0.4979 \\nGenerative \\nAdversarial \\nNetwork  [30] \\n \\n0.3992 \\n \\n0.1765 \\n \\n0.3671 \\nTFRSP [31] \\n0.2483 \\n0.2874 \\n0.2043 \\nTable II presents a comparative analysis of various models \\nbased on ROUGE-1, ROUGE-2, and ROUGE-L scores, \\nwhich \\nevaluate \\nsummary \\nquality \\nagainst \\nreference \\nsummaries. The RAG-PDF model demonstrates strong \\nperformance, achieving a ROUGE-1 score of 0.4604, \\nROUGE-2 score of 0.3576, and ROUGE-L score of 0.4283, \\nindicating its effectiveness in capturing both individual words \\nand longer sequences for coherent summaries. \\nWhile EdgeSumm excels in ROUGE-1 and ROUGE-L, its \\nlower ROUGE-2 score reveals limitations in bigram \\ncoherence. Our model balances coherence, particularly in \\ncomplex technical text. In contrast, the ML + RL ROUGE + \\nNovel model shows poorer performance, especially in \\nROUGE-2 (0.1738) and ROUGE-L (0.3752), suggesting \\nchallenges in capturing bi-gram sequences. COSUM \\nperforms well in ROUGE-1 (0.4908) but lacks coherence in \\nlonger sequences with lower ROUGE-2 (0.2379) and \\nROUGE-L (0.2834). \\nLatent Semantic Analysis is comparable to our model in \\nROUGE-1 (0.4621) but falls short in ROUGE-2 (0.2618) and \\nROUGE-L (0.3479). The Generative Adversarial Network \\nmodel exhibits low scores across metrics, particularly in \\nROUGE-2 (0.1765). Lastly, the TFRSP model scores the \\nlowest in ROUGE-1 (0.2483) and ROUGE-L (0.2043), \\nindicating significant challenges in summary generation. \\n \\nWhile ROUGE metrics provide useful insights, they may not \\nfully capture user experience or interaction quality. \\nTherefore, future work will focus on incorporating user-\\ncentered evaluations, including qualitative feedback and \\ninteraction analysis, to align the system’s performance with \\nreal-world needs. \\nVI. CONCLUSION AND FUTURE WORK \\nThe model introduces a unique approach for interacting with \\nPDF documents via a conversational interface, harnessing the \\npower of LLMs and RAG. This system enables users to \\nextract valuable insights from complex and text-heavy \\nmaterials effectively. One of its standout features is its focus \\non the specific content of uploaded PDFs, rather than relying \\non extensive external knowledge bases. By employing \\nsentence embeddings, the model converts text chunks into \\nnumerical vectors and utilizes cosine similarity for efficient \\nretrieval, aligning responses closely with user intent. \\nPerformance evaluations reveal competitive ROUGE \\nscores—0.4604 for ROUGE-1, 0.3576 for ROUGE-2, and \\n0.4283 for ROUGE-L—demonstrating the model's capability \\nto \\ncapture \\nessential \\ncontent \\nand \\nstructure \\nwhile \\noutperforming many existing models in summarization and \\nquestion answering. \\nTo enhance the practical application of this system, future \\nwork will aim to generalize its approach for a wider variety \\nof document types. This will include refining the retrieval \\nmechanism to accommodate diverse structures, such as legal, \\nfinancial, and multimodal documents, thereby increasing the \\nsystem's versatility in real-world scenarios. We also plan to \\nincorporate reinforcement learning techniques to improve \\nuser interactions, allowing the model to adapt dynamically \\nbased on feedback. Exploring the incorporation of knowledge \\ngraphs and ontologies may also improve semantic \\nunderstanding and contextualization. Furthermore, refining \\nthe model with user interaction data and reinforcement\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'subject': '', 'keywords': '', 'moddate': '2025-01-17T00:36:33+05:30', 'trapped': '', 'modDate': \"D:20250117003633+05'30'\", 'creationDate': \"D:20250117003633+05'30'\", 'page': 5}, page_content='learning can facilitate more personalized responses, ensuring \\nthat the system continuously evolves to meet user needs \\neffectively. \\nREFERENCES \\n[1] Khurana, D., Koli, A., Khatter, K., & Singh, S. (2023). Natural \\nlanguage processing: State of the art, current trends, and challenges. \\nMultimedia Tools and Applications, 82(4), 3713–3744. \\n[2] Bahl, L. R., Brown, P. F., de Souza, P. V., & Mercer, R. L. (1989). A \\ntree-based statistical language model for natural language speech \\nrecognition. IEEE Transactions on Acoustics, Speech, and Signal \\nProcessing, 37(7), 1001–1008. \\n[3] Curto, G., Jojoa Acosta, M. F., & Comim, F. (2024). Are AI systems \\nbiased against the poor? A machine learning analysis using Word2Vec \\nand GloVe embeddings. AI & Society, 39(3), 617–632. \\n[4] Zheng, X., Zhang, C., & Woodland, P. C. (2021). Adapting GPT, GPT-\\n2, and BERT language models for speech recognition. In 2021 IEEE \\nAutomatic Speech Recognition and Understanding Workshop (ASRU) \\n(pp. 162–168). \\n[5] Qu, Y., Liu, P., Song, W., Liu, L., & Cheng, M. (2020). A text \\ngeneration and prediction system: Pre-training on new corpora using \\nBERT and GPT-2. In 2020 IEEE 10th International Conference on \\nElectronics Information and Emergency Communication (ICEIEC) \\n(pp. 323–326). \\n[6] Wang, L., Ma, C., & Feng, X. (2024). A survey on large language \\nmodel-based autonomous agents. Frontiers of Computer Science, 18, \\n186345.  \\n[7] Xu, L., Lu, L., Liu, M., & others. (2024). Nanjing Yunjin intelligent \\nquestion-answering system based on knowledge graphs and retrieval-\\naugmented generation technology. Heritage Science, 12, 118. \\n[8] Louis, A., van Dijck, G., & Spanakis, G. (2024). Interpretable Long-\\nForm Legal Question Answering with Retrieval-Augmented Large \\nLanguage Models. Proceedings of the AAAI Conference on Artificial \\nIntelligence, 38(20), 22266-22275. \\n[9] Yang, X., Chen, A., PourNejatian, N., & others. (2022). A large \\nlanguage model for electronic health records. npj Digital Medicine, 5, \\n194.  \\n[10] Li, M., Kilicoglu, H., Xu, H., & Zhang, R. (2024). BiomedRAG: A \\nretrieval-augmented large language model for biomedicine. ArXiv: \\nComputation and Language. \\n[11] Hiesinger, W., Zakka, C., Chaurasia, A., Shad, R., Dalal, A., Kim, J., \\nMoor, M., Alexander, K., Ashley, E., Boyd, J., Boyd, K., Hirsch, K., \\nLanglotz, C., & Nelson, J. (2023). Almanac: Retrieval-augmented \\nlanguage models for clinical medicine. Research Square.  \\n[12] Yang, K., Swope, A., Gu, A., Chalamala, R., Song, P., Yu, S., Godil, \\nS., Prenger, R. J., & Anandkumar, A. (2023). LeanDojo: Theorem \\nproving with retrieval-augmented language models. In Advances in \\nNeural Information Processing Systems 36 (NeurIPS 2023). \\n[13] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., \\nKüttler, H., Lewis, M., Yih, W., Rocktäschel, T., Riedel, S., & Kiela, \\nD. (2020). Retrieval-augmented generation for knowledge-intensive \\nNLP tasks. In Advances in Neural Information Processing Systems 33 \\n(NeurIPS 2020). \\n[14] Feng, Z., Feng, X., Zhao, D., Yang, M., & Qin, B. (2024). Retrieval-\\ngeneration synergy augmented large language models. In ICASSP \\n2024 – IEEE International Conference on Acoustics, Speech and Signal \\nProcessing (pp. 11661–11665).  \\n[15] Miao, J., Thongprayoon, C., Suppadungsuk, S., Garcia Valencia, O., & \\nCheungpasitporn, \\nW. \\n(2024). \\nIntegrating \\nretrieval-augmented \\ngeneration with large language models in nephrology: Advancing \\npractical applications. Medicina.  \\n[16] Hao, T., Li, X., He, Y., & others. (2022). Recent progress in leveraging \\ndeep learning methods for question answering. Neural Computing & \\nApplications, 34, 2765–2783.  \\n[17] Zhang, Y., Qian, S., Fang, Q., & Xu, C. (2019). Multi-modal \\nknowledge-aware hierarchical attention network for explainable \\nmedical question answering. In Proceedings of the 27th ACM \\nInternational Conference on Multimedia (pp. 1178–1187). \\n[18] Sawarkar, K., Mangal, A., & Solanki, S. R. (2024). Blended RAG: \\nImproving RAG (Retriever-Augmented Generation) accuracy with \\nsemantic search and hybrid query-based retrievers. ArXiv: Information \\nRetrieval.  \\n[19] Arif, N., Latif, S., & Latif, R. (2021). Question classification using \\nUniversal Sentence Encoder and deep contextualized transformer. In \\n2021 14th International Conference on Developments in eSystems \\nEngineering (DeSE) (pp. 206–211).  \\n[20] Goswami, M., Panda, N., Mohanty, S., & Pattnaik, P. K. (2023). \\nMachine learning techniques and routing protocols in 5G and 6G \\nmobile network communication system – An overview. In 2023 7th \\nInternational Conference on Trends in Electronics and Informatics \\n(ICOEI) (pp. 1094–1101).  \\n[21] Li, H., Su, Y., Cai, D., Wang, Y., & Liu, L. (2022). A survey on \\nretrieval-augmented text generation. ArXiv: Computation and \\nLanguage. \\n[22] Bui, D. D. A., Del Fiol, G., & Jonnalagadda, S. (2016). PDF text \\nclassification to leverage information extraction from publication \\nreports. Journal of Biomedical Informatics, 61, 141–148.  \\n[23] Heimerl, F., Kralj, C., Möller, T., & Gleicher, M. (2022). embComp: \\nVisual interactive comparison of vector embeddings. IEEE \\nTransactions on Visualization and Computer Graphics, 28(8), 2953–\\n2969.  \\n[24] Goswami, M., Mohanty, S., & Pattnaik, P. K. (2024). Optimization of \\nmachine learning models through quantization and data bit reduction \\nin healthcare datasets. Franklin Open, 8. \\n[25] Singh, A., Ehtesham, S., Mahmud, R., & Kim, J.-H. (2024). \\nRevolutionizing mental health care through LangChain: A journey with \\na large language model. In 2024 IEEE 14th Annual Computing and \\nCommunication Workshop and Conference (CCWC) (pp. 73–78).  \\n[26] Kryściński, W., Paulus, R., Xiong, C., & Socher, R. (2018). Improving \\nabstraction in text summarization. ArXiv: Computation and Language.  \\n[27] Alguliyev, R. M., Aliguliyev, R. M., Isazade, N. R., Abdi, A., & Idris, \\nN. B. (2018). COSUM: Text summarization based on clustering and \\noptimization. Expert Systems, 36.  \\n[28] Ozsoy, M. G., Alpaslan, F. N., & Cicekli, I. (2011). Text \\nsummarization using latent semantic analysis. Journal of Information \\nScience, 37(4), 405–417.  \\n[29] El-Kassas, W. S., Salama, C. R., Rafea, A. A., & Mohamed, H. K. \\n(2020). EdgeSumm: Graph-based framework for automatic text \\nsummarization. Information Processing & Management, 57(6).  \\n[30] Liu, L., Lu, Y., Yang, M., Qu, Q., Zhu, J., & Li, H. (2018). Generative \\nadversarial network for abstractive text summarization. Proceedings of \\nthe AAAI Conference on Artificial Intelligence, 32(1). \\n[31] M. S. M., R. M. P., A. R. E., & E. S. G. S. R. (2020). Text \\nsummarization using text frequency ranking sentence prediction. In \\n2020 4th International Conference on Computer, Communication and \\nSignal Processing (ICCCSP) (pp. 1–6).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 0}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\nLANG MEI, Huawei Cloud BU, China\\nSIYU MO, Huawei Cloud BU, China\\nZHIHAN YANG, Huawei Cloud BU, China\\nCHONG CHEN∗, Huawei Cloud BU, China\\nMultimodal Retrieval-Augmented Generation (MRAG) represents a significant advancement in enhancing the\\ncapabilities of large language models (LLMs) by integrating multimodal data, such as text, images, and videos,\\ninto the retrieval and generation processes. Traditional Retrieval-Augmented Generation (RAG) systems, which\\nprimarily rely on textual data, have shown promise in reducing hallucinations and improving response accuracy\\nby dynamically incorporating external knowledge. However, these systems are limited by their reliance on\\ntext-only modalities, which restricts their ability to leverage the rich, contextual information available in\\nmultimodal data. MRAG addresses this limitation by extending the RAG framework to include multimodal\\nretrieval and generation, thereby enabling more comprehensive and contextually relevant responses. In MRAG,\\nthe retrieval step involves locating and integrating relevant knowledge from diverse modalities, while the\\ngeneration step utilizes multimodal large language models (MLLMs) to produce answers that incorporate\\ninformation from multiple data types. This approach not only enhances the quality of question-answering\\nsystems but also significantly reduces the incidence of hallucinations by grounding responses in factual,\\nmultimodal knowledge. Recent research has demonstrated that MRAG outperforms traditional text-modal\\nRAG, particularly in scenarios where visual and textual information are both critical for understanding and\\nresponding to queries. This survey systematically reviews the current state of MRAG research, focusing\\non four key aspects: essential components and technologies, datasets, evaluation methods and metrics, and\\nexisting limitations. By analyzing these dimensions, we aim to provide a comprehensive understanding of\\nhow MRAG can be effectively constructed and improved. Additionally, we highlight current challenges and\\npropose future research directions, encouraging further exploration into this promising paradigm. Our work\\nunderscores the potential of MRAG to revolutionize multimodal information retrieval and generation, offering\\na forward-looking perspective on its development and applications.\\nCCS Concepts: • Information systems →Multimedia and multimodal retrieval; Language models; •\\nComputing methodologies →Natural language processing.\\nAdditional Key Words and Phrases: Multimodal Retrieval-Augmented Generation, Multimodal Large Language\\nModel, Multimodal Document Parsing and Indexing, Multimodal Search Planning\\nACM Reference Format:\\nLang Mei, Siyu Mo, Zhihan Yang, and Chong Chen. 2018. A Survey on Multimodal Retrieval-Augmented\\nGeneration. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email\\n(Conference acronym ’XX). ACM, New York, NY, USA, 80 pages. https://doi.org/XXXXXXX.XXXXXXX\\n∗Chong Chen is the corresponding author.\\nAuthors’ Contact Information: Lang Mei, Huawei Cloud BU, Beijing, China, meilang1@huawei.com; Siyu Mo, Huawei Cloud\\nBU, Beijing, China, mosiyu@huawei.com; Zhihan Yang, Huawei Cloud BU, Beijing, China, yangzhihan4@huawei.com;\\nChong Chen, Huawei Cloud BU, Beijing, China, chenchong55@huawei.com.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\\nConference acronym ’XX, Woodstock, NY\\n© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 978-1-4503-XXXX-X/2018/06\\nhttps://doi.org/XXXXXXX.XXXXXXX\\n, Vol. 1, No. 1, Article . Publication date: April 2018.\\narXiv:2504.08748v1  [cs.IR]  26 Mar 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 1}, page_content='2\\nTrovato et al.\\n1\\nIntroduction\\nLarge language models (LLMs), especially the Transformer-based variants, have achieved extraor-\\ndinary success in many language-related tasks. Through pre-training on extensive, high-quality\\ninstruction datasets, LLMs can learn a wide range of language patterns, structures, and factual\\nknowledge. These pre-trained LLMs can generate human-like text with high degrees of fluency and\\ncoherence, and attain strong performance on question-answering tasks, which demonstrates their\\nability to understand and respond to a wide range of queries. However, despite their impressive\\ncapabilities, LLMs still face significant limitations. One of the primary challenges lies in their\\nperformance within specific domains or knowledge-intensive tasks. While these models are often\\ntrained on diverse and extensive datasets, such datasets may not cover the depth of knowledge\\nrequired for highly specialized fields or real-time information updates. This can be particularly\\nproblematic in areas like medicine, law, finance, and other technical fields where precision and\\nup-to-date knowledge are to be prioritized. When handling queries that extend beyond the scope\\nof their training knowledge or require the most current information, LLMs may generate responses\\nthat are speculative or based on patterns they have learned, rather than on verified facts. This\\ncan result in misleading, incorrect, or even entirely fabricated answers, a phenomenon known as\\n\"hallucination\". Minimizing the incidence of hallucinations is important for enhancing the reliability\\nof LLMs in providing accurate and context-relevant information across different domains.\\nRecently, Retrieval-Augmented Generation (RAG) has emerged as an effective solution to mitigate\\nhallucinations, by enhancing the generation capabilities of large language models (LLMs) through\\nthe retrieval of relevant external knowledge. Existing RAG systems typically operate through\\na two-step process: retrieval and generation. In the retrieval step, the goal is to quickly locate\\nrelevant knowledge that is semantically similar to the query from a large-scale document collection.\\nSince the relevant knowledge is often scattered across various parts of documents, each document\\nis pre-processed into multiple chunks. Additional chunks may be created through manual or\\nautomated methods. This process, known as document chunkerization, ensures that fine-grained\\nknowledge can be retrieved more efficiently. In the generation step, the retrieved document chunks\\nare combined with the query to form an augmented input. This augmented input provides the LLM\\nwith context that includes external knowledge. Furthermore, RAG allows LLMs to dynamically\\nintegrate the latest information during the inference stage. This capability ensures that the model’s\\nresponses are not only based on static, pre-trained knowledge but are continuously updated\\nwith current and relevant data. By retrieving and referencing external knowledge, RAG grounds\\nthe generated responses in factual information, thereby significantly reducing the occurrence of\\nhallucinations. However, previous research on RAG systems has primarily focused on knowledge\\nbases built from plain text and LLMs pre-trained on plain text, ignoring other rich sources of\\nknowledge available for query responses in the real world, such as videos and images, referred to\\nas \"multimodal data\".\\nMultimodal data refers to data that comes from multiple sources or formats. This can include text,\\nimages, audio, video, and other types of data. In real-world scenarios, humans naturally interact\\nwith multimodal data, such as browsing web pages that combine text, images, and videos in mixed\\nlayouts. By analyzing images or videos alongside text, the user can better understand the context\\nof the content, and thus improve the satisfaction with the quality of the answers. For example, if\\na passenger inquires about how to store luggage while flying, it will be clearer that the system\\nprovides relevant graphic guides or instructional videos. However, transferring the capabilities\\nof LLMs to the domain of multimodal text and images remains an active area of research, as\\nplain-text LLMs are typically trained only on textual corpora and lack perceptual abilities for visual\\nsignals. How to effectively incorporate multimodal data is important to enhance the capability of\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 2}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n3\\nLLMs. In recent years, the development of multimodal generative models has showcased additional\\napplication possibilities. Apart from textual generative models, multimodal generative models\\nhave been increasingly applied in fields such as human-computer interaction, robot control, image\\nsearch, and speech generation. Similarly, based on multimodal generative models and multimodal\\ndata, how to effectively process Multimodal Retrieval-Augmented Generation (MRAG) is an issue\\nthat needs to be explored.\\nRecently, some research have demonstrated that MRAG with multimodal data outperforms\\ntraditional text-modal RAG. By enhancing the generation capabilities of multimodal large language\\nmodels (MLLMs) through the retrieval of external multimodal knowledge, MRAG system can\\nfurther enhance question answering capabilities and quality, thereby further reducing hallucination\\nissues. The main differences between text-modal RAG and MRAG lie in retrieval and generation. In\\nthe retrieval step, the former only needs to consider retrieving relevant textual knowledge from\\na large document collection, while the latter needs to consider how to retrieve and integrate the\\nrelevant knowledge under different modalities, as well as the relationships between knowledge in\\ndifferent modalities. In the generation step, the former only needs to consider the input text query\\nand relevant textual knowledge, and output a text answer based on the LLM. The latter, however,\\nneeds to consider how to utilize the input query from different modalities and multimodal retrieval\\nknowledge, and output an answer that includes information from different modalities based on the\\nMLLM.\\nConsidering the immense potential of MRAG in this field, this survey aims to systematically\\nreview and analyze the current state and main challenges of MRAG. We discuss existing research\\nfrom several key perspectives: 1) What important components and technologies are involved in\\nMRAG? 2) What types of datasets can be used for the evaluation of MRAG? 3) What methods and\\nmetrics are used to evaluate MRAG? 4) What limitations exist in the different aspects of MRAG? We\\nexplore the main challenges faced by MRAG, and hope to provide clearer guidelines for their future\\ndevelopment. In summary, the main contributions of this paper are as follows:\\n• Comprehensive and Timely Survey: We conducted an extensive survey on the emerging\\nparadigm of multimodal Retrieval-Augmented Generation, systematically reviewing the current\\nstate of research and development in this field.\\n• Systematic Analysis from Four Key Perspectives: Our survey is organized around four key\\naspects: essential components and technologies, datasets, evaluation methods and metrics, and\\nlimitations. This structured approach allows for a detailed understanding of how MRAG can be\\nefficiently constructed, its reliability issues, and how it can be further improved.\\n• Current Challenges and Future Research Directions: We discuss the existing challenges of\\nMRAG, highlight potential research opportunities and directions, and provide a forward-looking\\nperspective on the future development of this paradigm, encouraging researchers to delve deeper\\ninto this exciting field.\\nWe have provided an overall introduction to this survey paper. The section 2 presents a compre-\\nhensive overview of multimodal retrieval-augmented generation, covering multiple developmental\\nstages. The section 3 delves into the technical details of multimodal retrieval-augmented generation,\\nfocusing on key components such as multimodal retrieval, multimodal generation, etc. In section 4,\\nwe discuss how to comprehensively evaluate multimodal retrieval-augmented generation systems\\nusing datasets, including specialized assessments for different competency areas. The section 5\\nintroduces relevant metrics for evaluating multimodal retrieval-augmented generation systems.\\nIn section 6, we outline the current technical challenges associated with multimodal retrieval-\\naugmented generation. In section 7, based on previous investigation of MRAG, we summarize\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 3}, page_content='4\\nTrovato et al.\\nfuture work in this field, and provide some suggestions. Finally, we give the conclusion of the paper\\nin section 8.\\n2\\nOverview of MRAG\\nMultimodal Retrieval-Augmented Generation (MRAG) represents a significant evolution of the\\ntraditional Retrieval-Augmented Generation (RAG) framework, building upon its foundational\\nstructure while extending its capabilities to process diverse data modalities. While RAG is limited\\nto processing plain text, MRAG integrates multimodal data, including images, audio, video, and\\ntext, enabling it to address more complex and diverse real-world applications where information\\nspans multiple modalities.\\nIn the early stages of MRAG development, researchers converted multimodal data into unified\\ntextual representations. This approach allowed for a seamless transition from RAG to MRAG\\nby leveraging existing text-based retrieval and generation mechanisms. Although this strategy\\nsimplified multimodal data integration and improved the end-to-end user experience, it introduced\\nsignificant limitations. For instance, the conversion process often resulted in the loss of modality-\\nspecific information, such as visual details in images or tonal nuances in audio, restricting the\\nsystem’s ability to fully exploit the potential of multimodal inputs. Subsequent research has focused\\non addressing these limitations by developing more advanced methods to optimize MRAG systems.\\nThese advancements have substantially enhanced MRAG’s performance and versatility, achieving\\nstate-of-the-art results across various multimodal tasks. This paper categorizes the evolution of\\nMRAG into three distinct stages:\\n2.1\\nMRAG1.0\\nThe initial stage of the MRAG framework, commonly termed \"pseudo-MRAG\", emerged as a\\nstraightforward extension of the highly successful RAG paradigm. This stage was rapidly adopted\\ndue to its adherence to RAG’s core principles, with modifications to support multimodal data. As\\nillustrated in Figure 1, the MRAG1.0 architecture consists of three key components: Document\\nParsing and Indexing, Retrieval, and Generation.\\n• Document Parsing and Indexing: This component is responsible for processing multimodal\\ndocuments in formats such as Word, Excel, PDF, and HTML. It extracts textual content using\\nOptical Character Recognition (OCR) or format-specific parsing techniques. A document layout\\ndetection model is then utilized to segment the document into structured elements, including\\ntitles, paragraphs, images, videos, tables, and footers. For textual content, a chunking strategy is\\napplied to segment or group semantically coherent passages. For multimodal data, specialized\\nmodels are used to generate captions describing images, videos, and other non-textual elements.\\nThese chunks and captions are encoded into vector representations using an embedding model\\nand stored in a vector database. The choice of embedding model is crucial, as it significantly\\nimpacts the performance and effectiveness of downstream retrieval tasks.\\n• Retrieval: This component processes user queries by encoding them into vector representations\\nusing the same embedding model applied during indexing. The query vectors are then utilized\\nto retrieve the top-𝑘most relevant chunks and captions from the vector database, typically\\nemploying cosine similarity as the relevance metric. Duplicate or overlapping information from\\nchunks and captions is merged to create a consolidated set of external knowledge, which is\\nsubsequently integrated into the prompt for the generation phase. This ensures the system\\nretrieves contextually relevant information to deliver accurate and informed responses.\\n• Generation: In the Generation phase, the MRAG system synthesizes the user’s query and\\nretrieved documents into a coherent prompt. A large language model (LLM) generates a response\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 4}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n5\\nDocuments Parsing\\n(Extractive-Based, Plain Text)\\nTable \\nCaptions\\nGeneration\\nRetrieval\\nIndexing\\nDocuments\\n……\\nText\\nText Parsing  \\nModel\\nImage Caption \\nModel\\nMultimodal Data\\nImage\\nTable\\n……\\nTable Parsing \\nModel\\nText Chunks\\nImage \\nCaptions\\nText Embedding \\nModel\\nText \\nVector DB\\nRelevant Text \\nChunks\\nRelevant Image \\nCaptions\\nPrompt\\nLLMs\\nQuery + History\\n（Text Only）\\nOCR-Based \\nModel\\nText Embedding \\nModel\\nAnswer\\n（Text Only）\\nFig. 1. The architecture of MRAG1.0, often termed \"pseudo-MRAG\", closely resembles traditional RAG,\\nconsisting of three modules: Document Parsing and Indexing, Retrieval, and Generation. While the overall\\nprocess remains largely unchanged, the key distinction lies in the Document Parsing stage. In this stage,\\nspecialized models are employed to convert diverse modal data into modality-specific captions. These captions\\nare then stored alongside textual data for utilization in subsequent stages.\\nby integrating its parametric knowledge with the retrieved external information. This approach\\nenhances response accuracy and timeliness, particularly in domain-specific contexts, while\\nreducing the risk of hallucinations common in LLM outputs. In multi-turn dialogues, the system\\nincorporates conversational history into the prompt, enabling contextually aware and seamless\\ninteractions.\\nDespite its initial success, MRAG1.0 exhibited several notable limitations that constrained its\\neffectiveness:\\n• Cumbersome Document Parsing: Converting multimodal data into textual captions introduced\\nsubstantial complexity to the system. This necessitated distinct models for processing different\\ndata modalities, increasing both computational overhead and system intricacy. Additionally,\\nthe conversion process frequently often to multimodal information loss. For instance, image\\ncaptions typically provided only coarse-grained descriptions, failing to capture fine-grained\\ndetails essential for accurate retrieval and generation.\\n• Bottleneck of Retrieval: While text vector retrieval technology is well-established, MRAG1.0\\nencountered challenges in achieving high recall accuracy. Similar to traditional RAG, the chunking\\nstrategy for text segmentation often fragmented keywords, making some content irretrievable.\\nAdditionally, transforming multimodal data into text, while enabling non-textual data retrieval,\\nintroduced additional information loss. These issues collectively created a bottleneck, limiting\\nthe system’s ability to retrieve comprehensive and accurate information.\\n• Challenges in Generation: Unlike traditional RAG, MRAG1.0 required processing not only\\ntext chunks but also image captions and other multimodal data. Effectively organizing these\\ndiverse elements into coherent prompts while minimizing redundancy and preserving relevant\\ninformation posed a significant challenge. Additionally, the \"Garbage In, Garbage Out\" (GIGO)\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 5}, page_content='6\\nTrovato et al.\\nIndexing\\nDocuments Parsing\\n(Extractive-Based, Multimodal)\\nGeneration\\nRetrieval\\nDocuments\\n……\\nText\\nText Parsing  \\nModel\\nMultimodal Data\\nImage\\nTable\\n……\\nText Chunks\\nText Embedding \\nModel\\nText Vector \\nDB\\nText Only \\nPrompt\\nLMMs\\nQuery + History\\n(Text with Multimodal Data)\\nOCR-Based \\nModel\\nText Embedding \\nModel\\nAnswer\\n(Text Only)\\nMultimodal  \\nEmbedding Model\\nMLLMs\\nMultimoda \\nData Captions\\nMultimodal \\nEmbedding Model\\nRelevant \\nPlain Text  Data\\n……\\nText Chunks\\nImage \\nCaptions\\nRelevant \\nMultimodal Data\\n……\\nTable\\nImage\\nMultimodal \\nVector DB\\nText Only？\\nMultimodal \\nPrompt\\nY\\nN\\nMLLMs\\nFig. 2. The architecture of MRAG2.0 retains multimodal data through document parsing and indexing, while\\nintroducing multimodal retrieval and MLLMs for answer generation, truly entering the multimodal era.\\nprinciple highlighted the sensitivity of LLMs to input quality. Information loss during parsing\\nand retrieval increased the risk of incorporating irrelevant data, compromising the robustness\\nand reliability of the generated responses.\\nThe limitations of MRAG1.0 created a performance ceiling, highlighting the need for more advanced\\ntechnological solutions. The system’s reliance on text-based representations for multimodal data,\\nalong with inherent challenges in retrieval and generation, revealed critical gaps in multimodal\\nunderstanding, retrieval efficiency, and generation robustness. Subsequent iterations of MRAG\\nmust address these issues by adopting more sophisticated models, enhancing information retention\\nduring parsing, and improving the integration of multimodal data into retrieval and generation\\nprocesses.\\n2.2\\nMRAG2.0\\nWith the rapid evolution of multimodal technologies, MRAG has transitioned into a \"true mul-\\ntimodal\" era, termed MRAG2.0. Unlike its predecessor MRAG1.0, MRAG2.0 not only supports\\nuser queries with multimodal inputs but also preserves the original multimodal data within the\\nknowledge base. By leveraging the capabilities of MLLMs, the generation module can now process\\nmultimodal data directly, minimizing information loss during data conversion. As illustrated in\\nFigure 2, the MRAG2.0 architecture incorporates several key optimizations:\\n• MLLMs Captions: The representational capabilities of MLLMs have significantly advanced, es-\\npecially in captioning tasks. MRAG2.0 leverages a single, unified MLLM—or multiple MLLMs—to\\nextract captions from multimodal documents. This approach replaces the conventional paradigm\\nof using separate models for different modalities, simplifying the document parsing module and\\nreducing its complexity.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 6}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n7\\n• Multimodal Retrieval: MRAG2.0 enhances its retrieval module to support multimodal user\\ninputs by preserving original multimodal data and enabling cross-modal retrieval. This allows text-\\nbased queries to directly retrieve relevant multimodal data, combining caption-based recall with\\ncross-modal search capabilities. The dual retrieval approach enriches data sources for downstream\\ntasks while minimizing data loss, improving accuracy and robustness for downstream tasks.\\n• Multimodal Generation: To fully leverage original multimodal data, the generation module\\nin MRAG2.0 has been enhanced by integrating MLLMs, enabling the synthesis of user queries\\nand retrieval results into a coherent prompt. When retrieval results are accurate and the input\\ncomprises original multimodal data, the generation module mitigates information loss typically\\nassociated with modality conversion. This enhancement has significantly improved the accuracy\\nof question-answering (QA) tasks, especially in scenarios involving interrelated multimodal data.\\nDespite these advancements, MRAG2.0 encounters several emerging challenges: 1) Integrating\\nmultimodal data inputs may reduce the accuracy of traditional textual query descriptions. Further-\\nmore, current multimodal retrieval capabilities remain inferior to text-based retrieval, potentially\\nlimiting the overall accuracy of the retrieval module. 2) The diversity of data formats presents new\\nchallenges for the generation module. Efficiently organizing these diverse data forms and clearly\\ndefining inputs for generation are critical areas requiring further exploration and prioritization.\\n2.3\\nMRAG3.0\\nAs illustrated in Figure 3, the MRAG3.0 system represents a significant evolution from its predeces-\\nsors, introducing structural and functional innovations that enhance its capabilities across multiple\\ndimensions. This new paradigm shift is characterized by three key advancements: 1) Enhanced\\nDocument Parsing: A novel approach retains document page screenshots during parsing, minimiz-\\ning information loss in database storage. 2) True End-to-End Multimodality: While earlier versions\\nemphasized multimodal capabilities in knowledge base construction and system input, MRAG3.0\\nintroduces multimodal output capabilities, completing the end-to-end multimodal framework. 3)\\nScenario Expansion: Moving beyond traditional focus on understanding capabilities—primarily ap-\\nplied in VQA (Visual Question Answering) scenarios reliant on knowledge bases, the new paradigm\\nintegrates understanding and generation capabilities through module adjustments and additions.\\nThis unification significantly broadens the system’s applicability. In the following sections, we will\\ndetail the scenarios supported by MRAG3.0 and the specific module modifications enabling these\\nadvanced capabilities.\\n2.3.1\\nScenario for MRAG.\\n• Retrieval-Augmented Scenario: This scenario addresses cases where LLMs or MLLMs alone\\ncannot adequately answer user queries. MRAG3.0 retrieves relevant content from external\\nknowledge bases to provide accurate answers, leveraging its enhanced retrieval capabilities.\\n• VQA Scenario: This scenario serves as a critical test for evaluating the fundamental capabilities\\nof MLLMs, which generate responses directly from user inputs containing text and multimodal\\nqueries without retrieval. The new MRAG paradigm introduces a search planning module,\\nenabling dynamic routing and retrieval to minimize unnecessary searches and the inclusion of\\nirrelevant information.\\n• Multimodal Generation Scenario: This primarily pertains to multimodal generation tasks,\\nsuch as text-to-image or text-to-video generation. While the original MRAG framework primarily\\naddressed understanding tasks, the new MRAG paradigm extends its capabilities by modifying\\nmultiple generation modules, unifying the solutions for both understanding and generation\\ntasks within a single framework. Following integration, the generation scenarios are further\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 7}, page_content='8\\nTrovato et al.\\nIndexing\\nDocuments Parsing\\nRepresentation-based Module\\nGeneration\\nRetrieval\\nMultimodal\\nSearch Planning\\nDocuments\\n……\\nExtractive-Based Module\\n（Same with MRAG2.0）\\nDocuments \\nScreenShots \\nEmbedding Model\\nText Embedding \\nModel\\nMultiModal \\nEmbedding Model\\nText \\nVector DB\\nMultimodal \\nVector DB\\nRelevant \\nPlain Text  Data\\n……\\nText Chunks\\nImage Captions\\nRelevant \\nMultimodal Data\\n……\\nTable\\nImage\\nRelevant \\nDocuments \\nScreenshots\\nText Embedding \\nModel\\nMultiModal \\nEmbedding Model\\nDocuments \\nScreenShoots \\nEmbedding Model\\nRetrieval \\nClassification\\nQuery Reformulation\\nNeed Search？\\nQuery + History\\n（Text with Multimodal data）\\nNew Query\\nText Only?\\nText Only?\\nText Only Prompt\\n(No search)\\nMultimodal Prompt\\n(No search)\\nText Only Prompt\\n(With search)\\nMultimodal Prompt\\n(With search)\\nLMMs\\nMLLMs\\nDocuments \\nScreenShots \\nVector DB\\nY\\nN\\nY\\nN\\nMultimodal Answer\\nDocuments \\nScreenshots\\nText Only?\\nAugmented \\nMultimodal Output\\nPosition identification \\nCandidate set Retrieval\\nMatching and Insertion\\nY\\nN\\nN\\nY\\nFig. 3. MRAG3.0 architecture integrates document screenshots during the document parsing and indexing\\nstages to minimize information loss. At the input stage, it incorporates a Multimodal Search Planning module,\\nunifying Visual Question Answering (VQA) and Retrieval-Augmented Generation (RAG) tasks while refining\\nuser query precision. At the output stage, the Multimodal Retrieval-Augmented Composition module enhances\\nanswer generation by transforming plain text into multimodal formats, thereby enriching information delivery.\\nenhanced by Retrieval-Augmentation (RA), which significantly improves the overall performance\\nof generation tasks (see Figure 4).\\n• Fusion Multimodal Output Scenario: This scenario is distinct from those previously men-\\ntioned but represents a significant aspect of the new paradigm, warranting separate discussion. In\\ntraditional settings, the final output is typically a plain text response. However, the new paradigm\\nenhances the generation module to produce outputs that integrate multiple modalities within a\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 8}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n9\\nPlease create a pixel art illustration of Huawei \\nStream Back Slope Village with whimsical fairy tale \\nelements?\\nAccording to the user‘s description, the generated \\npictures are as follows.\\nTraditional Multimodal Generation\\nIt’s not Huawei Stream Back Slope Village\\nPlease create a pixel art illustration of Huawei \\nStream Back Slope Village with whimsical fairy tale \\nelements?\\nI have found these pictures related to Huawei \\nStream Back Slope Village.\\nMultimodal Generation with MRAG\\nGood!\\nThe final generated pictures are as follows.\\nFig. 4. The user aims to generate images depicting \"Huawei Stream Back Slope Village.\" Due to the location’s\\nobscurity and the model’s limited knowledge, it may produce inaccurate representations, such as images of\\nhouses by a stream. By integrating retrieval-augmented capabilities, the model can access relevant information\\nbeforehand, enabling the generation of precise and contextually accurate images.\\nsingle response (e.g., combining text, images, or videos). This can be further categorized into\\nthree sub-scenarios (see Figure 5).\\n– Multimodal Data is Answer: The query can be answered directly through multimodal data\\nwithout any text, as the adage \"a picture is worth a thousand words\" suggests.\\n– Multimodal Data Enhances Accuracy: The integration of multimodal data enhances the\\naccuracy of responses, particularly in instructional contexts such as \"How to register for\\na Gmail account.\". By generating answers that interweave text and image, users can more\\neffectively comprehend and follow the required operations.\\n– Multimodal Data Enhances Richness: While multimodal data is not essential, its inclusion\\ncan significantly enhance user experience. For instance, when responding to a query such\\nas \"Please introduce the Eiffel Tower.\", supplementing the textual explanation with relevant\\nimages or a brief video can offer users a more engaging and visually enriched experience.\\n2.3.2\\nModified Modules.\\n• Documents Parsing and Indexing: To minimize information loss and enhance the accuracy\\nof document retrieval, the document parsing and indexing module has been upgraded with\\ninnovative technologies. This new approach preserves document screenshots during parsing,\\naddressing the information loss issues inherent in previous methods. By utilizing fine-tuned\\nMLLMs, the system vectorizes and indexes these document screenshots, enabling efficient\\nretrieval of relevant document screenshots based on user queries. This optimization not only\\nimproves the reliability of the knowledge base but also paves the way for advanced multimodal\\nretrieval capabilities.\\n• Generation: Previously, the generation module relied exclusively on large models with under-\\nstanding capabilities. The new paradigm integrates large models with generation capabilities,\\nunifying reasoning and generation scenarios at the system architecture level. Additionally, by\\nincorporating a multimodal output enhancement submodule, it facilitates a shift from text-based\\nanswers to mixed multimodal answers. The implementation methods can be categorized into\\ntwo types:\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 9}, page_content=\"10\\nTrovato et al.\\nYou can learn the route of Columbus's voyage through \\nthe video below.\\n1: Place the eggs and a small amount of salt into a container, \\nand mix well.\\n2: Add warm water and continue to stir, then strain it.\\n3: Steam for 10 minutes, drizzle with sesame oil. It's done!\\nWatch the video below for more detailed instructions.\\nPlease introduce the Egyptian pyramids.\\nThe Egyptian pyramids are among the most iconic and \\nfascinating structures in history.\\nBuilt primarily as tombs for the pharaohs of ancient Egypt, \\nthey served as grand final resting places to ensure their \\nsafe passage into the afterlife. The pyramids are mainly \\nlocated in Giza, near Cairo. The Great Pyramid of Khufu \\nis the largest and most well-known, standing at an \\nastonishing height.\\nConstructed with precision, these pyramids were built \\nusing massive limestone blocks. Thousands of workers, \\nincluding skilled artisans, were involved in their \\nconstruction, which took many years. The architecture is \\nremarkable, with the sides of the pyramids aligned almost \\nperfectly with the four cardinal directions.\\nWhat was the route of Columbus's voyage?\\nScenario II\\nScenario I\\nMultimodal Data is Answer\\nMultimodal Data Enhances Accuracy\\nScenario III\\nMultimodal Data Enhances Richness\\nWhat do the Alaskan Malamute and the \\nSiberian Husky look like, respectively?\\nThe left is the Alaskan Malamute, and the right is the \\nSiberian Husky .\\nHow to make steamed egg custard?\\nFig. 5. Multimodal output in QA scenarios can be categorized into three distinct types. In sub-scenario I, the\\nuser’s query can be fully addressed using only images or videos, without requiring supplementary textual\\ninformation. Sub-scenario II involves a step-by-step explanation that combines text and images to ensure\\nclarity and precision; omitting the images may lead to user confusion at specific steps. In sub-scenario III,\\nsupplementary images enrich the information conveyed in the answer, but their removal does not compromise\\nthe answer’s accuracy.\\n– Native MLLM-Based Output: In this task, the generation of multimodal data is entirely model-\\ndriven, eliminating the need for external data sources to supplement the model responses.\\nThe most straightforward approach involves using a unified MLLM to produce the desired\\nmultimodal output in a single step, ensuring seamless integration of diverse data types, such\\nas text, images, or audio, within a cohesive framework.\\n– Augmented Multimodal Output: This method utilizes pre-existing multimodal data to\\nenhance textual responses. After generating the text, the system executes three sequential\\nsubtasks to create the final multimodal output: 1) Position Identification: The system deter-\\nmines optimal insertion points within the text where multimodal elements (e.g., images, videos,\\ngraphs) can be integrated to complement or clarify the content. This step ensures that the mul-\\ntimodal data aligns contextually with the text. 2) Candidate Set Retrieval: Relevant multimodal\\ndata is retrieved from external sources, such as the web or a knowledge base, by querying and\\nfiltering potential candidates that best match the text’s context and intent. 3) Matching and\\nInsertion: The system selects the most appropriate multimodal element from the retrieved\\ncandidate set based on relevance, quality, and coherence. The chosen data is then seamlessly\\nintegrated into the identified positions, producing a cohesive and enriched multimodal answer.\\n2.3.3\\nNew Modules.\\n• Multimodal Search Planning: This module tackles key decision-making challenges in MRAG\\nsystems by focusing on two core tasks: retrieval classification and query reformulation. Given a\\nmultimodal query Q = (𝑞, 𝑣), where 𝑞is the textual component and 𝑣is the visual component,\\nthe module is designed to optimize information acquisition. Specifically, retrieval classification\\ninvolves determining the relevance and category of the multimodal query to guide the search\\ntoward the most appropriate data sources. Query reformulation, on the other hand, refines the\\n, Vol. 1, No. 1, Article . Publication date: April 2018.\"),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 10}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n11\\nquery by integrating textual and visual cues to improve retrieval accuracy and comprehensive-\\nness. By combining these tasks, the module strengthens the system’s ability to handle complex\\nmultimodal inputs, ensuring more effective and contextually relevant information retrieval.\\n– Retrieval Classification: This task determines the optimal retrieval strategy 𝑎∗from the\\naction space A = {𝑎𝑛𝑜𝑛𝑒,𝑎𝑡𝑒𝑥𝑡,𝑎𝑖𝑚𝑎𝑔𝑒} based on the current query and optionally the retrieved\\nhistorical documents. The decision process is formulated as:\\n𝑎∗= 𝑎𝑟𝑔𝑚𝑎𝑥\\n𝑎∈A\\nF𝑅𝐶(𝑎| Q, D)\\n(1)\\nwhere the retrieval control module F𝑅𝐶evaluates the utility of retrieval actions by considering\\nquery characteristics, the MLLM’s inherent capabilities, and, when available, the retrieved\\ndocuments D from previous iterations. For example, in multi-hop scenarios, after retrieving\\nvisual information in the initial round, the module may leverage accumulated knowledge\\nto determine subsequent actions, such as text-based retrieval or direct generation. Existing\\nMRAG frameworks typically follow a rigid pipeline with predetermined retrieval actions, which\\nposes significant limitations. Recent studies [125] have shown that compulsive image-to-image\\nretrieval can be counterproductive, as retrieved images may introduce misleading information,\\ndegrading MLLM performance. This highlights the necessity of dynamic retrieval strategy\\nselection.\\n– Query Reformulation: In scenarios where external information is required (𝑎∗≠𝑎𝑛𝑜𝑛𝑒)\\nfor queries, the task of query reformulation involves generating an enhanced query Q∗by\\nintegrating visual information and, when applicable, retrieved documents from previous\\niterations. This process can be formulated as:\\nQ∗= F𝑄𝑅(Q, D)\\n(2)\\nwhere F𝑄𝑅denotes the query enhancement function, which utilizes visual cues and, if available,\\nhistorical retrieval results to refine the query’s precision. This task is particularly critical in real-\\nworld human interactions, where queries often rely heavily on visual context and frequently\\nemploy anaphoric references. The inherent challenges of visual incompleteness and textual\\nambiguity pose significant obstacles to retrieving relevant information through straightforward\\nsearch mechanisms. For complex queries that necessitate multi-hop reasoning, the enhanced\\nquery Q∗may be further decomposed into a series of atomic sub-queries {𝑞∗\\n1, ...,𝑞∗\\n𝑛}. Each\\nsub-query is meticulously formulated by considering both textual and visual contexts, as well\\nas the accumulated knowledge from previous iterations, when relevant. This decomposition\\nallows for a more granular and precise retrieval process, addressing the nuanced dependencies\\nand ambiguities present in real-world queries.\\nThis dual-task approach optimizes information acquisition by minimizing unnecessary retrievals\\nwhile maximizing the relevance of retrieved content. The structured planning framework signifi-\\ncantly enhances the MRAG system’s ability to gather comprehensive and accurate information,\\nensuring computational efficiency.\\n3\\nComponents & Technologies of MRAG\\nIn this section, we will sequentially introduce the details of the five key technical components\\nof MRAG: Multimodal Document Parsing and Indexing (section3.1), Multimodal Search Planning\\n(section 3.2), Multimodal Retrieval (section 3.3), Multimodal Generation (section 3.4)\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 11}, page_content='12\\nTrovato et al.\\n3.1\\nMultimodal Document Parsing and Indexing\\nMRAG systems significantly enhance the reliability and quality of generated answers, by integrating\\ntarget multimodal knowledge from external multimodal knowledge bases. Target multimodal knowl-\\nedge can be derived from various granularity in knowledge bases, including localized segments\\nwithin a single document, cross-segment references within a document, or even cross-document\\nknowledge collections. Thus, how to effectively parse, index, and organize the multimodal docu-\\nments in external knowledge bases, can largely affect the model’s utilization of target multimodal\\nknowledge, thereby determining end-to-end performance. In this section, we first classify docu-\\nments in multimodal knowledge bases according to their structure, then we provide a detailed\\nintroduction to the parsing methods and the evolution of these methods for different types of\\nmultimodal documents. Specifically, multimodal documents can be categorized into the following\\nthree types:\\n• Unstructured Multimodal Data: refers to various multimodal information that does not have\\na specific format or schema, such as text, images, videos, and audio. Among the unstructured\\ndata, documents with images are widely studied in MRAG. For example, SlideVQA [347] is a\\ntypical dataset for visual question-answering task, where all documents are input as images.\\n• Semi-structured Multimodal Data: mainly refers to multimodal information that lacks the\\nrigid schema of traditional relational databases but retains some organizational features, such\\nas PDFs, HTML, XML, and JSON. In such documents, rule-based methods can directly extract\\nstructural characteristics. For instance, in HTML, the title can be identified using the <title>\\ntag. A common challenge in processing these documents is that their inherent structure, easily\\ninterpretable by humans, is often lost during parsing, resulting in information loss.\\n• Structured Multimodal Data: refers to multimodal information arranged in a predefined format,\\ntypically following a fixed schema, such as relational databases and knowledge graphs. The\\nprimary challenge in handling such data is formulating an accurate structured query language\\ncorresponding to natural language.\\nIn MRAG scenarios, the primary focus is on processing and leveraging unstructured and semi-\\nstructured documents. Document parsing methods in MRAG are broadly categorized into two\\napproaches: extraction-based and representation-based. Each approach has distinct advantages\\nand limitations, with the choice depending on task-specific requirements such as scalability or\\ncomputational efficiency. Extraction-based methods involve a two-step process: first, multimodal\\ninformation is extracted from documents, and second, the extracted data is parsed and structured\\nfor storage and downstream use. In contrast, representation-based methods do not require explicit\\nextraction of multimodal information. Instead, these methods focus on storing document content\\nholistically, often employing representation techniques for document segments. This approach\\nenables a more comprehensive processing of document content.\\n3.1.1\\nExtraction-based. Early document parsing solutions were entirely extractive. They evolved\\ngradually from plain text extraction to multimodal data extraction, depending on the type of content\\nbeing extracted. This subsection will present the process in this sequence.\\n• Plain Text Extraction. In this phase, only textual information from all modal data in the docu-\\nment was extracted. For example, for tables and images, only their textual content was captured.\\nSemi-structured documents, such as PDFs, XML, and HTML, can be parsed directly according to\\ntheir structural rules. Numerous open-source tools support such capabilities, including pymupdf\\n[3] and pdfminer [2] for PDF parsing, and jsoup [1] for HTML extraction. While this approach\\nenables simple and efficient document parsing, it has limitations: it cannot extract multimodal\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 12}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n13\\ninformation (e.g., text within images) and struggles with complex document formats. Additionally,\\nthe parsed results often suffer from significant loss of document structure information.\\nTo enhance document parsing accuracy and address the limitations of rule-based methods\\nin handling complex real-world documents, such as in Visual Document Understanding (VDU)\\ntasks, OCR (Optical Character Recognition)-based approaches have become widely adopted. The\\ntraditional OCR-based document parsing pipeline consists of three main stages: text detection,\\ntext recognition, and text parsing. Text Detection involves locating and extracting text regions\\nfrom documents. Early methods primarily relied on Connected Component Analysis (CCA) [26,\\n363] and Edge Detection algorithms [261]. For more complex layouts, techniques such as Contour\\nAnalysis and Stroke Width Transform (SWT) [73, 344] were employed to handle multi-oriented\\ntext. With advancements in machine learning, hybrid models combining regression-based object\\ndetection frameworks (e.g., Faster R-CNN) with semantic segmentation networks were developed\\nto address arbitrary-shaped text instances [15, 206, 481]. This stage outputs precise bounding\\nboxes or polygon coordinates around text elements, serving as the basis for subsequent processing.\\nText Recognition converts visual text representations into machine-readable text, playing a critical\\nrole in digitizing unstructured data. Its evolution can be divided into three phases: The classical\\nphase relied on handcrafted features [290] and statistical models [22], but faced challenges\\nwith fragmented processing and limited robustness. The deep learning phase introduced CNNs\\n(Convolutional Neural Networks) for feature extraction and CTC (Connectionist Temporal\\nClassification)/RNNs (Recurrent Neural Networks) for sequence modeling, with breakthroughs\\nlike CRNN enabling unified pipelines and improved accuracy on irregular text. The modern phase\\nleverages transformer architectures [191], achieving global context awareness and robustness\\nto arbitrary-shaped text. Text Parsing reconstructs semantic relationships through three key\\nsteps: Layout Analysis segments documents into logical components using rule-based heuristics\\nand graph models based on spatial and typographic cues. Syntactic Parsing extracts structured\\ndata from unstructured text using regular expressions and finite-state machines. Post-processing\\ncorrects recognition errors through contextual algorithms like language model interpolation (e.g.,\\nn-gram models and dictionary lookups). This comprehensive process ensures accurate semantic\\nreconstruction from complex documents.\\nHowever, the OCR-dependent approach has critical problems: It is not conducive to paralleliza-\\ntion and occupies a large amount of computing resources, besides, errors in the pipeline will\\npropagate downward through the system, affecting the overall performance. In recent years, with\\nthe development of Transformer architectures, the aforementioned issues have been effectively\\naddressed. It enhances global context modeling through the self-attention mechanism, signifi-\\ncantly improves processing efficiency by leveraging parallel computing, and directly maps images\\nto structured text in an end-to-end training mode. This effectively eliminates the cumulative\\nerror issues associated with the multi-stage cascading of traditional OCR systems. LayoutLM\\n[412] uses the BERT architecture as the backbone and adds two new input embeddings: a 2-D\\nposition embedding and an image embedding to jointly model interactions between text and\\nlayout information across scanned document images. LayoutLMv2 [413] and LayoutLMv3 [133]\\nfurther propose a new single multimodal framework to model the interaction among text, layout,\\nand image. DocFormer [12] based on the multimodal transformer architecture proposes a novel\\nmultimodal attention layer to fuse text, vision, and spatial features in a document, thereby\\nachieving end-to-end document parsing.\\n• Multimodal Extraction. In this phase, the original format of multimodal data is preserved\\nduring extraction, allowing downstream tasks to autonomously determine subsequent operations.\\nFor semi-structured documents, extraction can be performed similarly using rule-based methods.\\nRelevant multimodal data is identified through specific tags, such as extracting images from\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 13}, page_content='14\\nTrovato et al.\\nHTML files using the \"<img>\" tag. However, this approach faces similar challenges to plain text\\nextraction.\\nThe pipeline for multimodal document parsing based on OCR consists of three steps: page\\nsegmentation, text recognition, and text parsing. Page segmentation, similar to text detection in\\nplain text extraction, locates and extracts target regions while annotating them with semantic\\nlabels (e.g., title, table, footnote). This subtask of semantic segmentation commonly employs CNN-\\nbased methods, categorized into region-based, FCN-based, and weakly supervised approaches\\n[111]. Text recognition, similar to plain text extraction, focuses on parsing text data such as\\ntitles and page text. Text parsing involves layout analysis and other operations, processing\\nmultimodal data according to downstream task requirements. In the era of LLMs, multimodal\\ndata is often converted into text for utilization, as seen in models like TableNet [286] for tables and\\nUniChart [265] for charts. This necessitates distinct models for extracting captions from different\\nmodalities. With the advancement of MLLMs, there is a trend toward unifying these models\\ninto a single MLLM framework, leveraging their robust representation capabilities [173, 227].\\nFurther developments in MLLMs enable the direct retention and input of original multimodal\\ndata during generation [312, 437, 474].\\n3.1.2\\nRepresentation-based. Although extractive-based methods have been widely adopted, they\\nsuffer from several inherent limitations: (1) The parsing process is time-consuming, involves\\nmultiple steps, and requires different models for different document types; (2) Critical information,\\nsuch as document structure, may be lost during extraction; and (3) Parsing errors can propagate\\nto downstream tasks. Recent advancements in MLLMs [6, 20, 218] have enabled a novel approach\\nthat directly uses document screenshots as primary data for metadata indexing, addressing these\\nissues [78, 170, 253, 342, 463]. To capture both global and local information, DSE [253] processes the\\ndocument screenshot along with its sub-images through a unified encoding framework. Additionally,\\na late interaction mechanism, inspired by ColBERT [163], has been introduced to improve recall\\nefficiency [78]. However, page-level document splitting may hinder the model’s ability to capture\\nfull context and inter-part relationships. To address this problem, a holistic document representation\\nmethod has been proposed [170], which segments large documents into passages within the token\\nlimit of MLLMs. Empirical studies reveal a performance gap between multimodal and text-only\\nretrieval, highlighting differences in effectiveness when using raw multimodal data versus text or\\ncombined modalities [312, 463]. Consequently, a new paradigm has emerged that leverages OCR\\nfor text indexing, document screenshots for multimodal indexing, and executes textual and visual\\nRAG in parallel. The results from both streams are then fused through modality integration to\\nproduce the final answer [342].\\n3.2\\nMultimodal Search Planning\\nMultimodal search planning refers to the strategies employed by MRAG systems, to effectively\\nretrieve and integrate information from multiple modalities to address complex queries. The\\nplanning can be broadly categorized into two main approaches: fixed planning and adaptive\\nplanning.\\n3.2.1\\nFixed Planning. Early MRAG systems typically adopt fixed planning strategies for handling\\nmultimodal queries, characterized by predetermined processing pipelines that lack flexibility in\\nadapting to diverse query requirements. These approaches can be broadly categorized based on\\ntheir retrieval modality choices:\\n• Planning for Single-modal Retrieval. Early fixed planning strategies usually focus on a single\\nmodality for retrieval, despite the multimodal nature of input queries. These approaches can be\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 14}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n15\\nbroadly classified into text-centric and image-centric paradigms, reflecting initial efforts to adapt\\ntraditional IR query processing techniques [171, 184] to the multimodal domain.\\n– Text-centric planning approaches prioritize textual retrieval by transforming multimodal\\nqueries into text-only formats. For instance, Plug-and-Play [357] employs vision-language\\nmodels to convert the visual component of a query into textual descriptions, followed by text-\\nbased retrieval planning. This strategy simplifies the multimodal problem into a traditional\\ntext-based RAG pipeline, leveraging established multi-stage query processing techniques from\\nconventional IR systems. However, this approach often introduces a semantic gap between\\nthe user’s original intent and the generated textual descriptions. The conversion of visual\\nqueries to text may fail to precisely capture the user’s specific information needs, leading to\\nthe retrieval of irrelevant or noisy documents that diverge from the query’s focus.\\n– Image-centric planning strategies rely solely on image-based retrieval regardless of the query\\ncharacteristics. Systems such as Wiki-LLaVA [24] demonstrate this paradigm by consistently\\ntriggering image retrieval from knowledge bases for multimodal queries. While this approach\\nensures visual information preservation, it presents practical limitations. Recent empirical\\nstudies [125] highlight that compulsive image retrieval can be counterproductive, particularly\\nwhen textual information suffices or when retrieved images introduce misleading visual\\ncontexts, impairing MLLM performance.\\nThe inflexibility of single-modality planning strategies highlights their inherent limitations: they\\ncannot adapt to the diverse information needs of real-world scenarios. For example, while a\\ntext-centric approach may be suitable for queries referencing visual content but focused on\\nfactual information, an image-centric strategy is more effective for queries requiring detailed\\nvisual comparisons.\\n• Planning for Multimodal Retrieval. Recent studies have begun investigating the use of\\nmultimodal information retrieval to enhance the performance of MRAG systems. Unlike single-\\nmodality approaches, these methods integrate both textual and visual knowledge sources, albeit\\nthrough fixed processing pipelines. For instance, MMSearch [147] employs a rigid multimodal\\nplanning pipeline, mandating Google Lens image searches for all image-containing queries.\\nThis is followed by a \"Requery\" phase, where LLMs reformulate the search query using the\\noriginal query, image, and Google Lens results. While this structured approach ensures systematic\\ninformation retrieval, its inflexible design often leads to unnecessary image searches, increasing\\ncomputational overhead when visual information is irrelevant to the query.\\nFixed pipeline approaches, whether single-modality or multimodality, exhibit several critical limi-\\ntations. First, their rigid retrieval strategies struggle to adapt to the diverse nature of real-world\\nqueries, where the optimal retrieval modality depends on specific information needs. Second,\\nmandatory retrieval operations often introduce redundant or irrelevant information, particularly\\nwhen certain knowledge types are unnecessary for addressing the query. Third, these approaches\\nincur significant computational overhead, especially in multimodal pipelines handling large-scale\\nknowledge bases. As highlighted by mR2AG [474], a more fundamental issue is that not all queries\\nrequire external knowledge retrieval. Current MRAG systems frequently perform retrieval indiscrim-\\ninately, resulting in unnecessary computational costs and potential noise in response generation.\\nThese limitations emphasize the need to transition from predetermined pipelines to adaptive plan-\\nning mechanisms that dynamically adjust retrieval strategies based on query characteristics and\\nintermediate results.\\n3.2.2\\nAdaptive Planning. Recent studies have highlighted two key limitations in fixed pipeline\\napproaches [197]: 1) Non-adaptive Retrieval Queries: inflexible retrieval strategies that fail to adjust\\nto evolving contexts or intermediate results; and 2) Overloaded Retrieval Queries: concatenating\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 15}, page_content='16\\nTrovato et al.\\nvisual content descriptions with input questions into a single query, leading to ambiguous retrievals\\nand irrelevant knowledge. To address these issues, OmniSearch [197] introduces a self-adaptive\\nplanning agent for multimodal retrieval, mimicking human problem-solving behavior. Instead of\\nrelying on a fixed pipeline, the system dynamically breaks down complex multimodal questions\\ninto sub-question chains with retrieval actions. At each step, the agent adapts its next action based\\non the problem-solving state and retrieved content, enabling deeper understanding of retrieved\\ninformation and adaptive refinement of retrieval strategies. CogPlanner [440] iteratively refines\\nqueries and selects retrieval strategies, enabling both parallel and sequential modeling approaches.\\n3.3\\nMultimodal Retrieval\\nIn this section, we present a comprehensive overview of the three critical components of multimodal\\nretrieval in the MRAG system: retriever (section 3.3.1), reranker, and refiner. Each component plays\\na distinct yet interconnected role in enhancing the quality and relevance of information retrieval\\nand utilization for LLMs. We summarize the taxonomy of multimodal retrieval research in Figure 6.\\n3.3.1\\nRETRIEVER. The retriever is a core component that sources relevant documents from a large\\nexternal knowledge base using advanced indexing and search algorithms. It retrieves candidate\\ninformation aligned with user queries, aiming to provide a broad yet relevant set of documents to\\nsupport high-quality LLM responses. Its performance is crucial, as it directly influences the quality\\nof the downstream retrieval pipeline. As shown in Figure 7, existing retrieval methods fall into\\ntwo categories based on architecture: Single/Dual-stream Structure and Generative Structure, each\\ninvolves single-modal (e.g., text, images) and cross-modal information retrieval.\\n• Single/Dual-stream Structure: The Single-stream Structure integrates multimodal fusion\\nmodules to model image-text relationships in a unified semantic space, capturing fine-grained\\ninteractions but incurring higher computational costs and slower inference, limiting scalability\\nfor large-scale multimodal retrieval tasks in real-world applications. In contrast, the Dual-stream\\nStructure uses separate vision and language streams, leveraging contrastive learning to align\\nglobal features in a shared semantic space efficiently. However, it lacks explicit multimodal\\ninteraction and struggles with feature alignment due to information imbalance, exacerbated by\\nthe brevity of dataset captions.\\n– Retrieval for Single-Modal. In MRAG systems, single-modal retrieval focuses on text and\\nimage retrieval. Text retrieval uses NLP techniques to extract relevant information from\\ndatasets, identifying contextually aligned documents. Image retrieval employs computer vision\\nalgorithms and feature extraction methods to encode visual data into high-dimensional vectors\\nfor similarity matching. Both modalities are essential for enhancing MRAG system performance.\\n∗Text-centric. Text retrieval, a core component of information retrieval (IR), identifies\\nrelevant textual information from large corpora or web resources in response to user queries.\\nIt is widely used in downstream applications such as question answering [161, 304], dialogue\\nsystems [334, 436, 456], web search [92, 269, 270], and retrieval-augmented generation\\nsystems [33, 45, 102, 330]. Recent advancements categorize text retrieval methods into two\\ntypes: sparse retrieval and dense retrieval.\\n· Sparse Text Retrieval. Early research in text retrieval focused on extracting representative\\nterms from documents, leading to the development of vector space models [320] based\\non the \"bag-of-words\" assumption, which represents documents and queries as sparse\\nterm vectors, ignoring term order. Term weighting methods like tf-idf [9, 314, 319] and\\nBM25 models [315, 316] were introduced to assign weights based on term importance\\nwithin and across corpora, while inverted indexes [513] improved retrieval efficiency\\nby organizing corpora into term-document ID pairs. Statistical language modeling [452]\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 16}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n17\\nMultimodal\\nRetrieval\\nRefiner\\nSoft\\nCross-\\nModal\\nPromptMM [393], RACC [395], VTC-CLS [364], VisToG [128]\\nSingle-\\nModal\\nAskell et al. [13], PI [53], Context Distillation [331], Wingate et al. [396], Sun et al.\\n[339], Distilling step-by-step [122], Gist [279], AutoCompressor [49], ICAE [99],\\nPOD [190], xRAG [48], COCOM [308], Gist-COCO [193], UltraGist [467], LLoCO\\n[346], 500xCompressor [203], RDRec [379], SelfCP [90], QGC [25], UniICL [91]\\nHard\\nCross-\\nModal\\nLLaVolta [34], PyramidDrop [407], DeCo [425], MustDrop [226], G-Search [484],\\nG-Prune [151]\\nSingle-\\nModal\\nDynaICL [499], FILCO [385], Selective Context [196], CoT-Influx [132], RECOMP\\n[410], MEMWALKER [32], TCRA-LLM [220], LLMLingua [148], LongLLMLingua\\n[149], CPC [213], AdaComp [469], Prompt-SAW [11], PCRL [159], LLMLingua-2\\n[288], Nano-Capsulator [56], CompAct [432], Style-Compress [297], TACO-RL [323],\\nFaviComp [158]\\nReranker\\nPrompt\\nCross-\\nModal\\nTIGeR [303], Lin et al. [210]\\nSingle-\\nModal\\nZhuang et al. [509], MCRanker [109], UPR [318], Zhuang et al. [511], Co-Prompt\\n[52], PaRaDe [70], DemoRank [228], PRP-AllPair [302], PRP-Graph [248], Yan et al.\\n[416], RankGPT [341], LRL [256], Tang et al. [349], TourRank [43], TDPart [292],\\nFIRST [309]\\nFine-tune\\nCross-\\nModal\\nWen et al. [394], RagVL [311]\\nSingle-\\nModal\\nNogueira and Cho [282], monoBERT [285], Nogueira et al. [283], Ju et al. [157],\\nDuoT5 [296], RankT5 [510], ListT5 [433], RankLLaMA [255], TSARankLLM [464],\\nQ-PEFT [294], Zhang et al. [475], PE-Rank [222]\\nRetriever\\nGenerative\\nCross-\\nModal\\nIRGen [479], GeMKR [238], GRACE [199], ACE [76], AVG [195]\\nSingle-\\nModal\\nGENRE [61], DSI [352], DynamicRetriever [502], SEAL [19], DSI-QG [512], NCI\\n[383], Ultron [501], LTRGR [201], GenRRL [500], DGR [202], GenRet [340], MINDER\\n[200], NOVO [389], LMIndexer [153], ASI [420], RIPOR [449], GLEN [175]\\nSingle/Dual-\\nStream\\nCross-\\nModal\\n• Text/Image–Image:\\nMSDS [370], VSE++ [75], Liu et al. [231], Wehrmann and Barros [390], Guo et al.\\n[110], DSCMR [489], DRCE [384], ESSE [386], SDCMR [388], DSVEL [72], CRAN\\n[299], CAAN [468], RANet [408], PVSE [333], PCME [57], RLCMR [422], DREN\\n[419], TGDT [215], HREM [87], TransTPS [18], IEFT [350], TEAM [405], CSIC\\n[234], LAPS [88], COTS [240], AGREE [380], IRRA [146], USER [477], EI-CLIP [250],\\nMAKE [492]\\n• Text–Video:\\nLLVE [361], Mithun et al. [276], Miech et al. [274], MMT [89], HiT [224], CLIP4Clip\\n[247], VoP [131], Cap4Video [402], DGL [421], TeachCLIP [356], T-MASS [371]\\n• Text–Audio:\\nATR [239], OML [271], MGRL [485], TAP-PMR [406], CMRF [494], TTMR++ [63]\\n• Unified:\\nFLAVA [326], UniVL-DR [237], MARVEL [498], FLMR [211], UniIR [391], VISTA\\n[495], E5-V [150], VLM2VEC [152], GME [476], Ovis [244], ColPali [79], CREAM\\n[462], DSE [254]\\nSingle-\\nModal\\nTf-Idf [319], BM25 [316], statistical [452], DeepCT [60], HDCT [59], COIL [94],\\nuniCOIL [208], DocTTTTTquery [284], SPLADE [84], SPLADE v2 [83], Poly [137],\\nME-BERT [246], ColBERT [163], ColBERTer [120], MVR [471], MADRM [166],\\nrandom [127], in-batch [118], hard [161], STAR [454], ANCE [409], ADORE [454],\\nRocketQA [304], AR2 [459], SimANS [497], Lee et al. [172], Chang et al. [27], Prop\\n[251], B-PROP [252], Condenser [92], co-Condenser [93], Contriever [139], SimLM\\n[373], RetroMAE [491], Liu and Yang [214]\\nFig. 6. Taxonomy of recent advancements in multimodal retrieval research.\\nfurther advanced retrieval by estimating term probability distributions for probabilistic\\nranking. However, early sparse retrieval methods face limitations, such as assuming term\\nindependence and relying on lexical matching, which hinders their ability to capture\\ncontextual term importance or semantic relationships between terms. Consequently, these\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 17}, page_content='18\\nTrovato et al.\\nSingle/Dual-Stream\\nGenerative\\nSingle-Modal\\nCross-Modal\\nText Query\\n------------------\\nWhat is a new \\nenergy vehicle?\\nText Candidate\\n------------------------\\nA new energy \\nvehicle (NEV) is a \\ntype of vehicle that \\nutilizes alternative \\nenergy sources \\ninstead of ......\\nText Query\\n------------------------------------\\nWhat is a new energy vehicle?\\nVisual Candidate\\n(Image, Video, Audio, …)\\n------------------------------------\\nSingle-Modal\\nCross-Modal\\nText Query\\n------------------\\nWhat is a new \\nenergy vehicle?\\nText Candidate\\n---------------------------\\nA new energy vehicle \\n(NEV) is a type of \\nvehicle that utilizes \\nalternative energy \\nsources instead of \\ntraditional internal \\ncombustion engines \\nthat run on fossil fuels \\nlike gasoline or diesel.\\nLLM\\nTextID\\nIdentifier\\nTextID\\nEncoder\\nText Query\\n------------------\\nWhat is a new \\nenergy vehicle?\\nVisual Candidate\\n(Image, Video, Audio, …)\\n------------------------------------\\nMLLM\\nVisualID\\nIdentifier\\nVisualID\\nEncoder\\nText \\nRetriever\\nCross-Modal \\nRetriever\\nVisual Query\\n------------------------\\nVisual Candidate\\n------------------------\\nVisual\\nRetriever\\nFig. 7. The architectures of retriever in multimodal retrieval.\\nmethods struggle to understand deeper textual meanings and contextual relevance between\\nqueries and documents.\\nRecent advancements in sparse retrieval models have been driven by the integration of\\npre-trained language models (PLMs). While these approaches leverage PLMs, they remain\\nfundamentally rooted in lexical matching, enabling the reuse of traditional sparse index\\nstructures by incorporating auxiliary information such as contextualized embeddings\\n[94, 208] and extended tokens [83, 84, 284]. This research domain focuses on two main\\napproaches: term weighting and term expansion. Term weighting enhances relevance\\nestimation by leveraging context-specific token representations. DeepCT [60] and HDCT\\n[59] use learned token representations to estimate the context-specific importance of terms\\nwithin passages, while COIL [94] and uniCOIL [208] employ contextualized token repre-\\nsentations of exact matching terms to compute relevance via dot products and summed\\nsimilarity scores. Term expansion mitigates vocabulary mismatch by expanding queries or\\ndocuments using PLMs. For instance, DocTTTTTquery [284] predicts relevant queries\\nfor documents to enrich the document’s content, while SPLADE [84] and SPLADEv2 [83]\\nproject terms onto vocabulary-sized weight vectors derived from masked language model\\nlogits. These vectors, aggregated via methods like summing or max pooling, effectively\\nexpand content by incorporating absent terms. Sparsity regularization ensures efficient\\nsparse representations for inverted index usage.\\nIn summary, sparse retrieval models achieve an optimal balance in cross-domain transfer,\\nretrieval efficiency, and overall effectiveness.\\n· Dense Text Retrieval. Recent advancements in deep learning [5, 50, 51, 105, 119, 167,\\n169], particularly pre-trained language models (PLMs) [23, 62, 229] based on the Trans-\\nformer architecture [80, 362], have increasingly adopted dense vector embeddings in\\nlow-dimensional Euclidean spaces for modeling semantic relationships between queries\\nand documents. These embeddings enable relevance measurement through Euclidean dis-\\ntances or inner products. Dense retrieval methods have demonstrated strong performance\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 18}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n19\\nacross various information retrieval tasks [161, 163, 270]. Additionally, Approximate Near-\\nest Neighbor Search (ANNS) algorithms [98, 142, 156], particularly quantization-based\\nmethods [98, 142] and their retrieval-oriented variants [415, 453, 455, 461], enable efficient\\nretrieval of top-ranked documents from large collections using precomputed ANNS indices.\\nDense retrieval techniques primarily focus on two key aspects: model architecture and\\ntraining methods.\\nFor model architecture, dense retrieval methods employ a two-tower architecture to\\nbalance retrieval efficiency and effectiveness by modeling semantic interactions between\\nqueries and documents through their representations. These methods vary in represen-\\ntation granularity, primarily falling into two categories: single-vector and multi-vector\\nrepresentations. Then, the relevance scores are computed using similarity functions (e.g.,\\ncosine similarity, inner product) between these embeddings. A common technique in-\\nvolves placing a special token (e.g., “[CLS]”) at the beginning of a text sequence, with\\nits learned representation capturing the overall semantics. The existing dense retrieval\\nmodels learn the query and document representations by fine-tuning PLMs like BERT [62],\\nRoBERTa [229], or Mamba Gu and Dao [106], Zhang et al. [458], or large language models\\n(LLMs) like RepLLaMA [255] on annotated datasets (e.g., MSMARCO [281], BEIR [354]).\\nHowever, single-vector bi-encoders struggle to model fine-grained semantic interactions\\nbetween queries and documents. To address this limitation, multi-vector representation en-\\nhance text representation and semantic interaction by employing multiple-representation\\nbi-encoders. The Poly-encoder [137] generates multiple context codes to capture text se-\\nmantics from multiple views. ME-BERT [246] produces 𝑚representations for a candidate\\ntext using the contextualized embeddings of the first 𝑚tokens. ColBERT [163] maintains\\nper-token contextualized embeddings with a late interaction mechanism. ColBERTer [120]\\nextends ColBERT by combining single- (“[CLS]”) and multi-representation (per-token)\\nmechanisms for better performance. MVR [471] introduces multiple “[VIEW]” tokens to\\nlearn diverse representations, with a local loss to identify the best-matched view. MADRM\\n[166] learns multiple aspect embeddings for queries and texts, supervised by explicit aspect\\nannotations.\\nFor training method, to achieve optimal retrieval performance, dense retrieval mod-\\nels are typically trained using two key techniques: negative sampling and pretraining.\\nNegative sampling focuses on selecting high-quality negatives to compute the negative\\nlog-likelihood loss used for training dense retrieval models. Basic methods include ran-\\ndom sampling [127] and in-batch negatives [118, 161, 304], which increase the number\\nof negatives within memory limits but do not guarantee the inclusion of hard negatives,\\ni.e., irrelevant texts with high semantic similarity to the query. Hard negatives are critical\\nfor improving the model’s ability to distinguish relevant from irrelevant texts. Various\\napproaches have been proposed to incorporate hard negatives. BM25-retrieved documents\\nare used as static hard negatives [95, 161]. STAR [454] combines static hard negatives\\nwith random negatives, while ANCE [409] retrieves hard negatives using a warm-up\\ndense retrieval model and refreshes the document index during training. ADORE [454]\\nemploys an adaptive query encoder to retrieve top-ranked texts as hard negatives, keeping\\nthe text encoder and document index fixed. However, hard negatives may include false\\nnegatives, introducing noise that can degrade performance. RocketQA [304] addresses\\nthis by using a cross-encoder to filter out likely false negatives. AR2 [459] integrates a\\ndual-encoder retriever with a cross-encoder ranker, jointly optimized through a minimax\\nadversarial objective to produce harder negatives and improve the retriever. SimANS [497]\\nintroduces the concept of sampling ambiguous negatives, i.e., texts ranked near positives\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 19}, page_content='20\\nTrovato et al.\\nwith moderate similarity to the query. These negatives are more informative and less likely\\nto be false negatives, further enhancing model performance.\\nPretraining aims to learn universal semantic representations that generalize to down-\\nstream dense retrieval tasks. To enhance the modeling capacity of PLMs, self-supervised\\npretraining tasks, such as those proposed by Lee et al. [172] (selecting random sentences\\nas queries) and Chang et al. [27] (leveraging hyperlinks for constructing query-passage\\npairs), mimic retrieval objectives. Prop [251] and B-PROP [252] use document language\\nmodels (e.g., unigram, BERT) to sample word sets, training PLMs to predict pairwise\\npreferences. To enhance dense retrieval models, studies focus on improving the “[CLS]”\\ntoken embedding. Condenser [92] aggregates global text information for masked token\\nrecovery, while co-Condenser [93] adds a query-agnostic contrastive loss to cluster related\\ntext segments while distancing unrelated ones. Contriever [139] generates positive pairs\\nby sampling two spans from the same text and negatives using in-batch and cross-batch\\ntexts. Following with an unbalanced architecture (strong encoder, simple decoder), SimLM\\n[373] pretrains the encoder and decoder with replaced language modeling, recovering\\noriginal tokens after replacement. It further optimizes the retriever through hard negative\\ntraining and cross-encoder distillation. RetroMAE [491] utilizes a high masking ratio for\\nthe decoder and a standard ratio for the encoder, incorporating an enhanced decoding\\nmechanism with two-stream and position-specific attention masks. Liu and Yang [214]\\nintroduces a two-stage pretraining approach, combining general-corpus pretraining with\\ndomain-specific continual pretraining, achieving strong benchmark performance.\\nHowever, single-modal retrieval is inherently limited by its inability to capture cross-modal\\nrelationships, which underscores the importance of integrating multimodal retrieval strategies\\nto bridge textual and visual semantics for more comprehensive information retrieval and\\ngeneration.\\n– Retrieval for Cross-modal. Cross-modal retrieval enables the identification of relevant data\\nin one modality (e.g., images) using a query from another (e.g., text). It enhances MRAG systems\\nby facilitating the retrieval and generation of information across diverse modalities, including\\ntext, images, audio, and video.\\n∗Text–Image Retrieval. Text–Image Retrieval aims to match images with corresponding\\ntextual queries by leveraging multimodal data co-occurrence, such as paired text-image\\ninstances or manual annotations, to capture semantic correlations. Existing methods can be\\ncategorized into three groups: CNN/RNN-based approaches, Transformer-based techniques,\\nand Vision-Language Pretraining (VLP) model-based methods.\\nEarly CNN/RNN-based methods [75, 110, 174, 231, 370, 390] extract features from each\\nmodality separately using MLP, CNN, and RNN, enforcing cross-modal constraints through\\npositive/negative sample construction. MSDS [370] uses CNN with a maximum likelihood-\\nbased scheme for image-text relevance. VSE++ [75] combines CNN and RNN with hard\\nsample mining in ranking loss. Advances include residual learning [231], character-level\\nconvolution [390], and disentangled representation [110] for improved feature mapping and\\nretrieval. DSCMR [489] maps multimodal data into a shared space using modality-specific\\nnetworks and fully connected layers, leveraging label constraints and pairwise loss for\\ndiscriminant learning. Recent CNN/RNN-based methods improve image-text matching by\\naddressing key challenges. DRCE [384] enhances rare content representation and association\\nusing a dual-path structure, adaptive fusion, and reranking to mitigate long-tail issues. ESSE\\n[386] tackles one-to-many correspondence by projecting data as sectors with uncertainty\\napertures. SDCMR [388] employs diverse CNNs for multimodal feature extraction and a dual\\nadversarial mechanism to isolate semantic-shared features, ensuring retrieval consistency.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 20}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n21\\nThese methods collectively advance cross-modal retrieval robustness and accuracy. Spatial\\nattention [72, 135, 299, 408, 468] is widely used in CNN/RNN-based cross-modal retrieval to\\nuncover fine-grained associations by generating weighted masks for local regions, enhancing\\nkey features while suppressing irrelevant ones. DSVEL [72] employs spatial-aware pooling\\nto align image regions with text, while CRAN [299] and CAAN [468] improve global-local\\nalignment through relation alignment and context-aware selection. RANet [408] refines\\nattention mechanisms with reference attention to reduce incorrect scores and adaptive\\naggregation to amplify relevant information and minimize redundancy.\\nTransformer-based methods [18, 57, 87, 215, 333, 350, 419, 422, 451] leverage multi-head\\nself-attention to encode multimodal relationships and optimize modality-specific encoders,\\ndemonstrating superior performance in multimodal modeling and cross-modal retrieval tasks.\\nRecent advancements in multimodal representation learning have focused on enhancing\\nTransformer architectures and feature alignment. PVSE [333] integrates self-attention and\\nresidual learning, while PCME [57] uses probabilistic embeddings to model one-to-many\\nand many-to-many correlations. RLCMR [422] tokenizes multimodal data and trains with\\na unified Transformer encoder for cross-modal semantic correlation. DREN [419] refines\\nfeature representation through character-level and context-driven augmentation. TGDT\\n[215] unifies coarse- and fine-grained learning with multimodal contrastive loss for feature\\nalignment. HREM [87] improves image-text matching by capturing multi-level intra- and\\ninter-modal relationships. TransTPS [18] extends Transformers with cross-modal multi-\\ngranularity matching and contrastive loss for better feature distinction. IEFT [350] models\\ntext-image pairs as unified entities to model their intrinsic correlation.\\nWith the rapid advancement of pretraining paradigms, Vision-Language Pretraining (VLP)\\nmodels [46, 62, 68, 107, 136, 144, 183, 194, 305], including both single- and dual-stream\\narchitectures, have leveraged large-scale visual-linguistic datasets for joint pretraining. Re-\\nsearchers have utilized the strong representational capabilities [88, 146, 234, 240, 250, 380,\\n405, 477, 492] of VLP models to significantly enhance cross-modal retrieval performance.\\nSingle-stream models like TEAM [405] align multimodal token embeddings for token-level\\nmatching, while dual-stream approaches such as COTS [240] integrate contrastive learning\\nwith token- and task-level interactions. Methods like CSIC [234] and LAPS [88] improve mul-\\ntimodal alignment by quantifying semantic significance and associating patch features with\\nwords, respectively. AGREE [380] fine-tunes and reranks cross-modal entities to harmonize\\ntheir alignment. IRRA [146] employs text-specific mask mechanism to capture fine-grained\\nintra- and inter-modal relationships. USER [477], EI-CLIP [250], and MAKE [492] leverage\\nCLIP [305] or ALIGN [144] to integrate contrastive learning and keyword enhancement\\nfor enriching representations. Overall, VLP models, through strategies such as fine-tuning,\\nreranking, and follow-up training, have become essential for improving cross-modal align-\\nment and interaction.\\n∗Text–Video Retrieval. Text-video retrieval involves matching textual descriptions with\\ncorresponding videos, requiring spatiotemporal representations to address temporal dy-\\nnamics, scene transitions, and precise text-video alignment. This task is more complex\\nthan text-image retrieval due to the need to model both visual and sequential information\\neffectively.\\nEarly CNN/RNN-based methods [64, 273, 274, 276, 361, 441] encode videos and texts into\\na shared latent space for similarity measurement. LLVE [361] employs CNNs and LSTMs\\nto extract latent features from images and texts, with LSTMs further capturing temporal\\nrelationships between video frames. Subsequent studies [274, 276] apply mean/max pooling\\nto frame sequences to generate compact video-level representations, prioritizing efficiency\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 21}, page_content='22\\nTrovato et al.\\nover granularity. Later advancements incorporate additional modalities, such as audio and\\nmotion, to enhance video semantics [273]. For text encoding, simpler methods like Word2Vec,\\nLSTMs, or GRUs are commonly used [274, 276, 441], with evidence suggesting that combining\\nmultiple text encoding strategies improves retrieval performance [64].\\nTransformer-based methods [89, 224] utilize self-attention mechanisms to jointly encode\\nvideos and texts, enabling cross-modal interaction. MMT [89] employs mutual attention\\nbetween video and text modalities, integrating temporal information to enhance feature\\nrepresentation. Inspired by MoCo [116], HiT [224] introduces hierarchical cross-modal\\ncontrastive matching at both feature and semantic levels. Additionally, these methods [89,\\n224] encode diverse modalities in video data, such as audio and motion, further enriching\\nvideo representations.\\nRecently, VLP-based models [131, 247, 356, 371, 402, 421] utilize pretrained models like\\nCLIP [305] to enhance text-video tasks in text-video retrieval tasks by capturing cross-modal\\nand temporal dependencies. CLIP4Clip [247] adapt CLIP for text-video retrieval and caption-\\ning, analyzing temporal dependencies. VoP [131] introduces prompt tuning and fine-tunes\\nCLIP to model spatiotemporal video aspects, while Cap4Video [402] leverages zero-shot\\ncaptioning with CLIP and GPT-2 [306] for auxiliary captions. DGL [421] proposes dynamic\\nglobal-local prompt tuning, emphasizing intermodal interaction and global video informa-\\ntion through shared latent spaces and attention mechanisms. TeachCLIP [356] improves\\nCLIP4Clip by integrating fine-grained cross-modal knowledge from advanced models, and re-\\nfining text-video similarity with an frame-feature aggregation block. T-MASS [371] addresses\\ndataset limitations by enriching text embeddings with stochastic text modeling.\\n∗Text–Audio Retrieval. Text-audio retrieval involves matching textual queries with corre-\\nsponding audio content, requiring alignment of semantic text information with dynamic\\nacoustic patterns in speech, music, or environmental sounds. The challenge lies in bridging\\nthe gap between discrete text and continuous audio signals.\\nEarly CNN/RNN-based approaches [239, 271, 485] focus on encoding text and audio\\nseparately and aligning them in a shared space for similarity measurement. ATR [239] uses\\npretrained CNN-based audio networks with NetRVLAD pooling [143] to aggregate features\\ninto a unified representation. OML [271] employs CNNs for robust audio feature extraction\\nand metric learning to enhance audio-text alignment. MGRL [485] leverages CNNs for\\nlocalized audio features and introduces adaptive aggregation to handle varying text–audio\\ngranularities.\\nFurthermore, Transformer-based methods [63, 406, 494] utilize multi-head attention mech-\\nanisms and fine-tuning to enhance cross-modal interactions. TAP-PMR [406] employs scaled\\ndot-product attention to enable text to focus on relevant audio frames, reducing mislead-\\ning information, while its prior matrix revised loss optimizes dual matching by addressing\\nsimilarity inconsistencies. CMRF [494] enhances audio-lyrics retrieval through directional\\ncross-modal attention and reinforcement learning to refine multimodal embeddings and\\ninteractions. TTMR++ [63] integrates fine-tuned LLMs and rich metadata to generate detailed\\ntext descriptions, improving retrieval by addressing musical attributes and user preferences.\\n∗Unified-Modal Retrieval. Unified-Modal Retrieval aims to process diverse hybrid-modal\\ndata (e.g., text, images, videos) within a unified model architecture, such as transformer-based\\nPLMs, to encode all modalities into a shared feature space. This enables efficient cross-modal\\nretrieval between any pairwise combination of hybrid-modal data. With the growing demand\\nfor multimodal applications, there is an increasing need for unified multimodal retrieval\\nmodels tailored to complex scenarios. Current approaches leverage pre-trained models like\\nCLIP [305], BLIP [186], and ALIGN [144] for multimodal embedding. For instance, FLAVA\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 22}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n23\\n[326] integrates multiple modalities into a unified framework, leveraging joint pretraining\\non multimodal data with cross-modal alignment and fusion objectives. Similarly, UniVL-DR\\n[237] encodes queries and multimodal resources into a shared embedding space, employing\\na universal embedding optimization strategy with modality-balanced hard negatives and an\\nimage verbalization method to bridge the gap between images and texts. MARVEL [498]\\naddresses the modality gap between images and texts by incorporating visual features\\ninto the encoding process. FLMR [211] enhances image representations by using a visual\\nmodel aligned with existing text-based retrievers to supplement the image representation of\\nimage-to-text transforms. UniIR [391] introduces a unified instruction-guided multimodal\\nretriever, achieving robust generalization through instruction tuning on diverse multimodal-\\nIR tasks. VISTA [495] extends image understanding capability by integrating visual token\\nembeddings into a text encoder, supported by high-quality composed image-text data and\\na multi-stage training algorithm. E5-V [150] fine-tunes MLLMs on single-text or vision-\\ncentric relevance data, outperforming traditional image-text pair training. VLM2VEC [152]\\nproposes a contrastive training framework to convert vision-language models into embedding\\nmodels using the MMEB dataset [152]. To address modality imbalance, GME [476] trains\\nan MLLM-based dense retriever on the large-scale UMRB dataset[476]. Ovis [244] aligns\\nvisual and textual embeddings by integrating a learnable visual embedding table, enabling\\nprobabilistic combinations of indexed embeddings for rich visual semantics. ColPali [79]\\nleverages Vision Language Models and the ViDoRe benchmark [79] to index documents from\\ntheir visual features, facilitating efficient query matching with late interaction mechanisms.\\nCREAM [462] employs a coarse-to-fine retrieval and ranking approach, combining similarity\\ncalculations with large language model-based grouping and attention pooling for MLLM-\\nbased multi-page document processing. DSE [254] fine-tunes a large vision-language model\\non 1.3 million Wikipedia web page screenshots, enabling direct encoding of document\\nscreenshots into dense representations.\\n• Generative Structure: Traditional information retrieval (IR) methods, which rely on similarity\\nmatching to return ranked lists of documents, have long been a cornerstone of information\\nacquisition, dominating the field for decades. However, with the advent of pre-trained language\\nmodels, generative retrieval (GR) has emerged as a novel paradigm, garnering increasing attention\\nin recent years. GR primarily consists of two fundamental components: model training and\\ndocument identifier. Model Training aims to train generative models to effectively index and\\nretrieve documents, while enhancing the model’s capacity to memorize information from the\\ndocument corpus. This is typically achieved through sequence-to-sequence (seq2seq) training,\\nwhere the model learns to map queries to their corresponding Document Identifiers (DocIDs).\\nThe training process emphasizes optimizing the model’s understanding of semantic relationships\\nbetween queries and documents, thereby improving retrieval accuracy. Document Identifiers\\n(DocIDs) serve as the target output for the generative retrieval model, and unique representations\\nof each document in the corpus. The quality of these identifiers is crucial, as they directly impact\\nthe model’s ability to memorize and retrieve document information. Effective DocIDs are often\\ngenerated using dense, low-dimensional embeddings or structured representations that capture\\nthe essential content and context of documents, enabling the model to distinguish between\\ndocuments more accurately and enhancing retrieval performance. By overcoming the limitations\\nof traditional IR in terms of content granularity and relevance matching, GR offers enhanced\\nflexibility, efficiency, and creativity, better aligning with practical demands.\\n– Retrieval for Text-modal. The recent advancements in generative language models have\\ndemonstrated their ability to memorize knowledge from documents and recall knowledge to\\nrespond to user queries effectively, which focuses on the use of document identifiers (DocIDs)\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 23}, page_content='24\\nTrovato et al.\\nand their optimization for retrieval tasks. The approaches can be categorized into static DocID-\\nbased methods and learnable DocID-based methods.\\nStatic DocID-based methods rely on pre-defined, fixed document identifiers. They often\\nuse unique names, numeric formats, or structured identifiers to represent documents. GENRE\\n[61] generates entity names via constrained beam search using a prefix tree, with document\\ntitles serving as DocIDs. DSI [352] introduces numeric DocID formats, including unstructured,\\nnaively structured, and semantically structured identifiers, trained through indexing and\\nretrieval strategies. DynamicRetriever [502] uses unstructured atomic DocIDs and enhances\\nmemorization with pseudo queries. SEAL [19] representing documents with N-gram sub-string\\nidentifiers, leveraging FM-Index [82] for retrieval. DSI-QG [512] represents documents with\\ngenerated queries, re-ranked by a cross-encoder. NCI [383] generates document identifiers using\\na seq2seq network with a prefix-aware decoder. It is trained on both labeled and augmented\\npseudo query-document pairs. Ultron [501] combines URLs and titles as DocIDs to uniquely\\nidentify web documents. It encodes documents into a latent semantic space using BERT [62]\\nand compresses vectors via Product Quantization (PQ) [98, 142], with PQ codes serving as\\nsemantic identifiers. Additional digits ensure DocID uniqueness. LTRGR [201] focuses on\\nlearning to rank passages directly using generative retrieval models, optimizing autoregressive\\nmodels via rank loss. GenRRL [500] integrates reinforcement learning for aligning token-level\\nDocID generation with document-level relevance estimation. DGR [202] enhances generative\\nretrieval through knowledge distillation, using a cross-encoder as a teacher model to provide\\nfine-grained ranking supervision. Despite these innovations, most approaches rely on static\\nDocIDs, which are not optimized for retrieval tasks, limiting their ability to capture document\\nsemantics and relationships, thereby hindering retrieval performance.\\nTo address this limitation, Learnable DocID-based methods introduce learnable document\\nrepresentations, where DocIDs are optimized during training to better capture document\\nsemantics and improve retrieval performance. GenRet [340] employs a discrete autoencoder\\nto encode documents into compact DocIDs, minimizing reconstruction error. MINDER [200]\\nenhances document representations using multi-view identifiers, including pseudo-queries,\\ntitles, and sub-strings. NOVO [389] introduces learnable continuous N-gram DocIDs, refining\\nembeddings through query denoising and retrieval tasks. LMIndexer [153] generates neural\\nsequential discrete IDs via progressive training and contrastive learning, addressing semantic\\nmismatches. ASI [420] automates DocID learning, assigning similar IDs to semantically close\\ndocuments and optimizing end-to-end retrieval using an generative model. RIPOR [449]\\nimproves relevance scoring during sequential DocID generation using dense encoding and\\nResidual Quantization [264]. GLEN [175] employs a dynamic lexical identifier with a two-phase\\nindex learning strategy. Firstly, the keyword-based DocID are defined by extracting keywords\\nfrom documents using self-supervised signals. Secondly, dynamic DocIDs are refined by\\nintegrating query-document relevance, enabling efficient inference. The field of generative text\\nretrieval is evolving from static, pre-defined DocIDs to dynamic, learnable DocIDs that better\\ncapture document semantics and relationships. Learnable DocIDs, combined with advanced\\ntechniques like reinforcement learning, knowledge distillation, and contrastive learning, are\\ndriving improvements in retrieval performance.\\n– Retrieval for Cross-modal. Similarly, MLLMs are considered to memorize and retrieve\\nmultimodal content, such as images and videos, within their parameters. When presented\\nwith a user query for visual content, the MLLM is expected to \"recall\" the relevant image\\nfrom its parameters as a response. Achieving this capability presents significant challenges,\\nparticularly in developing effective visual memory and recall mechanisms within MLLMs.\\nIRGen [479] employs a seq2seq model to predict discrete visual tokens (image identifiers) from\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 24}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n25\\nquery images. Its key innovation is a semantic image tokenizer that encodes global features\\ninto discrete visual tokens, enabling end-to-end differentiable search for improved accuracy\\nand efficiency. GeMKR [238] integrates LLMs with visual-text features through a generative\\nmultimodal knowledge retrieval framework. It first guides multi-granularity visual learning\\nusing object-aware prefix tuning techniques to align visual features with LLMs’ text feature\\nspace, then adopts a two-step retrieval process: generating knowledge clues relevant to the\\nquery and retrieving documents based on these clues. GRACE [199] assigns unique identifier\\nstrings to represent images, training MLLMs to memorize and retrieve image identifiers from\\ntextual queries. ACE [76] combines K-Means and RQ-VAE to construct coarse and fine tokens\\nas multimodal data identifiers, aligning natural language queries with candidate identifiers.\\nAVG [195] introduces autoregressive voken (i.e., visual token) generation, tokenizing images\\ninto vokens that serve as image identifiers while preserving visual and semantic alignment.\\nBy framing text-to-image retrieval as a token-to-voken generation task, AVG bridges the gap\\nbetween generative training and retrieval objectives through discriminative training, refining\\nthe learning direction during token-to-voken generation.\\n3.3.2\\nRERANKER. Reranker, as a critical second-stage component in multimodal retrieval, is\\ndesigned to re-rank a multimodal document list initially retrieved by a first-stage retriever. It\\nachieves this by employing advanced relevance scoring mechanisms, such as cross-attention\\nmodels, which enable more contextual interactions between queries and documents. Based on\\nthe utilization of large models, including LLMs and MLLMs, existing reranking methods can be\\ncategorized into two primary paradigms: fine-tuning-as-reranker and prompting-as-reranker.\\n• Fine-tuning-as-Reranker: The fine-tuning-as-reranker paradigm adapts PLMs to domain-\\nspecific reranking tasks through supervised fine-tuning on domain-specific datasets, addressing\\ntheir inherent lack of ranking awareness and inability to effectively measure query-document\\nrelevance.\\n– Reranking for Text-Modal: According the development of large models’ architecture,\\nreranker can be divided to three categories: encoder-only, encoder-decoder, and decoder-\\nonly.\\nEncoder-only rerankers have advanced document ranking by fine-tuning PLMs (e.g., BERT\\n[62]) to achieve precise relevance estimation. Key examples include Nogueira and Cho [282]\\nand monoBERT [285], which format query-document pairs as query-document sequences.\\nThe relevance score is derived from the “[CLS]” token’s representation via a linear layer, with\\noptimization achieved through negative sampling and cross-entropy loss.\\nExisting research on encoder-decoder rerankers primarily formulates document ranking as a\\ngeneration task [157, 283, 296, 510], fine-tuning models like T5 to generate classification tokens\\n(e.g., “true” or “false”) for query-document pairs, with relevance scores derived from token logits\\n[283]. Extensions include multi-view learning approaches [157] that simultaneously generate\\nclassification tokens for query-document pairs and queries conditioned on documents, and\\nDuoT5 [296], which compares the classification tokens of document pairs to determine relative\\nrelevance. Beyond these approaches, studies have explored alternative training losses and\\narchitectures. Contrast with previous methods that rely on text generation losses, RankT5 [510]\\ndirectly produces numerical relevance scores for each query-document pair, optimizing with\\nranking losses instead of generation losses. ListT5 [433] further advances this by processing\\nmultiple documents simultaneously, directly generating reranked lists using the Fusion-in-\\nDecoder architecture.\\nRecent studies [222, 255, 294, 464, 475] have explored fine-tuning decoder-only models\\nlike LLaMA for document reranking. RankLLaMA [255] formats query-document pairs into\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 25}, page_content='26\\nTrovato et al.\\nprompts and uses the last token representation for relevance scoring. TSARankLLM [464]\\nemploys a two-stage training approach: continuous pretraining on web-sourced relevant text\\npairs to align LLMs with ranking tasks, followed by fine-tuning with supervised data and\\ntailored loss functions. Q-PEFT [294] introduces query-dependent parameter-efficient fine-\\ntuning to generate accurate queries from documents. In contrast, listwise approaches like\\nthose in [475] and PE-Rank [222] focus on directly outputting reranked document lists. Zhang\\net al. [475] highlight the limitations of point-wise datasets with binary labels, and instead use\\nranking outputs from existing systems as gold standards to train a listwise reranker. PE-Rank\\n[222] compresses documents into single embeddings, reducing input length and improving\\nreranking efficiency.\\n– Reranking for Cross-Model: The multi-modal reranking uses the multi-modal question\\nand multi-modal knowledge items to obtain the relevance score, as reranking have already\\nshown its importance in various knowledge-intensive tasks. Wen et al. [394] fine-tunes a\\npretrained MLLM to facilitate cross-item interaction between questions and knowledge items.\\nThe reranker is trained on the same dataset as the answer generator, using distant supervision\\nby checking whether answer candidates appear in the knowledge text. RagVL RETRIEVAL\\net al. [311] introduces a novel framework featuring knowledge-enhanced reranking and noise-\\ninjected training. The approach involves instruction-tuning the MLLM with a simple yet\\neffective template to enhance its ranking capability, enabling it to serve as a reranker for\\naccurately filtering the top-𝑘retrieved images.\\nIn summary, these approaches leverages the representational capacity of large models while\\noptimizing them for task-specific relevance signals, often achieving high reranking accuracy.\\nHowever, it requires substantial computational resources and labeled training data, resulting in\\nincreased costs.\\n• Prompting-as-Reranker: In contrast, the prompting-as-reranker paradigm leverages large\\nmodels in a zero-shot or few-shot manner by designing prompts that direct the model to gen-\\nerate relevance scores or rankings directly. This approach exploits the inherent knowledge\\nand reasoning capabilities of large models, eliminating the need for extensive fine-tuning and\\noffering greater flexibility and resource efficiency. Researchers have explored prompting LLMs\\nand MLLMs to perform ranking tasks on multimodal documents, with prompting strategies\\ngenerally categorized into three types: point-wise, pair-wise, and list-wise methods.\\n– Reranking for Text-Model: LLMs are increasingly employed in text-modal reranking tasks,\\nleveraging their advanced capabilities to optimize the ranking of textual documents.\\nPoint-wise methods evaluate the relevance between a query and individual documents,\\nreranking them based on relevance scores.. Zhuang et al. [509] integrates fine-grained rele-\\nvance labels into prompts for better document distinction. MCRanker [109] addresses biases\\nin existing point-wise rerankers by generating relevance scores based on multi-perspective\\ncriteria. UPR [318] re-scores retrieved passages using a zero-shot question generation model.\\nZhuang et al. [511] show that LLMs pre-trained without supervised instruction fine-tuning\\n(e.g., LLaMA) also exhibit strong zero-shot ranking capabilities. Despite their effectiveness,\\nthese methods often rely on suboptimal handcrafted prompts. To improve prompts for rank-\\ning tasks, Co-Prompt [52] introduces a discrete prompt optimization method for improving\\nprompt generation in reranking tasks. PaRaDe [70] proposes a difficulty-based approach to\\nselect the most challenging in-context demonstrations for prompts, though experiments reveal\\nthat this method does not significantly outperform random selection. To improve demonstra-\\ntion selection, DemoRank [228] advances demonstration selection with a dependency-aware\\ndemonstration reranker, optimizing top-ranked examples through efficient training sample\\nconstruction and a novel list-pairwise loss.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 26}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n27\\nPair-wise methods involve presenting LLMs with a query and a document pair, instructing\\nthem to identify the more relevant document. PRP-AllPair [302] generates all possible pairs,\\nassigns discrete relevance judgments, and aggregates these into a final relevance score per\\ndocument. PRP-Graph [248] improves this by using judgment generation probabilities and a\\ngraph-based aggregation for scoring relevance. Additionally, a post-processing technique [416]\\nrefines LLM-generated labels by aligning them with pairwise preferences while minimizing\\ndeviations from original values.\\nListwise methods directly rank document lists by incorporating queries and documents into\\nprompts, instructing LLMs to output reranked document identifiers. RankGPT [341] introduces\\ninstructional permutation generation and a sliding window strategy to address context length\\nlimits, while LRL [256] reorders document identifiers for candidate documents. However, these\\nmethods face challenges: (1) performance is highly sensitive to document order, revealing\\npositional bias, and (2) the sliding window strategy limits the number of documents ranked\\nper iteration. Recent advancements have attempted to address these issues: Tang et al. [349]\\npropose permutation self-consistency to mitigate bias. TourRank [43] introduces a tournament\\nmechanism, parallelizing reranking to minimize the impact of initial document order. TDPart\\n[292] employs a top-down partitioning algorithm, which processes documents to depth using\\na pivot element. FIRST [309] leverages the output logits of the first generated identifier to\\ndirectly obtain a ranked ordering of candidates.\\n– Reranking for Cross-Model: Prompt-Based Multimodal Reranker uses prompts to guide a\\nMLLM in reranking items. TIGeR [303] proposes a framework leveraging multimodal LLMs\\nfor zero-shot reranking via a generative retrieval approach. However, their method is limited\\nto text-only query retrieval tasks. In contrast, Lin et al. [210] extends this scope by utilizing\\nmultimodal LLMs to address diverse multimodal reranking tasks, supporting queries and\\ndocuments in text, image, or interleaved text-image formats.\\nIn summary, these approaches leverages the pre-existing knowledge and reasoning capabilities\\nof LLMs, reducing the need for extensive task-specific fine-tuning. Consequently, it provides\\ngreater flexibility and resource efficiency, particularly in scenarios with limited labeled data or\\ncomputational resources. However, its effectiveness depends heavily on the quality and design of\\nthe prompts, as well as the model’s ability to generalize its pre-trained knowledge to the specific\\ndemands of the target task.\\n3.3.3\\nREFINER. Theoretically, LLMs improves with more comprehensive task-relevant knowledge\\nin the retrieved and reranked context. However, unlimited input length poses practical deployment\\nchallenges: (1) Limited Context Window: LLMs have a fixed input length determined during pre-\\ntraining, and any text exceeding this limit is truncated, leading to loss of contextual semantics.\\n(2) Catastrophic Forgetting: Insufficient cache space can cause LLMs to forget previously learned\\nknowledge when processing long sequences. (3) Slow Inference Speed. Consequently, refined\\nprompts are crucial for optimizing LLM performance.\\nThe refiner is an optional yet highly impactful component that optimizes retrieved and reranked\\ninformation before its utilization by the LLM. It performs advanced processing tasks, such as\\nsummarization, distillation, or contextualization, to condense and refine content into a more\\ndigestible and actionable format. By extracting key insights, eliminating redundancies, and aligning\\ninformation with the query’s context, the refiner enhances the utility of the retrieved data, enabling\\nthe LLM to generate more coherent, accurate, and contextually relevant responses.\\nPrompt refinement can be achieved through two primary approaches: hard prompt methods and\\nsoft prompt methods. Hard prompt methods involve filtering out unnecessary or low-information\\ncontent, still using natural language tokens and resulting in less fluent but generalizable prompts\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 27}, page_content='28\\nTrovato et al.\\nthat can be used across LLMs with different embedding configurations. Soft prompt methods, in\\ncontrast, encode prompt information into continuous representations, producing latent vectors\\n(special tokens) that are not human-readable but optimized for model performance.\\n• Hard Prompt Refiner: Hard prompts consist of natural language tokens from the LLM/MLLM’s\\nvocabulary, representing specific words or sub-words, and can be generated by humans or\\nmodels.\\n– Refining for Text-Model: Recent advancements in prompt compression and context distil-\\nlation aim to optimize the efficiency of LLMs. DynaICL [499] employs a meta controller to\\ndynamically allocate in-context demonstrations based on input complexity and computational\\nconstraints. FILCO [385] distills retrieved documents using lexical and information-theoretic\\nmethods—String Inclusion, Lexical Overlap, and CXMI—training both context filtering and\\ngeneration models for RAG tasks. CPC [213] preserves semantic integrity by using a context-\\naware encoder to remove irrelevant sentences, while AdaComp [469] dynamically selects\\noptimal documents via a compression-rate predictor. LLMLingua [148] introduces a coarse-\\nto-fine approach, compressing prompt components (instructions, questions, demonstrations)\\nusing a small language model (SLM) to measure token informativeness via perplexity (PPL).\\nLongLLMLingua [149] extends this to long documents, employing a linear scheduler, reorder-\\ning mechanism, and contrastive perplexity to retain question-relevant tokens while ensuring\\nkey information integrity. CoT-Influx [132] compresses GPT-4-generated Chain-of-Thought\\n(CoT) prompts using a shot-pruner and token-pruner, both implemented as MLPs trained\\nvia reinforcement learning. These methods collectively improve performance while reducing\\nuseless CoT examples and redundant tokens. Selective Context [196] evaluates lexical unit in-\\nformativeness using a causal language model and a percentile-based filtering method to remove\\nredundancy. It calculates token self-information by predicting next-token probabilities, aggre-\\ngating these at phrase and sentence levels. Prompt-SAW [11] preserves syntactic and semantic\\nstructures by extracting key tokens via relation-aware graphs, integrating them into com-\\npressed prompts. PCRL [159] treats prompt compression as a binary classification task, using a\\nfrozen pre-trained policy language model with trainable MLP layers. The compression policy\\nlabels tokens as include or exclude, optimizing a reward function that balances faithfulness and\\nprompt length reduction. LLMLingua-2 [288] employs a bidirectional encoder-only model with\\na linear classification layer for compression, determining token retention or removal. RECOMP\\n[410] employs extractive and abstractive compressors to generate query-focused summaries,\\nleveraging contrastive learning and knowledge distillation. Nano-Capsulator [56] optimizes\\ncompression using reward feedback from response differences and enforces strict length con-\\nstraints. MEMWALKER [32] uses interactive prompting to build and navigate a memory tree\\nfor context summarization. CompAct [432] sequentially compresses document segments for\\nlong-context question-answering, achieving high compression rates. Style-Compress [297]\\niteratively refines prompts using diverse styles and task-specific examples, evaluated by larger\\nLLMs. TCRA-LLM [220] combines summarization and semantic compression to reduce token\\nsize. TACO-RL [323] employs reinforcement learning for task-aware prompt compression,\\nensuring low latency. FaviComp [158] enhances evidence familiarity by combining token\\nprobabilities from compression and target models, reducing perplexity.\\n– Refining for Cross-Model: Recent advancements in visual token compression for MLLMs\\nfocus on enhancing efficiency without significant performance loss. LLaVolta [34] introduces\\na method to reduce the number of visual tokens, enhancing training and inference efficiency\\nwithout compromising performance. To minimize information loss during compression while\\nmaintaining training efficiency, it employs a lightweight, staged training scheme. This scheme\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 28}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n29\\nprogressively compresses visual tokens from heavy to light compression during training, en-\\nsuring no information loss during testing. PyramidDrop [407] is a visual redundancy reduction\\nstrategy for MLLMs, designed to improve efficiency in both inference and training with negli-\\ngible performance loss. It partitions the MLLM into several stages and drops a predefined ratio\\nof image tokens at the end of each stage. DeCo [425] proposes the principle of \"Decouple Com-\\npression from Abstraction,\" which involves compressing visual tokens at the patch level using\\nprojectors while allowing the LLM to handle visual semantic abstraction entirely. MustDrop\\n[226] measures the importance of each token throughout its lifecycle, including the vision\\nencoding, prefilling, and decoding stages. During vision encoding, it merges spatially adjacent\\ntokens with high similarity and establishes a key token set to retain vision-critical tokens.\\nIn the prefilling stage, it further compresses vision tokens guided by text semantics using a\\ndual-attention filtering strategy. In the decoding stage, an output-aware cache policy reduces\\nthe size of the KV cache. By employing tailored strategies across these stages, MustDrop\\nachieves an optimal balance between performance and efficiency. G-Search [484] proposes a\\ngreedy search algorithm to determine the minimum number of vision tokens to retain at each\\nlayer, from shallow to deep. Based on this strategy, a parametric sigmoid function (P-Sigmoid)\\nis designed to guide token reduction at each layer of the MLLM, with parameters optimized\\nusing Bayesian Optimization. G-Prune [151] introduces a graph-based method for training-free\\nvisual token pruning. It treats visual tokens as nodes and constructs connections based on\\nsemantic similarities. Information flow is propagated through weighted links, and the most\\nimportant tokens are retained for MLLMs after iterations.\\nAlthough interpretable and transparent, the inherent ambiguity of hard prompts often hinders\\nthe precise expression of intent, limiting their effectiveness in diverse or complex scenarios.\\nCrafting accurate and impactful hard prompts demands significant human effort and may require\\nmodel-based refinement or optimization. Moreover, even minor variations in hard prompts can\\nlead to inconsistent LLM performance for identical tasks.\\n• Soft Prompt Refiner: Soft prompts are trainable, continuous vectors that match the dimension-\\nality of token embeddings in LLM’s vocabulary. Unlike hard prompts, which rely on discrete\\ntokens from a predefined vocabulary, soft prompts are optimized through training to capture\\nnuanced meanings that discrete tokens cannot express. When fine-tuned on diverse datasets,\\nsoft prompts enhance the LLM’s performance across various tasks.\\n– Refining for Text-Model: Language models convert text prompts into vectors for denser\\nrepresentation, enabling compression of discrete text into continuous vectors within the model.\\nThese vectors can serve as internal parameters (internalization) or additional soft prompts\\n(encoding). Such compression extends the context window and enhances inference speed,\\nparticularly with repeated prompt usage.\\nEarly work focused on system prompt internalization. Askell et al. [13] used Knowledge\\nDistillation to align models with human values, while Choi et al. [53] introduced Pseudo-Input\\nGeneration, generating pseudo-inputs from prompts and distilling knowledge between teacher\\nand student models to avoid redundant inference computations. Later research compressed user\\nprompt contexts. Snell et al. [331] distilled abstract instructions, reasoning, and examples into\\nprompts with distinct distribution differences, enabling task execution without explicit prompts.\\nSun et al. [339] internalized ranking techniques for zero-shot relevance tasks, while Distilling\\nStep-by-Step [122] improved reasoning tasks by distilling rationales as additional supervision.\\nIn retrieval-augmented generation, xRAG [48] integrated compressed document embeddings\\nvia a plug-and-play projector, using self-distillation for robustness. For context compression,\\nCOCOM [308] reduced long contexts to few embeddings, balancing trade-offs between decoding\\ntime and answer quality. LLoCO [346] learned offline compressed representations for efficient\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 29}, page_content='30\\nTrovato et al.\\nQA retrieval. QGC [25] retained key information under high compression using query-guided\\ndynamic strategies. UniICL [91] unified demonstration selection, compression, and generation\\nwithin a single frozen LLM, projecting demonstrations and inputs into virtual tokens for\\nsemantic-based processing.\\nRecent advancements in prompt compression for LLMs focus on encoding hard prompts\\ninto reusable soft prompts to enhance efficiency and generalization across tasks. Early work by\\nWingate et al. [396] distilled complex hard prompts into concise soft prompts by minimizing\\noutput distribution differences, reducing inference costs. A series of works aim to enhance\\ngeneralization across diverse prompts. Gist [279] used meta-learning to encode multi-task in-\\nstructions into gist tokens, while Gist-COCO [193] employed an encoder-decoder architecture\\nto compresses original prompts into shorter gist prompts, via the Minimum Description Length\\nprinciple. UltraGist [467] optimized cross-attention for compressing ultra-long contexts into\\nnear-lossless UltraGist tokens. AutoCompressor [49] iteratively compressed contexts segments\\ninto summary vectors using a Recurrent Memory Transformer, reducing computational load.\\nOther approaches, like ICAE [99] and 500xCompressor [203], fine-tuned LoRA-adapted LLMs\\nfor context encoding and prompt compression. For LLM-based recommendations, POD [190]\\ndistilled discrete prompt templates into continuous prompt vectors with an whole-word em-\\nbedding to integrate the item ID, while RDRec [379] synthesizes training data and internalizes\\nrationales into a smaller model. SelfCP [90] balances training cost, inference efficiency, and\\ngeneration quality by compressing over-limit prompts asynchronously using frozen LLMs\\nas the compressor and generator and trainable linear layers to project hidden states into\\nLLM-acceptable memory tokens.\\n– Refining for Cross-Model: PromptMM [393] tackles overfitting and side information inac-\\ncuracies in multi-modal recommenders by using Multi-modal Knowledge Distillation with\\nprompt-tuning. It compresses models by distilling user-item relationships and multi-modal\\ncontent from complex teacher models to lightweight student models, eliminating extra pa-\\nrameters. Soft prompt-tuning bridges the semantic gap between multi-modal context and\\ncollaborative signals, enhancing robustness. Additionally, a disentangled multi-modal list-wise\\ndistillation with modality-aware re-weighting addresses multimedia data inaccuracies. RACC\\n[395] compresses and aggregates retrieved knowledge for image-question pairs, generating a\\ncompact Key-Value (KV) cache modulation to adapt downstream frozen MLLMs for efficient\\ninference. VTC-CLS [364] uses the prior knowledge of the association between the [CLS] token\\nand visual tokens in the visual encoder to evaluate visual token importance, enabling Visual\\nToken Compression and shortening visual context. VisToG [128] introduces a grouping mecha-\\nnism using pretrained vision encoders to group similar image segments without segmentation\\nmasks. Semantic tokens represent image segments after linear projection and before input\\ninto the vision encoder. Isolated attention identifies and eliminates redundant visual tokens,\\nreducing computational demands.\\nHowever, as dataset size increases, so do the computational resource requirements. Additionally,\\nsoft prompts are less interpretable than hard prompts, as their continuous vectors are not directly\\nreadable or explainable by humans.\\n3.4\\nMultimodal Generation\\nMultimodal generation based on Multimodal Large Language Models (MLLMs) represents a sig-\\nnificant advancement, enabling the generation of content across multiple modalities such as text,\\nimages, audio, and video. These models leverage the strengths of large language models (LLMs) and\\nextend them to handle and integrate diverse data types, creating rich, coherent, and contextually\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 30}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n31\\nMultimodal\\nGeneration\\nModality\\nAugmentation\\nInternLM-XComposer [465], InternLM-XComposer2 [67], InternLM-XComposer-2.5 [466], MuRAR [508],\\n𝑀2𝑅𝐴𝐺[259]\\nMLLM\\n• Image ⊕Text →Text\\nBLIP-2 [185], ChatSpot [483], OpenFlamingo [14], ASM [378], Qwen-VL [16], Kosmos-2.5 [249], InternLM-\\nXComposer [465], JAM [8], Kosmos-1 [130], PaLM-E [69], ViperGPT [343], PandaGPT [335], PaLI-X [40], LLaVA-\\nMed [182], LLaVAR [478], mPLUG-DocOwl [426], P-Former [145], MiniGPT-v2 [35], LLaVA [219], MiniGPT-4\\n[504], mPLUG-Owl [427], Otter [181], MultiModal-GPT [103], CogVLM [377], mPLUG-Owl2 [428], Monkey\\n[205], DocPedia [81], ShareGPT4V [37], mPLUG-PaperOwl [123], RLHF-V [438], Silkie [189], Lyrics [241], VILA\\n[209], CogAgent [121], Volcano [176], DRESS [44], LION [31], Osprey [443], LLaVA-MoLE [39], VLGuard [514],\\nMobileVLM V2 [55], ViGoR [417], V* [399], MobileVLM [54], TinyGPT-V [444], DocLLM [367], LLaVA-Phi [507],\\nKAM-CoT [277], InternLM-XComposer2 [67], InternLM-XComposer-2.5 [466], MoE-LLaVA [207], VisLingInstruct\\n[505]\\n• Image ⊕Text →Image ⊕Text\\nVisual ChatGPT [397], DetGPT [295], FROMAGe [165], Shikra [36], GPT4RoI [472], SEED [100], LISA [168],\\nGILL [164], Kosmos-2 [293], DreamLLM [65], MiniGPT-5 [490], Kosmos-G [287], VisCPM [124], CM3Leon [434],\\nLaVIT [155], GLaMM [307], RPG [418], Vary-toy [392], CogCoM [298], SPHINX-X [216], LLaVA-Plus [223],\\nPixelLM [310], VL-GPT [506], CLOVA [96], Emu-2 [337], MM-Interleaved [355], DiffusionGPT [301]\\n• Video ⊕Text →Text\\nVideo-ChatGPT [260], VideoChat [187], Dolphins [257]\\n• Video ⊕Text →Video ⊕Text\\nVideo-LaVIT [154]\\n• Unified →Text\\nFlamingo [10], X-LLM [30], LanguageBind [503], InstructBLIP [219], MM-REACT [423], X-InstructBLIP [289],\\nEmbodiedGPT [280], Video-LLaMA [460], Lynx [450], LLaMA-VID [198], InternVL [47], AnyMAL [278]\\n• Unified →Image ⊕Text\\nBuboGPT [487], Emu [338], GroundingGPT [204]\\n• Unified →Unified\\nTEAL [424], GPT-4 [7], Gemini [353], HuggingGPT [325], CoDi-2 [351], AudioGPT [129], ModaVerse [382],\\nMLLM-Tool [366], ControlLLM [236], NExT-GPT [400]\\nFig. 8. Taxonomy of recent advancements in multimodal generation research.\\nrelevant outputs. We classify MLLMs from generative perspectives of inputs and outputs, and\\nsummarize the related researches in Figure 8.\\n3.4.1\\nMODALITY INPUT. With the rapid advancement of large language models in the domain\\nof textual knowledge comprehension and question-answering, researchers try to explore how\\nto enable these models to understand and process inputs from a broader range of modalities,\\nthereby facilitating more extensive multimodal question-answering tasks. Initial efforts focused on\\nincorporating image modality into the input of large models. For instance, Blip-2 [185] proposes\\na generic and efficient pre-training strategy that bootstraps vision-language pre-training from\\noff-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges\\nthe modality gap with a lightweight Querying Transformer, which is pre-trained in two stages.\\nThe first stage bootstraps vision-language representation learning from a frozen image encoder.\\nThe second stage bootstraps vision-to-language generative learning from a frozen language model.\\nInternlm-xcomposer2 [66] proposes a vision-language model excelling in free-form text-image com-\\nposition and comprehension. This model goes beyond conventional vision-language understanding,\\nadeptly crafting interleaved text-image content from diverse inputs like outlines, detailed textual\\nspecifications, and reference images, enabling highly customizable content creation. DiffusionGPT\\n[301] leverages Large Language Models (LLM) to offer a unified generation system capable of\\nseamlessly accommodating various types of prompts and integrating domain-expert models. Diffu-\\nsionGPT constructs domain-specific Trees for various generative models based on prior knowledge.\\nWhen provided with an input, the LLM parses the prompt and employs the Trees-of-Thought to\\nguide the selection of an appropriate model.\\nAs the variety of modal data continues to expand, more complex modalities, such as video, have\\nbeen integrated into the inputs of large models. For instance, Video-ChatGPT [260] proposes a\\nmultimodal model that merges a video-adapted visual encoder with an LLM. The resulting model is\\ncapable of understanding and generating detailed conversations about videos. Video-LaVIT [154]\\naddress spatiotemporal dynamics limitations in video-language pretraining with an efficient video\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 31}, page_content='32\\nTrovato et al.\\ndecomposition that represents each video as keyframes and temporal motions. These are then\\nadapted to an LLM using well-designed tokenizers that discretize visual and temporal information as\\na few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference,\\nthe generated tokens from the LLM are carefully recovered to the original continuous pixel space\\nto create various video content. The proposed framework is both capable of comprehending and\\ngenerating image and video content.\\nRecently, the input for multimodal large models has evolved from specialized modal data to a\\nunified input that can handle arbitrary modal data. For instance, InstructBLIP [289] conduct a vision-\\nlanguage instruction tuning based on the pretrained BLIP-2 models. Additionally, we introduce an\\ninstruction-aware Query Transformer, which extracts informative features tailored to the given\\ninstruction. InternVL [47] design a large-scale vision-language foundation model (InternVL) which\\nscales up the vision foundation model to 6 billion parameters and progressively aligns it with\\nthe LLM using web-scale image-text data from various sources. GPT-4 [7] proposes a large-scale,\\nmultimodal model which can accept image and text inputs and produce text outputs. While less\\ncapable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on\\nvarious professional and academic benchmarks. HuggingGPT [325] proposes an LLM-powered agent\\nthat leverages LLMs (eg, ChatGPT) to connect various AI models in machine learning communities\\n(eg, Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when\\nreceiving a user request, select models according to their function descriptions available in Hugging\\nFace, execute each subtask with the selected AI model, and summarize the response according to\\nthe execution results. By leveraging the strong language capability of ChatGPT and abundant AI\\nmodels in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning\\ndifferent modalities and domains. NExT-GPT [400] present an end-to-end general-purpose any-to-\\nany MM-LLM system. NExT-GPT connect an LLM with multimodal adaptors and different diffusion\\ndecoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations\\nof text, image, video, and audio. By leveraging the existing well-trained high-performing encoders\\nand decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection\\nlayers, which not only benefits low-cost training but also facilitates convenient expansion to more\\npotential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and\\nmanually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with\\ncomplex cross-modal semantic understanding and content generation.\\n3.4.2\\nMODALITY OUTPUT. With the explosive growth in the capabilities of MLLMs, the ability\\nto answer questions based on multimodal inputs and generate multimodal outputs has also seen a\\nqualitative improvement. There is also increasing attention from researchers on VQA scenarios that\\nshift from generating text results to generating multimodal results that include text.In this section,\\nwe are discussing multimodal outputs that are not scenarios like text-to-image or text-to-video,\\nwhich only generate a single modality, but rather scenarios where the answers includes text and at\\nleast one other modality of data, such as text-image output, or image-video output.In the basic VQA\\ntask, MIMOQA[328] was the first to propose the concept of multimodal output, which achieved\\nthe capability of multimodal output by transforming questions into an image-text matching task.\\nIt constructed a dual-tower model called MExBERT. The text stream, based on BERT, takes in\\nthe query and related documents to output the final text answer. The visual stream, based on\\nVGG-19, receives images related to the query and documents, outputting a relevance score between\\nthe image and text. The final insertion of the image is determined by this relevance score. Its\\ngroundbreaking introduction to multimodal output research has, however, certain limitations: 1) It\\nis necessary to screen out images related to the question. The model only needs to select and output\\nimages from the small number of screened ones. The task is relatively simple. 2) Multimodality is\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 32}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n33\\nstill limited to the image modality. 3) To simplify the issue, it is still limited to scenarios where the\\ninput images must include at least one relevant image. Based on the aforementioned limitations,\\nthe latest research has made corresponding improvements[67, 259, 401, 465, 466, 508].\\nA common workflow paradigm for implementing multimodal output is to first conduct position\\nidentification after generating a text answer to determine where to insert multimodal data. Subse-\\nquently, based on the surrounding context of the corresponding positions, candidate multimodal\\ndata is retrieved. Finally, a relevance matching model is utilized to determine the final data to\\nbe inserted. InternLM-XComposer [465] achieves multimodal output of text and images. After\\ngenerating each paragraph of text, it calls a model to determine whether to insert an image. If\\nit is determined that an image needs to be inserted, it will generate a caption of the image to be\\ninserted and search the web for candidate images, eventually allowing the model to select the most\\nrelevant image from candidate set for insertion. InternLM-XComposer2 and 2.5 [67, 466] allow\\nusers to directly input a set of candidate images on the basis of the above. MuRAR [508] has also\\nimplemented multi - modal output in RAG scenarios based on this paradigm, but it has innovated\\nthe methods of position identification and candidate set recall in RAG scenarios. It uses source\\nattribution to confirm the correspondence between the generated snippet and the retrieved snippet\\nfrom the large model input, thereby determining the insertion point, and the candidate set directly\\nuses the multimodal data associated with the retrieved snippet, simplifying the recall operation. In\\naddition, it has expanded the multimodal data from images to include tables and videos. 𝑀2𝑅𝐴𝐺\\n[259] employs an alternative paradigm to achieve multimodal output in the RAG scenario. It uses\\nthe user’s query to simultaneously recall associated text elements and images. Then, based on the\\nassociations of the images and text elements in the original document, they are refined. Subse-\\nquently, MLLMs are employed to vectorize the images or convert them into descriptions, which are\\ninput into the generative model in the form of placeholders. The output generates answer text and\\na simple description placeholder for the associated image. Finally, through a chain-of-thought(COT)\\nprocess, the placeholders are converted into actual images. NExT-GPT [401] employs an entirely\\ndifferent and novel paradigm. It directly trains a unified multimodal large model, unifying the\\nreasoning and generation process, and directly generates multimodal data including text, images,\\nvideos, etc., through the model [401].\\n4\\nDataset for MRAG\\nTo evaluate the general capabilities of MRAG systems in real-world multimodal understanding and\\nknowledge-based question-answering tasks, we curated a collection of existing datasets designed to\\ncomprehensively evaluate the MRAG pipeline. These datasets are categorized into two classes: (1)\\nRetrieval & Generation-Joint Components, which evaluate the synergy of retrieval and generation\\nby requiring systems to retrieve external knowledge and generate accurate responses; and (2)\\nGeneration, focusing solely on the model’s ability to produce contextually accurate outputs without\\nexternal retrieval. This categorization enables a detailed evaluation of MRAG systems’ strengths\\nand limitations in diverse scenarios.\\n4.1\\nDataset for Retrieval & Generation\\nDatasets for Retrieval & Generation in MRAG are designed to evaluate end-to-end systems capable\\nof retrieving relevant knowledge from multimodal sources (e.g., text, images, videos) and generating\\naccurate responses. These datasets evaluate the synergistic integration of retrieval and generation,\\nfocusing on the system’s ability to dynamically utilize external knowledge to improve response\\nquality and relevance. In this section, we introduce key benchmarks designed for diverse evaluation\\nof Retrieval & Generation tasks. Figure 9 provides an overview of existing benchmarks, while Table\\n1 summarizes the statistics of selected representative datasets.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 33}, page_content='34\\nTrovato et al.\\nDataset for\\nRetrieval & Generation\\nVisual Advertisement\\nAds [138]\\nVideo Understanding\\n& Reasoning\\nKnowIT VQA [97], SOK-Bench [365]\\nVisual Commonsense\\n& Reasoning\\nVCR [448], VisualCOMET [291]\\nComprehensive\\nKB-VQA [375], FVQA [374], KVQA [322], OK-VQA [263], S3VQA [140], ManyModalQA\\n[114], MultiModalQA [345], MIMOQA [329], A-OKVQA [321], WebQA [28], ViQuAE [177],\\nInfoSeek [42], Encyclopedic-VQA [272], MMSearch [147], MRAG-bench [126], MRAMG-\\nBench [435]\\nFig. 9. Categories of MRAG dataset for retrieval & generation.\\nTable 1. Summary of dataset for retrieval & generation.\\nDataset\\nTime\\nStatistics\\nComprehensive\\nKB-VQA [375]\\n2015\\n2,402 questions, 700 images, 1 knowledge bases.\\nFVQA [374]\\n2017\\n5,826 questions, 2,190 images, 3 knowledge bases.\\nKVQA [322]\\n2019\\n183,007 question-answer pairs about 18,880 unique entities contained within 24,602 images.\\nOK-VQA [263]\\n2019\\n14,055 questions, 10 scenarios.\\nS3VQA [140]\\n2021\\n6,765 question-image pairs.\\nManyModalQA [114]\\n2020\\n10,190 questions with 2,873 image, 3,789 text, and 3,528 table.\\nMultiModalQA [345]\\n2021\\n29,918 questions that requires knowledge from text, tables, and images (35.7% require cross-modality\\nreasoning).\\nMIMOQA [329]\\n2021\\n56,693 QA pairs, with 401,182 images.\\nA-OKVQA [321]\\n2022\\n24,903 multiple-choice questions.\\nWebQA [28]\\n2022\\n24,929 image-based and 24,343 text-based questions.\\nViQuAE [177]\\n2022\\n3.7K questions paired with images. A Knowledge base composed of 1.5M Wikipedia articles paired with\\nimages.\\nInfoSeek [42]\\n2023\\n8.9K human-written and 1.3M semi-automated questions, 9 image classification and retrieval datasets.\\nEncyclopedic-VQA\\n[272]\\n2023\\n1M Image-Question-Answer triplets derived from 221k textual QA pairs from 16.7k different categories.\\nEach QA pair is combined with (up to) 5 images. 514k unique images. 15k textual single-hop questions,\\n25k multi-answer questions, and 22k two-hop questions.\\nMMSearch [147]\\n2024\\n2,901 unique images, 300 manually collected queries spanning 14 subfields.\\nMRAG-bench [126]\\n2024\\n1,353 multiple-choice questions, 16,130 images, 9 scenarios.\\nMRAMG-Bench\\n[435]\\n2025\\n4,800 QA pairs across three distinct domains, containing 4,346 documents and 14,190 images, with tasks\\ncategorized into three difficulty levels.\\nVisual Commonsense Reasoning\\nVCR [448]\\n2019\\n290k multiple choice QA problems derived from 110k movie scenes.\\nVisualCOMET [291]\\n2020\\n1,465,704 commonsense inferences over 59,356 images, and 139,377 distinct events.\\nVideo Understanding & Commonsense Reasoning\\nKnowIT VQA [97]\\n2020\\n24,282 human-generated QA pairs about a popular sitcom.\\nSOK-Bench [365]\\n2024\\n44K QA pairs covers over 12 types of questions, sourcing from about 10K situations. Each question is\\naccompanied by two types of answers: a direct answer and a set of four multiple-choice options.\\nVisual Advertisement\\nAds [138]\\n2017\\n202,090 questions from 64,832 image ads and 3,477 video ads.\\nEarly knowledge-based datasets include KB-VQA [375] and FVQA [374], which rely on closed\\nknowledge. FVQA, for instance, uses a fixed knowledge graph, making questions straightforward\\nonce the knowledge is known, with minimal reasoning required. KVQA [322] focuses on images in\\nWikipedia articles, primarily testing named entity recognition and Wikipedia knowledge retrieval\\nrather than commonsense reasoning. OK-VQA [263] and A-OKVQA [321] evaluate multimodal\\nreasoning using external knowledge, with A-OKVQA introducing \"rationale\" annotations to better\\nevaluate knowledge acquisition and reasoning. S3VQA [140] extends OK-VQA by requiring object\\ndetection and web queries, but like OK-VQA, it often reduces to single retrieval tasks rather than\\ncomplex reasoning. MultiModalQA [345] pioneers complex questions requiring reasoning across\\nsnippets, tables, and images, focusing on cross-modal knowledge extraction. However, its template-\\nbased questions simplify the task to filling in blanks with modality-specific answering mechanisms.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 34}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n35\\nManyModalQA [114] also uses snippets, images, and tables but emphasizes answer modality\\nchoice over knowledge aggregation. MIMOQA [329] introduces “Multimodal Input Multimodal\\nOutput”, requiring both text and image selections to enhance understanding. WebQA [28] is a\\nmanually crafted, multi-hop multimodal QA dataset that retrieves visual content but provides\\nonly textual answers, relying solely on MLLMs for reasoning, making it unsuitable for models\\ndependent on linguistic context. ViQuAE [177] focuses on answering questions about named\\nentities grounded in a visual context using a Knowledge Base. InfoSeek [42] and Encyclopedic-VQA\\n[272] target knowledge-based questions beyond common sense knowledge, with Encyclopedic-\\nVQA using model-generated annotations. MMSearch [147] evaluates MLLMs as multimodal search\\nengines, focusing on image-to-image retrieval. Compared with previous works, MRAG-bench\\n[126] evaluates MLLMs in utilizing vision-centric retrieval-augmented knowledge, identifying\\nscenarios where visual knowledge outperforms textual knowledge. MRAMG-Bench [435] evaluates\\nanswers combining text and images, leveraging multimodal data within a corpus. Additionally,\\nVCR [448] and VisualCOMET [291], derived from movie scenes, evaluate Visual Commonsense\\nReasoning. KnowIT VQA [97] and SOK-Bench [365] focus on video understanding and reasoning\\ntask, combining visual, textual, and temporal reasoning with knowledge-based questions. Ads [138]\\nproposes an automatic advertisement understanding task, featuring rich annotations on topics,\\nsentiments, and persuasive reasoning.\\n4.2\\nDataset for Generation\\nThe Generation category evaluates a model’s intrinsic capacity to generate contextually accurate\\noutputs based solely on its pre-trained knowledge and internal reasoning, without external retrieval.\\nThis evaluation isolates the generation component, providing insights into the model’s foundational\\nlanguage understanding capabilities. It enables a detailed analysis of MRAG systems’ strengths\\nand limitations across diverse scenarios. In this section, we provide an overview of representative\\nbenchmarks developed for various evaluation of Generation tasks. The existing benchmarks are\\nsystematically organized in Figure 10, and the statistics of selected representative benchmarks are\\nsummarized in Table 2.\\n4.2.1\\nComprehensive. To rigorously evaluate the capabilities of MLLMs, a diverse range of evalua-\\ntion benchmarks has been developed. These benchmarks are designed to test various dimensions\\nof model performance. By utilizing these benchmarks, researchers can systematically quantify\\nthe strengths and limitations of MLLMs, ensuring their alignment with real-world applications\\nand user expectations. This evaluation framework not only supports the iterative improvement of\\nMLLMs but also provides a standardized basis for comparing models in terms of perceptual and\\nreasoning abilities.\\nVQA v2 [104], an early benchmark with 453K annotated QA pairs, focuses on open-ended\\nquestions with concise answers. VizWiz [112], introduced around the same time, includes 8K\\nQA pairs from visually impaired individuals’ daily lives, addressing real-world needs of disabled\\nusers. NLVR2 [336] explores multi-image vision capabilities by evaluating captions against image\\npairs. However, these benchmarks often fail to assess modern MLLMs’ emergent capabilities, such\\nas advanced reasoning. Recent efforts like LVLM-eHub [411], MDVP [212], and LAMM [430]\\ncompile extensive datasets for comprehensive evaluation, revealing that while MLLMs excel in\\ncommonsense tasks, they lag in image classification, OCR, VQA, large-scale counting, fine-grained\\nattribute differentiation, and precise object localization. Fine-tuning can mitigate some of these\\nlimitations.\\nResearchers are developing specialized benchmarks to address the limitations of traditional\\nevaluations for MLLMs. Notable examples include MME [29], which covers 14 perception and\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 35}, page_content='36\\nTrovato et al.\\nDataset for\\nGeneration\\nMultidisciplinary\\nScienceQA [243], MMMU [445], CMMU [117], CMMMU [457], MMMU-Pro [446]\\nConversational QA\\nSparklesDialogue [134], SciGraphQA [192], ConvBench [225], MMDU [235]\\nIndustry\\nMME-Industry [429]\\nVideo Understanding\\nTGIF-QA [141], ActivityNet-QA [442], EgoSchema [262], Video-MME [85], MVBench [188],\\nMMBench-Video [77], MLVU [496], LVBench [376], Event-Bench [71], VNBench [488], TempCompass\\n[232], MovieChat [332]\\nMathematics\\nMathVista [242], We-Math [300], Math-Vision [372], Olympiadbench [115], MathVerse [470]\\nStructural Document\\nFigureQA [160], DocVQA [268], VisualMRC [348], ChartQA [266], InfographicVQA [267], ChartBench\\n[414], SciGraphQA [192], MMC-Benchmark [217], MP-DocVQA [358], ChartX [404], DocGenome\\n[403], CharXiv [387], MMLongBench-Doc [258], ComTQA [486], Web2Code [447], VisualWebBench\\n[221], SciFIBench [313]\\nOptical Character\\nRecognition\\nTextVQA [327], OCR-VQA [275], WebSRC [41], OCRBench [233], VCR [473], SEED-Bench-2-Plus\\n[178]\\nComprehensive\\nVQA v2 [104], NLVR2 [336], VizWiz [112], MME [29], Visit-Bench [21], Touchstone [17], MM-Vet\\n[439], InfiMM-Eval [113], Q-Bench [398], Seed-Bench [180], SEED-Bench-2 [179], SEED-Bench-2-Plus\\n[178], LVLM-eHub [411], LAMM [430], MMT-Bench [431], RealWorldQA [4], WV-Bench [245], MME-\\nRealWorld [480], MMStar [38], CV-Bench [359], MDVP [212], FOCI [101], MMVP [360], V*-Bench\\n[399], MME-RealWorld [480], Visual COT [324], Mementos [381], MIRB [482], ReMI [162], MuirBench\\n[368], VEGA [493], MMBench [230], BLINK [86]\\nFig. 10. Categories of MRAG dataset for generation.\\ncognition tasks; MMBench [230], featuring 20 ability dimensions, including object localization\\nand social reasoning; and SEED-Bench [180], which focuses on multiple-choice questions. SEED-\\nBench-2 [179] expanded the scope to 24K QA pairs, including the evaluation of both text and image\\ngeneration. MMT-Bench [431] further scaled up to 31K QA pairs across diverse scenarios. Common\\nfindings reveal that model performance improves with scale, but challenges persist in fine-grained\\nperception tasks (e.g., spatial localization), chart and visual mathematics comprehension, and\\ninterleaved image-text understanding. Open-source MLLMs have shown rapid progress, often\\nmatching or surpassing closed-source models.\\nReal-world usage scenarios are critical for evaluating model performance in practical applications.\\nBenchmarks like RealWorldQA [4] evaluates spatial understanding capabilities sourced from real-\\nlife scenarios, while BLINK [86] highlights tasks such as visual correspondence and multi-view\\nreasoning that challenge current MLLMs despite being intuitive for humans. WV-Bench [245] and\\nVisit-Bench [21] emphasize human preferences and instruction-following capabilities, whereas\\nV*-Bench [399] evaluates high-resolution image processing and correct visual details through\\nattribute recognition and spatial reasoning tasks. MME-RealWorld [480] enhances quality and\\ndifficulty with extensive annotated QA pairs and high-resolution images. These benchmarks reveal\\nthat fine-grained perception tasks remain challenging for models, while artistic style recognition\\nand relative depth perception are relatively stronger. Although closed-source models like GPT-4o\\noutperform others, human performance still surpasses general models significantly.\\nMany studies simplify evaluation into binary or multi-choice problems for easier quantification,\\nbut this approach overlooks the importance of the reasoning process, which is critical for under-\\nstanding model capabilities. To address this, some works use open-ended generation and LLM-based\\nevaluators, though these face challenges with inaccurate LLM scoring. For instance, MM-Vet [439]\\nemploys diverse question formats to assess integrated vision-language capabilities, while Touch-\\nstone [17] emphasizes real-world dialogue evaluation, arguing that multiple-choice questions are\\ninsufficient for evaluating multimodal dialogue capabilities. InfiMM-Eval [113] evaluates models on\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 36}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n37\\ndeductive, abductive, and analogical reasoning across tasks, including intermediate reasoning steps,\\naligning with practical scenarios like mathematical problem-solving. These benchmarks highlight\\nthe strengths and limitations of MLLMs in complex tasks. Closed-source models excel in reasoning\\nbut struggle with complex localization, structural relationships, charts, and visual mathematics.\\nHigh-resolution data improves recognition of small objects, dense text, and fine-grained details.\\nWhile Chain-of-Thought (CoT) strategies significantly boost reasoning in closed-source models,\\ntheir impact on open-source models remains limited.\\nThe development of multimodal benchmarks emphasizes continuous refinement to accurately as-\\nsess model capabilities. MMStar [38] addresses data leakage by curating 1.5K visually-dependent QA\\npairs, while CV-Bench [359] tackles the scarcity of vision-centric benchmarks with 2.6K manually-\\ninspected samples for 2D/3D understanding. FOCI [101] evaluates MLLMs using domain-specific\\nsubsets and supplementary classification datasets, revealing challenges in fine-grained perception.\\nMMVP [360] identifies 9 distinct patterns in CLIP-based models, showing MLLMs’ struggles with\\nvisual details, with only Gemini and GPT-4V performing above random guessing. Q-Bench [398]\\nevaluates low-level attribute perception, highlighting GPT-4V’s near-human performance. Visual-\\nCOT [324] introduces visual chain-of-thought prompts to enhance MLLMs’ focus on specific image\\nregions. To further upgrading vision capabilities on multiple image understanding, Mementos [381]\\nevaluates sequential image understanding, while MIRB [482] focuses on multi-image reasoning\\nacross perception, visual knowledge, and multi-hop reasoning tasks. ReMI [162] designs 13 tasks\\nwith diverse image relationships and input formats, and MuirBench [368] includes 12 multi-image\\nunderstanding tasks with unanswerable variants for robust assessment. VEGA [493] is specifi-\\ncally designed to evaluate interleaved image-text comprehension. The task requires models to\\nidentify relevant images and text while filtering out irrelevant information to arrive at the correct\\nanswer. Evaluation results reveal that even advanced proprietary MLLMs, such as GPT-4V and\\nGemini 1.5 Pro, achieve only modest performance, highlighting significant room for improvement\\nin interleaved information processing capabilities.\\n4.2.2\\nOptical Character Recognition (OCR). Multimodal benchmarks are increasingly focusing\\non the evaluation of Optical Character Recognition (OCR) tasks, driving progress in document\\nunderstanding. Early benchmarks like TextVQA [327] and OCR-VQA [275] evaluated standard\\ntext recognition, while WebSRC [41] introduces advanced structural reasoning tasks like web\\npage layout interpretation. SEED-Bench-2-Plus [178] and OCRBench [233] expanded evaluation to\\ndiverse data types, including charts, maps, and web pages, showing models achieving near state-of-\\nthe-art performance in recognizing various OCR text. VCR [473] addresses OCR task with partially\\nobscured text embedded in images, requiring content reconstruction. Despite advancements, many\\nMLLMs struggle with fine-grained OCR tasks. While models like GPT-4V perform well, they lag\\nbehind specialized OCR models. Performance varies significantly by data type, with knowledge\\ngraphs and maps posing greater challenges than simpler formats like charts, suggesting potential\\nimprovements through data-specific optimization or dedicated OCR integration.\\n4.2.3\\nStructural Document. Structural documents, including charts, HTML web content, and vari-\\nous document formats, play a critical role in practical applications due to their ability to efficiently\\nconvey complex information. These data types are characterized by their highly structured nature\\nand information density, distinguishing them from natural images. Unlike images, which rely on\\nvisual patterns and textures, structural documents require models to comprehend intricate layouts,\\nspatial relationships, and semantic connections between embedded elements such as text, tables,\\nand graphical components.\\nTo advance models capable of understanding and reasoning with such data, several benchmarks\\nhave been proposed for different types of structural documents. Early dataset FigureQA [160]\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 37}, page_content='38\\nTrovato et al.\\nintroduces a visual reasoning corpus with synthetic images and scientific-style figures, focusing on\\nrelationships between plot elements. ChartQA [266] emphasizes VQA with charts, ranging from\\ntasks that require both data extraction and math reasoning. ChartX [404] collects a comprehensive\\ndataset with 22 topics, 18 chart types, and 7 tasks, incorporating multiple modalities. VisualMRC\\n[348] targets visual machine reading comprehension, emphasizing natural language understanding\\nand generation. ChartBench [414] evaluates chart comprehension and data reliability through\\ncomplex reasoning. MMC-Benchmark [217] provides a human-annotated benchmark to assess\\nMLLMs on visual chart understanding tasks like chart information extraction, reasoning, and\\nclassification. Web2Code [447] introduces a webpage-to-code dataset for instruction tuning and\\nan evaluation framework to assess MLLMs’ webpage understanding and HTML code translation\\ncapabilities. VisualWebBench [221] evaluates MLLMs on various web tasks at website, element,\\nand action levels. Many charts lack data point annotations, necessitating MLLMs to infer values\\nusing chart elements. ComTQA [486] introduces a table VQA benchmark for perception and\\ncomprehension tasks, while DocVQA [268] focuses on document image QA with an emphasis on\\ninformation extraction tasks. InfographicVQA [267] targets understanding infographics images,\\nwhich are designed to present information concisely. Infographics exhibit diverse layouts and\\nstructures, requiring basic reasoning and arithmetic skills. As MLLMs advance, benchmarks now\\nfocus on complex chart and document understanding. For instance, DocGenome [403] analyzes\\nscientific papers, covering tasks like information extraction, layout detection, VQA, and code\\ngeneration. CharXiv [387] targets challenging charts from scientific papers, while MP-DocVQA\\n[358] extends DocVQA to multi-page scenario, where questions are constructed based on multi-\\npage documents instead of single page. MMLongBench-Doc [258] focuses on long document\\nunderstanding, averaging 47.5 pages. SciGraphQA [192] is a synthetic dataset with 295K QA\\ndialogues about academic graphs, generated using Palm-2 from CS/ML ArXiv papers. SciFIBench\\n[313] benchmarks scientific figure interpretation, using adversarial filtering for negative examples\\nand human verification for quality assurance.\\nDespite advancements, a performance gap persists between proprietary and open-source models\\non conventional benchmarks. Current MLLMs continue to face challenges in reasoning tasks and\\nlong-context document comprehension, particularly in interpreting extended multimodal contexts,\\nwhich remains a critical limitation.\\n4.2.4\\nMathematics. Visual math problem-solving is key to evaluating MLLMs, leading to the\\ndevelopment of specialized benchmarks. MathVista [242] pioneered this effort by aggregating 28\\nexisting datasets and introducing 3 new ones, featuring diverse tasks like logical, algebraic, and\\nscientific reasoning with various visual inputs. Subsequent benchmarks, such as Math-Vision [372]\\nand OlympiadBench [115], introduced more complex tasks and fine-grained evaluation methods.\\nWe-Math [300] decomposes problems into sub-problems to assess fundamental understanding,\\nwhile MathVerse [470] further evaluates MLLMs’ comprehension of math diagrams by transforming\\nproblems into six versions with varying proportions of visual and textual content.\\nDespite promising results from MLLMs, significant challenges remain. existing MLLMs often\\nstruggle with interpreting complex diagrams, rely heavily on textual cues, and address composite\\nproblems through memorization rather than underlying reasoning. These limitations highlight the\\nneed for further development in MLLM capabilities.\\n4.2.5\\nVideo Understanding. Traditional video-QA benchmarks like TGIF-QA [141] and ActivityNet-\\nQA [442] are domain-specific, focusing on tasks related to human activities. With advancements in\\nMLLMs, new benchmarks have emerged to address more complex video understanding challenges.\\nVideo-MME [85] explores diverse video domains with multimodal inputs and manual annotations,\\nwhile MVBench [188] reannotates existing datasets using ChatGPT. MMBench-Video [77] features\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 38}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n39\\nfree-form questions for short to medium-length videos. Benchmarks like MLVU [496], LVBench\\n[376], Event-Bench [71], and VNBench [488] emphasize long-video understanding, testing models\\non extended multimodal contexts. VNBench [488] introduces a synthetic framework for evaluating\\ntasks like retrieval and ordering, by inserting irrelevant images or text into videos. Specialized\\nbenchmarks like EgoSchema [262] focus on egocentric videos. TempCompass [232] evaluates\\nfine-grained temporal perception, and MovieChat [332] targets long videos but often reduces\\ntasks to short-video problems. Current MLLMs, especially open-source ones, face challenges with\\nlong-context processing and temporal perception, underscoring the need for improved capabilities\\nin these areas.\\n4.2.6\\nIndustry. The absence of a comprehensive benchmark for evaluating MLLMs across diverse\\nindustry verticals has limited understanding of their applicability in specialized real-world scenarios.\\nTo address this gap, MME-Industry [429] was developed specifically for industrial applications,\\ncovering over 21 industrial sectors such as power generation, electronics manufacturing, textile\\nproduction, steel, and chemical processing. Domain experts from each sector meticulously annotated\\nand validated test cases, ensuring the benchmark’s reliability, accuracy, and practical relevance.\\nMME-Industry thus serves as a robust tool for assessing MLLMs in industrial contexts.\\n4.2.7\\nConversational QA. Current MLLMs are primarily designed for multi-round chatbot inter-\\nactions, yet most benchmarks focus on single-round QA tasks. To better align with real-world\\nconversational scenarios, multi-round QA benchmarks have been developed to simulate human-AI\\ninteractions with extended contextual histories. SparklesDialogue [134] evaluates conversational\\nproficiency across multiple images and dialogue turns, featuring flexible text-image interleaving\\nwith two rounds and four images per instance. SciGraphQA [192] constructs multi-turn QA con-\\nversations based on scientific graphs from Arxiv papers, emphasizing complex scientific discourse.\\nConvBench [225] assesses perception, reasoning, and creation capabilities across individual rounds\\nand overall conversations, revealing that MLLMs’ reasoning and creation failures often stem from\\ninadequate fine-grained perception. MMDU [235] engages models in multi-turn, multi-image con-\\nversations, with up to 20 images and 27 turns, highlighting that the performance gap between\\nopen-source and closed-source models is largely due to limited conversational instruction tuning\\ndata. These benchmarks collectively enhance the evaluation of MLLMs in complex, real-world\\ninteraction scenarios.\\n4.2.8\\nMultidisciplinary. The mastery of multidisciplinary knowledge is a key indicator of a model’s\\nexpertise, and several benchmarks have been developed to evaluate this capability. ScienceQA\\n[243] comprises scientific questions annotated with lectures and explanations, designed to facilitate\\nchain-of-thought evaluation. It spans grade-level knowledge across diverse domains. MMMU [445]\\npresents a more challenging college-level benchmark across diverse subjects, including engineering,\\nart and design, business, science, humanities, social science, and medicine. Its question format\\nextends beyond single image-text pairs to include interleaved text and images. Similarly, CMMU\\n[117] and CMMMU [457] provide Chinese domain-specific benchmarks for grade-level and college-\\nlevel knowledge, respectively. MMMU-Pro [446] enhances MMMU with a more robust version for\\nadvanced evaluation.\\nTable 2. Summary of dataset for generation.\\nDataset\\nTime\\nStatistics\\nComprehensive\\nVQA v2 [104]\\n2017\\ncontains more than 443K training, 214K validation and 453K test image-question pairs.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 39}, page_content='40\\nTrovato et al.\\nNLVR2 [336]\\n2018\\ncontains 107,292 examples of English sentences paired with web photographs, including 29,680 unique\\nsentences and 127,502 images. The task is to determine whether a natural language caption is true about\\na pair of photographs.\\nVizWiz [112]\\n2018\\ncontains 20,000 training, 3,173 validation, and 8,000 test sets of visual questions originating from blind\\npeople.\\nMME [29]\\n2023\\nmeasures both perception and cognition abilities on a total of 14 subtasks\\nVisit-Bench [21]\\n2023\\ncomprising 592 instances and 1,159 public images. The instances are either from 45 newly assembled\\ninstruction families or reformatted from 25 existing datasets. 10 instruction families cater to multi-image\\nquery scenarios.\\nTouchstone [17]\\n2023\\n908 questions covering 27 subtasks. The highest proportion of questions pertains to recognition, ac-\\ncounting for about 44.1%, followed by comprehension questions at 29.6%. The proportions of the\\nother categories are 15.3% for basic descriptive ability, 7.4% for visual storytelling ability, and 3.6% for\\nmulti-image analysis ability.\\nMM-Vet [439]\\n2023\\ndefines 6 core vision-language capabilities and examines the 16 integrations of interest derived from\\ntheir combinations. It contains 200 images, and 218 questions (samples), all paired with their respective\\nground truths.\\nInfiMM-Eval [113]\\n2023\\nIt consists of 279 manually curated reasoning questions, associated with a total of 342 images. The\\ndataset is categorized into three reasoning paradigms: deductive, abductive, and analogical reasoning.\\n49 questions pertain to abductive reasoning, 181 require deductive reasoning, and 49 involve analogical\\nreasoning. Furthermore, the dataset is divided into two folds based on reasoning complexity, with 108\\nclassified as “High” reasoning complexity and 171 as “Moderate” reasoning complexity.\\nQ-Bench [398]\\n2023\\nconsists of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its\\nlow-level attributes.\\nSeed-Bench [180]\\n2023\\nconsists of 19K multiple-choice questions with accurate human annotations, which spans 12 evaluation\\ndimensions including the comprehension of both the image and video modality.\\nSEED-Bench-2 [179]\\n2024\\ncomprises 24K multiple-choice questions with accurate human annotations, which span 27 dimensions,\\nincluding the evaluation of both text and image generation.\\nLVLM-eHub [411]\\n2024\\ncontains 42 datasets in our LVLM-eHub. The sizes of specific datasets are 109.8K, 29.5K, 177.2K, 67.3K, and\\n8.9K for visual perception, knowledge acquisition, reasoning, commonsense, and object hallucination,\\nrespectively.\\nLAMM [430]\\n2024\\nevaluate 9 common image tasks, using a total of 11 datasets with over 62,439 samples, and 3 common\\npoint cloud tasks, by utilizing 3 datasets with over 12,788 data samples.\\nMMT-Bench [431]\\n2024\\ncomprises 31,325 meticulously curated multi-choice visual questions from various multimodal scenarios,\\ncovering 32 core meta-tasks and 162 subtasks in multimodal understanding.\\nRealWorldQA [4]\\n2024\\nconsists of 765 images, with a question and easily verifiable answer for each image.\\nWV-Bench [245]\\n2024\\nconstructed by selecting 500 high-quality samples from 8,000 user submissions in WV-ARENA.\\nMME-RealWorld\\n[480]\\n2024\\nconstructed by collecting more than 300K images from public datasets and the Internet, filtering 13,366\\nhigh-quality images for annotation and contributing to 29,429 question-answer pairs that cover 43\\nsubtasks across 5 real-world scenarios.\\nMMStar [38]\\n2024\\ncontains 1,500 challenging samples, each rigorously validated by humans. It identify 6 core capabili-\\nties (i.e., coarse perception, fine-grained perception, instance reasoning, logical reasoning, science &\\ntechnology, mathematics) along with 18 specific dimensions.\\nCV-Bench [359]\\n2024\\nprovides 2,638 manually-inspected examples, and formulate natural language questions that evaluates\\n2D understanding via spatial relationships & object counting, and 3D understanding via depth order &\\nrelative distance.\\nMDVP [212]\\n2024\\ncontains 1.6M unique image-visual prompt-text instruction-following samples, including natural images,\\ndocument images, OCR images, mobile screenshots, web screenshots, and multi-panel images.\\nFOCI [101]\\n2024\\nconstructed from 5 popular classification datasets for different domains: 1) aircraft contains images of\\n100 different aircraft types; 2) flowers contains images of 102 different flower species; 3) food covers 101\\ndishes; 4) pets contains images of 37 cat and dog breeds. 5) cars covers 196 car models. Additionally,\\nFOCI create 4 domain subsets for animals (1322 classes), plants (957 classes), food (563 classes), and\\nman-made objects (2631 classes).\\nMMVP [360]\\n2024\\nsummarizes 9 prevalent patterns of the CLIP-blind pairs, such as “orientation”, “counting”, and “view-\\npoint”. Utilizing the collected CLIP-blind pairs, MMVP design 150 pairs with 300 questions.\\nV*-Bench [399]\\n2024\\nIt is built based on 191 high-resolution images with an average image resolution of 2246×1582. V*-\\nBench contains two sub-tasks: attribute recognition and spatial relationship reasoning. The attribute\\nrecognition task has 115 samples. The spatial relationship reasoning task has 76 samples.\\nMME-RealWorld\\n[480]\\n2024\\ncontains 29,429 question-answer pairs that cover 43 subtasks across 5 real-world scenarios, where each\\none has at least 100 questions.\\nVisual COT [324]\\n2024\\n438k visual chain-of-thought question-answer pairs spans across five distinct domains, each consisting\\nof a question, an answer, and an intermediate bounding box as CoT contexts. About 98k question-answer\\npairs include extra detailed reasoning steps.\\nMementos [381]\\n2024\\nIt consists of 4,761 image sequences with varying episode lengths, encompassing diverse scenarios from\\ndailylife, robotics tasks, and comic-style storyboards. Each sequence is paired with a human-annotated\\ndescription of the primary objects and their behaviors within the sequence.\\nMIRB [482]\\n2024\\ncomprises 925 samples with average image number of 3.78, constructed from four distinct categories of\\nmulti-image understanding: perception, visual world knowledge, reasoning, and multi-hop reasoning.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 40}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n41\\nReMI [162]\\n2024\\nIt consists of 13 tasks that span a range of domains and properties. The tasks require reasoning over\\nup to six images, with all tasks requiring reasoning over at least two images. The images comprise a\\nvariety of heterogeneous image types.\\nMuirBench [368]\\n2024\\nIt consists of 12 distinctive multi-image understanding tasks that involve 10 categories of multi-image\\nrelations, comprising 11,264 images and 2,600 multiple-choice questions, with an average of 4.3 images\\nper instance.\\nVEGA [493]\\n2024\\ncontains 50k scientific literature entries, over 200k question-and-answer pairs, and a rich trove of\\n400k images. It includes the Interleaved Image-Text Comprehension subset, which is segmented into\\ntwo categories based on token length: one supports up to 4,000 tokens, while the other extends to\\n8,000 tokens. Here, images are equated to 256 tokens each. Both categories offer roughly 200k training\\ninstances and approximately 700 test samples.\\nMMBench [230]\\n2025\\ncontains 3,217 multiple-choice questions covering a diverse spectrum of 20 fine-grained skills.\\nBLINK [86]\\n2025\\nreformats 14 classic computer vision tasks, and contains 3,807 multiple-choice questions across 7.3K\\nimages, paired with single or multiple images and visual prompting.\\nOptical Character Recognition\\nTextVQA [327]\\n2019\\ncontains 45,336 questions asked by humans on 28,408 images that require reasoning about text to answer.\\nEach question-image pair has 10 ground truth answers provided by humans.\\nOCR-VQA [275]\\n2019\\ncomprises of 207,572 images of book covers and contains more than 1 million question-answer pairs\\nabout visual question answering by reading text in images.\\nWebSRC [41]\\n2021\\nIt consists of 400K question-answer pairs, which are collected from 6.4K web pages. Along with the QA\\npairs, corresponding HTML source code, screenshots, and metadata are also provided in the dataset.\\nEach question in WebSRC requires a certain structural understanding of a web page to answer.\\nOCRBench [233]\\n2024\\nincludes 1000 question-answer pairs, which is consist of five components: text recognition, scene\\ntext-centric VQA, document-oriented VQA, key information extraction, and handwritten mathematical\\nexpression recognition.\\nVCR [473]\\n2024\\nIt comprise 2.11M English and 346K Chinese entities, featuring captions in both languages across ‘easy’\\nand ‘hard’ difficulty levels.\\nSEED-Bench-2-Plus\\n[178]\\n2024\\ncomprises 2.3K multiple-choice questions with precise human annotations, spanning three broad\\ncategories: Charts, Maps, and Webs.\\nStructural Document\\nFigureQA [160]\\n2017\\nits training set contains 100,000 images with 1.3 million questions; the validation and test sets each\\ncontain 20,000 images with over 250, 000 questions. The images are synthetic, scientific-style figures\\nfrom five classes: line plots, dot-line plots, vertical and horizontal bar graphs, and pie charts.\\nDocVQA [268]\\n2021\\ncontains 50,000 question-answer pairs with 12,767 document images sourced from documents in UCSF\\nIndustry Documents Library.\\nVisualMRC [348]\\n2021\\nIt contains 30562 pairs of a question and an abstractive answer for 10,197 document images sourced\\nfrom multiple domains of webpages.\\nChartQA [266]\\n2022\\nconsists of 20,882 charts curated from four different online sources. It covers 9,608 human-written\\nquestions focusing on logical and visual reasoning questions, and generates another 23,111 questions\\nautomatically from human-written chart summaries.\\nInfographicVQA\\n[267]\\n2022\\ncomprises 30,035 questions over 5,485 images. Questions in the dataset include questions grounded on\\ntables, figures and visualizations and questions that require combining multiple cues.\\nChartBench [414]\\n2023\\nincludes over 68k charts and more than 600k high-quality instruction data, covering 9 major categories\\nand 42 subcategories of charts. 5 chart question-answering tasks to assess the models’ cognitive and\\nperceptual abilities.\\nSciGraphQA [192]\\n2023\\ngenerate 295K samples of open-vocabulary multi-turn question-answering dialogues about the graphs.\\nAs context, it provided the text-only Palm-2 with paper title, abstract, paragraph mentioning the\\ngraph, and rich text contextual data from the graph itself, obtaining dialogues with an average 2.23\\nquestion-answer turns for each graph.\\nMMC-Benchmark\\n[217]\\n2023\\nconsists of 2k QA pairs with 1,063 unique images, accompanied by 1,275 multiple-choice questions and\\n851 free-form questions. The average length of the questions is 15.6.\\nMP-DocVQA [358]\\n2023\\ncontains 46K questions and 6K documents, with a total of 48K pages (images). On average, each question\\nis associated with 8.27 pages.\\nChartX [404]\\n2024\\ncollected 48K multi-modal chart data covering 22 topics, 18 chart types, and 7 tasks. Each chart data\\nwithin this dataset includes 4 modalities, including image, Comma-Separated Values (CSV), python\\ncode, and text description. 7 chart tasks is classified into perception tasks and cognition tasks.\\nDocGenome [403]\\n2024\\nconstructed by annotating 500K scientific documents from 153 disciplines in the arXiv open-access\\ncommunity. It contains structure data from all modalities including 13 layout attributes along with their\\nLATEX source codes. It provides 6 logical relationships between different entities within each scientific\\ndocument. It covers various document-oriented tasks.\\nCharXiv [387]\\n2024\\ninvolves 2,323 real-world charts handpicked from scientific papers spanning 8 major subjects published\\non arXiv, and produces more than 10K questions.\\nMMLongBench-Doc\\n[258]\\n2024\\ncomprising 1,082 expert-annotated questions. It is constructed upon 135 lengthy PDF-formatted docu-\\nments with an average of 47.5 pages and 21,214 textual tokens. 494 questions are single-page questions\\n(with one evidence page). 365 questions are cross-page questions requiring evidence across multiple\\npages. 223 questions are designed to be unanswerable for detecting potential hallucinations.\\nComTQA [486]\\n2024\\ncomprises a total of 9,070 QA pairs across 1,591 images. It contains challenging questions, such as\\nmultiple answers, mathematical calculations, and logical reasoning.\\nWeb2Code [447]\\n2024\\ncontains a total of 1179.7k webpage based instruction-response pairs.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 41}, page_content='42\\nTrovato et al.\\nVisualWebBench\\n[221]\\n2024\\nconsists of 7 tasks, and comprises 1.5K human-curated instances from 139 real websites, covering 87\\nsub-domains.\\nSciFIBench [313]\\n2024\\nconsists of 2000 multiple-choice scientific figure interpretation questions split between two tasks across\\n8 categories. The questions are curated from arXiv paper figures and captions.\\nMathematics\\nMathVista [242]\\n2023\\nincorporates 28 existing multimodal datasets, including 9 math-targeted question answering (MathQA)\\ndatasets and 19 VQA datasets. In addition, it creates three new datasets (i.e., IQTest, FunctionQA,\\nPaperQA) which are tailored to evaluating logical reasoning on puzzle test figures, algebraic reasoning\\nover functional plots, and scientific reasoning with academic paper figures, respectively. It consists of\\n6,141 examples, with 736 of them being newly curated.\\nWe-Math [300]\\n2024\\nIt collect and categorize 6.5K visual math problems, spanning 67 hierarchical knowledge concepts and 5\\nlayers of knowledge granularity.\\nMath-Vision [372]\\n2024\\ncomprises 3,040 mathematical problems within visual contexts across 12 grades, selected from 19 math\\ncompetitions. It contains 1,532 problems in an open-ended format and 1,508 in a multiple-choice format.\\nAll problems encompass 16 subjects over 5 levels of difficulty.\\nOlympiadbench\\n[115]\\n2024\\nan Olympiad-level bilingual multimodal scientific benchmark, featuring 8,476 problems from Olympiad-\\nlevel mathematics and physics competitions, including the Chinese college entrance exam. Each problem\\nis detailed with expert-level annotations for step-by-step reasoning.\\nMathVerse [470]\\n2025\\nIt contains 2,612 math problems from three fundamental math subjects, i.e., plane geometry (1,746), solid\\ngeometry (332), and functions (534). Each problem is then transformed by human annotators into six\\ndistinct versions, each offering varying degrees of information content in multimodality, contributing\\nto 15K test samples in total.\\nVideo Understanding\\nTGIF-QA [141]\\n2017\\nconsists of 103,919 QA pairs collected from 56,720 animated GIFs. TGIF-QA includes four task types:\\nrepetition count, repeating action, state transition, frame QA.\\nActivityNet-QA\\n[442]\\n2019\\nIt exploits 5,800 videos from the ActivityNet dataset, which contains about 20,000 untrimmed web\\nvideos representing 200 action classes. Each video is annotated with ten question-answer pairs using\\ncrowdsourcing to finally obtain 58,000 question-answer pairs. The maximum question length is 20 and\\nthe maximum answer length is 5. The average question length is 8.67 and average answer length is 1.85.\\nEgoSchema [262]\\n2023\\nconsists of over 5000 human curated multiple choice question answer pairs, spanning over 250 hours\\nof real video data. For each question, it requires the correct answer to be selected between five given\\noptions based on a three-minute-long video clip.\\nVideo-MME [85]\\n2024\\nIt contains a annotated set of 2,700 high-quality multiple-choice questions (3 per video) from 900 videos,\\n744 subtitles and 900 audio files across various scenarios. For diversity in video types, it spans 6 visual\\ndomains, with 30 subfields. For duration in temporal dimension, it encompasses both short-, medium-,\\nand long-term videos, ranging from 11 seconds to 1 hour.\\nMVBench [188]\\n2024\\ncovers 20 video temporal understanding tasks that cannot be effectively solved with a single frame.\\nEach task produces 200 multiple-choice QA pairs by leveraging ChatGPT to automatically reannotate\\nexisting video datasets with their original annotations.\\nMMBench-Video [77]\\n2024\\nincorporates approximately 600 web videos from YouTube, spanning 16 major categories. Each video\\nranges in duration from 30 seconds to 6 minutes. The benchmark includes roughly 2,000 original\\nquestion-answer pairs, contributed by volunteers, covering a total of 26 fine-grained capabilities.\\nMLVU [496]\\n2024\\nconsists of 3,102 questions across 9 categories with 2,593 questions for dev set and 509 questions for test\\nset. It is made up of videos of diversified lengths, spanning from 3 min to more than 2 hours. Besides,\\neach video is further partitioned as incremental segments, e.g., the first 3 min, the first 6 min, and the\\nentire video.\\nLVBench [376]\\n2024\\ngathers an initial collection of 500 videos, each with a minimum duration of 30 minutes. Finally, these\\nvideos is annotated to select a subset of 103 videos.\\nEvent-Bench [71]\\n2024\\nincludes 6 event-related tasks and 2,190 test instances.\\nVNBench [488]\\n2024\\n1,350 samples with 9 sub-tasks.\\nTempCompass [232]\\n2024\\ncollects a total of 410 videos and 500 pieces of meta-information, with 9 content categories.\\nMovieChat [332]\\n2024\\n1K long videos and 13K manual question-answering pairs.\\nIndustry\\nMME-Industry [429]\\n2025\\nencompasses 21 distinct domain, comprising 1050 question-answer pairs with 50 questions per domain.\\nConversational QA\\nSparklesDialogue\\n[134]\\n2023\\nSparklesDialogueCC comprises 4.5K dialogues, each consisting of at least two images spanning two\\nconversational turns. SparklesDialogueVG includes 2K dialogues, each with at least three distinct images\\nacross two turns.\\nSciGraphQA [192]\\n2023\\nselected 290,000 Computer Science or Machine Learning ArXiv papers, and then used Palm-2 to generate\\n295K samples of open-vocabulary multi-turn question-answering dialogues about the graphs. As context,\\nit provided the text-only Palm-2 with paper title, abstract, paragraph mentioning the graph, and rich\\ntext contextual data from the graph itself, obtaining dialogues with an average 2.23 question-answer\\nturns for each graph.\\nConvBench [225]\\n2024\\ncomprises 577 image-instruction pairs tailored for multi-round conversations. Each pair is structured\\naround three sequential instructions, each targeting a distinct cognitive skill—beginning with perception,\\nfollowed by reasoning, and culminating in creation. Encompassing 215 tasks, the benchmark is divided\\ninto 71 tasks focused on perception, 65 on reasoning, and 79 on creation.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 42}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n43\\nMMDU [235]\\n2024\\ncomprises 110 multi-image multi-turn dialogues with more than 1600 questions, each accompanied by\\ndetailed long-form answers. The questions in MMDU involve 2 to 20 images, with an average image&text\\ntoken length of 8.2k tokens, a maximum turn length of 27, and a maximum image&text length reaching\\n18K tokens.\\nMultidisciplinary\\nScienceQA [243]\\n2022\\nmultiple-choice science question dataset containing 21,208 examples. It covers diverse topics across\\nthree subjects: natural science, social science, and language science.\\nMMMU [445]\\n2024\\nincludes 11.5K multimodal questions from college exams, quizzes, and textbooks, covering 6 core\\ndisciplines. These questions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous\\nimage types.\\nCMMU [117]\\n2024\\nIt consists of 3,603 questions in 7 subjects, covering knowledge from primary to high school. The\\nquestions can be categorized into 3 types: multiple-choice, multiple-response, and fill-in-the-blank.\\nCMMMU [457]\\n2024\\nA Chinese Multi-discipline multimodal Understanding, including 12k manually collected multimodal\\nquestions from college exams, quizzes, and textbooks, covering 6 core disciplines. These questions span\\n30 subjects and comprise 39 highly heterogeneous image typesbenchmark.\\nMMMU-Pro [446]\\n2024\\n3460 questions in total (1730 samples are in the standard format and the other 1730 are in the screenshot\\nor photo form)\\n5\\nEvaluation Metrics of MRAG\\nMultimodal RAG systems generally consist of four core components: document parsing, search\\nplanning, retrieval, and generation, which collectively influence their end-to-end performance.\\nAccurate and comprehensive evaluation of these components is essential, leveraging available\\nmultimodal benchmarks. In practice, three common evaluation strategies are typically employed:\\nhuman evaluation, rule-based evaluation, and LLM/MLLM-based evaluation. Each strategy offers\\ndistinct advantages and disadvantages in calculating evaluation metrics.\\n• Human evaluation: Human evaluation is widely regarded as the gold standard for assessing\\nMRAG systems, as their effectiveness is ultimately determined by human users. This method is\\nextensively used in research to ensure the reliability and relevance of model outputs. For instance,\\nBingo [58] employs human annotators to assess the accuracy of GPT-4V’s responses, with a focus\\non identifying and analyzing model biases. In hallucination detection, M-HalDetect [108] demon-\\nstrates that human evaluation outperforms model-based methods in detecting subtle inaccuracies,\\nhighlighting its precision. Additionally, WV-Arena [245] uses a human voting system combined\\nwith Elo ratings to rank and compare multiple models, providing a robust benchmarking frame-\\nwork. However, human evaluation presents challenges, including increased time and labor costs,\\nwhich limit its scalability for large-scale assessments. The reliability of results can also be affected\\nby the limited number of evaluators, as individual biases may influence outcomes. To address\\nthese issues, some studies employ diverse evaluator pools and cross-validation techniques to\\nenhance the balance and representativeness of assessments. Nonetheless, the trade-off between\\nevaluation accuracy and resource expenditure remains a critical consideration in designing RAG\\nmodel evaluation methodologies.\\n• Rule-based evaluation: Rule-based evaluation metrics [41, 430, 473] are essential for assessing\\nthe performance of MRAG systems. These metrics rely on standardized evaluation tools, enabling\\nobjective, reproducible assessments with minimal human intervention. Compared to subjective\\nhuman evaluations, deterministic metrics offer significant advantages, including reduced time\\nconsumption, lower susceptibility to bias, and greater consistency across multiple assessments.\\nSuch consistency is particularly crucial for large-scale evaluations or when comparing different\\nsystems or model iterations.\\n• LLM/MLLM-based evaluation: For evaluation of MRAG systems, LLMs/MLLMs are employed\\nto compare reference answers with generated outputs or to directly score responses. For example,\\nMM-Vet [439] uses GPT-4 to automate evaluation, generating scores for each sample based on\\nthe input question, ground truth, and model output. Similarly, TouchStone [17] and LLaVA-bench\\n[219] leverage GPT-4 to directly compare generated answers with reference answers, simplifying\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 43}, page_content='44\\nTrovato et al.\\nthe evaluation process. While integrating LLMs/MLLMs in evaluation reduces human effort, it\\nhas limitations. This approach is prone to systematic biases, such as sensitivity to the order of\\nresponse presentation. Additionally, evaluation outcomes are heavily influenced by the inherent\\ncapabilities and limitations of the LLMs/MLLMs themselves, leading to potential inconsistencies,\\nas different models may produce divergent results for the same task. These challenges underscore\\nthe need for careful model selection and evaluation design to mitigate biases and ensure reliable\\nassessments.\\n5.1\\nMetrics of Retrieval & Generation\\nThe evaluation of MRAG systems is essential for ensuring their effectiveness and reliability in\\nprocessing complex, multimodal data. Evaluation metrics can be broadly classified into rule-based\\nand LLM/MLLM-based approaches.\\n• Rule-based Metrics: Rule-based metrics evaluate the performance of MRAG systems using\\npredefined criteria and heuristics. These metrics are generally interpretable, transparent, and\\ncomputationally efficient, making them well-suited for tasks with well-defined benchmarks.\\nExamples of common rule-based metrics include:\\n– Exact Match (EM): This metric evaluates whether the model’s output exactly matches the\\nground truth, offering a clear and unambiguous performance measure. It is especially valuable\\nin tasks requiring high accuracy and fidelity to reference data, such as question answering,\\nfact verification, and information retrieval. While exact match (EM) provides a straightforward\\nand interpretable evaluation, it may fall short in scenarios where semantically equivalent but\\nlexically divergent responses are acceptable.\\n– ROUGE-N (N-gram Recall): The ROUGE metric is a widely used framework for evaluating\\ntext summarization and generation tasks. ROUGE-N measures the overlap of N-grams (con-\\ntiguous sequences of N words) between generated text and one or more reference texts, with\\na strong emphasis on recall. This metric assesses how well the generated text captures the\\nessential content of the reference. For example, ROUGE-1 evaluates unigram overlap, ROUGE-2\\nfocuses on bigrams, and higher-order N-grams (e.g., ROUGE-3) capture more complex linguis-\\ntic structures. While ROUGE-N provides a quantitative measure of lexical similarity, it is often\\nsupplemented by other metrics to account for semantic coherence, fluency, and relevance,\\nparticularly in multimodal contexts where textual and non-textual data interact.\\n– BLEU: BLEU is a widely used metric in NLP for evaluating the quality of machine-generated\\ntext by assessing its similarity to one or more reference texts. Initially designed for machine\\ntranslation, BLEU has been adapted to various NLP tasks, including multimodal generation. In\\nmultimodal settings, BLEU can evaluate the alignment between generated text and associated\\nmodalities (e.g., images, videos) by comparing the output to reference descriptions or captions.\\nHowever, while BLEU offers a quantitative measure of n-gram overlap, it has limitations\\nin capturing semantic depth, contextual coherence, and multimodal consistency, which are\\nessential for comprehensive evaluation in MRAG systems.\\n– Mean Reciprocal Rank (MRR): MRR is a widely used metric for evaluating the performance\\nof systems that produce ranked lists of results, such as search engines, recommendation\\nsystems, or retrieval-augmented models. MRR measures the rank position of the first relevant\\nitem in the returned list, reflecting the system’s ability to surface correct or useful information\\nquickly. It is calculated as the average of the reciprocal ranks of the first relevant result across\\nmultiple queries or tasks. A higher MRR indicates better performance, as it demonstrates the\\nsystem’s effectiveness in prioritizing relevant results at the top of the list.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 44}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n45\\n– CIDEr (Consensus-based Image Description Evaluation): CIDEr is specifically designed to\\nmeasure the agreement between machine-generated captions and human-authored reference\\ncaptions. It utilizes a TF-IDF weighting mechanism to quantify the similarity between generated\\nand reference texts.\\n– SPICE (Semantic Propositional Image Caption Evaluation): The evaluation of MRAG\\nsystems frequently utilizes the SPICE metric to assess the quality of generated captions.\\nSPICE prioritizes semantic fidelity by parsing captions into structured scene graphs, which\\ndepict objects, attributes, and relationships within the text. These generated scene graphs are\\nsubsequently compared to reference graphs derived from ground-truth captions. By emphasiz-\\ning semantic similarity over lexical overlap, SPICE offers a robust measure of how well the\\ngenerated content aligns with the intended meaning. This makes it particularly well-suited\\nfor evaluating multimodal systems that integrate visual and textual information, ensuring a\\nnuanced and contextually accurate assessment of MRAG outputs.\\n– BERTScore: Evaluation of MRAG focuses on assessing the quality and relevance of outputs\\nin contexts integrating both textual and non-textual data (e.g., images, audio). A key metric\\nfor evaluating textual components is BERTScore, which utilizes contextual embeddings from\\nBERT to measure semantic similarity between generated and reference texts. Unlike traditional\\nmetrics such as BLEU or ROUGE, which depend on exact word matches or n-gram overlap,\\nBERTScore captures deeper semantic relationships by aligning tokens based on their contextual\\nembeddings.\\n– Perplexity: It measures the model’s ability to predict the next word in a sequence, with lower\\nperplexity values indicating greater confidence and accuracy in predictions. This reflects a\\nstronger understanding of the underlying data distribution.\\nRule-based metrics offer objective and reproducible outcomes but frequently lack the adaptability\\nneeded to capture nuanced semantic or contextual understanding, especially in multimodal\\nenvironments where text, images, and other data types interact.\\n• LLM/MLLM-based Metrics: The emergence of LLMs and MLLMs has transformed evalua-\\ntion paradigms, enabling the use of their advanced reasoning and comprehension capabilities.\\nLLM/MLLM-based metrics now provide more holistic and context-aware assessments of MRAG\\nsystems, with key approaches including:\\n– Answer Precision: This metric measures the degree to which the knowledge in a model-\\ngenerated answer is supported or entailed by the ground truth. It assesses the accuracy and\\nrelevance of retrieved information by evaluating the overlap between the model’s output and\\nthe factual or contextual basis provided by the ground truth. High answer precision indicates\\nthat the model effectively utilizes retrieved multimodal data to produce responses aligned\\nwith the expected factual content. This metric is crucial for evaluating the reliability and\\nfactual consistency of multimodal RAG systems, ensuring that generated outputs are both\\ncontextually appropriate and informationally accurate.\\n– Ground Truth Recall: This metric measures the degree to which the knowledge in the\\nground truth is accurately captured and reflected in the model-generated response. It assesses\\nthe model’s ability to retrieve and integrate relevant information from the provided knowledge\\nbase or multimodal sources, ensuring the output aligns with the factual or contextual details\\nin the reference data. It is particularly crucial for evaluating retrieval-augmented systems,\\nas it directly quantifies the fidelity of the model’s output to the intended knowledge. Higher\\nscores indicate stronger alignment with the ground truth, reflecting enhanced retrieval and\\ngeneration capabilities.\\n– Retrieved Context Precision: This metric measures the alignment between the knowledge\\nin the retrieved context and the information in the ground truth response. It evaluates the\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 45}, page_content='46\\nTrovato et al.\\nproportion of relevant and accurate information in the retrieved context that is directly\\nsupported or entailed by the ground truth, assessing the retrieval system’s precision and\\ncontextual appropriateness in generating accurate responses. This metric is especially vital\\nin multimodal RAG systems, where integrating diverse data types (e.g., text, images, audio)\\nrequires robust evaluation of relevance and precision across modalities.\\n– Retrieved Context Recall: This metric measures the degree to which the retrieved context\\naligns with and encompasses the knowledge necessary to generate ground truth responses.\\nIt evaluates the proportion of relevant information from the ground truth captured within\\nthe retrieved context, serving as a key indicator of the retrieval system’s effectiveness in\\nsupporting accurate and comprehensive response generation. High values indicate that the\\nretrieval mechanism effectively identifies and incorporates essential knowledge, thereby\\nenhancing the overall performance of the MRAG system.\\n– Faithfulness: This metric evaluates the extent to which generated text maintains factual\\nconsistency with the information in the retrieved documents, ensuring the output accurately\\nreflects the source material and minimizes hallucinations or deviations from the evidence.\\nIn MRAG systems, it also ensures alignment with multimodal retrieved content, including\\ntextual, visual, and auditory elements, maintaining consistency across modalities.\\n– Hallucination: This metric measures the proportion of generated outputs containing hal-\\nlucinated content, such as unsupported claims, fabricated information, or inaccuracies not\\nsubstantiated by the retrieved data. It is essential for evaluating the reliability and factual\\nconsistency of the model’s responses.\\nLLM/MLLM-based metrics are highly effective at capturing complex semantic relationships and\\ncontextual nuances, making them particularly suitable for multimodal RAG systems. However,\\nthey may inherit biases from the underlying models and demand substantial computational\\nresources.\\n• Metric Calculation: When evaluating multimodal retrieval-augmented generation systems,\\nimplementation methods for the same metric can vary significantly, primarily categorized into\\ncoarse-grained and fine-grained approaches. These methodologies differ in their granularity\\nand the depth of analysis applied to assess the quality of model-generated responses against\\nreference answers.\\n– Coarse-Grained Evaluation: Coarse-grained evaluation utilizes LLMs or MLLMs to compare\\nmodel-generated responses with reference answers. This method involves inputting both\\nthe generated output and the reference into the LLM/MLLM, which evaluates the overall\\nsemantic alignment, coherence, and relevance between the two. The model assesses whether\\nthe generated content captures the core meaning and intent of the reference, providing a holistic\\nscore or qualitative feedback. This approach is computationally efficient and scalable, making\\nit suitable for rapid benchmarking and high-level quality checks in large-scale applications.\\nHowever, its broad focus may overlook fine-grained inaccuracies, such as subtle factual\\nerrors, logical inconsistencies, or nuanced contextual mismatches. Consequently, coarse-\\ngrained evaluation is best used as an initial screening tool or in scenarios where high-level\\nsemantic fidelity is prioritized over detailed precision. For more rigorous evaluation, it is\\noften supplemented by fine-grained metrics that address specific aspects of content quality.\\nIn summary, coarse-grained evaluation offers a pragmatic balance between efficiency and\\neffectiveness, particularly in applications requiring quick assessments or large-scale model\\ncomparisons.\\n– Fine-Grained Evaluation: Fine-grained evaluation, such as RAGChecker [317] and RAGAS\\n[74], offers a nuanced and detailed approach to assessing MRAG systems, surpassing the limita-\\ntions of coarse-grained methods. This approach involves decomposing both model-generated\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 46}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n47\\nresponses and reference answers into granular knowledge points or semantic units, which are\\nindividually evaluated based on criteria such as accuracy, relevance, and alignment with the\\nreference. By analyzing responses at this level of detail, the method enables precise identifica-\\ntion of a model’s strengths and weaknesses, particularly in capturing and reproducing intricate\\ninformation. The fine-grained approach is especially valuable for diagnosing performance\\nissues in handling complex or nuanced content. However, it is computationally intensive,\\nrequiring robust mechanisms for extracting, matching, and evaluating multiple semantic units.\\nCareful design of these mechanisms is essential to ensure evaluation consistency and reliability.\\nDespite its challenges, this method provides a rigorous and comprehensive framework for\\nadvancing the development and refinement of MRAG systems, making it a critical tool in the\\nfield.\\nThe choice between coarse-grained and fine-grained evaluation depends on the assessment\\nobjectives. Coarse-grained methods are ideal for obtaining quick, high-level insights, whereas\\nfine-grained approaches are better suited for detailed analysis and iterative model refinement.\\nIntegrating both strategies can provide a balanced perspective, combining the efficiency of\\ncoarse-grained evaluation with the precision of fine-grained analysis to comprehensively assess\\nMRAG systems.\\n6\\nChallenges of MRAG\\nIn this section, we delineate the challenges associated with various modules in a MRAG system.\\nThese challenges span multiple critical components, including document parsing and indexing,\\nsearch planning, retrieval, generation, dataset, and evaluation. Each module presents unique\\ncomplexities that must be addressed to ensure the system’s effectiveness and robustness.\\n6.1\\nDocument Parsing and Indexing\\nDocument parsing and indexing has established the data foundation based on MRAG, which plays\\na crucial role in the entire system. The relevant technologies extensively studied even before the\\nadvent of LLMs, have seen significant advancements in the LLM era. However, they continue to\\nface several challenges that necessitate further exploration and refinement.\\n• Challenges in Data Accuracy and Completeness: As the primary input source, the accuracy\\nand completeness of upstream data are critical. Errors or omissions in the upstream data can\\npropagate and amplify downstream, significantly degrading system performance. For example,\\nwhile MRAG systems have enhanced document information preservation—such as capturing\\nper-page screenshots—they still face challenges in maintaining inter-page relationships. This\\nlimitation is particularly problematic in long documents with associated segments. Preserving\\nthese relationships is essential for ensuring contextually accurate outputs.\\n• Balancing Multimodal and Textual Data: The document parsing module has grown increas-\\ningly complex as modern MRAG systems must handle multimodal data, including images, tables,\\nand text. To address this, contemporary approaches preserve the original multimodal inputs\\nto minimize information loss, while also converting them into textual captions or descriptions.\\nAlthough retaining the original data reduces information loss, relying solely on it has proven\\nsuboptimal. Recent studies highlight the benefits of leveraging textual representations derived\\nfrom multimodal data. For example, Riedler and Langer [312] showed that models generate\\nhigher-quality responses using textual captions from images rather than processing raw images\\ndirectly. Similarly, Ma et al. [259] found that LLMs outperform MLLMs in text generation tasks,\\nrevealing a performance gap between multimodal and text-focused systems. This gap highlights\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 47}, page_content='48\\nTrovato et al.\\nthe limitations of current multimodal systems in effectively integrating diverse data types, ne-\\ncessitating additional components in document parsing pipelines. These enhancements, while\\nimproving functionality, increase system complexity and expand the volume of data requiring\\nprocessing, storage, and management.\\n6.2\\nMultimodal Search Planning\\nThe challenges in multimodal search planning can be more effectively understood through a\\nhierarchical framework similar to leveled RAG systems, where queries span a spectrum from simple\\nfactual retrievals to complex, creative tasks. This framework highlights three critical challenges\\nthat must be addressed to advance the field.\\n• Intelligent Adaptive Planning Mechanisms: The primary challenge is developing intelligent\\nadaptive planning mechanisms that can dynamically adjust to the diversity and complexity of\\nqueries. Current systems often rely on predetermined pipelines, which fail to accommodate\\nvariations in query characteristics or computational constraints, leading to inefficient resource\\nallocation and suboptimal performance [125, 474]. While fixed strategies may suffice for homo-\\ngeneous query types, real-world applications handle heterogeneous query patterns that demand\\ndynamic adjustment of retrieval strategies. For example, complex queries involving multi-hop\\nreasoning or creative problem-solving could greatly benefit from a multi-agent collaborative\\napproach [369]. In such a framework, specialized agents could explore parallel reasoning paths,\\npropose complementary retrieval strategies, and collaboratively synthesize findings to construct\\ncomprehensive search plans. This collaborative paradigm not only simulates diverse perspec-\\ntives but also facilitates intricate interactions between knowledge sources and reasoning steps.\\nBy evaluating search plans from multiple angles, such systems can balance effectiveness and\\nefficiency, ensuring robust performance across diverse query types.\\n• Query Reformulation and Semantic Alignment: A second major challenge is query reformu-\\nlation, particularly in maintaining semantic alignment between the original multimodal query\\nintent and the reformulated queries [197]. As queries become more sophisticated, accurately\\ncapturing and maintaining their intent grows increasingly complex. This challenge is amplified\\nin multimodal contexts, where queries may integrate text, images, audio, or other data types,\\neach requiring precise interpretation. To address this, multi-perspective reformulation strategies\\ncould be employed, leveraging diverse interpretations of the query to generate reformulations\\nthat better align with the original intent. Such strategies might integrate contextual under-\\nstanding, domain-specific knowledge, and cross-modal alignment techniques to ensure semantic\\nconsistency with the user’s intent.\\n• Comprehensive Evaluation Benchmarks: The third critical challenge is the absence of com-\\nprehensive evaluation benchmarks capable of assessing planning mechanisms across diverse\\nquery complexities and scenarios. Existing benchmarks often focus on narrow performance\\naspects, failing to capture the full spectrum of real-world applications. To address this gap, future\\nbenchmarks should evaluate systems across multiple dimensions, including adaptability to query\\ndiversity, robustness in handling complex queries, and efficiency in resource utilization. These\\nbenchmarks should incorporate a wide range of query types, from simple factual retrievals to\\nmulti-hop reasoning and creative tasks, ensuring rigorous testing under realistic conditions. Addi-\\ntionally, benchmarks should incorporate metrics for semantic alignment in query reformulation,\\ncomputational efficiency, and scalability.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 48}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n49\\nThese interconnected challenges highlight the need for future research to develop adaptive planning\\nmechanisms capable of addressing both query diversity and complexity. This could involve multi-\\nagent coordination for advanced cases, alongside robust query reformulation and comprehensive\\nevaluation frameworks.\\n6.3\\nRetrieval\\nMultimodal retrieval has made significant progress but continues to face challenges that can be\\ncategorized into methodological and practical issues. These challenges arise from the inherent\\ncomplexity of integrating and retrieving information across diverse data modalities such as text,\\nimages, audio, and video. Below, we outline the key challenges in this field:\\n• Heterogeneity of Cross-Modal Data: The heterogeneity of data across modalities poses a\\nsignificant challenge in multimodal retrieval and representation learning. Text, being sequential\\nand discrete, relies on syntactic and semantic structures best captured by language models, while\\nimages, being spatial and continuous, require convolutional or transformer-based architectures to\\nextract hierarchical visual features. This structural divergence complicates cross-modal alignment\\nand comparison, as each modality demands specialized feature extraction techniques tailored to\\nits unique characteristics. Extracting meaningful and comparable features from each modality is\\nnon-trivial, requiring domain-specific expertise and sophisticated models capable of capturing\\nnuanced data properties. For instance, while transformers excel in processing sequential data like\\ntext, their adaptation to spatial data like images often necessitates architectural modifications,\\nsuch as vision transformers (ViTs), to handle pixel arrays. Aligning these features into a unified\\nrepresentation space that preserves cross-modal semantic relationships remains a major challenge.\\nCurrent approaches, including cross-modal transformers and MLLMs, often fail to create a\\ncommon embedding space that adequately captures the semantic richness of each modality while\\nensuring inter-modal consistency.\\n• Cross-modal components (reranker, refiner): While the dual-tower architecture has made\\nsignificant strides in first-stage retrieval by efficiently encoding and aligning multimodal data (e.g.,\\ntext and images), developing advanced reranking models that enable fine-grained multimodal\\ninteraction remains a challenge. Additionally, refining external multimodal knowledge post-\\nretrieval and reranking remains underexplored, despite its potential to enhance result accuracy\\nand relevance. Addressing these gaps requires innovative methodologies that leverage MLLMs\\nand LLMs to enable sophisticated cross-modal understanding and reasoning.\\n6.4\\nGeneration\\nThe multimodal module in MRAG achieves human-aligned sensory representation through diver-\\nsified modality integration, which significantly enhances user experience and system usability.\\nHowever, achieving these enhancement objectives entails addressing shared challenges across both\\nQA systems and multimodal generation:\\n• Multimodal Input: Multimodal systems face the challenge of integrating diverse data structures\\nand representations across modalities such as text, images, audio, and video. As multimodal\\nmodels evolve, they are increasingly required to process arbitrary combinations of modalities (e.g.,\\ntext+image, text+video, image+audio). This necessitates a highly flexible and adaptive framework\\ncapable of dynamically accommodating diverse input configurations. Such frameworks must be\\nmodality-agnostic, enabling seamless integration of any input combination without predefined\\nstructures or extensive retraining. Achieving this flexibility involves designing architectures that\\ngeneralize across modalities, extract relevant features, and fuse them meaningfully, regardless of\\ninput composition.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 49}, page_content='50\\nTrovato et al.\\n• Multimodal Output:\\n– Coherent and Contextually Relevant Generation: Ensuring consistency across different\\nmodalities in the output presents a significant challenge. For instance, in a text-image pair, the\\nimage must accurately reflect the scene or object described in the text, while the text should\\nprecisely convey the visual content.\\n– Positioning and Integration of Multimodal Elements: In multimodal outputs, such as\\ntext with embedded images or videos, the model must intelligently determine where to\\nintegrate non-textual elements. This requires an understanding of the narrative flow and the\\nidentification of optimal insertion points to enhance coherence and readability. Additionally,\\nthe model should dynamically generate or retrieve relevant multimodal content based on\\ncontext. For instance, when creating a text-image pair, the model may need to generate an\\nimage caption, search for relevant images, and select the most appropriate one. This process\\nmust be efficient and seamless to ensure the final output is both relevant and high-quality.\\n– Diversity of Outputs: In some applications, generating diverse outputs—such as multiple\\nimages or videos corresponding to a given text description—is essential. However, balancing\\ndiversity with relevance and quality poses a significant challenge. The model must explore a\\nbroad range of possibilities while ensuring each output remains contextually appropriate and\\nadheres to high-quality standards.\\n6.5\\nDataset & Evaluation\\nThe advancement of MLLMs has heightened the need for comprehensive evaluation. Despite\\nthe introduction of over a hundred benchmarks by both academic and industrial communities,\\nseveral challenges remain in the current evaluation landscape. First, there is a lack of a universally\\naccepted, standardized capability taxonomy, with existing benchmarks often defining their own\\ndisparate ability dimensions. Second, current benchmarks exhibit significant gaps in critical areas\\nsuch as instruction following, complex multimodal reasoning, multi-turn dialogue, and creativity\\nassessment. Third, task-specific evaluations for MLLMs are insufficient, particularly in commercially\\nrelevant domains like invoice recognition, multimodal knowledge base comprehension, and UI\\nunderstanding and industry. Finally, while existing multimodal benchmarks primarily focus on\\nimage and video modalities, there is a notable deficit in assessing capabilities related to audio\\nand 3D representations. Addressing these challenges is essential for developing more robust and\\ncomprehensive evaluation methodologies for MLLMs in the future.\\nDespite rapid advancements, current evaluations of MLLMs remain insufficiently comprehensive,\\nprimarily focusing on perception and reasoning abilities through objective questions. This creates a\\nsignificant gap between evaluation methodologies and real-world applications. Moreover, optimiz-\\ning models based on objective assessments often leads developers to prioritize objective question\\ncorpora during instruction tuning, potentially degrading the quality of dialogue experiences. Al-\\nthough subjective multimodal evaluation platforms like WildVision and OpenCompass MultiModal\\nArena have emerged, further research is needed to develop assessment methods that better align\\nwith practical usage scenarios. Current evaluation strategies predominantly rely on curated or\\ncrafted questions to assess specific capabilities, yet complex multimodal tasks typically require\\nthe integration of multiple skills. For instance, a chart-related question may involve OCR, spatial\\nrelationship recognition, reasoning, and calculations. The absence of decoupled assessments for\\nthese distinct capabilities represents a major limitation in existing frameworks. Additionally, crucial\\nabilities such as instruction following remain under-evaluated. Multiturn dialogue, the primary\\nmode of human interaction with multimodal models, remains a weakness for most models, and\\ncorresponding evaluations, are still in their infancy. In the realm of complex multimodal reasoning,\\ncurrent evaluations predominantly focus on mathematical and examination problems, necessitating\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 50}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n51\\nimprovements in both difficulty and relevance to everyday use cases. Notably, the evaluation of\\nmultimodal creative tasks, a key application area for these models—such as text generation based on\\nimage and textual prompts—remains largely unexplored, highlighting a critical gap in the current\\nevaluation landscape.\\nMLLMs are still in the early stages of development, with limited business applications to date.\\nAs a result, current evaluations primarily focus on assessing foundational capabilities rather than\\nreal-world performance. Moving forward, it is critical to develop evaluation frameworks that\\nmeasure MLLM performance on specific tasks with commercial value, such as large-scale document\\nprocessing, multimodal knowledge base comprehension, anomaly detection, and industrial visual\\ninspection. When designing task-specific evaluations, it is essential to consider not only performance\\nmetrics but also computational costs and inference speeds, benchmarking them against traditional\\ncomputer vision methods like OCR, object detection, and action recognition to determine practical\\napplicability. Additionally, a key potential of MLLMs lies in their ability to plan and interact with\\nenvironments as agents to solve complex problems. Developing diverse virtual environments\\nfor MLLMs to demonstrate agent-based problem-solving capabilities will likely become a critical\\ncomponent of future evaluations. Current efforts in this domain remain nascent, highlighting a\\npromising area for future research in multimodal AI assessment.\\n7\\nFuture Directions\\nIn this chapter, we propose several suggestions to the future development of multimodal Retrieval-\\nAugmented Generation (MRAG) systems, informed by related research and identified challenges.\\nThese recommendations collectively aim to overcome existing limitations and unlock the full\\npotential of MRAG in complex, real-world scenarios.\\n7.1\\nDocuments Parsing\\nMultimodal document parsing has become a crucial element in MRAG systems, particularly with\\nthe emergence of large language models (LLMs) and multimodal large models (MLLMs). The fusion\\nof text, images, and other data types into a cohesive framework presents both transformative\\nopportunities and notable challenges. This paper provides a detailed analysis of future directions\\nin this evolving field.\\n• Enhancing Data Accuracy and Completeness:\\n– Contextual Relationship Preservation: To improve the accuracy and coherence of multi-\\nmodal document parsing, especially for long and complex documents, advanced algorithms are\\nneeded to capture and preserve both inter-page and intra-document relationships. Techniques\\nsuch as graph-based representations and hierarchical document modeling can help maintain\\ncontextual coherence across the document. These methods enable the system to understand\\nstructural and semantic dependencies between sections, tables, figures, and other elements,\\nensuring the preservation of the document’s logical flow. Additionally, cross-referencing mech-\\nanisms are essential for linking related content across pages. These mechanisms dynamically\\nconnect sections, tables, and figures, facilitating seamless retrieval and utilization of contextual\\nrelationships in downstream tasks like information extraction, summarization, or question\\nanswering. By integrating these approaches, the system can better handle the complexities of\\nlong documents, ensuring accurate maintenance and leveraging of contextual relationships\\nfor enhanced performance in multimodal document understanding tasks. This is particularly\\nrelevant when combining Optical Character Recognition (OCR), LLMs, and MLLMs to process\\nand interpret documents with diverse content types.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 51}, page_content='52\\nTrovato et al.\\n– Error Detection and Correction: To improve the accuracy and reliability of multimodal\\ndocument parsing, integrating advanced error detection and correction mechanisms is crucial.\\nLeveraging LLMs and MLLMs, systems can validate extracted text against the original docu-\\nment, identifying and correcting inaccuracies or omissions. These models can be enhanced\\nwith consistency-checking algorithms to ensure coherence and accuracy across multimodal\\ndata, including text, images, and tables. For critical documents, a human-in-the-loop (HITL)\\napproach is advisable. This involves human reviewers verifying and refining parsed data,\\nespecially in cases where systems may struggle with complex layouts, ambiguous content,\\nor domain-specific nuances. By combining the strengths of LLMs, MLLMs, and human ex-\\npertise, this hybrid approach ensures high accuracy and reliability, making it suitable for\\nprecision-demanding applications such as legal, medical, or financial document processing.\\n• Improving Multimodal Data Integration:\\n– Unified Multimodal Representation: Advancing multimodal document parsing requires\\nthe development of unified representation frameworks that seamlessly integrate diverse data\\ntypes, such as text, images, and tables, into a cohesive structure. Such frameworks enable\\nrobust, context-aware analysis by leveraging multimodal transformers—like CLIP, Flamingo,\\nor other state-of-the-art models—to encode disparate modalities into a shared embedding\\nspace. This interoperability enhances downstream tasks, including information extraction,\\nquestion answering, and summarization. A promising approach involves hybrid strategies\\nthat combine raw multimodal data with textual representations. For instance, raw images\\ncan support visual tasks (e.g., object detection or layout analysis), while textual captions\\nor OCR-derived text can improve text generation tasks (e.g., summarization or translation).\\nThis dual methodology leverages the strengths of each modality, ensuring both accuracy and\\nefficiency in processing complex documents, as demonstrated by recent research. Additionally,\\nintegrating LLMs and MLLMs with Optical Character Recognition (OCR) systems can enhance\\nthe parsing of scanned or image-based documents. By aligning OCR outputs with multimodal\\nembeddings, these systems improve the handling of noisy or unstructured data, enabling more\\naccurate interpretation and contextual understanding.\\n– Advanced Captioning and Description Generation: To improve multimodal data inte-\\ngration, particularly in document parsing, enhancing automated captioning and description\\ngeneration for non-textual elements like images, tables, and charts is critical. Leveraging state-\\nof-the-art vision-language models (VLMs) and MLLMs can boost the accuracy and contextual\\nrelevance of textual descriptions. These models bridge the gap between visual and textual\\ndata, enabling more comprehensive document understanding. Integrating domain-specific\\nknowledge into captioning models is essential for generating accurate and contextually tai-\\nlored descriptions. This can be achieved by fine-tuning pre-trained models on domain-specific\\ndatasets or incorporating external knowledge bases. Such an approach ensures that descrip-\\ntions align with the document’s content, enhancing the utility of multimodal data integration.\\n• Leveraging LLMs and MLLMs for Enhanced Parsing:\\n– LLM/MLLM-Driven Parsing and Indexing: LLMs and MLLMs can be fine-tuned on domain-\\nspecific corpora to improve their ability to parse and interpret complex document structures.\\nLeveraging their advanced multimodal understanding, these models can accurately identify\\nand extract key information—such as legal clauses, scientific hypotheses, or technical speci-\\nfications—even from dense or unstructured text. Fine-tuning enhances their proficiency in\\nrecognizing domain-specific terminology, relationships, and contextual nuances. Furthermore,\\nLLMs and MLLMs can generate metadata, tags, and summaries. By automatically annotat-\\ning documents with relevant keywords, classifications, or concise summaries, these models\\nstreamline the organization and accessibility of large document repositories. This capability\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 52}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n53\\nis particularly valuable in applications like legal case management, academic research, and\\nenterprise knowledge bases. In multimodal contexts, MLLMs extend these capabilities by\\nintegrating and interpreting data from diverse sources, such as text, images, tables, and dia-\\ngrams. This enables a more comprehensive parsing process, where visual and textual elements\\nare jointly analyzed to extract richer, more accurate information. For example, in scientific\\ndocuments, MLLMs can parse and correlate data from textual descriptions and accompanying\\ncharts, facilitating a deeper understanding of the content.\\n– Bridging the Gap Between LLMs and MLLMs: A promising approach involves hybrid\\narchitectures that combine the strengths of MLLMs and LLMs. MLLMs process raw multimodal\\ninputs (e.g., images, audio, video) to extract meaningful representations, while LLMs generate\\ncoherent and contextually accurate textual outputs. This division of labor optimizes perfor-\\nmance, as MLLMs excel in multimodal feature extraction and LLMs in linguistic precision. For\\nexample, in document parsing, MLLMs analyze visual layouts, tables, or embedded graphics,\\nwhile LLMs synthesize this information into structured textual formats.\\n7.2\\nMultimodal Search Planning\\nThe future of multimodal search planning should focus on addressing three key challenges within\\nthe hierarchical framework: intelligent adaptive planning mechanisms, query reformulation and\\nsemantic alignment, and comprehensive evaluation benchmarks. Below are targeted suggestions\\nfor advancing each area.\\n• Intelligent Adaptive Planning Mechanisms:\\n– Multi-Agent Collaborative Systems: To address the challenges of multimodal search and\\ncomplex query resolution, multi-agent collaborative systems can be designed to leverage\\nspecialized agents working in tandem. These systems enhance efficiency, adaptability, and\\nrobustness in handling multi-hop, creative, or cross-modal queries. Key mechanisms include:\\n1) Parallel Reasoning Paths: Specialized agents can simultaneously explore multiple reasoning\\ntrajectories, enabling faster and more comprehensive solutions. This approach is particularly\\neffective for multi-hop queries, where intermediate reasoning steps are critical, or for creative\\ntasks requiring diverse perspectives. By evaluating multiple pathways in parallel, the system\\ncan identify optimal solutions while mitigating the risk of local optima. 2) Complementary Re-\\ntrieval Strategies: Agents can employ diverse retrieval methodologies, such as keyword-based,\\nsemantic, or cross-modal retrieval, to address different aspects of a query. For instance, one\\nagent might focus on extracting structured data, while another leverages semantic embeddings\\nor visual-textual alignment for multimodal contexts. The synthesis of these strategies ensures\\nrobust and contextually relevant search outcomes, enhancing the system’s ability to handle\\nheterogeneous data sources. 3) Dynamic Resource Allocation: Agents can monitor computa-\\ntional resources and system constraints in real-time, dynamically adjusting retrieval strategies\\nto optimize performance. For example, under limited computational bandwidth, agents might\\nprioritize lightweight retrieval methods or redistribute tasks to balance load. This adaptive\\nmechanism ensures efficient resource utilization while maintaining high-quality query resolu-\\ntion. 4) Integration with MLLMs and LLMs: The collaborative multi-agent framework can be\\nseamlessly integrated with MLLMs and LLMs to enhance their capabilities. MLLMs can serve\\nas central orchestrators, interpreting multimodal inputs and coordinating agent tasks, while\\nLLMs provide deep contextual understanding and reasoning support. This integration enables\\nthe system to handle complex, multimodal queries with greater precision and adaptability.\\n– Hierarchical Planning Frameworks: To address the challenges of ambiguous or highly\\ncreative queries in multimodal search and planning, integrating human feedback into the\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 53}, page_content='54\\nTrovato et al.\\ndecision-making process is essential. Human-in-the-loop (HITL) systems facilitate iterative\\nrefinement by leveraging user expertise to guide and validate intermediate results. These\\ninteractive systems enable users to dynamically adjust search parameters, prioritize modalities,\\nor correct misinterpretations, ensuring more accurate and contextually relevant outcomes. By\\ncombining the strengths of multimodal large language models (MLLMs) with human intuition,\\nHITL systems enhance adaptability, build trust, and improve the robustness of intelligent\\nplanning frameworks. This collaborative approach is particularly valuable in domains requiring\\nnuanced understanding, creativity, or domain-specific knowledge.\\n– Reinforcement Learning for Adaptation: Intelligent adaptive planning mechanisms can\\nbe developed using reinforcement learning (RL) to enable dynamic, context-aware decision-\\nmaking. By modeling the search process as a sequential decision problem, RL agents can\\nbe trained to optimize resource allocation and retrieval accuracy. These agents adapt their\\nstrategies by receiving rewards for minimizing computational overhead, reducing latency, and\\ndelivering precise results tailored to query characteristics, such as modality, complexity, and\\nuser intent.\\n• Query Reformulation and Semantic Alignment:\\n– Multi-Perspective Reformulation: To address the complexity of multimodal search, query\\nreformulation strategies must generate diverse interpretations while preserving the original\\nintent across modalities. This involves: 1) Contextual Understanding: Leveraging contextual\\nembeddings (e.g., from transformer-based models) to capture semantic nuances and contex-\\ntual dependencies, ensuring reformulated queries retain the richness of the original input. 2)\\nCross-Modal Alignment: Employing advanced techniques like contrastive learning to align rep-\\nresentations across text, images, and audio modalities. By embedding queries and multimodal\\ndata into a shared latent space, this ensures consistent interpretation and retrieval across\\ndiverse data types. 3) Domain-Specific Knowledge Integration: Incorporating domain-specific\\nontologies or knowledge graphs to enhance reformulation accuracy, particularly in specialized\\nfields. This leverages structured domain knowledge to improve the relevance and precision of\\nreformulated queries.\\n– Interactive Query Refinement: A pivotal strategy is interactive query refinement, which\\nallows users to iteratively adjust queries based on intermediate results. Intelligent systems can\\nfacilitate this process by suggesting alternative query formulations, identifying ambiguities,\\nand providing contextual feedback to better align queries with user intent. By integrating user\\nfeedback loops and real-time semantic analysis, these systems dynamically bridge the gap\\nbetween user input and multimodal data, ensuring more precise and contextually relevant\\nsearch outcomes.\\n– Explainable Reformulation: A critical aspect of query reformulation is its explainability.\\nBy offering clear and concise explanations for how queries are transformed, users gain insight\\ninto the system’s reasoning and decision-making processes. For example, when a user submits\\na vague or ambiguous query, the system can generate a reformulated version while detailing\\nthe rationale behind the changes, such as term disambiguation, incorporation of contextual\\ncues, or alignment with multimodal data (e.g., text, images, or audio). This transparency fosters\\nuser trust, enables validation of the system’s interpretation, and enhances user control and\\nsatisfaction. Furthermore, explainable reformulation underscores the importance of semantic\\nalignment, where the system bridges the gap between user intent and the underlying data\\nrepresentation, ensuring the reformulated query accurately reflects the user’s needs.\\n• Comprehensive Evaluation Benchmarks:\\n– Diverse Query Datasets: It is crucial to establish robust benchmarks. These benchmarks\\nshould incorporate diverse query datasets spanning a wide range of query types, from simple\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 54}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n55\\nfactual retrievals to complex multi-hop reasoning and creative tasks. The datasets must\\nreflect real-world heterogeneity in query patterns and modalities, capturing the intricacies of\\nuser interactions across text, image, audio, and video inputs. By integrating such diversity,\\nbenchmarks can more accurately assess model performance, generalization capabilities, and\\nadaptability to varied real-world applications.\\n– Multi-Dimensional Metrics: To ensure the effectiveness and reliability of Multimodal Search\\nPlanning systems, robust evaluation frameworks must be established. These frameworks\\nshould employ multi-dimensional metrics to comprehensively assess system performance\\nacross diverse operational scenarios. Key dimensions include: 1) Adaptability: The system’s\\nability to handle a broad spectrum of query types, from simple to highly complex, while\\nintegrating multiple modalities (e.g., text, images, audio). This metric evaluates the model’s\\nflexibility in addressing varied user needs and its capacity to generalize across domains. 2)\\nRobustness: The system’s resilience under challenging conditions, such as computational con-\\nstraints, noisy or incomplete inputs, and adversarial scenarios. Robustness ensures consistent\\nperformance in real-world applications, where ideal conditions are seldom present. 3) Effi-\\nciency: The optimization of resource utilization (e.g., memory, processing power) and response\\ntime. This metric is critical for scalability and user satisfaction, especially in time-sensitive\\nor resource-constrained environments. 4) Semantic Alignment: The system’s accuracy in\\npreserving and interpreting the intent of user queries during reformulation or multimodal\\nintegration. This ensures that outputs remain contextually and semantically aligned with the\\nuser’s original request.\\n7.3\\nRetrieval\\nThe challenges in multimodal retrieval underscore the complexity of integrating and retrieving\\ninformation across diverse data types. To address these issues and advance the field, future research\\nshould prioritize the following directions:\\n• Unified Cross-Modal Representation Learning: The primary objective is to develop robust\\nand unified representation learning frameworks that effectively align and compare data across\\ndiverse modalities, including text, images, audio, and video. A key element of this framework is\\nthe enhancement of cross-modal attention mechanisms to better model complex interactions\\nbetween modalities. Cross-modal attention layers, inspired by transformer architectures, are\\ncentral to capturing fine-grained relationships. These mechanisms enable one modality to focus\\non relevant features in another, allowing the model to dynamically prioritize the most informative\\naspects of the data. For example, text can guide attention over visual regions, or audio cues can\\nemphasize relevant temporal segments in video data. Techniques such as multi-head cross-modal\\nattention and hierarchical attention further refine this process, ensuring robust and context-aware\\nrepresentations.\\n• Cross-Modal Context: The primary objective is to improve the ability of models to perform\\nfine-grained interactions between modalities, particularly in the reranking and refinement stages.\\n– Reranker: Multi-Modal Reranking Models rerank retrieved document list by incorporating\\ndetailed cross-modal interactions, such as text-image, text-audio, or video-text relationships.\\nBy integrating LLMs and MLLMs, reranking models enhance their ability to capture nuanced\\nsemantic alignments between modalities.\\n– Refiner: Leveraging their cross-modal reasoning abilities, MLLMs refine retrieval results\\nthrough knowledge-enhanced refinement, yielding more accurate, contextually relevant, and\\nsemantically rich outputs. This refinement process utilizes MLLMs’ contextual understanding\\nand multimodal alignment to re-rank, filter, or augment retrieved content.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 55}, page_content='56\\nTrovato et al.\\n7.4\\nGeneration\\nThe future of multimodal generation should focus on overcoming existing challenges on multimodal\\ninput and output. Below are key suggestions for advancing multimodal generation systems.\\n• Flexible and Adaptive Multimodal Input Frameworks: To address the growing complexity\\nof multimodal data, it is crucial to develop modality-agnostic architectures that dynamically\\nadapt to diverse and arbitrary input modality combinations, such as text+image, text+video, or\\nimage+audio. These frameworks should process inputs without relying on predefined structures\\nor extensive retraining.\\n• Coherent and Contextually Relevant Multimodal Output: Achieving coherent and con-\\ntextually relevant outputs in multimodal generation necessitates the development of advanced\\nmodels capable of maintaining consistency across modalities. For example, in text-image gen-\\neration tasks, the generated image must precisely align with the textual description, and the\\ntext should accurately reflect the visual content of the image. This cross-modal consistency is\\nessential for ensuring the reliability and usability of multimodal systems.\\n• Intelligent Positioning and Integration of Multimodal Elements: To seamlessly integrate\\nnon-textual elements (e.g., images, videos, audio) into a narrative, models must be trained to\\nidentify optimal insertion points. This requires a deep understanding of the content’s structure,\\nflow, and contextual nuances to ensure coherence, readability, and enhanced user engagement.\\nAdvanced techniques, such as attention mechanisms, can analyze the narrative’s semantic and\\nsyntactic structure, enabling the model to determine where multimodal elements can comple-\\nment or enrich the text. Modern multimodal systems must dynamically retrieve or generate\\ncontextually relevant non-textual content. For example, when generating a text-image pair,\\nthe model should use cross-modal alignment techniques to either retrieve an existing image\\nfrom a database or synthesize a new one that aligns with the textual context. This relies on\\nrobust multimodal representation learning, where embeddings from different modalities (text,\\nimage, video) are mapped into a shared latent space, enabling precise cross-modal retrieval or\\ngeneration.\\n• Diversity in Multimodal Outputs: Achieving a balance between diversity, relevance, and\\nquality in multimodal generation requires controlled mechanisms. For instance, in text-to-image\\ngeneration, models should produce diverse yet faithful representations of textual descriptions.\\nTechniques like conditional sampling can guide models to explore varied latent spaces while\\nadhering to input constraints.\\n7.5\\nDataset & Evaluation\\nThe future direction of datasets and evaluation in MRAG should focus on addressing current gaps\\nand challenges while harnessing the unique capabilities of MLLMs. Below are refined suggestions\\nfor advancing datasets and evaluation methodologies in this field.\\n• Comprehensive Benchmark Development: To enhance the evaluation of MLLMs and LLMs\\nin retrieval-augmented generation, it is crucial to develop comprehensive benchmarks that\\naddress key limitations in current assessment frameworks. These benchmarks should focus on\\nthe following areas: 1) Instruction Following: Create tasks to evaluate the model’s ability to\\ncomprehend and execute complex, multi-step instructions across diverse modalities. This includes\\nassessing precision in adhering to nuanced directives and handling ambiguous or incomplete\\ninputs. 2) Multiturn Dialogue: Develop datasets that simulate real-world conversational dynamics,\\nemphasizing the model’s capacity for context retention, coherence, and adaptability over extended\\ninteractions. Scenarios should include cross-modal references and long-term memory challenges.\\n3) Complex Multimodal Reasoning: Design tasks requiring the integration of multiple modalities\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 56}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n57\\n(e.g., text, images, audio) to solve real-world problems, such as interpreting charts, maps, or\\ncombining visual and textual data for decision-making. 4) Creativity Evaluation: Introduce\\nbenchmarks to assess generative capabilities in creative tasks, such as composing stories, poems,\\nor designing visual artifacts from multimodal inputs. These tasks should measure originality,\\nrelevance, and the ability to synthesize diverse inputs into coherent outputs. 5) Diverse Modalities:\\nExpand evaluation frameworks to include emerging modalities like audio, 3D models, and sensor\\ndata, ensuring robustness and versatility in handling a wide range of input types.\\n• Multimodal Retrieval-Augmented Generation: The development of robust metrics for eval-\\nuating retrieval and generation in multimodal systems requires assessing relevance, precision,\\ndiversity, and cross-modal alignment to ensure semantic consistency and contextual appropri-\\nateness. Metrics should quantify the system’s ability to filter noise and redundancy, delivering\\nconcise and meaningful outputs. For generation quality, coherence, fluency, creativity, and adapt-\\nability are essential, alongside factual accuracy and consistency with retrieved data and external\\nknowledge. Effective multimodal integration is crucial to unify diverse inputs into contextually\\nrich outputs. Comprehensive benchmarks must simulate real-world scenarios, incorporating\\nvaried queries, multimodal sources, and differing complexity levels to evaluate the end-to-end\\nperformance of retrieval-augmented generation (RAG) pipelines.\\n8\\nConclusion\\nIn conclusion, this survey comprehensively examines the emerging field of Multimodal Retrieval-\\nAugmented Generation (MRAG), highlighting its potential to enhance the capabilities of large\\nlanguage models (LLMs) by integrating multimodal data such as text, images, and videos. Unlike\\ntraditional text-based RAG systems, MRAG addresses the challenges of retrieving and generat-\\ning information across different modalities, thereby improving the accuracy and relevance of\\nresponses while reducing hallucinations. The survey systematically analyzes MRAG from four key\\nperspectives: essential components and technologies, datasets, evaluation methods and metrics,\\nand existing limitations. It identifies current challenges, such as effectively integrating multimodal\\nknowledge and ensuring the reliability of generated outputs, while also proposing future research\\ndirections. By providing a structured overview and forward-looking insights, this survey aims to\\nguide researchers in advancing MRAG, ultimately contributing to the development of more robust\\nand versatile Multimodal Retrieval-Augmented Generation.\\nReferences\\n[1] [n. d.]. jsoup. https://jsoup.org/\\n[2] [n. d.]. pdfminer. https://github.com/pdfminer/pdfminer.six\\n[3] [n. d.]. PyMuPDF. https://github.com/pymupdf/PyMuPDF\\n[4] [n. d.]. realworldQA. https://huggingface.co/datasets/visheratin/realworldqa\\n[5] Ossama Abdel-Hamid, Abdel-rahman Mohamed, Hui Jiang, Li Deng, Gerald Penn, and Dong Yu. 2014. Convolutional\\nneural networks for speech recognition. IEEE/ACM Transactions on audio, speech, and language processing 22, 10\\n(2014), 1533–1545.\\n[6] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree,\\nArash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin\\nCai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao\\nCheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao,\\nAmit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett,\\nWenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero\\nKauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li,\\nYunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong\\nLiu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio César Teodoro Mendes,\\nArindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid\\nPryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase,\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 57}, page_content='58\\nTrovato et al.\\nOlli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen,\\nSwadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua\\nWang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu,\\nXiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei\\nYang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang,\\nYi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. 2024. Phi-3 Technical Report: A Highly Capable Language\\nModel Locally on Your Phone. arXiv:2404.14219 [cs.CL] https://arxiv.org/abs/2404.14219\\n[7] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774\\n(2023).\\n[8] Emanuele Aiello, Lili Yu, Yixin Nie, Armen Aghajanyan, and Barlas Oguz. 2023. Jointly training large autoregressive\\nmultimodal models. arXiv preprint arXiv:2309.15564 (2023).\\n[9] Akiko Aizawa. 2003. An information-theoretic perspective of tf–idf measures. Information Processing & Management\\n39, 1 (2003), 45–65.\\n[10] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\\nKatherine Millican, Malcolm Reynolds, et al. 2022. Flamingo: a visual language model for few-shot learning. Advances\\nin neural information processing systems 35 (2022), 23716–23736.\\n[11] Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Guimin Hu, Weimin Lyu,\\nLijie Hu, Lu Yu, et al. 2024. Prompt-saw: Leveraging relation-aware graphs for textual prompt compression. arXiv\\npreprint arXiv:2404.00489 (2024).\\n[12] Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and R Manmatha. 2021. Docformer: End-to-end\\ntransformer for document understanding. In Proceedings of the IEEE/CVF international conference on computer vision.\\n993–1003.\\n[13] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph,\\nBen Mann, Nova DasSarma, et al. 2021. A general language assistant as a laboratory for alignment. arXiv preprint\\narXiv:2112.00861 (2021).\\n[14] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton,\\nSamir Gadre, Shiori Sagawa, et al. 2023. Openflamingo: An open-source framework for training large autoregressive\\nvision-language models. arXiv preprint arXiv:2308.01390 (2023).\\n[15] Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk Lee. 2019. Character Region Awareness for\\nText Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\\n[16] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\\nZhou. 2023. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966\\n1, 2 (2023), 3.\\n[17] Shuai Bai, Shusheng Yang, Jinze Bai, Peng Wang, Xingxuan Zhang, Junyang Lin, Xinggang Wang, Chang Zhou,\\nand Jingren Zhou. 2023. Touchstone: Evaluating vision-language models by language models. arXiv preprint\\narXiv:2308.16890 (2023).\\n[18] Liping Bao, Longhui Wei, Wengang Zhou, Lin Liu, Lingxi Xie, Houqiang Li, and Qi Tian. 2023. Multi-Granularity\\nMatching Transformer for Text-Based Person Search. IEEE Transactions on Multimedia (2023).\\n[19] Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Scott Yih, Sebastian Riedel, and Fabio Petroni. 2022. Autore-\\ngressive search engines: Generating substrings as document identifiers. Advances in Neural Information Processing\\nSystems 35 (2022), 31668–31683.\\n[20] Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann,\\nIbrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda\\nKoppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos,\\nRishabh Kabra, Matthias Bauer, Matko Bošnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana\\nBalazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and\\nXiaohua Zhai. 2024. PaliGemma: A versatile 3B VLM for transfer. arXiv:2407.07726 [cs.CV] https://arxiv.org/abs/\\n2407.07726\\n[21] Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, and\\nLudwig Schmidt. 2023. Visit-bench: A benchmark for vision-language instruction following inspired by real-world\\nuse. arXiv preprint arXiv:2308.06595 (2023).\\n[22] Chinmoy B Bose and Shyh-Shiaw Kuo. 1994. Connected and degraded text recognition using hidden Markov model.\\nPattern Recognition 27, 10 (1994), 1345–1363.\\n[23] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\\nPranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural\\ninformation processing systems 33 (2020), 1877–1901.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 58}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n59\\n[24] Davide Caffagni, Federico Cocchi, Nicholas Moratelli, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara.\\n2024. Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition. 1818–1826.\\n[25] Zhiwei Cao, Qian Cao, Yu Lu, Ningxin Peng, Luyang Huang, Shanbo Cheng, and Jinsong Su. 2024. Retaining key\\ninformation under high compression ratios: Query-guided compressor for llms. arXiv preprint arXiv:2406.02376\\n(2024).\\n[26] Bing-Bing Chai, Jozsef Vass, and Xinhua Zhuang. 1999. Significance-linked connected component analysis for wavelet\\nimage coding. IEEE Transactions on Image processing 8, 6 (1999), 774–784.\\n[27] Wei-Cheng Chang, Felix X Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. 2020. Pre-training tasks for\\nembedding-based large-scale retrieval. arXiv preprint arXiv:2002.03932 (2020).\\n[28] Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk. 2022. Webqa: Multihop\\nand multimodal qa. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 16495–16504.\\n[29] Yunhang Shen Yulei Qin Mengdan Zhang Xu Lin Jinrui Yang Xiawu Zheng Ke Li Xing Sun Yunsheng Wu Rongrong Ji\\nChaoyou Fu, Peixian Chen. 2023. Mme: A comprehensive evaluation benchmark for multimodal large language\\nmodels. arXiv preprint arXiv:2306.13394 (2023).\\n[30] Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu. 2023. X-llm: Bootstrap-\\nping advanced large language models by treating multi-modalities as foreign languages. arXiv preprint arXiv:2305.04160\\n(2023).\\n[31] Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, and Liqiang Nie. 2024. Lion: Empowering multimodal large\\nlanguage model with dual-level visual knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition. 26540–26550.\\n[32] Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. 2023. Walking down the memory maze:\\nBeyond context limit through interactive reading. arXiv preprint arXiv:2310.05029 (2023).\\n[33] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking large language models in retrieval-augmented\\ngeneration. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 17754–17762.\\n[34] Jieneng Chen, Luoxin Ye, Ju He, Zhaoyang Wang, Daniel Khashabi, and Alan L Yuille. 2024. Efficient large multi-modal\\nmodels via visual context compression. Advances in Neural Information Processing Systems 37 (2024), 73986–74007.\\n[35] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas\\nChandra, Yunyang Xiong, and Mohamed Elhoseiny. 2023. Minigpt-v2: large language model as a unified interface for\\nvision-language multi-task learning. arXiv preprint arXiv:2310.09478 (2023).\\n[36] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. 2023. Shikra: Unleashing multimodal\\nllm’s referential dialogue magic. arXiv preprint arXiv:2306.15195 (2023).\\n[37] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2024. Sharegpt4v:\\nImproving large multi-modal models with better captions. In European Conference on Computer Vision. Springer,\\n370–387.\\n[38] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao,\\nDahua Lin, et al. 2024. Are We on the Right Way for Evaluating Large Vision-Language Models? arXiv preprint\\narXiv:2403.20330 (2024).\\n[39] Shaoxiang Chen, Zequn Jie, and Lin Ma. 2024. Llava-mole: Sparse mixture of lora experts for mitigating data conflicts\\nin instruction finetuning mllms. arXiv preprint arXiv:2401.16160 (2024).\\n[40] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz,\\nSebastian Goodman, Xiao Wang, Yi Tay, et al. 2023. Pali-x: On scaling up a multilingual vision and language model.\\narXiv preprint arXiv:2305.18565 (2023).\\n[41] Xingyu Chen, Zihan Zhao, Lu Chen, Danyang Zhang, Jiabao Ji, Ao Luo, Yuxuan Xiong, and Kai Yu. 2021. Websrc: A\\ndataset for web-based structural reading comprehension. arXiv preprint arXiv:2101.09465 (2021).\\n[42] Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-Wei Chang. 2023. Can\\npre-trained vision and language models answer visual information-seeking questions? arXiv preprint arXiv:2302.11713\\n(2023).\\n[43] Yiqun Chen, Qi Liu, Yi Zhang, Weiwei Sun, Xinyu Ma, Wei Yang, Daiting Shi, Jiaxin Mao, and Dawei Yin. 2024.\\nTourrank: Utilizing large language models for documents ranking with a tournament-inspired strategy. arXiv preprint\\narXiv:2406.11678 (2024).\\n[44] Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran. 2024. Dress: Instructing large vision-\\nlanguage models to align and interact with humans via natural language feedback. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition. 14239–14250.\\n[45] Yiqun Chen, Lingyong Yan, Weiwei Sun, Xinyu Ma, Yi Zhang, Shuaiqiang Wang, Dawei Yin, Yiming Yang, and Jiaxin\\nMao. 2025. Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning. arXiv preprint\\narXiv:2501.15228 (2025).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 59}, page_content='60\\nTrovato et al.\\n[46] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020.\\nUniter: Universal image-text representation learning. In European conference on computer vision. Springer, 104–120.\\n[47] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu,\\nLewei Lu, et al. 2024. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In\\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition. 24185–24198.\\n[48] Xin Cheng, Xun Wang, Xingxing Zhang, Tao Ge, Si-Qing Chen, Furu Wei, Huishuai Zhang, and Dongyan Zhao. 2024.\\nxrag: Extreme context compression for retrieval-augmented generation with one token. arXiv preprint arXiv:2405.13792\\n(2024).\\n[49] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. 2023. Adapting language models to compress\\ncontexts. arXiv preprint arXiv:2305.14788 (2023).\\n[50] Kyunghyun Cho. 2014. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint\\narXiv:1409.1259 (2014).\\n[51] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and\\nYoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation.\\narXiv preprint arXiv:1406.1078 (2014).\\n[52] Sukmin Cho, Soyeong Jeong, Jeongyeon Seo, and Jong C Park. 2023. Discrete prompt optimization via constrained\\ngeneration for zero-shot re-ranker. arXiv preprint arXiv:2305.13729 (2023).\\n[53] Eunbi Choi, Yongrae Jo, Joel Jang, and Minjoon Seo. 2022. Prompt injection: Parameterization of fixed inputs. arXiv\\npreprint arXiv:2206.11349 (2022).\\n[54] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang,\\nXiaolin Wei, et al. 2023. Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices.\\narXiv preprint arXiv:2312.16886 1, 2 (2023), 3.\\n[55] Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang\\nLin, Bo Zhang, et al. 2024. Mobilevlm v2: Faster and stronger baseline for vision language model. arXiv preprint\\narXiv:2402.03766 (2024).\\n[56] Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, and Xia Hu. 2024. Learning to compress\\nprompt in natural language formats. arXiv preprint arXiv:2402.18700 (2024).\\n[57] Sanghyuk Chun, Seong Joon Oh, Rafael Sampaio De Rezende, Yannis Kalantidis, and Diane Larlus. 2021. Probabilistic\\nembeddings for cross-modal retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition. 8415–8424.\\n[58] Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. 2023. Holistic\\nanalysis of hallucination in gpt-4v (ision): Bias and interference challenges. arXiv preprint arXiv:2311.03287 (2023).\\n[59] Zhuyun Dai and Jamie Callan. 2020. Context-aware document term weighting for ad-hoc search. In Proceedings of\\nThe Web Conference 2020. 1897–1907.\\n[60] Zhuyun Dai and Jamie Callan. 2020. Context-aware term weighting for first stage passage retrieval. In Proceedings of\\nthe 43rd International ACM SIGIR conference on research and development in Information Retrieval. 1533–1536.\\n[61] Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2020. Autoregressive entity retrieval. arXiv\\npreprint arXiv:2010.00904 (2020).\\n[62] Jacob Devlin. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint\\narXiv:1810.04805 (2018).\\n[63] SeungHeon Doh, Minhee Lee, Dasaem Jeong, and Juhan Nam. 2024. Enriching Music Descriptions with A Finetuned-\\nLLM and Metadata for Text-to-Music Retrieval. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech\\nand Signal Processing (ICASSP). IEEE, 826–830.\\n[64] Jianfeng Dong, Xirong Li, and Cees GM Snoek. 2018. Predicting visual features from text for image and video caption\\nretrieval. IEEE Transactions on Multimedia 20, 12 (2018), 3377–3388.\\n[65] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu\\nZhou, Haoran Wei, et al. 2023. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint\\narXiv:2309.11499 (2023).\\n[66] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong\\nDuan, Maosong Cao, et al. 2024. Internlm-xcomposer2: Mastering free-form text-image composition and comprehen-\\nsion in vision-language large model. arXiv preprint arXiv:2401.16420 (2024).\\n[67] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong\\nDuan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen,\\nConghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. 2024. InternLM-XComposer2: Mastering\\nFree-form Text-Image Composition and Comprehension in Vision-Language Large Model. arXiv:2401.16420 [cs.CV]\\nhttps://arxiv.org/abs/2401.16420\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 60}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n61\\n[68] Alexey Dosovitskiy. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\\narXiv:2010.11929 (2020).\\n[69] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson,\\nQuan Vuong, Tianhe Yu, Wenlong Huang, et al. 2023. Palm-e: An embodied multimodal language model. (2023).\\n[70] Andrew Drozdov, Honglei Zhuang, Zhuyun Dai, Zhen Qin, Razieh Rahimi, Xuanhui Wang, Dana Alon, Mohit Iyyer,\\nAndrew McCallum, Donald Metzler, et al. 2023. PaRaDe: Passage ranking using demonstrations with LLMs. In\\nFindings of the Association for Computational Linguistics: EMNLP 2023. 14242–14252.\\n[71] Yifan Du, Kun Zhou, Yuqi Huo, Yifan Li, Wayne Xin Zhao, Haoyu Lu, Zijia Zhao, Bingning Wang, Weipeng Chen,\\nand Ji-Rong Wen. 2024. Towards Event-oriented Long Video Understanding. arXiv preprint arXiv:2406.14129 (2024).\\n[72] Martin Engilberge, Louis Chevallier, Patrick Pérez, and Matthieu Cord. 2018. Finding beans in burgers: Deep semantic-\\nvisual embedding with localization. In Proceedings of the IEEE conference on computer vision and pattern recognition.\\n3984–3993.\\n[73] Boris Epshtein, Eyal Ofek, and Yonatan Wexler. 2010. Detecting text in natural scenes with stroke width transform.\\nIn 2010 IEEE computer society conference on computer vision and pattern recognition. IEEE, 2963–2970.\\n[74] Shahul Es, Jithin James, Luis Espinosa Anke, and Steven Schockaert. 2024. Ragas: Automated evaluation of re-\\ntrieval augmented generation. In Proceedings of the 18th Conference of the European Chapter of the Association for\\nComputational Linguistics: System Demonstrations. 150–158.\\n[75] Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. 2017. Vse++: Improving visual-semantic embeddings\\nwith hard negatives. arXiv preprint arXiv:1707.05612 (2017).\\n[76] Minghui Fang, Shengpeng Ji, Jialong Zuo, Hai Huang, Yan Xia, Jieming Zhu, Xize Cheng, Xiaoda Yang, Wenrui Liu,\\nGang Wang, et al. 2024. Ace: A generative cross-modal retrieval framework with coarse-to-fine semantic modeling.\\narXiv preprint arXiv:2406.17507 (2024).\\n[77] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. 2024. MMBench-Video:\\nA Long-Form Multi-Shot Benchmark for Holistic Video Understanding. arXiv preprint arXiv:2406.14515 (2024).\\n[78] Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, and Pierre Colombo. 2024.\\nColPali: Efficient Document Retrieval with Vision Language Models. arXiv:2407.01449 [cs.IR] https://arxiv.org/abs/\\n2407.01449\\n[79] Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, and Pierre Colombo. 2024.\\nColpali: Efficient document retrieval with vision language models. In The Thirteenth International Conference on\\nLearning Representations.\\n[80] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with\\nsimple and efficient sparsity. Journal of Machine Learning Research 23, 120 (2022), 1–39.\\n[81] Hao Feng, Qi Liu, Hao Liu, Jingqun Tang, Wengang Zhou, Houqiang Li, and Can Huang. 2024. Docpedia: Unleashing\\nthe power of large multimodal model in the frequency domain for versatile document understanding. Science China\\nInformation Sciences 67, 12 (2024), 1–14.\\n[82] Paolo Ferragina and Giovanni Manzini. 2000. Opportunistic data structures with applications. In Proceedings 41st\\nannual symposium on foundations of computer science. IEEE, 390–398.\\n[83] Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and Stéphane Clinchant. 2021. SPLADE v2: Sparse lexical\\nand expansion model for information retrieval. arXiv preprint arXiv:2109.10086 (2021).\\n[84] Thibault Formal, Benjamin Piwowarski, and Stéphane Clinchant. 2021. SPLADE: Sparse lexical and expansion model\\nfor first stage ranking. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in\\nInformation Retrieval. 2288–2292.\\n[85] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang\\nShen, Mengdan Zhang, et al. 2024. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal\\nllms in video analysis. arXiv preprint arXiv:2405.21075 (2024).\\n[86] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu Ma,\\nand Ranjay Krishna. 2025. Blink: Multimodal large language models can see but not perceive. In European Conference\\non Computer Vision. Springer, 148–166.\\n[87] Zheren Fu, Zhendong Mao, Yan Song, and Yongdong Zhang. 2023. Learning semantic relationship among instances\\nfor image-text matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\\n15159–15168.\\n[88] Zheren Fu, Lei Zhang, Hou Xia, and Zhendong Mao. 2024. Linguistic-Aware Patch Slimming Framework for Fine-\\ngrained Cross-Modal Alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\\n26307–26316.\\n[89] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. 2020. Multi-modal transformer for video retrieval.\\nIn Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IV 16.\\nSpringer, 214–229.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 61}, page_content='62\\nTrovato et al.\\n[90] Jun Gao, Ziqiang Cao, and Wenjie Li. 2024. SelfCP: Compressing over-limit prompt via the frozen large language\\nmodel itself. Information Processing & Management 61, 6 (2024), 103873.\\n[91] Jun Gao, Ziqiang Cao, and Wenjie Li. 2024. Unifying demonstration selection and compression for in-context learning.\\narXiv preprint arXiv:2405.17062 (2024).\\n[92] Luyu Gao and Jamie Callan. 2021. Condenser: a pre-training architecture for dense retrieval. arXiv preprint\\narXiv:2104.08253 (2021).\\n[93] Luyu Gao and Jamie Callan. 2021. Unsupervised corpus aware language model pre-training for dense passage retrieval.\\narXiv preprint arXiv:2108.05540 (2021).\\n[94] Luyu Gao, Zhuyun Dai, and Jamie Callan. 2021. COIL: Revisit exact lexical match in information retrieval with\\ncontextualized inverted list. arXiv preprint arXiv:2104.07186 (2021).\\n[95] Luyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Benjamin Van Durme, and Jamie Callan. 2021. Complement\\nlexical retrieval model with semantic residual embeddings. In European Conference on Information Retrieval. Springer,\\n146–160.\\n[96] Zhi Gao, Yuntao Du, Xintong Zhang, Xiaojian Ma, Wenjuan Han, Song-Chun Zhu, and Qing Li. 2024. Clova: A\\nclosed-loop visual assistant with tool usage and update. In Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition. 13258–13268.\\n[97] Noa Garcia, Mayu Otani, Chenhui Chu, and Yuta Nakashima. 2020. KnowIT VQA: Answering knowledge-based\\nquestions about videos. In Proceedings of the AAAI conference on artificial intelligence, Vol. 34. 10826–10834.\\n[98] Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. 2013. Optimized product quantization. IEEE transactions on pattern\\nanalysis and machine intelligence 36, 4 (2013), 744–755.\\n[99] Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. 2023. In-context autoencoder for context\\ncompression in a large language model. arXiv preprint arXiv:2307.06945 (2023).\\n[100] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. 2023. Planting a seed of vision in large language\\nmodel. arXiv preprint arXiv:2307.08041 (2023).\\n[101] Gregor Geigle, Radu Timofte, and Goran Glavaš. 2024. African or European Swallow? Benchmarking Large Vision-\\nLanguage Models for Fine-Grained Object Classification. arXiv preprint arXiv:2406.14496 (2024).\\n[102] Peiyuan Gong, Jiamian Li, and Jiaxin Mao. 2024. Cosearchagent: a lightweight collaborative search agent with large\\nlanguage models. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in\\nInformation Retrieval. 2729–2733.\\n[103] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping\\nLuo, and Kai Chen. 2023. Multimodal-gpt: A vision and language model for dialogue with humans. arXiv preprint\\narXiv:2305.04790 (2023).\\n[104] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the v in vqa matter:\\nElevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition. 6904–6913.\\n[105] Alex Graves and Alex Graves. 2012. Long short-term memory. Supervised sequence labelling with recurrent neural\\nnetworks (2012), 37–45.\\n[106] Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint\\narXiv:2312.00752 (2023).\\n[107] Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu Minzhe, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang,\\nXin Jiang, et al. 2022. Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark. Advances in\\nNeural Information Processing Systems 35 (2022), 26418–26431.\\n[108] Anisha Gunjal, Jihan Yin, and Erhan Bas. 2024. Detecting and preventing hallucinations in large vision language\\nmodels. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 18135–18143.\\n[109] Fang Guo, Wenyu Li, Honglei Zhuang, Yun Luo, Yafu Li, Qi Zhu, Le Yan, and Yue Zhang. 2024. Generating diverse\\ncriteria on-the-fly to improve point-wise LLM rankers. arXiv preprint arXiv:2404.11960 (2024).\\n[110] Weikuo Guo, Huaibo Huang, Xiangwei Kong, and Ran He. 2019. Learning disentangled representation for cross-\\nmodal retrieval with deep mutual information estimation. In Proceedings of the 27th ACM International Conference on\\nMultimedia. 1712–1720.\\n[111] Yanming Guo, Yu Liu, Theodoros Georgiou, and Michael S Lew. 2018. A review of semantic segmentation using deep\\nneural networks. International journal of multimedia information retrieval 7 (2018), 87–93.\\n[112] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.\\n2018. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition. 3608–3617.\\n[113] Xiaotian Han, Quanzeng You, Yongfei Liu, Wentao Chen, Huangjie Zheng, Khalil Mrini, Xudong Lin, Yiqi Wang,\\nBohan Zhai, Jianbo Yuan, et al. 2023. InfiMM-Eval: Complex Open-Ended Reasoning Evaluation For Multi-Modal\\nLarge Language Models. arXiv e-prints (2023), arXiv–2311.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 62}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n63\\n[114] Darryl Hannan, Akshay Jain, and Mohit Bansal. 2020. Manymodalqa: Modality disambiguation and qa over diverse\\ninputs. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 7879–7886.\\n[115] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang,\\nYuxiang Zhang, et al. 2024. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual\\nmultimodal scientific problems. arXiv preprint arXiv:2402.14008 (2024).\\n[116] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momentum contrast for unsupervised\\nvisual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition.\\n9729–9738.\\n[117] Zheqi He, Xinya Wu, Pengfei Zhou, Richeng Xuan, Guang Liu, Xi Yang, Qiannan Zhu, and Hua Huang. 2024.\\nCMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning. arXiv preprint\\narXiv:2401.14011 (2024).\\n[118] Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun-Hsuan Sung, László Lukács, Ruiqi Guo, Sanjiv Kumar, Balint\\nMiklos, and Ray Kurzweil. 2017. Efficient natural language response suggestion for smart reply. arXiv preprint\\narXiv:1705.00652 (2017).\\n[119] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent\\nVanhoucke, Patrick Nguyen, Tara N Sainath, et al. 2012. Deep neural networks for acoustic modeling in speech\\nrecognition: The shared views of four research groups. IEEE Signal processing magazine 29, 6 (2012), 82–97.\\n[120] Sebastian Hofstätter, Omar Khattab, Sophia Althammer, Mete Sertkan, and Allan Hanbury. 2022. Introducing neural\\nbag of whole-words with colberter: Contextualized late interactions using enhanced reduction. In Proceedings of the\\n31st ACM International Conference on Information & Knowledge Management. 737–747.\\n[121] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao\\nDong, Ming Ding, et al. 2024. Cogagent: A visual language model for gui agents. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition. 14281–14290.\\n[122] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna,\\nChen-Yu Lee, and Tomas Pfister. 2023. Distilling step-by-step! outperforming larger language models with less\\ntraining data and smaller model sizes. arXiv preprint arXiv:2305.02301 (2023).\\n[123] Anwen Hu, Yaya Shi, Haiyang Xu, Jiabo Ye, Qinghao Ye, Ming Yan, Chenliang Li, Qi Qian, Ji Zhang, and Fei Huang.\\n2024. mplug-paperowl: Scientific diagram analysis with the multimodal large language model. In Proceedings of the\\n32nd ACM International Conference on Multimedia. 6929–6938.\\n[124] Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye\\nZhang, et al. 2023. Large multilingual models pivot zero-shot multimodal learning across languages. arXiv preprint\\narXiv:2308.12038 (2023).\\n[125] Wenbo Hu, Jia-Chen Gu, Zi-Yi Dou, Mohsen Fayyaz, Pan Lu, Kai-Wei Chang, and Nanyun Peng. 2024. MRAG-Bench:\\nVision-Centric Evaluation for Retrieval-Augmented Multimodal Models. arXiv preprint arXiv:2410.08182 (2024).\\n[126] Wenbo Hu, Jia-Chen Gu, Zi-Yi Dou, Mohsen Fayyaz, Pan Lu, Kai-Wei Chang, and Nanyun Peng. 2024. MRAG-Bench:\\nVision-Centric Evaluation for Retrieval-Augmented Multimodal Models. arXiv preprint arXiv:2410.08182 (2024).\\n[127] Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanabhan, Giuseppe\\nOttaviano, and Linjun Yang. 2020. Embedding-based retrieval in facebook search. In Proceedings of the 26th ACM\\nSIGKDD International Conference on Knowledge Discovery & Data Mining. 2553–2561.\\n[128] Minbin Huang, Runhui Huang, Han Shi, Yimeng Chen, Chuanyang Zheng, Xiangguo Sun, Xin Jiang, Zhenguo Li,\\nand Hong Cheng. 2024. Efficient Multi-modal Large Language Models via Visual Token Grouping. arXiv preprint\\narXiv:2411.17773 (2024).\\n[129] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong,\\nJiawei Huang, Jinglin Liu, et al. 2024. Audiogpt: Understanding and generating speech, music, sound, and talking\\nhead. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 23802–23804.\\n[130] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan\\nMohammed, Barun Patra, et al. 2023. Language is not all you need: Aligning perception with language models.\\nAdvances in Neural Information Processing Systems 36 (2023), 72096–72109.\\n[131] Siteng Huang, Biao Gong, Yulin Pan, Jianwen Jiang, Yiliang Lv, Yuyuan Li, and Donglin Wang. 2023. Vop: Text-video\\nco-operative prompt tuning for cross-modal retrieval. In Proceedings of the IEEE/CVF conference on computer vision\\nand pattern recognition. 6565–6574.\\n[132] Xijie Huang, Li Lyna Zhang, Kwang-Ting Cheng, Fan Yang, and Mao Yang. 2023. Fewer is more: Boosting LLM\\nreasoning with reinforced context pruning. arXiv preprint arXiv:2312.08901 (2023).\\n[133] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. 2022. Layoutlmv3: Pre-training for document ai with\\nunified text and image masking. In Proceedings of the 30th ACM International Conference on Multimedia. 4083–4091.\\n[134] Yupan Huang, Zaiqiao Meng, Fangyu Liu, Yixuan Su, Nigel Collier, and Yutong Lu. 2023. Sparkles: Unlocking chats\\nacross multiple images for multimodal instruction-following models. arXiv preprint arXiv:2308.16463 (2023).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 63}, page_content='64\\nTrovato et al.\\n[135] Yan Huang, Wei Wang, and Liang Wang. 2017. Instance-aware image and sentence matching with selective multimodal\\nlstm. In Proceedings of the IEEE conference on computer vision and pattern recognition. 2310–2318.\\n[136] Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, and Jianlong Fu. 2021. Seeing out of the box:\\nEnd-to-end pre-training for vision-language representation learning. In Proceedings of the IEEE/CVF conference on\\ncomputer vision and pattern recognition. 12976–12985.\\n[137] S Humeau. 2019. Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-\\nsentence scoring. arXiv preprint arXiv:1905.01969 (2019).\\n[138] Zaeem Hussain, Mingda Zhang, Xiaozhong Zhang, Keren Ye, Christopher Thomas, Zuha Agha, Nathan Ong, and\\nAdriana Kovashka. 2017. Automatic understanding of image and video advertisements. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition. 1705–1715.\\n[139] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard\\nGrave. 2021.\\nTowards unsupervised dense information retrieval with contrastive learning.\\narXiv preprint\\narXiv:2112.09118 2, 3 (2021).\\n[140] Aman Jain, Mayank Kothyari, Vishwajeet Kumar, Preethi Jyothi, Ganesh Ramakrishnan, and Soumen Chakrabarti.\\n2021. Select, substitute, search: A new benchmark for knowledge-augmented visual question answering. In Proceedings\\nof the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2491–2498.\\n[141] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. 2017. Tgif-qa: Toward spatio-temporal\\nreasoning in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition.\\n2758–2766.\\n[142] Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010. Product quantization for nearest neighbor search. IEEE\\ntransactions on pattern analysis and machine intelligence 33, 1 (2010), 117–128.\\n[143] Hervé Jégou, Matthijs Douze, Cordelia Schmid, and Patrick Pérez. 2010. Aggregating local descriptors into a compact\\nimage representation. In 2010 IEEE computer society conference on computer vision and pattern recognition. IEEE,\\n3304–3311.\\n[144] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and\\nTom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. In\\nInternational conference on machine learning. PMLR, 4904–4916.\\n[145] Yiren Jian, Chongyang Gao, and Soroush Vosoughi. 2023. Bootstrapping vision-language learning with decoupled\\nlanguage pre-training. Advances in Neural Information Processing Systems 36 (2023), 57–72.\\n[146] Ding Jiang and Mang Ye. 2023. Cross-modal implicit relation reasoning and aligning for text-to-image person retrieval.\\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2787–2797.\\n[147] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Guanglu Song,\\nPeng Gao, et al. 2024. Mmsearch: Benchmarking the potential of large models as multi-modal search engines. arXiv\\npreprint arXiv:2409.12959 (2024).\\n[148] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. Llmlingua: Compressing prompts for\\naccelerated inference of large language models. arXiv preprint arXiv:2310.05736 (2023).\\n[149] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. Longllmlingua:\\nAccelerating and enhancing llms in long context scenarios via prompt compression. arXiv preprint arXiv:2310.06839\\n(2023).\\n[150] Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen\\nZhuang. 2024. E5-v: Universal embeddings with multimodal large language models. arXiv preprint arXiv:2407.12580\\n(2024).\\n[151] Yutao Jiang, Qiong Wu, Wenhao Lin, Wei Yu, and Yiyi Zhou. 2025. What Kind of Visual Tokens Do We Need?\\nTraining-free Visual Token Pruning for Multi-modal Large Language Models from the Perspective of Graph. arXiv\\npreprint arXiv:2501.02268 (2025).\\n[152] Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu Chen. 2024. Vlm2vec: Training vision-\\nlanguage models for massive multimodal embedding tasks. arXiv preprint arXiv:2410.05160 (2024).\\n[153] Bowen Jin, Hansi Zeng, Guoyin Wang, Xiusi Chen, Tianxin Wei, Ruirui Li, Zhengyang Wang, Zheng Li, Yang Li,\\nHanqing Lu, et al. 2023. Language models as semantic indexers. arXiv preprint arXiv:2310.07815 (2023).\\n[154] Yang Jin, Zhicheng Sun, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang\\nSong, et al. 2024. Video-lavit: Unified video-language pre-training with decoupled visual-motional tokenization.\\narXiv preprint arXiv:2402.03161 (2024).\\n[155] Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Quzhe Huang, Bin Chen, Chenyi Lei, An Liu, Chengru Song,\\net al. 2023. Unified language-vision pretraining in llm with dynamic discrete visual tokenization. arXiv preprint\\narXiv:2309.04669 (2023).\\n[156] Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with gpus. IEEE Transactions on\\nBig Data 7, 3 (2019), 535–547.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 64}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n65\\n[157] Jia-Huei Ju, Jheng-Hong Yang, and Chuan-Ju Wang. 2021. Text-to-text multi-view learning for passage re-ranking.\\nIn Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval.\\n1803–1807.\\n[158] Dongwon Jung, Qin Liu, Tenghao Huang, Ben Zhou, and Muhao Chen. 2024. Familiarity-aware evidence compression\\nfor retrieval augmented generation. arXiv preprint arXiv:2409.12468 (2024).\\n[159] Hoyoun Jung and Kyung-Joong Kim. 2024. Discrete prompt compression with reinforcement learning. IEEE Access\\n(2024).\\n[160] Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Ákos Kádár, Adam Trischler, and Yoshua Bengio. 2017.\\nFigureqa: An annotated figure dataset for visual reasoning. arXiv preprint arXiv:1710.07300 (2017).\\n[161] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau\\nYih. 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906 (2020).\\n[162] Mehran Kazemi, Nishanth Dikkala, Ankit Anand, Petar Devic, Ishita Dasgupta, Fangyu Liu, Bahare Fatemi, Pranjal\\nAwasthi, Dee Guo, Sreenivas Gollapudi, et al. 2024. ReMI: A Dataset for Reasoning with Multiple Images. arXiv\\npreprint arXiv:2406.09175 (2024).\\n[163] Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage search via contextualized late\\ninteraction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in\\nInformation Retrieval. 39–48.\\n[164] Jing Yu Koh, Daniel Fried, and Russ R Salakhutdinov. 2023. Generating images with multimodal language models.\\nAdvances in Neural Information Processing Systems 36 (2023), 21487–21506.\\n[165] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. 2023. Grounding language models to images for multimodal\\ninputs and outputs. In International Conference on Machine Learning. PMLR, 17283–17300.\\n[166] Weize Kong, Swaraj Khadanga, Cheng Li, Shaleen Kumar Gupta, Mingyang Zhang, Wensong Xu, and Michael\\nBendersky. 2022. Multi-aspect dense retrieval. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge\\nDiscovery and Data Mining. 3178–3186.\\n[167] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural\\nnetworks. Advances in neural information processing systems 25 (2012).\\n[168] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. 2024. Lisa: Reasoning segmentation\\nvia large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\\n9579–9589.\\n[169] Yann LeCun, Yoshua Bengio, et al. 1995. Convolutional networks for images, speech, and time series. The handbook\\nof brain theory and neural networks 3361, 10 (1995), 1995.\\n[170] Jaewoo Lee, Joonho Ko, Jinheon Baek, Soyeong Jeong, and Sung Ju Hwang. 2024. Unified Multimodal Interleaved\\nDocument Representation for Retrieval. arXiv:2410.02729 [cs.CL] https://arxiv.org/abs/2410.02729\\n[171] Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi Chen. 2020. Learning dense representations of phrases at scale.\\narXiv preprint arXiv:2012.12624 (2020).\\n[172] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain\\nquestion answering. arXiv preprint arXiv:1906.00300 (2019).\\n[173] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal,\\nPeter Shaw, Ming-Wei Chang, and Kristina Toutanova. 2023. Pix2struct: Screenshot parsing as pretraining for visual\\nlanguage understanding. In International Conference on Machine Learning. PMLR, 18893–18912.\\n[174] Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He. 2018. Stacked cross attention for image-text\\nmatching. In Proceedings of the European conference on computer vision (ECCV). 201–216.\\n[175] Sunkyung Lee, Minjin Choi, and Jongwuk Lee. 2023. GLEN: Generative retrieval via lexical index learning. arXiv\\npreprint arXiv:2311.03057 (2023).\\n[176] Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Minjoon Seo. 2023. Volcano: mitigating multimodal hallucination\\nthrough self-feedback guided revision. arXiv preprint arXiv:2311.07362 (2023).\\n[177] Paul Lerner, Olivier Ferret, Camille Guinaudeau, Hervé Le Borgne, Romaric Besançon, José G Moreno, and Jesús\\nLovón Melgarejo. 2022. ViQuAE, a dataset for knowledge-based visual question answering about named entities. In\\nProceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval.\\n3108–3120.\\n[178] Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. 2024. Seed-bench-2-plus: Benchmarking\\nmultimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790 (2024).\\n[179] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. 2024. SEED-Bench:\\nBenchmarking Multimodal Large Language Models. In Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition. 13299–13308.\\n[180] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. 2023. Seed-bench: Benchmarking\\nmultimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125 (2023).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 65}, page_content='66\\nTrovato et al.\\n[181] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. 2023.\\nMimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425 (2023).\\n[182] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung\\nPoon, and Jianfeng Gao. 2023. Llava-med: Training a large language-and-vision assistant for biomedicine in one day.\\nAdvances in Neural Information Processing Systems 36 (2023), 28541–28564.\\n[183] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang. 2020. Unicoder-vl: A universal encoder for vision\\nand language by cross-modal pre-training. In Proceedings of the AAAI conference on artificial intelligence, Vol. 34.\\n11336–11344.\\n[184] Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. 2022. A survey on retrieval-augmented text generation.\\narXiv preprint arXiv:2202.01110 (2022).\\n[185] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pre-training\\nwith frozen image encoders and large language models. In International conference on machine learning. PMLR,\\n19730–19742.\\n[186] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training\\nfor unified vision-language understanding and generation. In International conference on machine learning. PMLR,\\n12888–12900.\\n[187] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. 2023.\\nVideochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355 (2023).\\n[188] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. 2024.\\nMvbench: A comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition. 22195–22206.\\n[189] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng\\nKong. 2023. Silkie: Preference distillation for large visual language models. arXiv preprint arXiv:2312.10665 (2023).\\n[190] Lei Li, Yongfeng Zhang, and Li Chen. 2023. Prompt distillation for efficient llm-based recommendation. In Proceedings\\nof the 32nd ACM International Conference on Information and Knowledge Management. 1348–1357.\\n[191] Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, and Furu Wei.\\n2023. Trocr: Transformer-based optical character recognition with pre-trained models. In Proceedings of the AAAI\\nconference on artificial intelligence, Vol. 37. 13094–13102.\\n[192] Shengzhi Li and Nima Tajbakhsh. 2023. Scigraphqa: A large-scale synthetic multi-turn question-answering dataset\\nfor scientific graphs. arXiv preprint arXiv:2308.03349 (2023).\\n[193] Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yukun Yan, Shuo Wang, and Ge Yu. 2024. Say more with less:\\nUnderstanding prompt learning behaviors through gist compression. arXiv preprint arXiv:2402.16058 (2024).\\n[194] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu\\nWei, et al. 2020. Oscar: Object-semantics aligned pre-training for vision-language tasks. In Computer Vision–ECCV\\n2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX 16. Springer, 121–137.\\n[195] Yongqi Li, Hongru Cai, Wenjie Wang, Leigang Qu, Yinwei Wei, Wenjie Li, Liqiang Nie, and Tat-Seng Chua. 2024. Rev-\\nolutionizing Text-to-Image Retrieval as Autoregressive Token-to-Voken Generation. arXiv preprint arXiv:2407.17274\\n(2024).\\n[196] Yucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. 2023. Compressing context to enhance inference efficiency\\nof large language models. arXiv preprint arXiv:2310.06201 (2023).\\n[197] Yangning Li, Yinghui Li, Xinyu Wang, Yong Jiang, Zhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao Zheng, Pengjun\\nXie, Philip S. Yu, Fei Huang, and Jingren Zhou. 2024. Benchmarking Multimodal Retrieval Augmented Generation\\nwith Dynamic VQA Dataset and Self-adaptive Planning Agent. (2024). arXiv:2411.02937 [cs.CL] https://arxiv.org/\\nabs/2411.02937\\n[198] Yanwei Li, Chengyao Wang, and Jiaya Jia. 2024. Llama-vid: An image is worth 2 tokens in large language models. In\\nEuropean Conference on Computer Vision. Springer, 323–340.\\n[199] Yongqi Li, Wenjie Wang, Leigang Qu, Liqiang Nie, Wenjie Li, and Tat-Seng Chua. 2024. Generative cross-modal\\nretrieval: Memorizing images in multimodal language models for retrieval and beyond. arXiv preprint arXiv:2402.10805\\n(2024).\\n[200] Yongqi Li, Nan Yang, Liang Wang, Furu Wei, and Wenjie Li. 2023. Multiview identifiers enhanced generative retrieval.\\narXiv preprint arXiv:2305.16675 (2023).\\n[201] Yongqi Li, Nan Yang, Liang Wang, Furu Wei, and Wenjie Li. 2024. Learning to rank in generative retrieval. In\\nProceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 8716–8723.\\n[202] Yongqi Li, Zhen Zhang, Wenjie Wang, Liqiang Nie, Wenjie Li, and Tat-Seng Chua. 2024. Distillation Enhanced\\nGenerative Retrieval. arXiv preprint arXiv:2402.10769 (2024).\\n[203] Zongqian Li, Yixuan Su, and Nigel Collier. 2024. 500xCompressor: Generalized Prompt Compression for Large\\nLanguage Models. arXiv preprint arXiv:2408.03094 (2024).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 66}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n67\\n[204] Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li, Van Tu Vu, et al.\\n2024. LEGO: language enhanced multi-modal grounding model. arXiv e-prints (2024), arXiv–2401.\\n[205] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. 2024.\\nMonkey: Image resolution and text label are important things for large multi-modal models. In proceedings of the\\nIEEE/CVF conference on computer vision and pattern recognition. 26763–26773.\\n[206] Minghui Liao, Baoguang Shi, Xiang Bai, Xinggang Wang, and Wenyu Liu. 2016. TextBoxes: A Fast Text Detector with\\na Single Deep Neural Network. arXiv:1611.06779 [cs.CV] https://arxiv.org/abs/1611.06779\\n[207] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Jinfa Huang, Junwu Zhang, Yatian Pang, Munan Ning,\\net al. 2024. Moe-llava: Mixture of experts for large vision-language models. arXiv preprint arXiv:2401.15947 (2024).\\n[208] Jimmy Lin and Xueguang Ma. 2021. A few brief notes on deepimpact, coil, and a conceptual framework for information\\nretrieval techniques. arXiv preprint arXiv:2106.14807 (2021).\\n[209] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. 2024. Vila: On pre-training\\nfor visual language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition.\\n26689–26699.\\n[210] Sheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, and Wei Ping. 2024. Mm-embed:\\nUniversal multimodal retrieval with multimodal llms. arXiv preprint arXiv:2411.02571 (2024).\\n[211] Weizhe Lin, Jinghong Chen, Jingbiao Mei, Alexandru Coca, and Bill Byrne. 2023. Fine-grained late-interaction\\nmulti-modal retrieval for retrieval augmented visual question answering. Advances in Neural Information Processing\\nSystems 36 (2023), 22820–22840.\\n[212] Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, and\\nHongsheng Li. 2024. Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you\\nwant. arXiv preprint arXiv:2403.20271 (2024).\\n[213] Barys Liskavets, Maxim Ushakov, Shuvendu Roy, Mark Klibanov, Ali Etemad, and Shane Luke. 2024. Prompt com-\\npression with context-aware sentence encoding for fast and improved llm inference. arXiv preprint arXiv:2409.01227\\n(2024).\\n[214] Alexander Liu and Samuel Yang. 2022. Masked autoencoders as the unified learners for pre-trained sentence\\nrepresentation. arXiv preprint arXiv:2208.00231 (2022).\\n[215] Chong Liu, Yuqi Zhang, Hongsong Wang, Weihua Chen, Fan Wang, Yan Huang, Yi-Dong Shen, and Liang Wang.\\n2023. Efficient token-guided image-text retrieval with consistent multimodal contrastive training. IEEE Transactions\\non Image Processing 32 (2023), 3622–3633.\\n[216] Dongyang Liu, Renrui Zhang, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng\\nJin, Kaipeng Zhang, et al. 2024. Sphinx-x: Scaling data and parameters for a family of multi-modal large language\\nmodels. arXiv preprint arXiv:2402.05935 (2024).\\n[217] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong\\nYu. 2023. Mmc: Advancing multimodal chart understanding with large-scale instruction tuning. arXiv preprint\\narXiv:2311.10774 (2023).\\n[218] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual Instruction Tuning. arXiv:2304.08485 [cs.CV]\\nhttps://arxiv.org/abs/2304.08485\\n[219] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. Advances in neural\\ninformation processing systems 36 (2023), 34892–34916.\\n[220] Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, and Yiming Qian. 2023. Tcra-llm: Token compression retrieval\\naugmented large language model for inference cost reduction. arXiv preprint arXiv:2310.15556 (2023).\\n[221] Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. 2024. Visualwebbench:\\nHow far have multimodal llms evolved in web page understanding and grounding? arXiv preprint arXiv:2404.05955\\n(2024).\\n[222] Qi Liu, Bo Wang, Nan Wang, and Jiaxin Mao. 2024. Leveraging passage embeddings for efficient listwise reranking\\nwith large language models. In THE WEB CONFERENCE 2025.\\n[223] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu,\\net al. 2024. Llava-plus: Learning to use tools for creating multimodal agents. In European Conference on Computer\\nVision. Springer, 126–142.\\n[224] Song Liu, Haoqi Fan, Shengsheng Qian, Yiru Chen, Wenkui Ding, and Zhongyuan Wang. 2021. Hit: Hierarchical\\ntransformer with momentum contrast for video-text retrieval. In Proceedings of the IEEE/CVF international conference\\non computer vision. 11915–11925.\\n[225] Shuo Liu, Kaining Ying, Hao Zhang, Yue Yang, Yuqi Lin, Tianle Zhang, Chuanhao Li, Yu Qiao, Ping Luo, Wenqi\\nShao, et al. 2024. Convbench: A multi-turn conversation evaluation benchmark with hierarchical capability for large\\nvision-language models. arXiv preprint arXiv:2403.20194 (2024).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 67}, page_content='68\\nTrovato et al.\\n[226] Ting Liu, Liangtao Shi, Richang Hong, Yue Hu, Quanjun Yin, and Linfeng Zhang. 2024. Multi-Stage Vision Token\\nDropping: Towards Efficient Multimodal Large Language Model. arXiv preprint arXiv:2411.10803 (2024).\\n[227] Weihao Liu, Fangyu Lei, Tongxu Luo, Jiahe Lei, Shizhu He, Jun Zhao, and Kang Liu. 2023. MMHQA-ICL: Multimodal\\nIn-context Learning for Hybrid Question Answering over Text, Tables and Images. arXiv preprint arXiv:2309.04790\\n(2023).\\n[228] Wenhan Liu, Yutao Zhu, and Zhicheng Dou. 2024. Demorank: Selecting effective demonstrations for large language\\nmodels in ranking task. arXiv preprint arXiv:2406.16332 (2024).\\n[229] Yinhan Liu. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 364\\n(2019).\\n[230] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui\\nHe, Ziwei Liu, et al. 2025. Mmbench: Is your multi-modal model an all-around player?. In European conference on\\ncomputer vision. Springer, 216–233.\\n[231] Yu Liu, Yanming Guo, Erwin M Bakker, and Michael S Lew. 2017. Learning a recurrent residual fusion network for\\nmultimodal matching. In Proceedings of the IEEE international conference on computer vision. 4107–4116.\\n[232] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. [n. d.].\\nTempcompass: Do video llms really understand videos?, 2024c. URL https://arxiv. org/abs/2403.00476 ([n. d.]).\\n[233] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen\\nJin, and Xiang Bai. 2024. OCRBench: on the hidden mystery of OCR in large multimodal models. Science China\\nInformation Sciences 67, 12 (2024), 220102.\\n[234] Zejun Liu, Fanglin Chen, Jun Xu, Wenjie Pei, and Guangming Lu. 2022. Image-text retrieval with cross-modal semantic\\nimportance consistency. IEEE Transactions on Circuits and Systems for Video Technology 33, 5 (2022), 2465–2476.\\n[235] Ziyu Liu, Tao Chu, Yuhang Zang, Xilin Wei, Xiaoyi Dong, Pan Zhang, Zijian Liang, Yuanjun Xiong, Yu Qiao, Dahua\\nLin, et al. 2024. MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset\\nfor LVLMs. arXiv preprint arXiv:2406.11833 (2024).\\n[236] Zhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Ziheng Li, Xizhou Zhu, Lewei Lu, Qifeng Chen, Yu Qiao, Jifeng\\nDai, et al. 2024. Controlllm: Augment language models with tools by searching on graphs. In European Conference on\\nComputer Vision. Springer, 89–105.\\n[237] Zhenghao Liu, Chenyan Xiong, Yuanhuiyi Lv, Zhiyuan Liu, and Ge Yu. 2022. Universal vision-language dense\\nretrieval: Learning a unified representation space for multi-modal retrieval. arXiv preprint arXiv:2209.00179 (2022).\\n[238] Xinwei Long, Jiali Zeng, Fandong Meng, Zhiyuan Ma, Kaiyan Zhang, Bowen Zhou, and Jie Zhou. 2024. Generative\\nmulti-modal knowledge retrieval with large language models. In Proceedings of the AAAI Conference on Artificial\\nIntelligence, Vol. 38. 18733–18741.\\n[239] Siyu Lou, Xuenan Xu, Mengyue Wu, and Kai Yu. 2022. Audio-text retrieval in context. In ICASSP 2022-2022 IEEE\\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 4793–4797.\\n[240] Haoyu Lu, Nanyi Fei, Yuqi Huo, Yizhao Gao, Zhiwu Lu, and Ji-Rong Wen. 2022. Cots: Collaborative two-stream\\nvision-language pre-training model for cross-modal retrieval. In Proceedings of the IEEE/CVF conference on computer\\nVision and pattern recognition. 15692–15701.\\n[241] Junyu Lu, Dixiang Zhang, Songxin Zhang, Zejian Xie, Zhuoyang Song, Cong Lin, Jiaxing Zhang, Bingyi Jing, and\\nPingjian Zhang. 2023. Lyrics: Boosting fine-grained language-vision alignment and comprehension via semantic-aware\\nvisual objects. arXiv preprint arXiv:2312.05278 (2023).\\n[242] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang,\\nMichel Galley, and Jianfeng Gao. 2023. Mathvista: Evaluating mathematical reasoning of foundation models in visual\\ncontexts. arXiv preprint arXiv:2310.02255 (2023).\\n[243] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and\\nAshwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering.\\nAdvances in Neural Information Processing Systems 35 (2022), 2507–2521.\\n[244] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. 2024. Ovis: Structural\\nembedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797 (2024).\\n[245] Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin. 2024. WildVision:\\nEvaluating Vision-Language Models in the Wild with Human Preferences. arXiv preprint arXiv:2406.11069 (2024).\\n[246] Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2021. Sparse, dense, and attentional representations\\nfor text retrieval. Transactions of the Association for Computational Linguistics 9 (2021), 329–345.\\n[247] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. 2022. Clip4clip: An empirical\\nstudy of clip for end to end video clip retrieval and captioning. Neurocomputing 508 (2022), 293–304.\\n[248] Jian Luo, Xuanang Chen, Ben He, and Le Sun. 2024. Prp-graph: Pairwise ranking prompting to llms with graph\\naggregation for effective text re-ranking. In Proceedings of the 62nd Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers). 5766–5776.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 68}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n69\\n[249] Tengchao Lv, Yupan Huang, Jingye Chen, Yuzhong Zhao, Yilin Jia, Lei Cui, Shuming Ma, Yaoyao Chang, Shaohan\\nHuang, Wenhui Wang, et al. 2023. Kosmos-2.5: A multimodal literate model. arXiv preprint arXiv:2309.11419 (2023).\\n[250] Haoyu Ma, Handong Zhao, Zhe Lin, Ajinkya Kale, Zhangyang Wang, Tong Yu, Jiuxiang Gu, Sunav Choudhary, and\\nXiaohui Xie. 2022. Ei-clip: Entity-aware interventional contrastive learning for e-commerce cross-modal retrieval. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 18051–18061.\\n[251] Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Xiang Ji, and Xueqi Cheng. 2021. Prop: Pre-training with\\nrepresentative words prediction for ad-hoc retrieval. In Proceedings of the 14th ACM International Conference on Web\\nSearch and Data Mining. 283–291.\\n[252] Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Yingyan Li, and Xueqi Cheng. 2021. B-PROP: bootstrapped\\npre-training with representative words prediction for ad-hoc retrieval. In Proceedings of the 44th International ACM\\nSIGIR Conference on Research and Development in Information Retrieval. 1513–1522.\\n[253] Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin. 2024. Unifying Multimodal Retrieval via\\nDocument Screenshot Embedding. arXiv:2406.11251 [cs.IR] https://arxiv.org/abs/2406.11251\\n[254] Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin. 2024. Unifying multimodal retrieval via\\ndocument screenshot embedding. arXiv preprint arXiv:2406.11251 (2024).\\n[255] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2024. Fine-tuning llama for multi-stage text retrieval.\\nIn Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval.\\n2421–2425.\\n[256] Xueguang Ma, Xinyu Zhang, Ronak Pradeep, and Jimmy Lin. 2023. Zero-shot listwise document reranking with a\\nlarge language model. arXiv preprint arXiv:2305.02156 (2023).\\n[257] Yingzi Ma, Yulong Cao, Jiachen Sun, Marco Pavone, and Chaowei Xiao. 2024. Dolphins: Multimodal language model\\nfor driving. In European Conference on Computer Vision. Springer, 403–420.\\n[258] Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong,\\net al. 2024. Mmlongbench-doc: Benchmarking long-context document understanding with visualizations. arXiv\\npreprint arXiv:2407.01523 (2024).\\n[259] Zi-Ao Ma, Tian Lan, Rong-Cheng Tu, Yong Hu, Heyan Huang, and Xian-Ling Mao. 2024. Multi-modal Retrieval\\nAugmented Multi-modal Generation: A Benchmark, Evaluate Metrics and Strong Baselines. arXiv:2411.16365 [cs.CL]\\nhttps://arxiv.org/abs/2411.16365\\n[260] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. 2023. Video-chatgpt: Towards detailed\\nvideo understanding via large vision and language models. arXiv preprint arXiv:2306.05424 (2023).\\n[261] Raman Maini and Himanshu Aggarwal. 2009. Study and comparison of various image edge detection techniques.\\nInternational journal of image processing (IJIP) 3, 1 (2009), 1–11.\\n[262] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. 2023. Egoschema: A diagnostic benchmark for very\\nlong-form video language understanding. Advances in Neural Information Processing Systems 36 (2023), 46212–46244.\\n[263] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019. Ok-vqa: A visual question answering\\nbenchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern\\nrecognition. 3195–3204.\\n[264] Julieta Martinez, Holger H Hoos, and James J Little. 2014. Stacked quantizers for compositional vector compression.\\narXiv preprint arXiv:1411.2173 (2014).\\n[265] Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, and Shafiq Joty. 2023. Unichart: A universal\\nvision-language pretrained model for chart comprehension and reasoning. arXiv preprint arXiv:2305.14761 (2023).\\n[266] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. 2022. Chartqa: A benchmark for question\\nanswering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244 (2022).\\n[267] Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. 2022. Infographicvqa.\\nIn Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 1697–1706.\\n[268] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. 2021. Docvqa: A dataset for vqa on document images. In\\nProceedings of the IEEE/CVF winter conference on applications of computer vision. 2200–2209.\\n[269] Lang Mei, Jiaxin Mao, Gang Guo, and Ji-Rong Wen. 2022. Learning Probabilistic Box Embeddings for Effective and\\nEfficient Ranking. In Proceedings of the ACM Web Conference 2022. 473–482.\\n[270] Lang Mei, Jiaxin Mao, Juan Hu, Naiqiang Tan, Hua Chai, and Ji-Rong Wen. 2023. Improving first-stage retrieval of\\npoint-of-interest search by pre-training models. ACM Transactions on Information Systems 42, 3 (2023), 1–27.\\n[271] Xinhao Mei, Xubo Liu, Jianyuan Sun, Mark D Plumbley, and Wenwu Wang. 2022. On metric learning for audio-text\\ncross-modal retrieval. arXiv preprint arXiv:2203.15537 (2022).\\n[272] Thomas Mensink, Jasper Uijlings, Lluis Castrejon, Arushi Goel, Felipe Cadar, Howard Zhou, Fei Sha, André Araujo,\\nand Vittorio Ferrari. 2023. Encyclopedic VQA: Visual questions about detailed properties of fine-grained categories.\\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision. 3113–3124.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 69}, page_content='70\\nTrovato et al.\\n[273] Antoine Miech, Ivan Laptev, and Josef Sivic. 2018. Learning a text-video embedding from incomplete and heteroge-\\nneous data. arXiv preprint arXiv:1804.02516 (2018).\\n[274] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019.\\nHowto100m: Learning a text-video embedding by watching hundred million narrated video clips. In Proceedings of\\nthe IEEE/CVF international conference on computer vision. 2630–2640.\\n[275] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. 2019. Ocr-vqa: Visual question\\nanswering by reading text in images. In 2019 international conference on document analysis and recognition (ICDAR).\\nIEEE, 947–952.\\n[276] Niluthpol Chowdhury Mithun, Juncheng Li, Florian Metze, and Amit K Roy-Chowdhury. 2018. Learning joint\\nembedding with multimodal cues for cross-modal video-text retrieval. In Proceedings of the 2018 ACM on international\\nconference on multimedia retrieval. 19–27.\\n[277] Debjyoti Mondal, Suraj Modi, Subhadarshi Panda, Rituraj Singh, and Godawari Sudhakar Rao. 2024. Kam-cot:\\nKnowledge augmented multimodal chain-of-thoughts reasoning. In Proceedings of the AAAI conference on artificial\\nintelligence, Vol. 38. 18798–18806.\\n[278] Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh,\\nPrakash Murugesan, Peyman Heidari, Yue Liu, et al. 2024. Anymal: An efficient and scalable any-modality augmented\\nlanguage model. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry\\nTrack. 1314–1332.\\n[279] Jesse Mu, Xiang Li, and Noah Goodman. 2023. Learning to compress prompts with gist tokens. Advances in Neural\\nInformation Processing Systems 36 (2023), 19327–19352.\\n[280] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and\\nPing Luo. 2023. Embodiedgpt: Vision-language pre-training via embodied chain of thought. Advances in Neural\\nInformation Processing Systems 36 (2023), 25081–25094.\\n[281] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. Ms\\nmarco: A human-generated machine reading comprehension dataset. (2016).\\n[282] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT. arXiv preprint arXiv:1901.04085 (2019).\\n[283] Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020. Document ranking with a pretrained sequence-to-sequence\\nmodel. arXiv preprint arXiv:2003.06713 (2020).\\n[284] Rodrigo Nogueira, Jimmy Lin, and AI Epistemic. 2019. From doc2query to docTTTTTquery. Online preprint 6, 2\\n(2019).\\n[285] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-stage document ranking with BERT. arXiv\\npreprint arXiv:1910.14424 (2019).\\n[286] Shubham Singh Paliwal, D Vishwanath, Rohit Rahul, Monika Sharma, and Lovekesh Vig. 2019. Tablenet: Deep\\nlearning model for end-to-end table detection and tabular data extraction from scanned document images. In 2019\\nInternational Conference on Document Analysis and Recognition (ICDAR). IEEE, 128–133.\\n[287] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. 2023. Kosmos-g: Generating\\nimages in context with multimodal large language models. arXiv preprint arXiv:2310.02992 (2023).\\n[288] Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing\\nYang, Chin-Yew Lin, et al. 2024. Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt\\ncompression. arXiv preprint arXiv:2403.12968 (2024).\\n[289] Artemis Panagopoulou, Le Xue, Ning Yu, Junnan Li, Dongxu Li, Shafiq Joty, Ran Xu, Silvio Savarese, Caiming Xiong,\\nand Juan Carlos Niebles. 2023. X-instructblip: A framework for aligning x-modal instruction-aware representations\\nto llms and emergent cross-modal reasoning. arXiv preprint arXiv:2311.18799 (2023).\\n[290] Yanwei Pang, Yuan Yuan, Xuelong Li, and Jing Pan. 2011. Efficient HOG human detection. Signal processing 91, 4\\n(2011), 773–781.\\n[291] Jae Sung Park, Chandra Bhagavatula, Roozbeh Mottaghi, Ali Farhadi, and Yejin Choi. 2020. Visualcomet: Reasoning\\nabout the dynamic context of a still image. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK,\\nAugust 23–28, 2020, Proceedings, Part V 16. Springer, 508–524.\\n[292] Andrew Parry, Sean MacAvaney, and Debasis Ganguly. 2024. Top-down partitioning for efficient list-wise ranking.\\narXiv preprint arXiv:2405.14589 (2024).\\n[293] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. 2023. Kosmos-2:\\nGrounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824 (2023).\\n[294] Zhiyuan Peng, Xuyang Wu, Qifan Wang, Sravanthi Rajanala, and Yi Fang. 2024. Q-peft: Query-dependent parameter\\nefficient fine-tuning for text reranking with large language models. arXiv preprint arXiv:2404.04522 (2024).\\n[295] Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong, Jipeng Zhang, Lewei Yao, Jianhua Han, Hang Xu, Lingpeng\\nKong, et al. 2023. Detgpt: Detect what you need via reasoning. arXiv preprint arXiv:2305.14167 (2023).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 70}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n71\\n[296] Ronak Pradeep, Rodrigo Nogueira, and Jimmy Lin. 2021. The expando-mono-duo design pattern for text ranking\\nwith pretrained sequence-to-sequence models. arXiv preprint arXiv:2101.05667 (2021).\\n[297] Xiao Pu, Tianxing He, and Xiaojun Wan. 2024. Style-Compress: An LLM-Based Prompt Compression Framework\\nConsidering Task-Specific Styles. arXiv preprint arXiv:2410.14042 (2024).\\n[298] Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, et al.\\n2024. Cogcom: Train large vision-language models diving into details through chain of manipulations. arXiv preprint\\narXiv:2402.04236 (2024).\\n[299] Jinwei Qi, Yuxin Peng, and Yuxin Yuan. 2018. Cross-media multi-level alignment with relation attention network.\\narXiv preprint arXiv:1804.09539 (2018).\\n[300] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei,\\nZhe Wei, Miaoxuan Zhang, et al. 2024. We-math: Does your large multimodal model achieve human-like mathematical\\nreasoning? arXiv preprint arXiv:2407.01284 (2024).\\n[301] Jie Qin, Jie Wu, Weifeng Chen, Yuxi Ren, Huixia Li, Hefeng Wu, Xuefeng Xiao, Rui Wang, and Shilei Wen. 2024.\\nDiffusionGPT: LLM-driven text-to-image generation system. arXiv preprint arXiv:2401.10061 (2024).\\n[302] Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald\\nMetzler, et al. 2023. Large language models are effective text rankers with pairwise ranking prompting. arXiv preprint\\narXiv:2306.17563 (2023).\\n[303] Leigang Qu, Haochuan Li, Tan Wang, Wenjie Wang, Yongqi Li, Liqiang Nie, and Tat-Seng Chua. 2024. Unified\\ntext-to-image generation and retrieval. arXiv preprint arXiv:2406.05814 (2024).\\n[304] Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang.\\n2020. RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering.\\narXiv preprint arXiv:2010.08191 (2020).\\n[305] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda\\nAskell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision.\\nIn International conference on machine learning. PMLR, 8748–8763.\\n[306] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are\\nunsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.\\n[307] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M\\nAnwer, Eric Xing, Ming-Hsuan Yang, and Fahad S Khan. 2024. Glamm: Pixel grounding large multimodal model. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 13009–13018.\\n[308] David Rau, Shuai Wang, Hervé Déjean, and Stéphane Clinchant. 2024. Context embeddings for efficient answer\\ngeneration in rag. arXiv preprint arXiv:2407.09252 (2024).\\n[309] Revanth Gangi Reddy, JaeHyeok Doo, Yifei Xu, Md Arafat Sultan, Deevya Swain, Avirup Sil, and Heng Ji. 2024. FIRST:\\nFaster Improved Listwise Reranking with Single Token Decoding. arXiv preprint arXiv:2406.15657 (2024).\\n[310] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin. 2024. Pixellm:\\nPixel reasoning with large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition. 26374–26383.\\n[311] TIMODAL RETRIEVAL, KNOWLEDGE-ENHANCED RERANKING, and NOISE-INJECTED TRAINING. [n. d.]. MLLM\\nIS A STRONG RERANKER: ADVANCING MUL. ([n. d.]).\\n[312] Monica Riedler and Stefan Langer. 2024. Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial\\nApplications. arXiv:2410.21943 [cs.CL] https://arxiv.org/abs/2410.21943\\n[313] Jonathan Roberts, Kai Han, Neil Houlsby, and Samuel Albanie. 2024. Scifibench: Benchmarking large multimodal\\nmodels for scientific figure interpretation. arXiv preprint arXiv:2405.08807 (2024).\\n[314] Stephen Robertson. 2004. Understanding inverse document frequency: on theoretical arguments for IDF. Journal of\\ndocumentation 60, 5 (2004), 503–520.\\n[315] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations\\nand Trends® in Information Retrieval 3, 4 (2009), 333–389.\\n[316] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. 1995. Okapi at\\nTREC-3. Nist Special Publication Sp 109 (1995), 109.\\n[317] Dongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Cheng Jiayang, Cunxiang Wang,\\nShichao Sun, Huanyu Li, et al. 2024. Ragchecker: A fine-grained framework for diagnosing retrieval-augmented\\ngeneration. Advances in Neural Information Processing Systems 37 (2024), 21999–22027.\\n[318] Devendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke\\nZettlemoyer. 2022. Improving passage retrieval with zero-shot question generation. arXiv preprint arXiv:2204.07496\\n(2022).\\n[319] Gerard Salton and Christopher Buckley. 1988. Term-weighting approaches in automatic text retrieval. Information\\nprocessing & management 24, 5 (1988), 513–523.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 71}, page_content='72\\nTrovato et al.\\n[320] Gerard Salton, Anita Wong, and Chung-Shu Yang. 1975. A vector space model for automatic indexing. Commun.\\nACM 18, 11 (1975), 613–620.\\n[321] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. 2022. A-okvqa: A\\nbenchmark for visual question answering using world knowledge. In European conference on computer vision. Springer,\\n146–162.\\n[322] Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar. 2019. Kvqa: Knowledge-aware visual\\nquestion answering. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33. 8876–8884.\\n[323] Shivam Shandilya, Menglin Xia, Supriyo Ghosh, Huiqiang Jiang, Jue Zhang, Qianhui Wu, and Victor Rühle.\\n2024. TACO-RL: Task Aware Prompt Compression Optimization with Reinforcement Learning. arXiv preprint\\narXiv:2409.13035 (2024).\\n[324] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. 2024. Visual\\ncot: Advancing multi-modal language models with a comprehensive dataset and benchmark for chain-of-thought\\nreasoning. Advances in Neural Information Processing Systems 37 (2024), 8612–8642.\\n[325] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving\\nai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems 36 (2023),\\n38154–38180.\\n[326] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and\\nDouwe Kiela. 2022. Flava: A foundational language and vision alignment model. In Proceedings of the IEEE/CVF\\nconference on computer vision and pattern recognition. 15638–15650.\\n[327] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.\\n2019. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition. 8317–8326.\\n[328] Hrituraj Singh, Anshul Nasery, Denil Mehta, Aishwarya Agarwal, Jatin Lamba, and Balaji Vasan Srinivasan. 2021.\\nMIMOQA: Multimodal Input Multimodal Output Question Answering. In Proceedings of the 2021 Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language Technologies, Kristina\\nToutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell,\\nTanmoy Chakraborty, and Yichao Zhou (Eds.). Association for Computational Linguistics, Online, 5317–5332. doi:10.\\n18653/v1/2021.naacl-main.418\\n[329] Hrituraj Singh, Anshul Nasery, Denil Mehta, Aishwarya Agarwal, Jatin Lamba, and Balaji Vasan Srinivasan. 2021.\\nMimoqa: Multimodal input multimodal output question answering. In Proceedings of the 2021 conference of the north\\namerican chapter of the association for computational linguistics: Human language technologies. 5317–5332.\\n[330] Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana, and Suranga\\nNanayakkara. 2023. Improving the domain adaptation of retrieval augmented generation (RAG) models for open\\ndomain question answering. Transactions of the Association for Computational Linguistics 11 (2023), 1–17.\\n[331] Charlie Snell, Dan Klein, and Ruiqi Zhong. 2022. Learning by distilling context. arXiv preprint arXiv:2209.15189\\n(2022).\\n[332] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo,\\nTian Ye, Yanting Zhang, et al. 2024. Moviechat: From dense token to sparse memory for long video understanding. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 18221–18232.\\n[333] Yale Song and Mohammad Soleymani. 2019. Polysemous visual-semantic embedding for cross-modal retrieval. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1979–1988.\\n[334] Yiping Song, Rui Yan, Cheng-Te Li, Jian-Yun Nie, Ming Zhang, and Dongyan Zhao. 2018. An Ensemble of Retrieval-\\nBased and Generation-Based Human-Computer Conversation Systems. (2018).\\n[335] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. 2023. Pandagpt: One model to instruction-follow\\nthem all. arXiv preprint arXiv:2305.16355 (2023).\\n[336] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. 2018. A corpus for reasoning about\\nnatural language grounded in photographs. arXiv preprint arXiv:1811.00491 (2018).\\n[337] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun\\nHuang, and Xinlong Wang. 2024. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition. 14398–14409.\\n[338] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\\nHuang, and Xinlong Wang. 2023. Emu: Generative pretraining in multimodality. arXiv preprint arXiv:2307.05222\\n(2023).\\n[339] Weiwei Sun, Zheng Chen, Xinyu Ma, Lingyong Yan, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and\\nZhaochun Ren. 2023. Instruction distillation makes large language models efficient zero-shot rankers. arXiv preprint\\narXiv:2311.01555 (2023).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 72}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n73\\n[340] Weiwei Sun, Lingyong Yan, Zheng Chen, Shuaiqiang Wang, Haichao Zhu, Pengjie Ren, Zhumin Chen, Dawei Yin,\\nMaarten Rijke, and Zhaochun Ren. 2024. Learning to tokenize for generative retrieval. Advances in Neural Information\\nProcessing Systems 36 (2024).\\n[341] Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun\\nRen. 2023. Is ChatGPT good at search? investigating large language models as re-ranking agents. arXiv preprint\\narXiv:2304.09542 (2023).\\n[342] Manan Suri, Puneet Mathur, Franck Dernoncourt, Kanika Goswami, Ryan A. Rossi, and Dinesh Manocha. 2024.\\nVisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal Retrieval-Augmented Generation.\\narXiv:2412.10704 [cs.CL] https://arxiv.org/abs/2412.10704\\n[343] Dídac Surís, Sachit Menon, and Carl Vondrick. 2023. Vipergpt: Visual inference via python execution for reasoning.\\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision. 11888–11898.\\n[344] Adiba Tabassum and Shweta A Dhondse. 2015. Text detection using MSER and stroke width transform. In 2015 Fifth\\nInternational Conference on Communication Systems and Network Technologies. IEEE, 568–571.\\n[345] Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi,\\nand Jonathan Berant. 2021. Multimodalqa: Complex question answering over text, tables and images. arXiv preprint\\narXiv:2104.06039 (2021).\\n[346] Sijun Tan, Xiuyu Li, Shishir Patil, Ziyang Wu, Tianjun Zhang, Kurt Keutzer, Joseph E Gonzalez, and Raluca Ada Popa.\\n2024. Lloco: Learning long contexts offline. arXiv preprint arXiv:2404.07979 (2024).\\n[347] Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito. 2023. SlideVQA: A\\nDataset for Document Visual Question Answering on Multiple Images. arXiv:2301.04883 [cs.CL] https://arxiv.org/\\nabs/2301.04883\\n[348] Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. 2021. Visualmrc: Machine reading comprehension on document\\nimages. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 13878–13888.\\n[349] Raphael Tang, Xinyu Zhang, Xueguang Ma, Jimmy Lin, and Ferhan Ture. 2023. Found in the middle: Permutation\\nself-consistency improves listwise ranking in large language models. arXiv preprint arXiv:2310.07712 (2023).\\n[350] Xu Tang, Yijing Wang, Jingjing Ma, Xiangrong Zhang, Fang Liu, and Licheng Jiao. 2023. Interacting-enhancing\\nfeature transformer for cross-modal remote-sensing image and text retrieval. IEEE Transactions on Geoscience and\\nRemote Sensing 61 (2023), 1–15.\\n[351] Zineng Tang, Ziyi Yang, Mahmoud Khademi, Yang Liu, Chenguang Zhu, and Mohit Bansal. 2024. Codi-2: In-context\\ninterleaved and interactive any-to-any generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition. 27425–27434.\\n[352] Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta,\\net al. 2022. Transformer memory as a differentiable search index. Advances in Neural Information Processing Systems\\n35 (2022), 21831–21843.\\n[353] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,\\nAndrew M Dai, Anja Hauth, Katie Millican, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv\\npreprint arXiv:2312.11805 (2023).\\n[354] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. Beir: A heterogenous\\nbenchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663 (2021).\\n[355] Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong\\nLu, Jie Zhou, et al. 2024. Mm-interleaved: Interleaved image-text generative modeling via multi-modal feature\\nsynchronizer. arXiv preprint arXiv:2401.10208 (2024).\\n[356] Kaibin Tian, Ruixiang Zhao, Zijie Xin, Bangxiang Lan, and Xirong Li. 2024. Holistic Features are almost Sufficient\\nfor Text-to-Video Retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\\n17138–17147.\\n[357] Anthony Meng Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, and Steven CH Hoi. 2022. Plug-and-play vqa:\\nZero-shot vqa by conjoining large pretrained models with zero training. arXiv preprint arXiv:2210.08773 (2022).\\n[358] Rubèn Tito, Dimosthenis Karatzas, and Ernest Valveny. 2023. Hierarchical multimodal transformers for multipage\\ndocvqa. Pattern Recognition 144 (2023), 109834.\\n[359] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang,\\nShusheng Yang, Adithya Iyer, Xichen Pan, et al. 2024. Cambrian-1: A fully open, vision-centric exploration of\\nmultimodal llms. arXiv preprint arXiv:2406.16860 (2024).\\n[360] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. 2024. Eyes wide shut? exploring\\nthe visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition. 9568–9578.\\n[361] Atousa Torabi, Niket Tandon, and Leonid Sigal. 2016. Learning language-visual embedding for movie understanding\\nwith natural-language. arXiv preprint arXiv:1609.08124 (2016).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 73}, page_content='74\\nTrovato et al.\\n[362] A Vaswani. 2017. Attention is all you need. Advances in Neural Information Processing Systems (2017).\\n[363] Thorsten Wagner and Hans-Gerd Lipinski. 2013. IJBlob: an ImageJ library for connected component analysis and\\nshape analysis. Journal of Open Research Software 1, 1 (2013).\\n[364] Ao Wang, Fengyuan Sun, Hui Chen, Zijia Lin, Jungong Han, and Guiguang Ding. 2024. [CLS] Token Tells Everything\\nNeeded for Training-free Efficient MLLMs. arXiv preprint arXiv:2412.05819 (2024).\\n[365] Andong Wang, Bo Wu, Sunli Chen, Zhenfang Chen, Haotian Guan, Wei-Ning Lee, Li Erran Li, and Chuang Gan. 2024.\\nSOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. 13384–13394.\\n[366] Chenyu Wang, Weixin Luo, Qianyu Chen, Haonan Mai, Jindi Guo, Sixun Dong, Zhengxin Li, Lin Ma, Shenghua Gao,\\net al. 2024. Tool-lmm: A large multi-modal model for tool agent learning. arXiv e-prints (2024), arXiv–2401.\\n[367] Dongsheng Wang, Natraj Raman, Mathieu Sibue, Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh\\nNourbakhsh, and Xiaomo Liu. 2023. DocLLM: A layout-aware generative language model for multimodal document\\nunderstanding. arXiv preprint arXiv:2401.00908 (2023).\\n[368] Fei Wang, Xingyu Fu, James Y Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou,\\nKai Zhang, et al. 2024. MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding. arXiv\\npreprint arXiv:2406.09411 (2024).\\n[369] Jinyu Wang, Jingjing Fu, Rui Wang, Lei Song, and Jiang Bian. 2025. PIKE-RAG: sPecIalized KnowledgE and Rationale\\nAugmented Generation. arXiv:2501.11551 [cs.CL] https://arxiv.org/abs/2501.11551\\n[370] Jian Wang, Yonghao He, Cuicui Kang, Shiming Xiang, and Chunhong Pan. 2015. Image-text cross-modal retrieval via\\nmodality-specific feature learning. In Proceedings of the 5th ACM on International Conference on Multimedia Retrieval.\\n347–354.\\n[371] Jiamian Wang, Guohao Sun, Pichao Wang, Dongfang Liu, Sohail Dianat, Majid Rabbani, Raghuveer Rao, and Zhiqiang\\nTao. 2024. Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition. 16551–16560.\\n[372] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. 2024. Measuring multimodal\\nmathematical reasoning with math-vision dataset. arXiv preprint arXiv:2402.14804 (2024).\\n[373] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei.\\n2022. Simlm: Pre-training with representation bottleneck for dense passage retrieval. arXiv preprint arXiv:2207.02578\\n(2022).\\n[374] Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel. 2017. Fvqa: Fact-based visual question\\nanswering. IEEE transactions on pattern analysis and machine intelligence 40, 10 (2017), 2413–2427.\\n[375] Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, and Anthony Dick. 2015. Explicit knowledge-based\\nreasoning for visual question answering. arXiv preprint arXiv:1511.02570 (2015).\\n[376] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao\\nDong, et al. 2024. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035 (2024).\\n[377] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song\\nXiXuan, et al. 2024. Cogvlm: Visual expert for pretrained language models. Advances in Neural Information Processing\\nSystems 37 (2024), 121475–121499.\\n[378] Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhenhang Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu,\\nZhiguo Cao, et al. 2023. The all-seeing project: Towards panoptic visual recognition and understanding of the open\\nworld. arXiv preprint arXiv:2308.01907 (2023).\\n[379] Xinfeng Wang, Jin Cui, Yoshimi Suzuki, and Fumiyo Fukumoto. 2024. Rdrec: Rationale distillation for llm-based\\nrecommendation. arXiv preprint arXiv:2405.10587 (2024).\\n[380] Xiaodan Wang, Lei Li, Zhixu Li, Xuwu Wang, Xiangru Zhu, Chengyu Wang, Jun Huang, and Yanghua Xiao. 2023.\\nAgree: Aligning cross-modal entities for image-text retrieval upon vision-language pre-trained models. In Proceedings\\nof the Sixteenth ACM International Conference on Web Search and Data Mining. 456–464.\\n[381] Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas\\nBertasius, Mohit Bansal, et al. 2024. Mementos: A comprehensive benchmark for multimodal large language model\\nreasoning over image sequences. arXiv preprint arXiv:2401.10529 (2024).\\n[382] Xinyu Wang, Bohan Zhuang, and Qi Wu. 2024. Modaverse: Efficiently transforming modalities with llms. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 26606–26616.\\n[383] Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai\\nZhao, Zheng Liu, et al. 2022. A neural corpus indexer for document retrieval. Advances in Neural Information\\nProcessing Systems 35 (2022), 25600–25614.\\n[384] Yan Wang, Yuting Su, Wenhui Li, Jun Xiao, Xuanya Li, and An-An Liu. 2023. Dual-path rare content enhancement\\nnetwork for image and text matching. IEEE Transactions on Circuits and Systems for Video Technology 33, 10 (2023),\\n6144–6158.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 74}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n75\\n[385] Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. 2023. Learning to filter context\\nfor retrieval-augmented generation. arXiv preprint arXiv:2311.08377 (2023).\\n[386] Zheng Wang, Zhenwei Gao, Mengqun Han, Yang Yang, and Heng Tao Shen. 2024. Estimating the Semantics via\\nSector Embedding for Image-Text Retrieval. IEEE Transactions on Multimedia (2024).\\n[387] Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu,\\nSadhika Malladi, et al. 2024. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. arXiv\\npreprint arXiv:2406.18521 (2024).\\n[388] Zheng Wang, Xing Xu, Jiwei Wei, Ning Xie, Yang Yang, and Heng Tao Shen. 2024. Semantics disentangling for\\ncross-modal retrieval. IEEE Transactions on Image Processing 33 (2024), 2226–2237.\\n[389] Zihan Wang, Yujia Zhou, Yiteng Tu, and Zhicheng Dou. 2023. NOVO: learnable and interpretable document identifiers\\nfor model-based IR. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management.\\n2656–2665.\\n[390] Jônatas Wehrmann and Rodrigo C Barros. 2018. Bidirectional retrieval made simple. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition. 7718–7726.\\n[391] Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. 2024. Uniir:\\nTraining and benchmarking universal multimodal information retrievers. In European Conference on Computer Vision.\\nSpringer, 387–404.\\n[392] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, En Yu, Jianjian Sun, Chunrui Han, and Xiangyu\\nZhang. 2024. Small language model meets with reinforced vision vocabulary. arXiv preprint arXiv:2401.12503 (2024).\\n[393] Wei Wei, Jiabin Tang, Lianghao Xia, Yangqin Jiang, and Chao Huang. 2024. Promptmm: Multi-modal knowledge\\ndistillation for recommendation with prompt-tuning. In Proceedings of the ACM Web Conference 2024. 3217–3228.\\n[394] Haoyang Wen, Honglei Zhuang, Hamed Zamani, Alexander Hauptmann, and Michael Bendersky. 2024. Multimodal\\nreranking for knowledge-intensive visual question answering. arXiv preprint arXiv:2407.12277 (2024).\\n[395] Weixi Weng, Jieming Zhu, Xiaojun Meng, Hao Zhang, Rui Zhang, and Chun Yuan. 2024. Learning to Compress\\nContexts for Efficient Knowledge-based Visual Question Answering. arXiv preprint arXiv:2409.07331 (2024).\\n[396] David Wingate, Mohammad Shoeybi, and Taylor Sorensen. 2022. Prompt compression and contrastive conditioning\\nfor controllability and toxicity reduction in language models. arXiv preprint arXiv:2210.03162 (2022).\\n[397] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. 2023. Visual chatgpt:\\nTalking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671 (2023).\\n[398] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong\\nYan, Guangtao Zhai, et al. 2023. Q-bench: A benchmark for general-purpose foundation models on low-level vision.\\narXiv preprint arXiv:2309.14181 (2023).\\n[399] Penghao Wu and Saining Xie. 2024. V?: Guided visual search as a core mechanism in multimodal llms. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 13084–13094.\\n[400] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. 2024. Next-gpt: Any-to-any multimodal llm. In\\nForty-first International Conference on Machine Learning.\\n[401] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. 2024. NExT-GPT: Any-to-Any Multimodal LLM.\\narXiv:2309.05519 [cs.AI] https://arxiv.org/abs/2309.05519\\n[402] Wenhao Wu, Haipeng Luo, Bo Fang, Jingdong Wang, and Wanli Ouyang. 2023. Cap4video: What can auxiliary\\ncaptions do for text-video retrieval?. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition. 10704–10713.\\n[403] Renqiu Xia, Song Mao, Xiangchao Yan, Hongbin Zhou, Bo Zhang, Haoyang Peng, Jiahao Pi, Daocheng Fu, Wenjie\\nWu, Hancheng Ye, et al. 2024. DocGenome: An Open Large-scale Scientific Document Benchmark for Training and\\nTesting Multi-modal Large Language Models. arXiv preprint arXiv:2406.11633 (2024).\\n[404] Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Min Dou, Botian Shi, Junchi\\nYan, et al. 2024. Chartx & chartvlm: A versatile benchmark and foundation model for complicated chart reasoning.\\narXiv preprint arXiv:2402.12185 (2024).\\n[405] Chen-Wei Xie, Jianmin Wu, Yun Zheng, Pan Pan, and Xian-Sheng Hua. 2022. Token embeddings alignment for\\ncross-modal retrieval. In Proceedings of the 30th ACM International Conference on Multimedia. 4555–4563.\\n[406] Yifei Xin, Dongchao Yang, and Yuexian Zou. 2023. Improving text-audio retrieval by text-aware attention pooling and\\nprior matrix revised loss. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing\\n(ICASSP). IEEE, 1–5.\\n[407] Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang,\\nFeng Wu, et al. 2024. Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy\\nreduction. arXiv preprint arXiv:2410.17247 (2024).\\n[408] Guoxin Xiong, Meng Meng, Tianzhu Zhang, Dongming Zhang, and Yongdong Zhang. 2024. Reference-Aware Adaptive\\nNetwork for Image-Text Matching. IEEE Transactions on Circuits and Systems for Video Technology (2024).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 75}, page_content='76\\nTrovato et al.\\n[409] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020.\\nApproximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808\\n(2020).\\n[410] Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. Recomp: Improving retrieval-augmented lms with compression and\\nselective augmentation. arXiv preprint arXiv:2310.04408 (2023).\\n[411] Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao,\\nand Ping Luo. 2024. Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence (2024).\\n[412] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. 2020. Layoutlm: Pre-training of text\\nand layout for document image understanding. In Proceedings of the 26th ACM SIGKDD international conference on\\nknowledge discovery & data mining. 1192–1200.\\n[413] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang\\nChe, et al. 2020. Layoutlmv2: Multi-modal pre-training for visually-rich document understanding. arXiv preprint\\narXiv:2012.14740 (2020).\\n[414] Zhengzhuo Xu, Sinan Du, Yiyan Qi, Chengjin Xu, Chun Yuan, and Jian Guo. 2023. Chartbench: A benchmark for\\ncomplex visual reasoning in charts. arXiv preprint arXiv:2312.15915 (2023).\\n[415] Ikuya Yamada, Akari Asai, and Hannaneh Hajishirzi. 2021. Efficient passage retrieval with hashing for open-domain\\nquestion answering. arXiv preprint arXiv:2106.00882 (2021).\\n[416] Le Yan, Zhen Qin, Honglei Zhuang, Rolf Jagerman, Xuanhui Wang, Michael Bendersky, and Harrie Oosterhuis. 2024.\\nConsolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing. arXiv preprint\\narXiv:2404.11791 (2024).\\n[417] Siming Yan, Min Bai, Weifeng Chen, Xiong Zhou, Qixing Huang, and Li Erran Li. 2024. Vigor: Improving visual\\ngrounding of large vision language models with fine-grained reward modeling. In European Conference on Computer\\nVision. Springer, 37–53.\\n[418] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and CUI Bin. 2024. Mastering text-to-image\\ndiffusion: Recaptioning, planning, and generating with multimodal llms. In Forty-first International Conference on\\nMachine Learning.\\n[419] Song Yang, Qiang Li, Wenhui Li, Xuanya Li, and An-An Liu. 2022. Dual-level representation enhancement on\\ncharacteristic and context for image-text retrieval. IEEE Transactions on Circuits and Systems for Video Technology 32,\\n11 (2022), 8037–8050.\\n[420] Tianchi Yang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, and Qi Zhang. 2023. Auto\\nsearch indexer for end-to-end document retrieval. arXiv preprint arXiv:2310.12455 (2023).\\n[421] Xiangpeng Yang, Linchao Zhu, Xiaohan Wang, and Yi Yang. 2024. DGL: Dynamic Global-Local Prompt Tuning for\\nText-Video Retrieval. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 6540–6548.\\n[422] Yang Yang, Chubing Zhang, Yi-Chu Xu, Dianhai Yu, De-Chuan Zhan, and Jian Yang. 2021. Rethinking Label-Wise\\nCross-Modal Retrieval from A Semantic Sharing Perspective.. In IJCAI. 3300–3306.\\n[423] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael\\nZeng, and Lijuan Wang. 2023. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint\\narXiv:2303.11381 (2023).\\n[424] Zhen Yang, Yingxue Zhang, Fandong Meng, and Jie Zhou. 2023. Teal: Tokenize and embed all for multi-modal large\\nlanguage models. arXiv preprint arXiv:2311.04589 (2023).\\n[425] Linli Yao, Lei Li, Shuhuai Ren, Lean Wang, Yuanxin Liu, Xu Sun, and Lu Hou. 2024. Deco: Decoupling token\\ncompression from semantic abstraction in multimodal large language models. arXiv preprint arXiv:2405.20985 (2024).\\n[426] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng\\nTian, et al. 2023. mplug-docowl: Modularized multimodal large language model for document understanding. arXiv\\npreprint arXiv:2307.02499 (2023).\\n[427] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\\nYaya Shi, et al. 2023. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint\\narXiv:2304.14178 (2023).\\n[428] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. 2024.\\nmplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedings of the\\nieee/cvf conference on computer vision and pattern recognition. 13040–13051.\\n[429] Dongyi Yi, Guibo Zhu, Chenglin Ding, Zongshu Li, Dong Yi, and Jinqiao Wang. 2025. MME-Industry: A Cross-Industry\\nMultimodal Evaluation Benchmark. arXiv preprint arXiv:2501.16688 (2025).\\n[430] Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Xiaoshui Huang, Zhiyong Wang, Lu Sheng,\\nLei Bai, et al. 2024. Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark.\\nAdvances in Neural Information Processing Systems 36 (2024).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 76}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n77\\n[431] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo\\nLiu, et al. 2024. Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language models\\ntowards multitask agi. arXiv preprint arXiv:2404.16006 (2024).\\n[432] Chanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Minbyul Jeong, and Jaewoo Kang. 2024. Compact: Compressing\\nretrieved documents actively for question answering. arXiv preprint arXiv:2407.09014 (2024).\\n[433] Soyoung Yoon, Eunbi Choi, Jiyeon Kim, Hyeongu Yun, Yireun Kim, and Seung-won Hwang. 2024. Listt5: Listwise\\nreranking with fusion-in-decoder improves zero-shot retrieval. arXiv preprint arXiv:2402.15838 (2024).\\n[434] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang,\\nBrian Karrer, Shelly Sheynin, et al. 2023. Scaling autoregressive multi-modal models: Pretraining and instruction\\ntuning. arXiv preprint arXiv:2309.02591 2, 3 (2023), 3.\\n[435] Qinhan Yu, Zhiyou Xiao, Binghui Li, Zhengren Wang, Chong Chen, and Wentao Zhang. 2025. MRAMG-Bench: A\\nBeyondText Benchmark for Multimodal Retrieval-Augmented Multimodal Generation. arXiv preprint arXiv:2502.04176\\n(2025).\\n[436] Shi Yu, Zhenghao Liu, Chenyan Xiong, Tao Feng, and Zhiyuan Liu. 2021. Few-shot conversational dense retrieval.\\nIn Proceedings of the 44th International ACM SIGIR Conference on research and development in information retrieval.\\n829–838.\\n[437] Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan\\nLiu, et al. 2024. Visrag: Vision-based retrieval-augmented generation on multi-modality documents. arXiv preprint\\narXiv:2410.10594 (2024).\\n[438] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng,\\nMaosong Sun, et al. 2024. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional\\nhuman feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 13807–13816.\\n[439] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\\n2023. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490 (2023).\\n[440] Xiaohan Yu, Zhihan Yang, and Chong Chen. 2025. Unveiling the Potential of Multimodal Retrieval Augmented\\nGeneration with Planning. arXiv preprint arXiv:2501.15470 (2025).\\n[441] Youngjae Yu, Hyungjin Ko, Jongwook Choi, and Gunhee Kim. 2017. End-to-end concept word detection for video\\ncaptioning, retrieval, and question answering. In Proceedings of the IEEE conference on computer vision and pattern\\nrecognition. 3165–3173.\\n[442] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. 2019. Activitynet-qa: A dataset\\nfor understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial\\nIntelligence, Vol. 33. 9127–9134.\\n[443] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. 2024. Osprey:\\nPixel understanding with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition. 28202–28211.\\n[444] Zhengqing Yuan, Zhaoxu Li, Weiran Huang, Yanfang Ye, and Lichao Sun. 2023. Tinygpt-v: Efficient multimodal large\\nlanguage model via small backbones. arXiv preprint arXiv:2312.16862 (2023).\\n[445] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming\\nRen, Yuxuan Sun, et al. 2024. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark\\nfor expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9556–9567.\\n[446] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang,\\nHuan Sun, et al. 2024. Mmmu-pro: A more robust multi-discipline multimodal understanding benchmark. arXiv\\npreprint arXiv:2409.02813 (2024).\\n[447] Sukmin Yun, Haokun Lin, Rusiru Thushara, Mohammad Qazim Bhat, Yongxin Wang, Zutao Jiang, Mingkai Deng,\\nJinhong Wang, Tianhua Tao, Junbo Li, et al. 2024. Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation\\nFramework for Multimodal LLMs. arXiv preprint arXiv:2406.20098 (2024).\\n[448] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. From recognition to cognition: Visual commonsense\\nreasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 6720–6731.\\n[449] Hansi Zeng, Chen Luo, Bowen Jin, Sheikh Muhammad Sarwar, Tianxin Wei, and Hamed Zamani. 2024. Scalable and\\neffective generative information retrieval. In Proceedings of the ACM on Web Conference 2024. 1441–1452.\\n[450] Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, Tao Kong, and Ruihua\\nSong. 2024. What matters in training a gpt4-style language model with multimodal inputs?. In Proceedings of the\\n2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies (Volume 1: Long Papers). 7930–7957.\\n[451] Zhixiong Zeng and Wenji Mao. 2022. A comprehensive empirical study of vision-language pre-trained model for\\nsupervised cross-modal retrieval. arXiv preprint arXiv:2201.02772 (2022).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 77}, page_content='78\\nTrovato et al.\\n[452] ChengXiang Zhai et al. 2008. Statistical language models for information retrieval a critical review. Foundations and\\nTrends® in Information Retrieval 2, 3 (2008), 137–213.\\n[453] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2021. Jointly optimizing query\\nencoder and product quantization to improve retrieval performance. In Proceedings of the 30th ACM International\\nConference on Information & Knowledge Management. 2487–2496.\\n[454] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2021. Optimizing dense retrieval\\nmodel training with hard negatives. In Proceedings of the 44th International ACM SIGIR Conference on Research and\\nDevelopment in Information Retrieval. 1503–1512.\\n[455] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2022. Learning discrete representations\\nvia constrained clustering for effective and efficient dense retrieval. In Proceedings of the Fifteenth ACM International\\nConference on Web Search and Data Mining. 1328–1336.\\n[456] Erhan Zhang, Xingzhu Wang, Peiyuan Gong, Yankai Lin, and Jiaxin Mao. 2024. Usimagent: Large language models for\\nsimulating search users. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development\\nin Information Retrieval. 2687–2692.\\n[457] Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu,\\nShuyue Guo, et al. 2024. Cmmmu: A chinese massive multi-discipline multimodal understanding benchmark. arXiv\\npreprint arXiv:2401.11944 (2024).\\n[458] Hanqi Zhang, Chong Chen, Lang Mei, Qi Liu, and Jiaxin Mao. 2024. Mamba Retriever: Utilizing Mamba for Effective\\nand Efficient Dense Retrieval. In Proceedings of the 33rd ACM International Conference on Information and Knowledge\\nManagement. 4268–4272.\\n[459] Hang Zhang, Yeyun Gong, Yelong Shen, Jiancheng Lv, Nan Duan, and Weizhu Chen. 2021. Adversarial retriever-ranker\\nfor dense text retrieval. arXiv preprint arXiv:2110.03611 (2021).\\n[460] Hang Zhang, Xin Li, and Lidong Bing. 2023. Video-llama: An instruction-tuned audio-visual language model for\\nvideo understanding. arXiv preprint arXiv:2306.02858 (2023).\\n[461] Han Zhang, Hongwei Shen, Yiming Qiu, Yunjiang Jiang, Songlin Wang, Sulong Xu, Yun Xiao, Bo Long, and Wen-Yun\\nYang. 2021. Joint learning of deep retrieval model and product quantization based embedding index. In Proceedings of\\nthe 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 1718–1722.\\n[462] Jinxu Zhang, Yongqi Yu, and Yu Zhang. 2024. CREAM: coarse-to-fine retrieval and multi-modal efficient tuning for\\ndocument VQA. In Proceedings of the 32nd ACM International Conference on Multimedia. 925–934.\\n[463] Junyuan Zhang, Qintong Zhang, Bin Wang, Linke Ouyang, Zichen Wen, Ying Li, Ka-Ho Chow, Conghui He, and\\nWentao Zhang. 2024. OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation.\\narXiv:2412.02592 [cs.CV] https://arxiv.org/abs/2412.02592\\n[464] Longhui Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, and Min Zhang. 2023. A two-stage\\nadaptation of large language models for text ranking. arXiv preprint arXiv:2311.16720 (2023).\\n[465] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang\\nZhang, Shuangrui Ding, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He,\\nXingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. 2023. InternLM-XComposer: A Vision-Language Large Model\\nfor Advanced Text-image Comprehension and Composition. arXiv:2309.15112 [cs.CV] https://arxiv.org/abs/2309.15112\\n[466] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang,\\nLinke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li,\\nWenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang.\\n2024. InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and\\nOutput. arXiv:2407.03320 [cs.CV] https://arxiv.org/abs/2407.03320\\n[467] Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, and Zhicheng Dou. 2024. Compressing lengthy context\\nwith ultragist. arXiv preprint arXiv:2405.16635 (2024).\\n[468] Qi Zhang, Zhen Lei, Zhaoxiang Zhang, and Stan Z Li. 2020. Context-aware attention network for image-text retrieval.\\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 3536–3545.\\n[469] Qianchi Zhang, Hainan Zhang, Liang Pang, Hongwei Zheng, and Zhiming Zheng. 2024. AdaComp: Extractive\\nContext Compression with Adaptive Predictor for Retrieval-Augmented Large Language Models. arXiv preprint\\narXiv:2409.01579 (2024).\\n[470] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei\\nChang, Yu Qiao, et al. 2025. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?.\\nIn European Conference on Computer Vision. Springer, 169–186.\\n[471] Shunyu Zhang, Yaobo Liang, Ming Gong, Daxin Jiang, and Nan Duan. 2022. Multi-view document representation\\nlearning for open-domain dense retrieval. arXiv preprint arXiv:2203.08372 (2022).\\n[472] Shilong Zhang, Peize Sun, Shoufa Chen, Minn Xiao, Wenqi Shao, Wenwei Zhang, Yu Liu, Kai Chen, and Ping Luo.\\n[n. d.]. GPT4roi: Instruction tuning large language model on regionof-interest, 2024. In URL https://openreview.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 78}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\n79\\nnet/forum.\\n[473] Tianyu Zhang, Suyuchen Wang, Lu Li, Ge Zhang, Perouz Taslakian, Sai Rajeswar, Jie Fu, Bang Liu, and Yoshua Bengio.\\n2024. VCR: Visual Caption Restoration. arXiv preprint arXiv:2406.06462 (2024).\\n[474] Tao Zhang, Ziqi Zhang, Zongyang Ma, Yuxin Chen, Zhongang Qi, Chunfeng Yuan, Bing Li, Junfu Pu, Yuxuan Zhao,\\nZehua Xie, et al. 2024. m𝑅2AG: Multimodal Retrieval-Reflection-Augmented Generation for Knowledge-Based VQA.\\narXiv preprint arXiv:2411.15041 (2024).\\n[475] Xinyu Zhang, Sebastian Hofstätter, Patrick Lewis, Raphael Tang, and Jimmy Lin. 2023. Rank-without-gpt: Building\\ngpt-independent listwise rerankers on open-source large language models. arXiv preprint arXiv:2312.02969 (2023).\\n[476] Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie\\nLi, and Min Zhang. 2024. GME: Improving Universal Multimodal Retrieval by Multimodal LLMs. arXiv preprint\\narXiv:2412.16855 (2024).\\n[477] Yan Zhang, Zhong Ji, Di Wang, Yanwei Pang, and Xuelong Li. 2024. USER: Unified semantic enhancement with\\nmomentum contrast for image-text retrieval. IEEE Transactions on Image Processing (2024).\\n[478] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. 2023. Llavar: Enhanced\\nvisual instruction tuning for text-rich image understanding. arXiv preprint arXiv:2306.17107 (2023).\\n[479] Yidan Zhang, Ting Zhang, Dong Chen, Yujing Wang, Qi Chen, Xing Xie, Hao Sun, Weiwei Deng, Qi Zhang, Fan Yang,\\net al. 2024. Irgen: Generative modeling for image retrieval. In European Conference on Computer Vision. Springer,\\n21–41.\\n[480] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang,\\nQingsong Wen, Zhang Zhang, et al. 2024. MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution\\nReal-World Scenarios that are Difficult for Humans? arXiv preprint arXiv:2408.13257 (2024).\\n[481] Zheng Zhang, Chengquan Zhang, Wei Shen, Cong Yao, Wenyu Liu, and Xiang Bai. 2016. Multi-Oriented Text\\nDetection with Fully Convolutional Networks. arXiv:1604.04018 [cs.CV] https://arxiv.org/abs/1604.04018\\n[482] Bingchen Zhao, Yongshuo Zong, Letian Zhang, and Timothy Hospedales. 2024. Benchmarking Multi-Image Un-\\nderstanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning. arXiv\\npreprint arXiv:2406.12742 (2024).\\n[483] Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei, Hongyu Zhou, Jianjian Sun, Yuang Peng, Runpei Dong,\\nChunrui Han, et al. 2023. Chatspot: Bootstrapping multimodal llms via precise referring instruction tuning. arXiv\\npreprint arXiv:2307.09474 (2023).\\n[484] Shiyu Zhao, Zhenting Wang, Felix Juefei-Xu, Xide Xia, Miao Liu, Xiaofang Wang, Mingfu Liang, Ning Zhang,\\nDimitris N Metaxas, and Licheng Yu. 2024. Accelerating Multimodel Large Language Models by Searching Optimal\\nVision Token Reduction. arXiv preprint arXiv:2412.00556 (2024).\\n[485] Shengwei Zhao, Linhai Xu, Yuying Liu, and Shaoyi Du. 2023. Multi-grained representation learning for cross-modal\\nretrieval. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information\\nRetrieval. 2194–2198.\\n[486] Weichao Zhao, Hao Feng, Qi Liu, Jingqun Tang, Shu Wei, Binghong Wu, Lei Liao, Yongjie Ye, Hao Liu, Wengang\\nZhou, et al. 2024. Tabpedia: Towards comprehensive visual table understanding with concept synergy. arXiv preprint\\narXiv:2406.01326 (2024).\\n[487] Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang. 2023. Bubogpt: Enabling visual\\ngrounding in multi-modal llms. arXiv preprint arXiv:2307.08581 (2023).\\n[488] Zijia Zhao, Haoyu Lu, Yuqi Huo, Yifan Du, Tongtian Yue, Longteng Guo, Bingning Wang, Weipeng Chen, and Jing\\nLiu. 2024. Needle In A Video Haystack: A Scalable Synthetic Framework for Benchmarking Video MLLMs. arXiv\\npreprint arXiv:2406.09367 (2024).\\n[489] Liangli Zhen, Peng Hu, Xu Wang, and Dezhong Peng. 2019. Deep supervised cross-modal retrieval. In Proceedings of\\nthe IEEE/CVF conference on computer vision and pattern recognition. 10394–10403.\\n[490] Kaizhi Zheng, Xuehai He, and Xin Eric Wang. 2023. Minigpt-5: Interleaved vision-and-language generation via\\ngenerative vokens. arXiv preprint arXiv:2310.02239 (2023).\\n[491] Liu Zheng and Shao Yingxia. 2022. RetroMAE: Pre-training retrieval-oriented transformers via masked auto-encoder.\\narXiv: 2205.12035 (2022).\\n[492] Xiaoyang Zheng, Zilong Wang, Sen Li, Ke Xu, Tao Zhuang, Qingwen Liu, and Xiaoyi Zeng. 2023. Make: Vision-\\nlanguage pre-training based product retrieval in taobao search. In Companion Proceedings of the ACM Web Conference\\n2023. 356–360.\\n[493] Chenyu Zhou, Mengdan Zhang, Peixian Chen, Chaoyou Fu, Yunhang Shen, Xiawu Zheng, Xing Sun, and Rongrong\\nJi. 2024. VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models. arXiv preprint\\narXiv:2406.10228 (2024).\\n[494] Dong Zhou, Fang Lei, Lin Li, Yongmei Zhou, and Aimin Yang. 2024. Cross-Modal Interaction via Reinforcement\\nFeedback for Audio-Lyrics Retrieval. IEEE/ACM Transactions on Audio, Speech, and Language Processing (2024).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'format': 'PDF 1.5', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'author': '', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'keywords': '', 'moddate': '2025-04-15T00:00:41+00:00', 'trapped': '', 'modDate': 'D:20250415000041Z', 'creationDate': 'D:20250415000041Z', 'page': 79}, page_content='80\\nTrovato et al.\\n[495] Junjie Zhou, Zheng Liu, Shitao Xiao, Bo Zhao, and Yongping Xiong. 2024. VISTA: visualized text embedding for\\nuniversal multi-modal retrieval. arXiv preprint arXiv:2406.04292 (2024).\\n[496] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and\\nZheng Liu. 2024. MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding. arXiv preprint\\narXiv:2406.04264 (2024).\\n[497] Kun Zhou, Yeyun Gong, Xiao Liu, Wayne Xin Zhao, Yelong Shen, Anlei Dong, Jingwen Lu, Rangan Majumder, Ji-Rong\\nWen, Nan Duan, et al. 2022. Simans: Simple ambiguous negatives sampling for dense text retrieval. arXiv preprint\\narXiv:2210.11773 (2022).\\n[498] Tianshuo Zhou, Sen Mei, Xinze Li, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu, Yu Gu, and Ge Yu. 2023. MARVEL:\\nunlocking the multi-modal capability of dense retrieval via visual module plugin. arXiv preprint arXiv:2310.14037\\n(2023).\\n[499] Wangchunshu Zhou, Yuchen Eleanor Jiang, Ryan Cotterell, and Mrinmaya Sachan. 2023. Efficient prompting via\\ndynamic in-context learning. arXiv preprint arXiv:2305.11170 (2023).\\n[500] Yujia Zhou, Zhicheng Dou, and Ji-Rong Wen. 2023. Enhancing generative retrieval with reinforcement learning\\nfrom relevance feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing.\\n12481–12490.\\n[501] Yujia Zhou, Jing Yao, Zhicheng Dou, Ledell Wu, Peitian Zhang, and Ji-Rong Wen. 2022. Ultron: An ultimate retriever\\non corpus with a model-based indexer. arXiv preprint arXiv:2208.09257 (2022).\\n[502] Yu-Jia Zhou, Jing Yao, Zhi-Cheng Dou, Ledell Wu, and Ji-Rong Wen. 2023. DynamicRetriever: a pre-trained model-\\nbased IR system without an explicit index. Machine Intelligence Research 20, 2 (2023), 276–288.\\n[503] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei\\nLi, et al. 2023. Languagebind: Extending video-language pretraining to n-modality by language-based semantic\\nalignment. arXiv preprint arXiv:2310.01852 (2023).\\n[504] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language\\nunderstanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023).\\n[505] Dongsheng Zhu, Xunzhu Tang, Weidong Han, Jinghui Lu, Yukun Zhao, Guoliang Xing, Junfeng Wang, and Dawei\\nYin. 2024. Vislinginstruct: Elevating zero-shot learning in multi-modal language models with autonomous instruction\\noptimization. arXiv preprint arXiv:2402.07398 (2024).\\n[506] Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie Zhao, Hengshuang Zhao, Xiaohua Wang, and Ying Shan. 2023.\\nVl-gpt: A generative pre-trained transformer for vision and language understanding and generation. arXiv preprint\\narXiv:2312.09251 (2023).\\n[507] Yichen Zhu, Minjie Zhu, Ning Liu, Zhiyuan Xu, and Yaxin Peng. 2024. Llava-phi: Efficient multi-modal assistant\\nwith small language model. In Proceedings of the 1st International Workshop on Efficient Multimedia Computing under\\nLimited. 18–22.\\n[508] Zhengyuan Zhu, Daniel Lee, Hong Zhang, Sai Sree Harsha, Loic Feujio, Akash Maharaj, and Yunyao Li. 2024. MuRAR:\\nA Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering.\\narXiv:2408.08521 [cs.IR] https://arxiv.org/abs/2408.08521\\n[509] Honglei Zhuang, Zhen Qin, Kai Hui, Junru Wu, Le Yan, Xuanhui Wang, and Michael Bendersky. 2023. Beyond yes and\\nno: Improving zero-shot llm rankers via scoring fine-grained relevance labels. arXiv preprint arXiv:2310.14122 (2023).\\n[510] Honglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui, Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, and Michael Bendersky.\\n2023. Rankt5: Fine-tuning t5 for text ranking with ranking losses. In Proceedings of the 46th International ACM SIGIR\\nConference on Research and Development in Information Retrieval. 2308–2313.\\n[511] Shengyao Zhuang, Bing Liu, Bevan Koopman, and Guido Zuccon. 2023. Open-source large language models are\\nstrong zero-shot query likelihood models for document ranking. arXiv preprint arXiv:2310.13243 (2023).\\n[512] Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuccon, and Daxin Jiang. 2022. Bridging\\nthe gap between indexing and retrieval for differentiable search index with query generation. arXiv preprint\\narXiv:2206.10128 (2022).\\n[513] Justin Zobel and Alistair Moffat. 2006. Inverted files for text search engines. ACM computing surveys (CSUR) 38, 2\\n(2006), 6–es.\\n[514] Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, and Timothy Hospedales. 2024. Safety fine-tuning at\\n(almost) no cost: A baseline for vision large language models. arXiv preprint arXiv:2402.02207 (2024).\\nReceived 20 February 2007; revised 12 March 2009; accepted 5 June 2009\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'file_path': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'trapped': '', 'modDate': 'D:20210302011619Z', 'creationDate': 'D:20210302011619Z', 'page': 0}, page_content='TORFI et. al., NLP ADVANCEMENTS BY DEEP LEARNING\\n1\\nNatural Language Processing Advancements By\\nDeep Learning: A Survey\\nAmirsina Torﬁ, Member, IEEE, Rouzbeh A. Shirvani, Yaser Keneshloo, Nader Tavaf,\\nand Edward A. Fox, Fellow, IEEE\\nAbstract—Natural Language Processing (NLP) helps empower\\nintelligent machines by enhancing a better understanding of the\\nhuman language for linguistic-based human-computer communi-\\ncation. Recent developments in computational power and the ad-\\nvent of large amounts of linguistic data have heightened the need\\nand demand for automating semantic analysis using data-driven\\napproaches. The utilization of data-driven strategies is pervasive\\nnow due to the signiﬁcant improvements demonstrated through\\nthe usage of deep learning methods in areas such as Computer\\nVision, Automatic Speech Recognition, and in particular, NLP.\\nThis survey categorizes and addresses the different aspects and\\napplications of NLP that have beneﬁted from deep learning. It\\ncovers core NLP tasks and applications, and describes how deep\\nlearning methods and models advance these areas. We further\\nanalyze and compare different approaches and state-of-the-art\\nmodels.\\nIndex Terms—Natural Language Processing, Deep Learning,\\nArtiﬁcial Intelligence\\nI. INTRODUCTION\\nN\\nATURAL Language Processing (NLP) is a sub-discipline\\nof computer science providing a bridge between natural\\nlanguages and computers. It helps empower machines to un-\\nderstand, process, and analyze human language [1]. NLP’s sig-\\nniﬁcance as a tool aiding comprehension of human-generated\\ndata is a logical consequence of the context-dependency\\nof data. Data becomes more meaningful through a deeper\\nunderstanding of its context, which in turn facilitates text\\nanalysis and mining. NLP enables this with the communication\\nstructures and patterns of humans.\\nDevelopment of NLP methods is increasingly reliant on\\ndata-driven approaches which help with building more pow-\\nerful and robust models [2]–[4]. Recent advances in com-\\nputational power, as well as greater availability of big data,\\nenable deep learning, one of the most appealing approaches\\nin the NLP domain [2], [3], [5], especially given that deep\\nlearning has already demonstrated superior performance in\\nadjoining ﬁelds like Computer Vision [6]–[10] and Speech\\nRecognition [11]–[13]. These developments led to a paradigm\\nshift from traditional to novel data-driven approaches aimed\\nat advancing NLP. The reason behind this shift was simple:\\nnew approaches are more promising regarding results, and are\\neasier to engineer.\\nAmirsina Torﬁ, Yaser Keneshloo, and Edward A. Fox were with the\\nDepartment of Computer Science, Virginia Polytechnic Institute and State\\nUniversity, Blacksburg, VA, 24060 USA e-mail: (amirsina.torﬁ@gmail.com,\\nyaserkl@vt.edu, fox@vt.edu). Rouzbeh A. Shirvani is an independent re-\\nsearcher, e-mail: (rouzbeh.asghari@gmail.com). Nader Tavaf was with the\\nUniversity of Minnesota Twin Cities, Minneapolis, MN, 55455 USA e-mail:\\n(tavaf001@umn.edu).\\nAs a sequitur to remarkable progress achieved in adjacent\\ndisciplines utilizing deep learning methods, deep neural net-\\nworks have been applied to various NLP tasks, including part-\\nof-speech tagging [14]–[17], named entity recognition [18],\\n[18]–[21], and semantic role labeling [22]–[25]. Most of the\\nresearch efforts in deep learning associated with NLP appli-\\ncations involve either supervised learning1 or unsupervised\\nlearning2.\\nThis survey covers the emerging role of deep learning in the\\narea of NLP, across a broad range of categories. The research\\npresented in [26] is primarily focused on architectures, with\\nlittle discussion of applications. More recent works [4], [27]\\nare speciﬁc to certain applications or certain sub-ﬁelds of\\nNLP [21]. Here we build on previous works by describing\\nthe challenges, opportunities, and evaluations of the impact of\\napplying deep learning to NLP problems.\\nThis survey has six sections, including this introduction.\\nSection 2 lays out the theoretical dimensions of NLP and\\nartiﬁcial intelligence, and looks at deep learning as an ap-\\nproach to solving real-world problems. It motivates this study\\nby addressing the question: Why use deep learning in NLP?\\nThe third section discusses fundamental concepts necessary\\nto understand NLP, covering exemplary issues in representa-\\ntion, frameworks, and machine learning. The fourth section\\nsummarizes benchmark datasets employed in the NLP domain.\\nSection 5 focuses on some of the NLP applications where deep\\nlearning has demonstrated signiﬁcant beneﬁt. Finally, Section\\n6 provides a conclusion, also addressing some open problems\\nand promising areas for improvement.\\nII. BACKGROUND\\nNLP has long been viewed as one aspect of artiﬁcial\\nintelligence (AI), since understanding and generating natural\\nlanguage are high-level indications of intelligence. Deep learn-\\ning is an effective AI tool, so we next situate deep learning in\\nthe AI world. After that we explain motivations for applying\\ndeep learning to NLP.\\nA. Artiﬁcial Intelligence and Deep Learning\\nThere have been “islands of success” where big data are\\nprocessed via AI capabilities to produce information to achieve\\ncritical operational goals (e.g., fraud detection). Accordingly,\\n1Learning from training data to predict the type of new unseen test examples\\nby mapping them to known pre-deﬁned labels.\\n2Making sense of data without sticking to speciﬁc tasks and supervisory\\nsignals.\\narXiv:2003.01200v4  [cs.CL]  27 Feb 2021'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'file_path': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'trapped': '', 'modDate': 'D:20210302011619Z', 'creationDate': 'D:20210302011619Z', 'page': 1}, page_content='TORFI et. al., NLP ADVANCEMENTS BY DEEP LEARNING\\n2\\nscientists and consumers anticipate enhancement across a\\nvariety of applications. However, achieving this requires un-\\nderstanding of AI and its mechanisms and means (e.g., algo-\\nrithms). Ted Greenwald, explaining AI to those who are not\\nAI experts, comments: ”Generally AI is anything a computer\\ncan do that formerly was considered a job for a human” [28].\\nAn AI goal is to extend the capabilities of information\\ntechnology (IT) from those to (1) generate, communicate,\\nand store data, to also (2) process data into the knowledge\\nthat decision makers and others need [29]. One reason is\\nthat the available data volume is increasing so rapidly that\\nit is now impossible for people to process all available data.\\nThis leaves two choices: (1) much or even most existing data\\nmust be ignored or (2) AI must be developed to process the\\nvast volumes of available data into the essential pieces of\\ninformation that decision-makers and others can comprehend.\\nDeep learning is a bridge between the massive amounts of\\ndata and AI.\\n1) Deﬁnitions: Deep learning refers to applying deep neu-\\nral networks to massive amounts of data to learn a procedure\\naimed at handling a task. The task can range from simple\\nclassiﬁcation to complex reasoning. In other words, deep\\nlearning is a set of mechanisms ideally capable of deriving an\\noptimum solution to any problem given a sufﬁciently extensive\\nand relevant input dataset. Loosely speaking, deep learning\\nis detecting and analyzing important structures/features in the\\ndata aimed at formulating a solution to a given problem. Here,\\nAI and deep learning meet. One version of the goal or ambition\\nbehind AI is enabling a machine to outperform what the human\\nbrain does. Deep learning is a means to this end.\\n2) Deep Learning Architectures: Numerous deep learning\\narchitectures have been developed in different research areas,\\ne.g., in NLP applications employing recurrent neural networks\\n(RNNs) [30], convolutional neural networks (CNNs) [31], and\\nmore recently, recursive neural networks [32]. We focus our\\ndiscussion on a review of the essential models, explained in\\nrelevant seminal publications.\\nMulti Layer Perceptron: A multilayer perceptron (MLP)\\nhas at least three layers (input, hidden, and output layers). A\\nlayer is simply a collection of neurons operating to transform\\ninformation from the previous layer to the next layer. In the\\nMLP architecture, the neurons in a layer do not communicate\\nwith each other. An MLP employs nonlinear activation func-\\ntions. Every node in a layer connects to all nodes in the next\\nlayer, creating a fully connected network (Fig. 1). MLPs are\\nthe simplest type of Feed-Forward Neural Networks (FNNs).\\nFNNs represent a general category of neural networks in which\\nthe connections between the nodes do not create any cycle, i.e.,\\nin a FNN there is no cycle of information ﬂow.\\nConvolutional Neural Networks: Convolutional neural\\nnetworks (CNNs), whose architecture is inspired by the human\\nvisual cortex, are a subclass of feed-forward neural networks.\\nCNNs are named after the underlying mathematical operation,\\nconvolution, which yields a measure of the interoperability of\\nits input functions. Convolutional neural networks are usually\\nemployed in situations where data is or needs to be represented\\nwith a 2D or 3D data map. In the data map representation,\\nthe proximity of data points usually corresponds to their\\nFig. 1. The general architecture of a MLP.\\ninformation correlation.\\nIn convolutional neural networks where the input is an\\nimage, the data map indicates that image pixels are highly cor-\\nrelated to their neighboring pixels. Consequently, the convolu-\\ntional layers have 3 dimensions: width, height, and depth. That\\nassumption possibly explains why the majority of research\\nefforts dedicated to CNNs are conducted in the Computer\\nVision ﬁeld [33].\\nA CNN takes an image represented as an array of numeric\\nvalues. After performing speciﬁc mathematical operations, it\\nrepresents the image in a new output space. This operation is\\nalso called feature extraction, and helps to capture and rep-\\nresent key image content. The extracted features can be used\\nfor further analysis, for different tasks. One example is image\\nclassiﬁcation, which aims to categorize images according to\\nsome predeﬁned classes. Other examples include determining\\nwhich objects are present in an image and where they are\\nlocated. See Fig. 2.\\nIn the case of utilizing CNNs for NLP, the inputs are sen-\\ntences or documents represented as matrices. Each row of the\\nmatrix is associated with a language element such as a word\\nor a character. The majority of CNN architectures learn word\\nor sentence representations in their training phase. A variety\\nof CNN architectures were used in various classiﬁcation tasks\\nsuch as Sentiment Analysis and Topic Categorization [31],\\n[34]–[36]. CNNs were employed for Relation Extraction and\\nRelation Classiﬁcation as well [37], [38].\\nRecurrent Neural Network: If we line up a sequence of\\nFNNs and feed the output of each FNN as an input to the next\\none, a recurrent neural network (RNN) will be constructed.\\nLike FNNs, layers in an RNN can be categorized into input,\\nhidden, and output layers. In discrete time frames, sequences\\nof input vectors are fed as the input, one vector at a time,\\ne.g., after inputting each batch of vectors, conducting some\\noperations and updating the network weights, the next input\\nbatch will be fed to the network. Thus, as shown in Fig. 3,\\nat each time step we make predictions and use parameters of\\nthe current hidden layer as input to the next time step.\\nHidden layers in recurrent neural networks can carry infor-\\nmation from the past, in other words, memory. This character-\\nistic makes them speciﬁcally useful for applications that deal'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'file_path': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'trapped': '', 'modDate': 'D:20210302011619Z', 'creationDate': 'D:20210302011619Z', 'page': 2}, page_content='TORFI et. al., NLP ADVANCEMENTS BY DEEP LEARNING\\n3\\nFig. 2.\\nA typical CNN architecture for object detection. The network provides a feature representation with attention to the speciﬁc region of an image\\n(example shown on the left) that contains the object of interest. Out of the multiple regions represented (see an ordering of the image blocks, giving image\\npixel intensity, on the right) by the network, the one with the highest score will be selected as the main candidate.\\nFig. 3. Recurrent Neural Network (RNN), summarized on the left, expanded\\non the right, for N timesteps, with X indicating input, h hidden layer, and\\nO output\\nwith a sequence of inputs such as language modeling [39], i.e.,\\nrepresenting language in a way that the machine understands.\\nThis concept will be described later in detail.\\nRNNs can carry rich information from the past. Consider\\nthe sentence: “Michael Jackson was a singer; some people\\nconsider him King of Pop.” It’s easy for a human to identify\\nhim as referring to Michael Jackson. The pronoun him happens\\nseven words after Michael Jackson; capturing this dependency\\nis one of the beneﬁts of RNNs, where the hidden layers in an\\nRNN act as memory units. Long Short Term Memory Network\\n(LSTM) [40] is one of the most widely used classes of RNNs.\\nLSTMs try to capture even long time dependencies between\\ninputs from different time steps. Modern Machine Translation\\nand Speech Recognition often rely on LSTMs.\\nFig. 4. Schematic of an Autoencoder\\nAutoencoders:\\nAutoencoders\\nimplement\\nunsupervised\\nmethods in deep learning. They are widely used in dimension-\\nality reduction3 or NLP applications which consist of sequence\\n3Dimensionality reduction is an unsupervised learning approach which is\\nthe process of reducing the number of variables that were used to represent\\nthe data by identifying the most crucial information.\\nto sequence modeling (see Section III-B [39]. Fig. 4 illustrates\\nthe schematic of an Autoencoder. Since autoencoders are\\nunsupervised, there is no label corresponding to each input.\\nThey aim to learn a code representation for each input. The\\nencoder is like a feed-forward neural network in which the\\ninput gets encoded into a vector (code). The decoder operates\\nsimilarly to the encoder, but in reverse, i.e., constructing\\nan output based on the encoded input. In data compression\\napplications, we want the created output to be as close as\\npossible to the original input. Autoencoders are lossy, meaning\\nthe output is an approximate reconstruction of the input.\\nFig. 5. Generative Adversarial Networks\\nGenerative Adversarial Networks: Goodfellow [41] intro-\\nduced Generative Adversarial Networks (GANs). As shown in\\nFig. 5, a GAN is a combination of two neural networks, a\\ndiscriminator and a generator. The whole network is trained\\nin an iterative process. First, the generator network generates a\\nfake sample. Then the discriminator network tries to determine\\nwhether this sample (ex.: an input image) is real or fake, i.e.,\\nwhether it came from the real training data (data used for\\nbuilding the model) or not. The goal of the generator is to fool\\nthe discriminator in a way that the discriminator believes the\\nartiﬁcial (i.e., generated) samples synthesized by the generator\\nare real.\\nThis iterative process continues until the generator produces\\nsamples that are indistinguishable by the discriminator. In'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'file_path': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'trapped': '', 'modDate': 'D:20210302011619Z', 'creationDate': 'D:20210302011619Z', 'page': 3}, page_content='TORFI et. al., NLP ADVANCEMENTS BY DEEP LEARNING\\n4\\nother words, the probability of classifying a sample as fake\\nor real becomes like ﬂipping a fair coin for the discriminator.\\nThe goal of the generative model is to capture the distribution\\nof real data while the discriminator tries to identify the fake\\ndata. One of the interesting features of GANs (regarding being\\ngenerative) is: once the training phase is ﬁnished, there is no\\nneed for the discrimination network, so we solely can work\\nwith the generation network. In other words, having access to\\nthe trained generative model is sufﬁcient.\\nDifferent forms of GANs has been introduced, e.g., Sim\\nGAN [8], Wasserstein GAN [42], info GAN [43], and DC\\nGAN [44]. In one of the most elegant GAN implementations\\n[45], entirely artiﬁcial, yet almost perfect, celebrity faces are\\ngenerated; the pictures are not real, but fake photos produced\\nby the network. GAN’s have since received signiﬁcant atten-\\ntion in various applications and have generated astonishing\\nresult [46]. In the NLP domain, GANs often are used for text\\ngeneration [47], [48].\\nB. Motivation for Deep Learning in NLP\\nDeep learning applications are predicated on the choices\\nof (1) feature representation and (2) deep learning algo-\\nrithm alongside architecture. These are associated with data\\nrepresentation and learning structure, respectively. For data\\nrepresentation, surprisingly, there usually is a disjunction\\nbetween what information is thought to be important for\\nthe task at hand, versus what representation actually yields\\ngood results. For instance, in sentiment analysis, lexicon\\nsemantics, syntactic structure, and context are assumed by\\nsome linguists to be of primary signiﬁcance. Nevertheless,\\nprevious studies based on the bag-of-words (BoW) model\\ndemonstrated acceptable performance [49]. The bag-of-words\\nmodel [50], often viewed as the vector space model, involves\\na representation which accounts only for the words and\\ntheir frequency of occurrence. BoW ignores the order and\\ninteraction of words, and treats each word as a unique feature.\\nBoW disregards syntactic structure, yet provides decent results\\nfor what some would consider syntax-dependent applications.\\nThis observation suggests that simple representations, when\\ncoupled with large amounts of data, may work as well or better\\nthan more complex representations. These ﬁndings corroborate\\nthe argument in favor of the importance of deep learning\\nalgorithms and architectures.\\nOften the progress of NLP is bound to effective language\\nmodeling. A goal of statistical language modeling is the prob-\\nabilistic representation of word sequences in language, which\\nis a complicated task due to the curse of dimensionality. The\\nresearch presented in [51] was a breakthrough for language\\nmodeling with neural networks aimed at overcoming the curse\\nof dimensionality by (1) learning a distributed representation\\nof words and (2) providing a probability function for se-\\nquences.\\nA key challenge in NLP research, compared to other do-\\nmains such as Computer Vision, seems to be the complexity\\nof achieving an in-depth representation of language using\\nstatistical models. A primary task in NLP applications is to\\nprovide a representation of texts, such as documents. This in-\\nvolves feature learning, i.e., extracting meaningful information\\nto enable further processing and analysis of the raw data.\\nTraditional methods begin with time-consuming hand-\\ncrafting of features, through careful human analysis of a\\nspeciﬁc application, and are followed by development of\\nalgorithms to extract and utilize instances of those features.\\nOn the other hand, deep supervised feature learning methods\\nare highly data-driven and can be used in more general efforts\\naimed at providing a robust data representation.\\nDue to the vast amounts of unlabeled data, unsupervised\\nfeature learning is considered to be a crucial task in NLP. Un-\\nsupervised feature learning is, in essence, learning the features\\nfrom unlabeled data to provide a low-dimensional representa-\\ntion of a high-dimensional data space. Several approaches such\\nas K-means clustering and principal component analysis have\\nbeen proposed and successfully implemented to this end. With\\nthe advent of deep learning and abundance of unlabeled\\ndata, unsupervised feature learning becomes a crucial task for\\nrepresentation learning, a precursor in NLP applications. Cur-\\nrently, most of the NLP tasks rely on annotated data, while a\\npreponderance of unannotated data further motivates research\\nin leveraging deep data-driven unsupervised methods.\\nGiven the potential superiority of deep learning approaches\\nin NLP applications, it seems crucial to perform a com-\\nprehensive analysis of various deep learning methods and\\narchitectures with particular attention to NLP applications.\\nIII. CORE CONCEPTS IN NLP\\nA. Feature Representation\\nDistributed representations are a series of compact, low\\ndimensional representations of data, each representing some\\ndistinct informative property. For NLP systems, due to issues\\nrelated to the atomic representation of the symbols, it is\\nimperative to learn word representations.\\nAt ﬁrst, let’s concentrate on how the features are rep-\\nresented, and then we focus on different approaches for\\nlearning word representations. The encoded input features can\\nbe characters, words [32], sentences [52], or other linguistic\\nelements. Generally, it is more desirable to provide a compact\\nrepresentation of the words than a sparse one.\\nFig. 6. Considering a given sequence, the skip-thought model generates the\\nsurrounding sequences using the trained encoder. The assumption is that the\\nsurrounding sentences are closely related, contextually.\\nHow to select the structure and level of text representa-\\ntion used to be an unresolved question. After proposing the\\nword2vec approach [53], subsequently, doc2vec was proposed\\nin [52] as an unsupervised algorithm and was called Paragraph'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'file_path': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'trapped': '', 'modDate': 'D:20210302011619Z', 'creationDate': 'D:20210302011619Z', 'page': 4}, page_content='TORFI et. al., NLP ADVANCEMENTS BY DEEP LEARNING\\n5\\nVector (PV). The goal behind PV is to learn ﬁxed-length rep-\\nresentations from variable-length text parts such as sentences\\nand documents. One of the main objectives of doc2vec is\\nto overcome the drawbacks of models such as BoW and to\\nprovide promising results for applications such as text classi-\\nﬁcation and sentiment analysis. A more recent approach is the\\nskip-thought model which applies word2vec at the sentence-\\nlevel [54]. By utilizing an encoder-decoder architecture, this\\nmodel generates the surrounding sentences using the given\\nsentence (Fig. 6). Next, let’s investigate different kinds of\\nfeature representation.\\n1) One-Hot Representation:\\nIn one-hot encoding, each\\nunique element that needs to be represented has its dimen-\\nsion which results in a very high dimensional, very sparse\\nrepresentation. Assume the words are represented with the\\none-hot encoding method. Regarding representation structure,\\nthere is no meaningful connection between different words in\\nthe feature space. For example, highly correlated words such\\nas ‘ocean’ and ‘water’ will not be closer to each other (in the\\nrepresentation space) compared to less correlated pairs such as\\n‘ocean’ and ‘ﬁre.’ Nevertheless, some research efforts present\\npromising results using one-hot encoding [2].\\n2) Continuous Bag of Words: Continuous Bag-of-Words\\nmodel (CBOW) has frequently been used in NLP applica-\\ntions. CBOW tries to predict a word given its surrounding\\ncontext, which usually consists of a few nearby words [55].\\nCBOW is neither dependent on the sequential order of words\\nnor necessarily on probabilistic characteristics. So it is not\\ngenerally used for language modeling. This model is typi-\\ncally trained to be utilized as a pre-trained model for more\\nsophisticated tasks. An alternative to CBOW is the weighted\\nCBOW (WCBOW) [56] in which different vectors get different\\nweights reﬂective of relative importance in context. The sim-\\nplest example can be document categorization where features\\nare words and weights are TF-IDF scores [57] of the associated\\nwords.\\n3) Word-Level Embedding: Word embedding is a learned\\nrepresentation for context elements in which, ideally, words\\nwith related semantics become highly correlated in the rep-\\nresentation space. One of the main incentives behind word\\nembedding representations is the high generalization power\\nas opposed to sparse, higher dimensional representations [58].\\nUnlike the traditional bag-of-words model in which different\\nwords have entirely different representations regardless of their\\nusage or collocations, learning a distributed representation\\ntakes advantage of word usage in context to provide similar\\nrepresentations for semantically correlated words. There are\\ndifferent approaches to create word embeddings. Several re-\\nsearch efforts, including [53], [55], used random initialization\\nby uniformly sampling random numbers with the objective of\\ntraining an efﬁcient representation of the model on a large\\ndataset. This setup is intuitively acceptable for initialization\\nof the embedding for common features such as part-of-speech\\ntags. However, this may not be the optimum method for rep-\\nresentation of less frequent features such as individual words.\\nFor the latter, pre-trained models, trained in a supervised or\\nunsupervised manner, are usually leveraged for increasing the\\nperformance.\\n4) Character-Level Embedding: The methods mentioned\\nearlier are mostly at higher levels of representation. Lower-\\nlevel representations such as character-level representation\\nrequire special attention as well, due to their simplicity of\\nrepresentation and the potential for correction of unusual\\ncharacter combinations such as misspellings [2]. For generat-\\ning character-level embeddings, CNNs have successfully been\\nutilized [14].\\nCharacter-level embeddings have been used in different\\nNLP applications [59]. One of the main advantages is the\\nability to use small model sizes and represent words with\\nlower-level language elements [14]. Here word embeddings\\nare models utilizing CNNs over the characters. Another mo-\\ntivation for employing character-level embeddings is the out-\\nof-vocabulary word (OOV) issue which is usually encountered\\nwhen, for the given word, there is no equivalent vector in\\nthe word embedding. The character-level approach may sig-\\nniﬁcantly alleviate this problem. Nevertheless, this approach\\nsuffers from a weak correlation between characters and se-\\nmantic and syntactic parts of the language. So, considering\\nthe aforementioned pros and cons of utilizing character-level\\nembeddings, several research efforts tried to propose and im-\\nplement higher-level approaches such as using sub-words [60]\\nto create word embeddings for OOV instances as well as\\ncreating a semantic bridge between the correlated words [61].\\nB. Seq2Seq Framework\\nMost underlying frameworks in NLP applications rely on\\nsequence-to-sequence (seq2seq) models in which not only the\\ninput but also the output is represented as a sequence. These\\nmodels are common in various applications including machine\\ntranslation4, text summarization5, speech-to-text, and text-to-\\nspeech applications6.\\nThe most common seq2seq framework is comprised of an\\nencoder and a decoder. The encoder ingests the sequence of\\ninput data and generates a mid-level output which is subse-\\nquently consumed by the decoder to produce the series of ﬁnal\\noutputs. The encoder and decoder are usually implemented via\\na series of Recurrent Neural Networks or LSTM [40] cells.\\nThe encoder takes a sequence of length T, X\\n=\\n{x1, x2, · · · , xT }, where xt ∈V\\n= {1, · · · , |V |} is the\\nrepresentation of a single input coming from the vocabulary\\nV , and then generates the output state ht. Subsequently, the\\ndecoder takes the last state from the encoder, i.e., ht, and\\nstarts generating an output of size L, Y ′ = {y′\\n1, y′\\n2, · · · , y′\\nL},\\nbased on its current state, st, and the ground-truth output yt.\\nIn different applications, the decoder could take advantage\\nof more information such as a context vector [62] or intra-\\nattention vectors [63] to generate better outputs.\\nOne of the most widely training approaches for seq2seq\\nmodels is called Teacher Forcing [64]. Let us deﬁne y =\\n4The input is a sequence of words from one language (e.g., English) and\\nthe output is the translation to another language (e.g., French).\\n5The input is a complete document (sequence of words) and the output is\\na summary of it (sequence of words).\\n6The input is an audio recording of a speech (sequence of audible elements)\\nand the output is the speech text (sequence of words).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'file_path': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'trapped': '', 'modDate': 'D:20210302011619Z', 'creationDate': 'D:20210302011619Z', 'page': 5}, page_content='TORFI et. al., NLP ADVANCEMENTS BY DEEP LEARNING\\n6\\n{y1, y2, · · · , yL} as the ground-truth output sequence corre-\\nspondent to a given input sequence X. The model training\\nbased on the maximum-likelihood criterion employs the fol-\\nlowing cross-entropy (CE) loss minimization:\\nLCE = −\\nL\\nX\\nt=1\\nlog pθ(yt|yt−1, st, X)\\n(1)\\nwhere θ is the parameters of the model optimized during the\\ntraining.\\nOnce the model is optimized using the cross-entropy loss,\\nit can generate an entire sequence as follows. Let ˆyt denote\\nthe output generated by the model at time t. Then, the next\\noutput is generated by:\\nˆyt = arg max\\ny\\npθ(y|ˆyt−1, st)\\n(2)\\nIn NLP applications, one can improve the output by using\\nbeam search to ﬁnd a reasonably good output sequence [3].\\nDuring beam search, rather than using argmax for selecting\\nthe best output, we choose the top K outputs at each step,\\ngenerate K different paths for the output sequence, and ﬁnally\\nchoose the one that provides better performance as the ﬁnal\\noutput. Although, there has been some recent studies [65],\\n[66] on improving the beam search by incorporating a similar\\nmechanism during training of them model, studying this is\\noutside the scope of this paper.\\nGiven a series of the ground-truth output Y and the gener-\\nated model output ˆY , the model performance is evaluated us-\\ning a task-speciﬁc measures such as ROUGE [67], BLEU [68],\\nand METEOR [69]. As an example, ROUGEL, which is an\\nevaluation metric in NLP tasks, uses the largest common sub-\\nstring between ground-truth Y and model output ˆY to evaluate\\nthe generated output.\\nC. Reinforcement Learning in NLP\\nAlthough the seq2seq models explained in Section III-B\\nachieve great successes w.r.t. traditional methods, there are\\nsome issues with how these models are trained. Generally\\nspeaking, seq2seq models like the ones used in NLP applica-\\ntions face two issues: (1) exposure bias and (2) inconsistency\\nbetween training time and test time measurements [70].\\nMost of the popular seq2seq models are minimizing cross-\\nentropy loss as their optimization objective via Teacher Forc-\\ning (Section III-B). In teacher forcing, during the training of\\nthe model, the decoder utilizes two inputs, the former decoder\\noutput state st−1 and the ground-truth input yt, to determine its\\ncurrent output state st. Moreover, it employs them to create\\nthe next token, i.e., ˆyt. However, at test time, the decoder\\nfully relies on the previously created token from the model\\ndistribution. As the ground-truth data is not available, such\\na step is necessary to predict the next action. Henceforth, in\\ntraining, the decoder input is coming from the ground truth,\\nwhile, in the test phase, it relies on the previous prediction.\\nThis exposure bias [71] induces error growth through output\\ncreation at the test phase. One approach to remedy this\\nproblem is to remove the ground-truth dependency in training\\nby solely relying on model distribution to minimize the cross-\\nentropy loss. Scheduled sampling [64] is one popular method\\nto handle this setback. During scheduled sampling, we ﬁrst\\npre-train the model using cross-entropy loss and then slowly\\nreplace the ground-truth with samples the model generates.\\nThe second obstacle with seq2seq models is that, when\\ntraining is ﬁnished using the cross-entropy loss, it is typically\\nevaluated using non-differentiable measures such as ROUGE\\nor METEOR. This will form an inconsistency between the\\ntraining objective and the test evaluation metric. Recently, it\\nhas been demonstrated that both of these problems can be tack-\\nled by utilizing techniques from reinforcement learning [70].\\nAmong most of the well-known models in reinforcement\\nlearning, policy gradient techniques [72] such as the REIN-\\nFORCE algorithm [73] and actor-critic based models such as\\nvalue-based iteration [74], and Q-learning [75], are among the\\nmost common techniques used in deep learning in NLP.\\nUsing the model predictions (versus the ground-truth) for\\nthe sequence to sequence modeling and generation, at training\\ntime, was initially introduced by Daume et al. [76]. According\\nto their approach, SEARN, the structured prediction can be\\ncharacterized as one of the reinforcement learning cases as\\nfollows: The model employs its predictions to produce a\\nsequence of actions (words sequences). Then, at each time\\nstep, a greedy search algorithm is employed to learn the\\noptimal action, and the policy will be trained to predict that\\nparticular action.\\nFig. 7. A simple Actor-Critic framework.\\nIn Actor-Critic training, the actor is usually the same neural\\nnetwork used to generate the output, while the critic is a\\nregression model that estimates how the actor performed on\\nthe input data. The actor later receives the feedback from the\\ncritic and improves its actions. Fig 7 shows this framework.\\nIt is worth noting that action in most of the NLP-related\\napplications is like selecting the next output token while the\\nstate is the decoder output state at each stage of decoding.\\nThese models have mostly been used for robotic [77] and\\nAtari games [78] due to the small action space in these\\napplications. However, when we use them in NLP applications,\\nthey face multiple challenges. The action space in most of the\\nNLP applications could be deﬁned as the number of tokens\\nin the vocabulary (usually between 50K to 150K tokens).\\nComparing this to the action space in a simple Atari game,\\nwhich on average has less than 20 actions [78], shows why\\nthese Actor-Critic models face difﬁculties when applied to\\nNLP applications. A major challenge is the massive action\\nspace in NLP applications, which not only causes difﬁculty'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'file_path': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'trapped': '', 'modDate': 'D:20210302011619Z', 'creationDate': 'D:20210302011619Z', 'page': 6}, page_content='TORFI et. al., NLP ADVANCEMENTS BY DEEP LEARNING\\n7\\nfor the right action selection, but also will make the training\\nprocess very slow. This makes the process of ﬁnding the best\\nActor-Critic model very complicated and model convergence\\nusually requires a lot of tweaks to the models.\\nIV. DATASETS\\nMany different researchers for different tasks use bench-\\nmark datasets, such as those discussed below. Benchmarking\\nin machine learning refers to the assessment of methods\\nand algorithms, comparing those regarding their capability to\\nlearn speciﬁc patterns. Benchmarking aids validation of a new\\napproach or practice, relative to other existing methods.\\nBenchmark datasets typically take one of three forms.\\n1) The ﬁrst is real-world data, obtained from various real-\\nworld experiments.\\n2) The second is synthetic data, artiﬁcially generated to\\nmimic real-world patterns. Synthetic data is generated\\nfor use instead of real data. Such datasets are of spe-\\ncial interest in applications where the amount of data\\nrequired is much larger than that which is available, or\\nwhere privacy considerations are crucial and strict, such\\nas in the healthcare domain.\\n3) The third type are toy datasets, used for demonstration\\nand visualization purposes. Typically they are artiﬁcially\\ngenerated; often there is no need to represent real-world\\ndata patterns.\\nThe foundation of Deep Learning utilization is the avail-\\nability of data to teach the system about pattern identiﬁcation.\\nThe effectiveness of the model depends on the quality of\\nthe data. Despite the successful implementation of universal\\nlanguage modeling techniques such as BERT [79], however,\\nsuch models can be used solely for pre-training the models.\\nAfterward, the model needs to be trained on the data associated\\nwith the desired task. Henceforth, based on the everyday\\ndemands in different machine domains such as NLP, creating\\nnew datasets is crucial.\\nOn the other hand, creating new datasets is not usually an\\neasy matter. Informally speaking, the newly created dataset\\nshould be: the right data to train on, sufﬁcient for the eval-\\nuation, and accurate to work on. Answering the questions of\\n“what is the meaning of right and accurate data” is highly\\napplication-based. Basically, the data should have sufﬁcient\\ninformation, which depends on the quality and quantity of the\\ndata.\\nTo create a dataset, the ﬁrst step is always asking “what are\\nwe trying to do and what problem do we need to solve?”\\nand “what kind of data do we need and how much of it\\nis required?” The next step is to create training and testing\\nportions. The training data set is used to train a model to\\nknow how to ﬁnd the connections between the inputs and\\nthe associated outputs. The test data set is used to assess the\\nintelligence of the machine, i.e., how well the trained model\\ncan operate on the unseen test samples. Next, we must conduct\\ndata preparation to make sure the data and its format is simple\\nand understandable for human experts. After that, the issue\\nof data accessibility and ownership may arise. Distribution of\\ndata may need to have speciﬁc authorizations, especially if we\\nare dealing with sensitive or private data.\\nGiven the aforementioned roadmap, creating proper datasets\\nis complicated and of great importance. That’s why few\\ndatasets are frequently chosen by the researchers and develop-\\ners for benchmarking. A summary of widely used benchmark\\ndatasets is provided in Table I.\\nV. DEEP LEARNING FOR NLP TASKS\\nThis section describes NLP applications using deep learn-\\ning. Fig. 8 shows representative NLP tasks (and the categories\\nthey belong to). A fundamental question is: ”How can we\\nevaluate an NLP algorithm, model, or system?” In [80],\\nsome of the most common evaluation metrics have been\\ndescribed. This reference explains the fundamental principles\\nof evaluating NLP systems.\\nA. Basic Tasks\\n1) Part-Of-Speech Tagging: Part-of-Speech tagging is one\\nof the basic tasks in Natural Language Processing. It is the\\nprocess of labeling words with their part of speech categories.\\nPart of speech is leveraged for many crucial tasks such\\nas named entity recognition. One commonly used dataset\\nfor Part-of-Speech tagging is the WSJ corpus7. This dataset\\ncontains over a million tokens and has been utilized widely as\\na benchmark dataset for the performance assessment of POS\\ntagging systems. Traditional methods are still performing very\\nwell for this task [16]. However, neural network based methods\\nhave been proposed for Part-of-Speech tagging [81].\\nFor example, the deep neural network architecture named\\nCharWNN\\nhas been developed to join word-level and\\ncharacter-level representations using convolutional neural net-\\nworks for POS tagging [14]. The emphasis in [14] is the\\nimportance of character-level feature extraction as their exper-\\nimental results show the necessity of employing hand-crafted\\nfeatures in the absence of character-level features for achieving\\nthe state-of-the-art. In [82], a wide variety of neural network\\nbased models have been proposed for sequence tagging tasks,\\ne.g., LSTM networks, bidirectional LSTM networks, LSTM\\nnetworks with a CRF8 layer, etc. Sequence tagging itself\\nincludes part of speech tagging, chunking, and named entity\\nrecognition. Likewise, a globally normalized transition-based\\nneural network architecture has been proposed for POS-\\ntagging [83]. State-of-the-art results are summarized in Table\\nII. In [17], authors propose a bidirectional LSTM to perform\\nparts of speech tagging and show that it performs better than\\nconventional machine learning techniques on the same dataset.\\nMore recently, in [84], authors use a pretrained BERT model\\nin combination with one bidirectional LSTM layer and train\\nthe latter layer only and outperform the prior state-of-the art\\nPOS architectures.\\n2) Parsing: Parsing is assigning a structure to a recognized\\nstring. There are different types of parsing. Constituency\\nParsing refers in particular to assigning a syntactic structure\\nto a sentence. A greedy parser has been introduced in [92]\\nwhich performs a syntactic and semantic summary of content\\n7Penn Treebank Wall Street Journal (WSJ-PTB).\\n8Conditional Random Field.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'file_path': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'trapped': '', 'modDate': 'D:20210302011619Z', 'creationDate': 'D:20210302011619Z', 'page': 7}, page_content='TORFI et. al., NLP ADVANCEMENTS BY DEEP LEARNING\\n8\\nTABLE I\\nBENCHMARK DATASETS.\\nTask\\nDataset\\nLink\\nMachine Translation\\nWMT 2014 EN-DE\\nWMT 2014 EN-FR\\nhttp://www-lium.univ-lemans.fr/∼schwenk/cslm joint paper/\\nText Summarization\\nCNN/DM\\nNewsroom\\nDUC\\nGigaword\\nhttps://cs.nyu.edu/∼kcho/DMQA/\\nhttps://summari.es/\\nhttps://www-nlpir.nist.gov/projects/duc/data.html\\nhttps://catalog.ldc.upenn.edu/LDC2012T21\\nReading Comprehension\\nQuestion Answering\\nQuestion Generation\\nARC\\nCliCR\\nCNN/DM\\nNewsQA\\nRACE\\nSQuAD\\nStory Cloze Test\\nNarativeQA\\nQuasar\\nSearchQA\\nhttp://data.allenai.org/arc/\\nhttp://aclweb.org/anthology/N18-1140\\nhttps://cs.nyu.edu/∼kcho/DMQA/\\nhttps://datasets.maluuba.com/NewsQA\\nhttp://www.qizhexie.com/data/RACE leaderboard\\nhttps://rajpurkar.github.io/SQuAD-explorer/\\nhttp://aclweb.org/anthology/W17-0906.pdf\\nhttps://github.com/deepmind/narrativeqa\\nhttps://github.com/bdhingra/quasar\\nhttps://github.com/nyu-dl/SearchQA\\nSemantic Parsing\\nAMR parsing\\nATIS (SQL Parsing)\\nWikiSQL (SQL Parsing)\\nhttps://amr.isi.edu/index.html\\nhttps://github.com/jkkummerfeld/text2sql-data/tree/master/data\\nhttps://github.com/salesforce/WikiSQL\\nSentiment Analysis\\nIMDB Reviews\\nSST\\nYelp Reviews\\nSubjectivity Dataset\\nhttp://ai.stanford.edu/∼amaas/data/sentiment/\\nhttps://nlp.stanford.edu/sentiment/index.html\\nhttps://www.yelp.com/dataset/challenge\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/\\nText Classiﬁcation\\nAG News\\nDBpedia\\nTREC\\n20 NewsGroup\\nhttp://www.di.unipi.it/∼gulli/AG corpus of news articles.html\\nhttps://wiki.dbpedia.org/Datasets\\nhttps://trec.nist.gov/data.html\\nhttp://qwone.com/∼jason/20Newsgroups/\\nNatural Language Inference\\nSNLI Corpus\\nMultiNLI\\nSciTail\\nhttps://nlp.stanford.edu/projects/snli/\\nhttps://www.nyu.edu/projects/bowman/multinli/\\nhttp://data.allenai.org/scitail/\\nSemantic Role Labeling\\nProposition Bank\\nOneNotes\\nhttp://propbank.github.io/\\nhttps://catalog.ldc.upenn.edu/LDC2013T19\\nTABLE II\\nPOS TAGGING STATE-OF-THE-ART MODELS EVALUATED ON THE\\nWSJ-PTB DATASET.\\nModel\\nAccuracy\\nCharacter-aware neural language models [85]\\n97.53\\nTransfer Learning + GRU [86]\\n97.55\\nBi-directional LSTM + CNNs + CRF [87]\\n97.55\\nAdversarial Training + Bi-LSTM [88]\\n97.59\\nCharacter Composition + Bi-LSTM [89]\\n97.78\\nString Embedding + LSTM [90]\\n97.85\\nMeta-BiLSTM [91]\\n97.96\\nusing vector representations. To enhance the results achieved\\nby [92], the approach proposed in [93] focuses on learning\\nmorphological embeddings. Recently, deep neural network\\nmodels outperformed traditional algorithms. State-of-the-art\\nresults are summarized in Table III.\\nAnother type of parsing is called Dependency Parsing. De-\\npendency structure shows the structural relationships between\\nthe words in a targeted sentence. In dependency parsing,\\nphrasal elements and phrase-structure rules do not contribute\\nto the process. Rather, the syntactic structure of the sentence\\nis expressed only in terms of the words in the sentence and\\nthe associated relations between the words.\\nTABLE III\\nCONSTITUENCY PARSING STATE-OF-THE-ART MODELS EVALUATED ON\\nTHE WSJ-PTB DATASET.\\nModel\\nAccuracy\\nRecurrent neural network grammars (RNNG) [94]\\n93.6\\nIn-order traversal over syntactic trees + LSTM [95]\\n94.2\\nModel Combination and Reranking [96]\\n94.6\\nSelf-Attentive Encoder [97]\\n95.1\\nNeural networks have shown their superiority regarding\\ngeneralizability and reducing the feature computation cost. In\\n[98], a novel neural network-based approach was proposed for\\na transition-based dependency parser. Neural network based\\nmodels that operate on task-speciﬁc transition systems have\\nalso been utilized for dependency parsing [83]. A regularized\\nparser with bi-afﬁne classiﬁers has been proposed for the pre-\\ndiction of arcs and labels [99]. Bidirectional-LSTMs have been\\nused in dependency parsers for feature representation [100].\\nA new control structure has been introduced for sequence-to-\\nsequence neural networks based on the stack LSTM and has\\nbeen used in transition-based parsing [101]. [102] presents a\\ntransition based multilingual dependency parser which uses\\na bidirectional LSTM to adapt to target languages. In [103],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'file_path': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'trapped': '', 'modDate': 'D:20210302011619Z', 'creationDate': 'D:20210302011619Z', 'page': 8}, page_content='TORFI et. al., NLP ADVANCEMENTS BY DEEP LEARNING\\n9\\nFig. 8. NLP tasks investigated in this study.\\nthe authors provide a comparison on the state of the art deep\\nlearning based parsing methods on a clinical text parsing task.\\nMore recently, in [104], a second-order TreeCRF extension\\nwas added to the biafﬁne [105] parser to demonstrate that\\nstructural learning can further improve parsing performance\\nover the state-of-the-art bi-afﬁne models.\\n3) Semantic Role Labeling: Semantic Role Labeling (SRL)\\nis the process of identiﬁcation and classiﬁcation of text argu-\\nments. It is aimed at the characterization of elements to deter-\\nmine “who” did “what” to “whom” as well as “how,” “where,”\\nand “when.” It identiﬁes the predicate-argument structure of a\\nsentence. The predicate, in essence, refers to “what,” while the\\narguments consist of the associated participants and properties\\nin the text. The goal of SRL is to extract the semantic relations\\nbetween the predicate and the related arguments.\\nMost of the previously-reported research efforts are based\\non explicit representations of semantic roles. Recently, deep\\nlearning approaches have achieved the SRL state-of-the-art\\nwithout taking the explicit syntax representation into consider-\\nation [106]. On the other hand, it is argued that the utilization\\nof syntactic information can be leveraged to improve the per-\\nformance of syntactic-agnostic9 models [107]. A linguistically-\\ninformed self-attention (LISA) model has been proposed to\\nleverage both multi-task learning and self-attention for effec-\\n9Note that being syntactic-agnostic does not imply discarding syntactic\\ninformation. It means they are not explicitly employed.\\ntive utilization of the syntactic information for SRL [108].\\nCurrent state-of-the-art methods employ joint prediction of\\npredicates and arguments [109], novel word representation ap-\\nproaches [110], and self-attention models [111]; see Table IV.\\nResearchers in [25] focus on syntax and contextualized word\\nrepresentation to present a unique multilingual SRL model\\nbased on a biafﬁne scorer, argument pruning and bidirectional\\nLSTMs, (see also [112]).\\nTABLE IV\\nSEMANTIC ROLE LABELING CURRENT STATE-OF-THE-ART MODELS\\nEVALUATED ON THE ONTONOTES DATASET [113]. THE ACCURACY\\nMETRIC IS F1 SCORE.\\nModel\\nAccuracy (F1)\\nSelf-Attention + RNN [111]\\n83.9\\nContextualized Word Representations [110]\\n84.6\\nArgumented Representations + BiLSTM [109]\\n85.3\\nB. Text Classiﬁcation\\nThe primary objective of text classiﬁcation is to assign\\npredeﬁned categories to text parts (which could be a word,\\nsentence, or whole document) for preliminary classiﬁcation\\npurposes and further organization and analysis. A simple ex-\\nample is the categorization of given documents as to political\\nor non-political news articles.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'file_path': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'trapped': '', 'modDate': 'D:20210302011619Z', 'creationDate': 'D:20210302011619Z', 'page': 9}, page_content='TORFI et. al., NLP ADVANCEMENTS BY DEEP LEARNING\\n10\\nThe use of CNNs for sentence classiﬁcation, in which train-\\ning the model on top of pretrained word-vectors through ﬁne-\\ntuning, has resulted in considerable improvements in learning\\ntask-speciﬁc vectors [31]. Later, a Dynamic Convolutional\\nNeural Network (DCNN) architecture – essentially a CNN\\nwith a dynamic k-max pooling method – was applied to\\ncapture the semantic modeling of sentences [114]. In addi-\\ntion to CNNs, RNNs have been used for text classiﬁcation.\\nAn LSTM-RNN architecture has been utilized in [115] for\\nsentence embedding with particular superiority in a deﬁned\\nweb search task. A Hierarchical Attention Network (HAN) has\\nbeen utilized to capture the hierarchical structure of text, with\\na word-level and sentence-level attention mechanism [116].\\nSome models used the combination of both RNNs and\\nCNNs for text classiﬁcation such as [117]. This is a recurrent\\narchitecture in addition to max-pooling with an effective word\\nrepresentation method, and demonstrates superiority compared\\nto simple window-based neural network approaches. Another\\nuniﬁed architecture is the C-LSTM proposed in [118] for\\nsentence and document modeling in classiﬁcation. Current\\nstate-of-the-art methods are summarized in Table V. A more\\nrecent review of the deep learning based methods for text clas-\\nsiﬁcation is provided in [119]. The latter focuses on different\\narchitectures used for this task, including most recent works\\nin CNN based models, as well as RNN based models, and\\ngraph neural networks. In [120], authors provide a comparison\\nbetween various deep learning methods for text classiﬁcation,\\nconcluding that GRUs and LSTMs can actually perform better\\nthan CNN-based models.\\nTABLE V\\nTHE CLASSIFICATION ACCURACY OF STATE-OF-THE-ART METHODS,\\nEVALUATED ON THE AG NEWS CORPUS DATASET [2].\\nModel\\nAccuracy\\nCNN [121]\\n91.33\\nDeep Pyramid CNN [122]\\n93.13\\nCNN [123]\\n93.43\\nUniversal Language Model Fine-tuning (ULMFiT) [124]\\n94.99\\nC. Information Extraction\\nInformation extraction identiﬁes structured information\\nfrom “unstructured” data such as social media posts and\\nonline news. Deep learning has been utilized for information\\nextraction regarding subtasks such as Named Entity Recogni-\\ntion, Relation Extraction, Coreference Resolution, and Event\\nExtraction.\\n1) Named Entity Recognition:\\nNamed Entity Recogni-\\ntion (NER) aims to locate and categorize named entities in\\ncontext into pre-deﬁned categories such as the names of people\\nand places. The application of deep neural networks in NER\\nhas been investigated by the employment of CNN [125] and\\nRNN architectures [126], as well as hybrid bidirectional LSTM\\nand CNN architectures [19]. NeuroNER [127], a named-entity\\nrecognition tool, operates based on artiﬁcial neural networks.\\nState-of-the-art models are reported in Table VI. [21] provides\\nan extensive discussion on recent deep learning methods for\\nnamed entity recognition. The latter concludes that the work\\npresented in [128] outperforms other recent models (with an\\nF-score of 93.5 on the CoNLL03 dataset).\\nTABLE VI\\nSTATE OF THE ART MODELS REGARDING NAME ENTITY RECOGNITION.\\nEVALUATION IS PERFORMED ON THE CONLL-2003 SHARED TASK\\nDATASET [129]. THE EVALUATION METRIC IS F1 SCORE.\\nModel\\nAccuracy\\nSemi-supervised Sequence Modeling [130]\\n92.61\\nGoogle BERT [131]\\n92.8\\nContextual String Embeddings [90]\\n93.09\\n2) Relation Extraction: Relation Extraction aims to ﬁnd\\nthe semantic relationships between entity pairs. The recursive\\nneural network (RNN) model has been proposed for semantic\\nrelationship classiﬁcation by learning compositional vector\\nrepresentations [132]. For relation classiﬁcation, CNN archi-\\ntectures have been employed as well, by extracting lexical\\nand sentence level features [37]. More recently, in [133],\\nbidirectional tree-structured LSTMs were shown to perform\\nwell for relation extraction. [134] provides a more recent\\nreview on relation extraction.\\n3) Coreference Resolution: Coreference resolution includes\\nidentiﬁcation of the mentions in a context that refer to the\\nsame entity. For instance, the mentions “car,” “Camry,” and\\n“it” could all refer to the same entity. For the ﬁrst time in [135],\\nReinforcement Learning (RL) was applied to coreference\\nresolution. Current widely used methods leverage an attention\\nmechanism [136]. More recently, in [137], authors adopt a\\nreinforcement learning policy gradient approach to coreference\\nresolution and provide state-of-the art performance on the\\nEnglish OntoNotes v5.0 benchmark task. [138] reformulates\\ncoreference resolution as a span prediction task as in question\\nanswering and provide superior performance on the CoNLL-\\n2012 benchmark task.\\n4) Event Extraction: A speciﬁc type of extracted infor-\\nmation from text is an event. Such extraction may involve\\nrecognizing trigger words related to an event and assign-\\ning labels to entity mentions that represent event triggers.\\nConvolutional neural networks have been utilized for event\\ndetection; they handle problems with feature-based approaches\\nincluding exhaustive feature engineering and error propagation\\nphenomena for feature generation [139]. In 2018, Nguyen\\nand Grishman applied graph-CNN (GCCN) where the con-\\nvolutional operations are applied to syntactically dependent\\nwords as well as consecutive words [140]; their adding entity\\ninformation reﬂected the state-of-the-art using CNN models.\\n[141] uses a novel inverse reinforcement learning approach\\nbased on generative adversarial networks (imitation learning)\\nto tackle joint entity and event extraction. More recently, in\\n[142], authors proposed a model for document-level event\\nextraction using a combined dependency-based GCN (for\\nlocal context) and a hypergraph (as an aggregator for global\\ncontext).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'file_path': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'trapped': '', 'modDate': 'D:20210302011619Z', 'creationDate': 'D:20210302011619Z', 'page': 10}, page_content='TORFI et. al., NLP ADVANCEMENTS BY DEEP LEARNING\\n11\\nD. Sentiment analysis\\nThe primary goal in sentiment analysis is the extraction\\nof subjective information from text by contextual mining.\\nSentiment analysis is considered high-level reasoning based on\\nsource data. Sentiment analysis is sometimes called opinion\\nmining, as its primary goal is to analyze human opinion,\\nsentiments, and even emotions regarding products, problems,\\nand varied subjects. Seminal works on sentiment analysis or\\nopinion mining include [143], [144]. Since 2000, much atten-\\ntion has been given to sentiment analysis, due to its relation\\nto a wide variety of applications [145], its associations with\\nnew research challenges, and the availability of abundant data.\\n[146] provides a more recent review of the sentiment analysis\\nmethods relying on deep learning and gives an insightful\\ndiscussion on the drawbacks as well as merits of deep learning\\nmethods for sentiment analysis.\\nA critical aspect of research in sentiment analysis is content\\ngranularity. Considering this criterion, sentiment analysis is\\ngenerally divided into three categories/levels: document level,\\nsentence level, and aspect level.\\n1) Document-level Sentiment Analysis: At the document\\nlevel, the task is to determine whether the whole document\\nreﬂects a positive or negative sentiment about exactly one\\nentity. This differs from opinion mining regarding multiple\\nentries. The Gated Recurrent Neural Network architecture\\nhas been utilized successfully for effectively encoding the\\nsentences’ relations in the semantic structure of the docu-\\nment [147]. Domain adaptation has been investigated as well,\\nto deploy the trained model on unseen new sources [148].\\nMore recently, in [149] authors provide an LSTM-based model\\nfor document-level sentiment analysis that captures semantic\\nrelations between sentences. In [150], authors use a CNN-\\nbidirectional LSTM model to process long texts.\\n2) Sentence-level Sentiment Analysis:\\nAt the sentence-\\nlevel, sentiment analysis determines the positivity, negativity,\\nor neutrality regarding an opinion expressed in a sentence. One\\ngeneral assumption for sentence-level sentiment classiﬁcation\\nis the existence of only one opinion from a single opinion\\nholder in an expressed sentence. Recursive autoencoders have\\nbeen employed for sentence-level sentiment label prediction\\nby learning the vector space representations for phrases [151].\\nLong Short-Term Memory (LSTM) recurrent models have\\nalso been utilized for tweet sentiment prediction [152]. The\\nSentiment Treebank and Recursive Neural Tensor Networks\\n[153] have shown promise for predicting ﬁne-grained sen-\\ntiment labels. [154] provides a cloud-based hybrid machine\\nlearning model for sentence level sentiment analysis. More\\nrecently in [155], propose A Lexicalized Domain Ontology\\nand a Regularized Neural Attention model (ALDONAr) for\\nsentence-level aspect-based sentiment analysis that uses a\\nCNN classiﬁcation module with BERT word embeddings and\\nachieves state-of-the art results.\\n3) Aspect-level Sentiment Analysis: Document-level and\\nsentence-level sentiment analysis usually focus on the senti-\\nment itself, not the target of the sentiment, e.g., a product.\\nAspect-level sentiment analysis directly targets an opinion,\\nwith the assumption of the existence of the sentiment and its\\ntarget. A document or sentence may not have a generally posi-\\ntive or negative sentiment, but may have multiple subparts with\\ndifferent targets, each with a positive or negative sentiment.\\nThis can make aspect-level analysis even more challenging\\nthan other types of sentiment categorization.\\nAspect-level sentiment analysis usually involves Aspect\\nSentiment Classiﬁcation and Aspect Extraction. The former\\ndetermines opinions on different aspects (positive, neutral,\\nor negative) while the latter identiﬁes the target aspect for\\nevaluation in context. As an example consider the following\\nsentence: “This car is old. It must be repaired and sold!”.\\n“This car” is what is subject to evaluation and must be\\nextracted ﬁrst. Here, the opinion about this aspect is negative.\\nFor aspect-level sentiment classiﬁcation, attention-based\\nLSTMs are proposed to connect the aspect and sentence\\ncontent for sentiment classiﬁcation [156]. For aspect extrac-\\ntion, deep learning has successfully been proposed in opinion\\nmining [157]. State-of-the-art methods rely on converting\\naspect-based sentiment analysis to sentence-pair classiﬁcation\\ntasks [79], post-training approaches [158] on the popular\\nlanguage model BERT [131], and employment of pre-trained\\nembeddings [159]. [160] provides a recent comparative review\\non aspect-based sentiment analysis. Also recently, [161] pro-\\nposed a dual-attention model which tries to extract the implicit\\nrelation between the aspect and opinion terms. In [162] authors\\npropose a novel Aspect-Guided Deep Transition model for\\naspect-based sentiment analysis.\\nE. Machine Translation\\nMachine Translation (MT) is one of the areas of NLP\\nthat has been profoundly affected by the advances in deep\\nlearning. The ﬁrst subsection below explains methods used in\\nthe pre-deep learning period, as explained in reference NLP\\ntextbooks such as “Speech and Language Processing” [163].\\nThe remainder of this section is dedicated to delving into\\nrecent innovations in MT which are based on neural networks,\\nstarted by [164]. [165], [166] provide reviews on various deep\\nlearning architectures used for MT.\\n1) Traditional Machine Translation:\\nOne of the ﬁrst\\ndemonstrations of machine translation happened in 1954 [167]\\nin which the authors tried to translate from Russian to English.\\nThis translation system was based on six simple rules, but\\nhad a very limited vocabulary. It was not until the 1990s that\\nsuccessful statistical implementations of machine translation\\nemerged as more bilingual corpora became available [163].\\nIn [68] the BLEU score was introduced as a new evaluation\\nmetric, allowing more rapid improvement than when the only\\napproach involved using human labor for evaluation.\\n2) Neural Machine Translation: It was after the success\\nof the neural network in image classiﬁcation tasks that re-\\nsearchers started to use neural networks in machine translation\\n(NMT). Around 2013, research groups started to achieve\\nbreakthrough results in NMT. Unlike traditional statistical\\nmachine translation, NMT is based on an end-to-end neural\\nnetwork [168]. This implies that there is no need for extensive\\npreprocessing and word alignments. Instead, the focus shifted\\ntoward network structure.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'file_path': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'trapped': '', 'modDate': 'D:20210302011619Z', 'creationDate': 'D:20210302011619Z', 'page': 11}, page_content='TORFI et. al., NLP ADVANCEMENTS BY DEEP LEARNING\\n12\\nFig. 11 shows an example of an end-to-end recurrent neural\\nnetwork for machine translation. A sequence of input tokens\\nis fed into the network. Once it reaches an end-of-sentence\\n(EOS) token, it starts generating the output sequence. The\\noutput sequence is generated in the same recurrent manner as\\nthe input sequence until it reaches an end-of-sentence token.\\nOne major advantage of this approach is that there is no need\\nto specify the length of the sequence; the network takes it\\ninto account automatically. In other words, the end-of-sentence\\ntoken determines the length of the sequence. Networks implic-\\nitly learn that longer input sentences usually lead to longer\\noutput sentences with varying length, and that ordering can\\nchange. For instance, the second example in Fig. 9 shows that\\nadjectives generally come before nouns in English but after\\nnouns in Spanish. There is no need to explicitly specify this\\nsince the network can capture such properties. Moreover, the\\namount of memory that is used by NMT is just a fraction\\nof the memory that is used in traditional statistical machine\\ntranslation [169].\\nFig. 9. Alignment in Machine Translation\\n[164] was one of the early works that incorporated recurrent\\nneural networks for machine translation. They were able to\\nachieve a perplexity (a measure where lower values indicate\\nbetter models) that was 43% less than the state-of-the-art\\nalignment based translation models. Their recurrent continuous\\ntranslation model (RCTM) is able to capture word ordering,\\nsyntax, and meaning of the source sentence explicitly. It maps\\na source sentence into a probability distribution over sentences\\nin the target language. RCTM estimates the probability P(f|e)\\nof translating a sentence e = e1 + ... + ek in the source\\nlanguage to target language sentence f = f1+...+fm. RCTM\\nestimates P(f|e) by considering source sentence e as well as\\nthe preceding words in the target language f1:i−1:\\nP(f|e) =\\nm\\nY\\ni=1\\nP(fi|f1:i−1, e)\\n(3)\\nThe representation generated by RCTM acts on n-grams in\\nthe lower layers, and acts more on the whole sentence as one\\nmoves to the upper layers. This hierarchical representation is\\nperformed by applying different layers of convolution. First a\\ncontinuous representation of each word is generated; i.e., if\\nthe sentence is e = e1...ek, the representation of the word ei\\nwill be v(ei) ∈Rq×1. This will result in sentence matrix Ee ∈\\nRq×k in which Ee\\n:,i = v(ei). This matrix representation of the\\nsentence will be fed into a series of convolution layers in order\\nto generate the ﬁnal representation e for the recurrent neural\\nnetwork. The approach is illustrated in Fig. 10. Equations for\\nthe pipeline are as follows.\\ns = S.csm(e)\\n(4)\\nh1 = σ(I.v(f1) + s)\\n(5)\\nhi+1 = σ(R.hi + I.v(fi+1) + s)\\n(6)\\noi+1 = O.hi\\n(7)\\nIn order to take into account the sentence length, the authors\\nintroduced RCTM II which estimates the length of the target\\nsentence. RCTM II was able to achieve better perplexity on\\nWMT datasets (see top portion of Table I) than other existing\\nmachine translation systems.\\nFig. 10. Recurrent Continuous Translation Models (RCTM) [164].\\nIn another line of work,\\n[170] presented an end-to-end\\nsequence learning approach without heavy assumptions on\\nthe structure of the sequence. Their approach consists of two\\nLSTMs, one for mapping the input to a vector of ﬁxed di-\\nmension and another LSTM for decoding the output sequence\\nfrom the vector. Their model was able to handle long sentences\\nas well as sentence representations that are sensitive to word\\norder. As shown in Fig. 11, the model reads ”ABC” as an\\ninput sequence and produces ”WXYZ” as output sequence.\\nThe < EOS > token indicates the end of prediction. The\\nnetwork was trained by maximizing the log probability of the\\ntranslation (η) given the input sequence (ζ). In other words,\\nthe objective function is:\\n1/|D|\\nX\\n(η,ζ)∈D\\nlogP(η|ζ)\\n(8)\\nD is the training set and |D| is its size. One of the\\nnovelties of their approach was reversing word order of the\\nsource sentence. This helps the LSTM to learn long term\\ndependencies.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'file_path': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'trapped': '', 'modDate': 'D:20210302011619Z', 'creationDate': 'D:20210302011619Z', 'page': 12}, page_content='TORFI et. al., NLP ADVANCEMENTS BY DEEP LEARNING\\n13\\nFig. 11. Sequence to sequence learning with LSTM.\\nHaving a ﬁxed-length vector in the decoder phase is one\\nof the bottlenecks of the encoder-decoder approach.\\n[168]\\nargues that a network will have a hard time compressing\\nall the information from the input sentence into a ﬁxed-size\\nvector. They address this by allowing the network to search\\nsegments of the source sentence that are useful for predicting\\nthe translation. Instead of representing the input sentence as a\\nﬁxed-size vector, in [168] the input sentence is encoded to a\\nsequence of vectors and a subset of them is chosen by using\\na method called attention mechanism as shown in Fig. 12.\\nIn their approach P(yi|y1, ..., yi−1, X) = g(yi−1, si, ci), in\\nwhich si = f(si−1, yi−1, ci). While previously c was the same\\nfor all time steps, here c takes a different value, ci, at each time\\nstep. This accounts for the attention mechasim (context vector)\\naround that speciﬁc time step. ci is computed according to the\\nfollowing:\\nci = PTx\\nj=1 αijhj, αij =\\nexp(eij)\\nPTx\\nk=1 exp(eik), eij = a(si−1, hj).\\nHere a is the alignment model that is represented by a feed\\nforward neural network. Also hj = [\\n→\\nhT\\nj ,\\n←\\nhT\\nj ], which is a way to\\ninclude information both about preceding and following words\\nin hj. The model was able to outperform the simple encoder-\\ndecoder approach regardless of input sentence length.\\nImproved machine translation models continue to emerge,\\ndriven in part by the growth in people’s interest and need\\nto understand other languages Most of them are variants of\\nthe end-to-end decoder-encoder approach. For example, [171]\\ntries to deal with the problem of rare words. Their LSTM\\nnetwork consists of encoder and decoder layers using residual\\nlayers along with the attention mechanism. Their system\\nwas able to decrease training time, speed up inference, and\\nhandle translation of rare words. Comparisons between some\\nof the state-of-the-art neural machine translation models are\\nsummarized in Table VII.\\nTABLE VII\\nTHE MACHINE TRANSLATION STATE-OF-THE-ART MODELS EVALUATED\\nON THE English-German dataset of ACL 2014 Ninth Workshop on Statistical\\nMachine TRranslation. THE EVALUATION METRIC IS BLEU SCORE.\\nModel\\nAccuracy\\nConvolutional Seq-to-Seq [172]\\n25.2\\nAttention Is All You Need [173]\\n28.4\\nWeighted Transformer [174]\\n28.9\\nSelf Attention [175]\\n29.2\\nDeepL Translation Machine 10\\n33.3\\nBack-translation [176]\\n35.0\\nMore recently, [177] provides an interesting single-model\\nimplementation of massively multilingual NMT. In [178],\\nauthors use BERT to extract contextual embeddings and com-\\nFig. 12. Attention Mechasim for Neural Machine Translation [168].\\nbine BERT with an attention-based NMT model and provide\\nstate-of-the-art results on various benchmark datasets. [179]\\nproposes mBART which is a seq-to-seq denoising autoen-\\ncoder and reports that using a pretrained, locked (i.e. no\\nmodiﬁcations) mBART improves performance in terms of\\nthe BLEU point. [180] proposes an interesting adversarial\\nframework for robustifying NMT against noisy inputs and\\nreports performance gains over the Transformer model. [181]\\nis also an insightful recent work where the authors sample\\ncontext words from the predicted sequence as well as the\\nground truth to try to reconcile the training and inference\\nprocesses. Finally, [182] is a successful recent effort to prevent\\nthe forgetting that often accompanies in translating pre-trained\\nlanguage models to other NMT task. [182] achieves that aim\\nprimarily by using a dynamically gated model and asymptotic\\ndistillation.\\nF. Question Answering\\nQuestion answering (QA) is a ﬁne-grained version of Infor-\\nmation Retrieval (IR). In IR a desired set of information has to\\nbe retrieved from a set of documents. The desired information\\ncould be a speciﬁc document, text, image, etc. On the other\\nhand, in QA speciﬁc answers are sought, typically ones that\\ncan be inferred from available documents. Other areas of NLP\\nsuch as reading comprehension and dialogue systems intersect\\nwith question answering.\\nResearch in computerized question answering has pro-\\nceeded since the 1960s. In this section, we present a general\\noverview of question answering system history, and focus on\\nthe breakthroughs in the ﬁeld. Like all other ﬁelds in NLP,\\nquestion answering was also impacted by the advancement of\\ndeep learning [183], so we provide an overview of QA in deep\\nlearning contexts. We brieﬂy visit visual question answering\\nas well.\\n1) Rule-based Question Answering: Baseball [184] is one\\nof the early works (1961) on QA where an effort was made to\\nanswer questions related to baseball games by using a game\\ndatabase. The baseball system consists of (1) question read-in,\\n(2) dictionary lookup for words in the question, (3) syntactic\\n(POS) analysis of the words in question, (4) content analysis'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'file_path': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'trapped': '', 'modDate': 'D:20210302011619Z', 'creationDate': 'D:20210302011619Z', 'page': 13}, page_content='TORFI et. al., NLP ADVANCEMENTS BY DEEP LEARNING\\n14\\nfor extracting the input question, and (5) estimating relevance\\nregarding answering the input question.\\nIBM’s [185] statistical question answering system consisted\\nof four major components:\\n1) Question/Answer Type Classiﬁcation\\n2) Query Expansion/Information Retrieval\\n3) Name Entity Making\\n4) Answer Selection\\nSome QA systems fail when semantically equivalent re-\\nlationships are phrased differently.\\n[186] addressed this by\\nproposing fuzzy relation matching based on mutual informa-\\ntion and expectation maximization.\\n2) Question answering in the era of deep learning: Smart-\\nphones (Siri, Ok Google, Alexa, etc.) and virtual personal\\nassistants are common examples of QA systems with which\\nmany interact on a daily basis. While earlier such systems\\nemployed rule-based methods, today their core algorithm is\\nbased on deep learning. Table VIII presents some questions\\nand answers provided by Siri on an iPhone.\\nTABLE VIII\\nTYPICAL QUESTION ANSWERING PERFORMANCE BASED ON DEEP\\nLEARNING.\\nQuestion\\nAnswer\\nWho invented polio vaccine?\\nThe answer I found is Jonas Salk\\nWho wrote Harry Potter?\\nJ.K.Rowling wrote Harry Potter in\\n1997\\nWhen was Einstein born?\\nAlbert Einstein was born March\\n14, 1879\\n[188] was one of the ﬁrst machine learning based papers\\nthat reported results on QA for a reading comprehension\\ntest. The system tries to pick a sentence in the database that\\nhas an answer to a question, and a feature vector represents\\neach question-sentence pair. The main contribution of [188]\\nis proposing a feature vector representation framework which\\nis aimed to provide information for learning the model. There\\nare ﬁve classiﬁers (location, date, etc.), one for each type of\\nquestion. They were able to achieve accuracy competitive with\\nprevious approaches.\\nAs illustrated in Fig. 13, [187] uses convolutional neural\\nnetworks in order to encode Question-Answer sentence pairs\\nin the form of ﬁxed length vectors regardless of the length\\nof the input sentence. Instead of using distance measures like\\ncosine correlation, they incorporate a non-linear tensor layer to\\nmatch the relevance between question and answer. Equation 9\\ncalculates the matching degree between question q and its\\ncorresponding answer a.\\ns(q, a) = uT f(vT\\nq M[1:r]va + V\\n\\x14vq\\nva\\n\\x15\\n+ b)\\n(9)\\nf is the standard element-wise non-linearity function,\\nM[1:r]∈Rns×ns×r is a tensor, V ∈Rr×2ns, b ∈Rr, u ∈Rr.\\nThe model tries to capture the interaction between question\\nand answer. Inspired by ﬁndings in neuroscience, [81] incorpo-\\nrated episodic memory11 in their Dynamic Memory Network\\n11A kind of long-term memory that includes conscious recall of previous\\nactivities together with their meaning.\\n(DMN). By processing input sequences and questions, DMN\\nforms episodic memories to answer relevant questions. As\\nillustrated in Fig. 14, their system is trained based on raw\\nInput-Question-Answer triplets.\\nDMN consists of four modules that communicate with each\\nother as shown in Fig. 15. The input module encodes raw\\ninput text into a distributed vector representation; likewise\\nthe question module encodes a question into its distributed\\nvector representation. The episodic memory module uses the\\nattention mechanism in order to focus on a speciﬁc part of\\nthe input module. Through an iterative process, this module\\nproduces a memory vector representation that considers the\\nquestion as well as previous memory. The answer module\\nuses the ﬁnal memory vector to generate an answer. The model\\nimproved upon state-of-the-art results on tasks such as the ones\\nshown in Fig. 14. DMN is one of the architectures that could\\npotentially be used for a variety of NLP applications such as\\nclassiﬁcation, question answering, and sequence modeling.\\n[189] introduced a Dynamic Coattention Network (DCN)\\nin order to address local maxima corresponding to incorrect\\nanswers; it is considered to be one of the best approaches to\\nquestion answering.\\n3) Visual Question Answering: Given an input image, Vi-\\nsual Question Answering (VQA) tries to answer a natural\\nlanguage question about the image [190]. VQN addresses mul-\\ntiple problems such as object detection, image segmentation,\\nsentiment analysis, etc. [190] introduced the task of VQA\\nby providing a dataset containing over 250K images, 760K\\nquestions, and around 10M answers. [191] proposed a neural-\\nbased approach to answer the questions regarding the input\\nimages. As illustrated in Fig. 16, Neural-Image-QA is a deep\\nnetwork consisting of CNN and LSTM. Since the questions\\ncan have multiple answers, the problem is decomposed into\\npredicting a set of answer words aq,x = {a1, a2, ..., aN(q,x)}\\nfrom a ﬁnite vocabulary set ν where N(q, x) represents the\\ncount of answer words regarding a given question.\\nDo humans and computers look at the same regions to\\nanswer questions about an image? [193] tries to answer this\\nquestion by conducting large-scale studies on human attention\\nin VQA. Their ﬁndings show that VQAs do not seem to\\nbe looking at the same regions as humans. Finally, [192]\\nincorporates a spatial memory network for VQA. Fig. 17\\nshows the inference process of their model. As illustrated in\\nthe ﬁgure, the speciﬁc attention mechanism in their system can\\nhighlight areas of interest in the input image. [194] introduces\\nBLOCK, a bilinear fusion model based on superdiagonal\\ntensor decomposition for the VQA task, with state-of-the-\\nart performance and the code made public on github. To\\nimprove the generalization of existing models to test data of\\ndifferent distribution, [195] introduces a self-critical training\\nobjective to help ﬁnd visual regions of prominent visual/textual\\ncorrelation with a focus on recognizing inﬂuential objects and\\ndetecting and devaluing incorrect dominant answers.\\nG. Document Summarization\\nDocument summarization refers to a set of problems involv-\\ning generation of summary sentences given one or multiple\\ndocuments as input.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'file_path': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'trapped': '', 'modDate': 'D:20210302011619Z', 'creationDate': 'D:20210302011619Z', 'page': 14}, page_content='TORFI et. al., NLP ADVANCEMENTS BY DEEP LEARNING\\n15\\nFig. 13. Fixed length vector sentence representation for input Questions and Answers [187].\\nFig. 14.\\nExample of Dynamic Memory Network (DMN) input-question-\\nanswer triplet\\nFig. 15. Interaction between four modules of Dynamic Memory Network [78].\\nGenerally, text summarization ﬁts into two categories:\\n1) Extractive Summarization, where the goal is to iden-\\ntify the most salient sentences in the document and\\nreturn them as the summary.\\n2) Abstractive Summarization, where the goal is to gen-\\nerate summary sentences from scratch; they may contain\\nnovel words that do not appear in the original document.\\nEach of these methods has its own advantages and disad-\\nvantages. Extractive summarization is prone to generate long\\nand sometimes overlapping summary sentences; however, the\\nresult reﬂects the author’s mode of expression. Abstractive\\nFig. 16. Neural Image Question Answering [191].\\nFig. 17.\\nSpatial Memory Network for VQA. Bright Areas are regions the\\nmodel is attending [192].\\nmethods generate a shorter summary but they are hard to train.\\nThere is a vast amount of research on the topic of text\\nsummarization using extractive and abstractive methods. As\\none of the earliest works on using neural networks for ex-\\ntractive summarization,\\n[196] proposed a framework that\\nused a ranking technique to extract the most salient sentences\\nin the input. This model was improved by [197] which\\nused a document-level encoder to represent sentences, and\\na classiﬁer to rank these sentences. On the other hand, in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'file_path': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'trapped': '', 'modDate': 'D:20210302011619Z', 'creationDate': 'D:20210302011619Z', 'page': 15}, page_content='TORFI et. al., NLP ADVANCEMENTS BY DEEP LEARNING\\n16\\nabstractive summarization, it was\\n[198] which, for the ﬁrst\\ntime, used attention over a sequence-to-sequence (seq2seq)\\nmodel for the problem of headline generation. However, since\\nsimple attention models perform worse than extractive models,\\ntherefore more effective attention models such as graph-based\\nattention [199] and transformers [173] have been proposed for\\nthis task. To further improve abstractive text summarization\\nmodels, [200] proposed the ﬁrst pointer-generator model and\\napplied it to the DeepMind QA dataset [201]. As a result\\nof this work, the CNN/Daily Mail dataset emerged which is\\nnow one of the widely used datasets for the summarization\\ntask. A copy mechanism was also adopted by [202] for\\nsimilar tasks. But their analysis reveals a key problem with\\nattention-based encoder-decoder models: they often generate\\nunusual summaries consisting of repeated phrases. Recently,\\n[62] reached state-of-the-art results on the abstractive text\\nsummarization using a similar framework. They alleviated the\\nunnatural summaries by avoiding generating unknown tokens\\nand replacing these words with tokens from the input article.\\nLater, researchers moved their focus to methods that use\\nsentence-embedding to ﬁrst select the most salient sentence\\nin the document and then change them to make them more\\nabstractive [203], [204]. In these models, salient sentences\\nare extracted ﬁrst and then a paraphrasing model is used to\\nmake them abstractive. The extraction employs a sentence\\nclassiﬁer or ranker while the abstractor tries to remove the\\nextra information in a sentence and present it as a shorter\\nsummary. Fast-RL [203] is the ﬁrst framework in this family of\\nworks. In Fast-RL, the extractor is pre-trained to select salient\\nsentences and the abstractor is pre-trained using a pointer-\\ngenerator model to generate paraphrases. Finally, to merge\\nthese two non-differentiable components, they propose using\\nActor-Critic Q-learning methods in which the actor receives\\na single document and generates the output while the critic\\nevaluates the output based on comparison with the ground-\\ntruth summary.\\nThough the standard way to evaluate the performance of\\nsummarization models is with ROUGE [67] and BLEU [68],\\nthere are major problems with such measures. For instance, the\\nROUGE measure focuses on the number of shared n-grams\\nbetween two sentences. Such a method incorrectly assigns\\na low score to an abstractive summary that uses different\\nwords yet provides an excellent paraphrase that humans would\\nrate highly. Clearly, better automated evaluation methods are\\nneeded in such cases.\\nThere are additional problems with current summarization\\nmodels. Shi et al. [205] provides a comprehensive survey on\\ntext summarization.\\n[206] provides a recent survey on summarization methods.\\n[207] provides an advanced composite deep learning model,\\nbased on LSTMs and Restricted Boltzmann Machine, for\\nmulti-doc opinion summarization. A very inﬂuential recent\\nwork, [208], introduces HIBERT ( HIerachical\\nBidirectional\\nEncoder Representations from Transformers) as a pre-trained\\ninitialization for document summarization and report state-of-\\nthe-art performance.\\nH. Dialogue Systems\\nDialogue Systems are quickly becoming a principal in-\\nstrument in human-computer interaction, due in part to their\\npromising potential and commercial value [209]. One appli-\\ncation is automated customer service, supporting both online\\nand bricks-and-mortar businesses. Customers expect an ever-\\nincreasing level of speed, accuracy, and respect while dealing\\nwith companies and their services. Due to the high cost of\\nknowledgeable human resources, companies frequently turn\\nto intelligent conversational machines. Note that the phrases\\nconversational machines and dialogue machines are often used\\ninterchangeably.\\nDialogue systems are usually task-based or non-task-\\nbased (Fig. 18). Though there might be Automatic Speech\\nRecognition (ASR) and Language-to-Speech (L2S) compo-\\nnents in a dialogue system, the discussion of this section is\\nsolely about the linguistic components of dialogue systems;\\nconcepts associated with speech technology are ignored.\\nDespite useful statistical models employed in the backend\\nof dialogue systems (especially in language understanding\\nmodules), most deployed dialogue systems rely on expensive\\nhand-crafted and manual features for operation. Furthermore,\\nthe generalizability of these manually engineered systems to\\nother domains and functionalities is problematic. Hence, recent\\nattention has focused on deep learning for the enhancement of\\nperformance, generalizability, and robustness. Deep learning\\nfacilitates the creation of end-to-end task-oriented dialogue\\nsystems, which enriches the framework to generalize conver-\\nsations beyond annotated task-speciﬁc dialogue resources.\\n1) Task-based Systems: The structure of a task-based dia-\\nlogue system usually consists of the following elements:\\n• Natural Language Understanding (NLU): This compo-\\nnent deals with understanding and interpreting user’s\\nspoken context by assigning a constituent structure to the\\nspoken utterance (e.g., a sentence) and captures its syn-\\ntactic representation and semantic interpretation, to allow\\nthe back-end operation/task. NLU is usually leveraged\\nregardless of the dialogue context.\\n• Dialogue Manager (DM): The generated representation\\nby NLU would be handled by the dialogue manager,\\nwhich investigates the context and returns a reasonable\\nsemantic-related response.\\n• Natural Language Generation (NLG): The natural lan-\\nguage generation (NLG) component produces an utter-\\nance based on the response provided by the DM compo-\\nnent.\\nThe general pipeline is as follows: NLU module (i.e.,\\nsemantic decoder) transforms the output of the speech recog-\\nnition module to some dialogue elements. Then the DM\\nprocesses these dialogue elements and provides a suitable\\nresponse which is fed to the NLG for response generation.\\nThe main pipeline in NLU is to classify the user query domain\\nand user intent, and ﬁll a set of slots to create a semantic\\nframe. It is usually customary to perform the intent prediction\\nand the slot ﬁlling simultaneously [210]. Most of the task-\\noriented dialogue systems employ slot-ﬁlling approaches to\\nclassify user intent in the speciﬁc domain of the conversation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'file_path': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'trapped': '', 'modDate': 'D:20210302011619Z', 'creationDate': 'D:20210302011619Z', 'page': 16}, page_content='TORFI et. al., NLP ADVANCEMENTS BY DEEP LEARNING\\n17\\nFig. 18. The framework of a dialogue system. A dialogue system can be task oriented or used for natural language generation based on the user input which\\nis also known as a chat bot.\\nFor this aim, having predeﬁned tasks is required; this depends\\non manually crafted states with different associated slots.\\nHenceforth, a designed dialogue system would be of limited\\nor no use for other tasks.\\nRecent task-oriented dialogue systems have been designed\\nbased on deep reinforcement learning, which provided promis-\\ning results regarding performance [211], domain adapta-\\ntion [212], and dialogue generation [213]. This was due to\\na shift towards end-to-end trainable frameworks to design\\nand deploy task-oriented dialogue systems. Instead of the\\ntraditionally utilized pipeline, an end-to-end framework in-\\ncorporates and uses a single module that deals with external\\ndatabases. Despite the tractability of end-to-end dialogue\\nsystems (i.e., easy to train and simple to engineer), due to\\ntheir need for interoperability with external databases via\\nqueries, they are not well-suited for task-oriented settings.\\nSome approaches to this challenge include converting the user\\ninput into internal representations [214], combining supervised\\nand reinforced learning [215], and extending the memory\\nnetwork approach [216] for question-answering to a dialog\\nsystem [217].\\n2) Non-task-based Systems: As opposed to task-based dia-\\nlogue systems, the goal behind designing and deploying non-\\ntask-based dialogue systems is to empower a machine with\\nthe ability to have a natural conversation with humans [218].\\nTypically, chatbots are of one of the following types: retrieval-\\nbased methods and generative methods. Retrieval-based mod-\\nels have access to information resources and can provide more\\nconcise, ﬂuent, and accurate responses. However, they are\\nlimited regarding the variety of responses they can provide\\ndue to their dependency on backend data resources. Generative\\nmodels, on the other hand, have the advantage of being able\\nto produce suitable responses when such responses are not in\\nthe corpus. However, as opposed to retrieval-based models,\\nthey are more prone to grammatical and conceptual mistakes\\narising from their generative models.\\nRetrieval-based methods select an appropriate response\\nfrom the candidate responses. Therefore, the key element is the\\nquery-response operation. In general, this problem has been\\nformulated as a search problem and uses IR techniques for task\\ncompletion [219]. Retrieval-based methods usually employ\\neither Single-turn Response Matching or Multi-turn Response\\nMatching. In the ﬁrst type, the current query (message) is\\nsolely used to select a suitable response [220]. The latter\\ntype takes the current message and previous utterances as the\\nsystem input and retrieves a response based on the instant and\\ntemporal information. The model tries to choose a response\\nwhich considers the whole context to guarantee conversation\\nconsistency. An LSTM-based model has been proposed [221]\\nfor context and response vectors creation. In [222], various\\nfeatures and multiple data inputs have been incorporated to\\nbe ingested using a deep learning framework. Current base\\nmodels regarding retrieval-based chatbots rely on multi-turn\\nresponse selection augmented by an attention mechanism and\\nsequence matching [223].\\nGenerative models don’t assume the availability of pre-\\ndeﬁned responses. New responses are produced from scratch\\nand are based on the trained model. Generative models are\\ntypically based on sequence to sequence models and map an\\ninput query to a target element as the response. In general,\\ndesigning and implementing a dialogue agent to be able to\\nconverse at the human level is very challenging. The typical\\napproach usually consists of learning and imitating human\\nconversation. For this goal, the machine is generally trained on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'file_path': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'trapped': '', 'modDate': 'D:20210302011619Z', 'creationDate': 'D:20210302011619Z', 'page': 17}, page_content='TORFI et. al., NLP ADVANCEMENTS BY DEEP LEARNING\\n18\\nlarge corpora of conversations. However, this does not directly\\nremedy the issue of encountering out-of-corpus conversation.\\nThe question is: How can an agent be taught to generate\\nproper responses to conversations that it never has seen? It\\nmust handle content that is not exactly available in the data\\ncorpus that the machine has been trained on, due to the lack\\nof content matching between the query and the corresponding\\nresponse, resulting from the wide range of plausible queries\\nthat humans can provide.\\nTo tackle the aforementioned general problem, some fun-\\ndamental questions must be answered: (1) What are the core\\ncharacteristics of a natural conversation? (2) How can these\\ncharacteristics be measured? (3) How can we incorporate this\\nknowledge in a machine, i.e., the dialogue system? Effective\\nintegration of these three elements determines the intelligence\\nof a machine. A qualitative criterion is to observe if the\\ngenerated utterances can be distinguished from natural human\\ndialogues. For quantitative evaluation, adversarial evaluation\\nwas initially used for quality assessment of sentence gener-\\nation [224] and employed for quality evaluation of dialogue\\nsystems [225]. Recent advancements in sequence to sequence\\nmodeling encouraged many research efforts regarding natural\\nlanguage generation [226]. Furthermore, deep reinforcement\\nlearning yields promising performance in natural language\\ngeneration [213].\\n3) Final note on dialogue systems: Despite remarkable\\nadvancements in AI and much attention dedicated to dia-\\nlogue systems, in reality, successful commercial tools, such\\nas Apple’s Siri and Amazon’s Alexa, still heavily rely on\\nhandcrafted features. It still is very challenging to design and\\ntrain data-driven dialogue machines given the complexity of\\nthe natural language, the difﬁculties in framework design, and\\nthe complex nature of available data sources.\\nVI. CONCLUSION\\nIn this article, we presented a comprehensive survey of\\nthe most distinguished works in Natural Language Processing\\nusing deep learning. We provided a categorized context for\\nintroducing different NLP core concepts, aspects, and applica-\\ntions, and emphasized the most signiﬁcant conducted research\\nefforts in each associated category. Deep learning and NLP are\\ntwo of the most rapidly developing research topics nowadays.\\nDue to this rapid progress, it is hoped that soon, new effective\\nmodels will supersede the current state-of-the-art approaches.\\nThis may cause some of the references provided in the survey\\nto become dated, but those are likely to be cited by new\\npublications that describe improved methods\\nNeverthless, one of the essential characteristics of this\\nsurvey is its educational aspect, which provides a precise\\nunderstanding of the critical elements of this ﬁeld and explains\\nthe most notable research works. Hopefully, this survey will\\nguide students and researchers with essential resources, both\\nto learn what is necessary to know, and to advance further the\\nintegration of NLP with deep learning.\\nREFERENCES\\n[1] C. D. Manning, C. D. Manning, and H. Sch¨utze, Foundations of\\nstatistical natural language processing. MIT Press, 1999.\\n[2] X. Zhang, J. Zhao, and Y. LeCun, “Character-level convolutional\\nnetworks for text classiﬁcation,” in Advances in neural information\\nprocessing systems, pp. 649–657, 2015.\\n[3] K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares,\\nH. Schwenk, and Y. Bengio, “Learning phrase representations us-\\ning RNN encoder-decoder for statistical machine translation,” arXiv\\npreprint arXiv:1406.1078, 2014.\\n[4] S. Wu, K. Roberts, S. Datta, J. Du, Z. Ji, Y. Si, S. Soni, Q. Wang,\\nQ. Wei, Y. Xiang, B. Zhao, and H. Xu, “Deep learning in clinical\\nnatural language processing: a methodical review,” Journal of the\\nAmerican Medical Informatics Association, vol. 27, pp. 457–470, mar\\n2020.\\n[5] R. Collobert and J. Weston, “A uniﬁed architecture for natural lan-\\nguage processing: Deep neural networks with multitask learning,” in\\nProceedings of the 25th international conference on Machine learning,\\npp. 160–167, ACM, 2008.\\n[6] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and\\nL. Fei-Fei, “Large-scale video classiﬁcation with convolutional neural\\nnetworks,” in Proceedings of the IEEE conference on Computer Vision\\nand Pattern Recognition, pp. 1725–1732, 2014.\\n[7] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, “Learning and transferring\\nmid-level image representations using convolutional neural networks,”\\nin Proceedings of the IEEE conference on Computer Vision and Pattern\\nRecognition, pp. 1717–1724, 2014.\\n[8] A. Shrivastava, T. Pﬁster, O. Tuzel, J. Susskind, W. Wang, and R. Webb,\\n“Learning from simulated and unsupervised images through adversarial\\ntraining,” in Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pp. 2107–2116, 2017.\\n[9] A. Voulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis,\\n“Deep Learning for Computer Vision: A Brief Review,” Computational\\nIntelligence and Neuroscience, Feb 2018.\\n[10] N. O’Mahony, S. Campbell, A. Carvalho, S. Harapanahalli, G. V.\\nHernandez, L. Krpalkova, D. Riordan, and J. Walsh, “Deep learning vs.\\ntraditional computer vision,” in Advances in Computer Vision (K. Arai\\nand S. Kapoor, eds.), (Cham), pp. 128–144, Springer International\\nPublishing, 2020.\\n[11] A. Graves and N. Jaitly, “Towards end-to-end speech recognition with\\nrecurrent neural networks,” in International Conference on Machine\\nLearning, pp. 1764–1772, 2014.\\n[12] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg,\\nC. Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen, et al., “Deep\\nspeech 2: End-to-end speech recognition in English and Mandarin,” in\\nICML, pp. 173–182, 2016.\\n[13] U. Kamath, J. Liu, and J. Whitaker, Deep learning for NLP and speech\\nrecognition, vol. 84. Springer, 2019.\\n[14] C. D. Santos and B. Zadrozny, “Learning character-level representa-\\ntions for part-of-speech tagging,” in Proceedings of the 31st Interna-\\ntional Conference on Machine Learning (ICML-14), pp. 1818–1826,\\n2014.\\n[15] B. Plank, A. Søgaard, and Y. Goldberg, “Multilingual part-of-speech\\ntagging with bidirectional long short-term memory models and auxil-\\niary loss,” arXiv preprint arXiv:1604.05529, 2016.\\n[16] C. D. Manning, “Part-of-speech tagging from 97% to 100%: is it\\ntime for some linguistics?,” in International Conference on Intelligent\\nText Processing and Computational Linguistics, pp. 171–189, Springer,\\n2011.\\n[17] R. D. Deshmukh and A. Kiwelekar, “Deep learning techniques for\\npart of speech tagging by natural language processing,” in 2020\\n2nd International Conference on Innovative Mechanisms for Industry\\nApplications (ICIMIA), pp. 76–81, IEEE, 2020.\\n[18] G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and\\nC. Dyer, “Neural architectures for named entity recognition,” arXiv\\npreprint arXiv:1603.01360, 2016.\\n[19] J. P. Chiu and E. Nichols, “Named entity recognition with bidirectional\\nLSTM-CNNs,” arXiv preprint arXiv:1511.08308, 2015.\\n[20] V.\\nYadav\\nand\\nS.\\nBethard,\\n“A\\nsurvey\\non\\nrecent\\nadvances\\nin\\nnamed entity recognition from deep learning models,” arXiv preprint\\narXiv:1910.11470, 2019.\\n[21] J. Li, A. Sun, J. Han, and C. Li, “A survey on deep learning for\\nnamed entity recognition,” IEEE Transactions on Knowledge and Data\\nEngineering, 2020.\\n[22] J. Zhou and W. Xu, “End-to-end learning of semantic role labeling\\nusing recurrent neural networks,” in Proceedings of the 53rd Annual\\nMeeting of the Association for Computational Linguistics and the\\n7th International Joint Conference on Natural Language Processing\\n(Volume 1: Long Papers), vol. 1, pp. 1127–1137, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'file_path': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'trapped': '', 'modDate': 'D:20210302011619Z', 'creationDate': 'D:20210302011619Z', 'page': 18}, page_content='TORFI et. al., NLP ADVANCEMENTS BY DEEP LEARNING\\n19\\n[23] D. Marcheggiani, A. Frolov, and I. Titov, “A simple and accurate\\nsyntax-agnostic neural model for dependency-based semantic role\\nlabeling,” arXiv preprint arXiv:1701.02593, 2017.\\n[24] L. He, K. Lee, M. Lewis, and L. Zettlemoyer, “Deep semantic role\\nlabeling: What works and what’s next,” in Proceedings of the 55th\\nAnnual Meeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers), vol. 1, pp. 473–483, 2017.\\n[25] S. He, Z. Li, and H. Zhao, “Syntax-aware multilingual semantic role\\nlabeling,” arXiv preprint arXiv:1909.00310, 2019.\\n[26] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent trends in\\ndeep learning based natural language processing,” IEEE Computational\\nIntelligence Magazine, vol. 13, no. 3, pp. 55–75, 2018.\\n[27] Y. Kang, Z. Cai, C.-W. Tan, Q. Huang, and H. Liu, “Natural language\\nprocessing (NLP) in management research: A literature review,” Jour-\\nnal of Management Analytics, vol. 7, pp. 139–172, apr 2020.\\n[28] T.\\nGreenwald,\\n“What\\nexactly\\nis\\nartiﬁcial\\nin-\\ntelligence,\\nanyway?.”\\nhttps://www.wsj.com/articles/\\nwhat-exactly-is-artiﬁcial-intelligence-anyway-1525053960,\\nApril\\n2018. Wall Street Journal Online Article.\\n[29] U. Sivarajah, M. M. Kamal, Z. Irani, and V. Weerakkody, “Critical\\nanalysis of big data challenges and analytical methods,” Journal of\\nBusiness Research, vol. 70, pp. 263–286, 2017.\\n[30] Z. C. Lipton, J. Berkowitz, and C. Elkan, “A critical review of\\nrecurrent neural networks for sequence learning,” arXiv preprint\\narXiv:1506.00019, 2015.\\n[31] Y. Kim, “Convolutional neural networks for sentence classiﬁcation,”\\narXiv preprint arXiv:1408.5882, 2014.\\n[32] R. Socher, C. C. Lin, C. Manning, and A. Y. Ng, “Parsing natural scenes\\nand natural language with recursive neural networks,” in Proceedings\\nof the 28th international conference on machine learning (ICML-11),\\npp. 129–136, 2011.\\n[33] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\\nwith deep convolutional neural networks,” in Advances in neural\\ninformation processing systems, pp. 1097–1105, 2012.\\n[34] C. dos Santos and M. Gatti, “Deep convolutional neural networks for\\nsentiment analysis of short texts,” in Proceedings of COLING 2014, the\\n25th International Conference on Computational Linguistics: Technical\\nPapers, pp. 69–78, 2014.\\n[35] R. Johnson and T. Zhang, “Effective use of word order for text\\ncategorization with convolutional neural networks,” arXiv preprint\\narXiv:1412.1058, 2014.\\n[36] R. Johnson and T. Zhang, “Semi-supervised convolutional neural\\nnetworks for text categorization via region embedding,” in Advances\\nin neural information processing systems, pp. 919–927, 2015.\\n[37] D. Zeng, K. Liu, S. Lai, G. Zhou, and J. Zhao, “Relation classiﬁcation\\nvia convolutional deep neural network,” in Proceedings of COLING\\n2014, the 25th International Conference on Computational Linguistics:\\nTechnical Papers, pp. 2335–2344, 2014.\\n[38] T. H. Nguyen and R. Grishman, “Relation extraction: Perspective from\\nconvolutional neural networks,” in Proceedings of the 1st Workshop on\\nVector Space Modeling for Natural Language Processing, pp. 39–48,\\n2015.\\n[39] T. Mikolov, M. Karaﬁ´at, L. Burget, J. ˇCernock`y, and S. Khudanpur,\\n“Recurrent neural network based language model,” in Eleventh Annual\\nConference of the International Speech Communication Association,\\n2010.\\n[40] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\\n[41] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\\nS. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,”\\nin Advances in neural information processing systems, pp. 2672–2680,\\n2014.\\n[42] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,” arXiv\\npreprint arXiv:1701.07875, 2017.\\n[43] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and\\nP. Abbeel, “Infogan: Interpretable representation learning by informa-\\ntion maximizing generative adversarial nets,” in Advances in neural\\ninformation processing systems, pp. 2172–2180, 2016.\\n[44] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation\\nlearning with deep convolutional generative adversarial networks,”\\narXiv preprint arXiv:1511.06434, 2015.\\n[45] T. Karras, T. Aila, S. Laine, and J. Lehtinen, “Progressive growing\\nof GANs for improved quality, stability, and variation,” arXiv preprint\\narXiv:1710.10196, 2017.\\n[46] N.\\nTavaf,\\nA.\\nTorﬁ,\\nK.\\nUgurbil,\\nand\\nP.-F.\\nVan\\nde\\nMoortele,\\n“GRAPPA-GANs for Parallel MRI Reconstruction,” arXiv preprint\\narXiv:2101.03135, Jan 2021.\\n[47] L. Yu, W. Zhang, J. Wang, and Y. Yu, “Seqgan: Sequence generative\\nadversarial nets with policy gradient,” in Thirty-First AAAI Conference\\non Artiﬁcial Intelligence, 2017.\\n[48] J. Li, W. Monroe, T. Shi, S. Jean, A. Ritter, and D. Jurafsky,\\n“Adversarial learning for neural dialogue generation,” arXiv preprint\\narXiv:1701.06547, 2017.\\n[49] B. Pang, L. Lee, and S. Vaithyanathan, “Thumbs up?: sentiment classi-\\nﬁcation using machine learning techniques,” in Proceedings of the ACL-\\n02 conference on Empirical methods in natural language processing-\\nVolume 10, pp. 79–86, Association for Computational Linguistics,\\n2002.\\n[50] Z. S. Harris, “Distributional structure,” Word, vol. 10, no. 2-3, pp. 146–\\n162, 1954.\\n[51] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural proba-\\nbilistic language model,” Journal of machine learning research, vol. 3,\\nno. Feb., pp. 1137–1155, 2003.\\n[52] Q. Le and T. Mikolov, “Distributed representations of sentences\\nand documents,” in International Conference on Machine Learning,\\npp. 1188–1196, 2014.\\n[53] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,\\n“Distributed representations of words and phrases and their compo-\\nsitionality,” in Advances in neural information processing systems,\\npp. 3111–3119, 2013.\\n[54] R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Tor-\\nralba, and S. Fidler, “Skip-thought vectors,” in Advances in neural\\ninformation processing systems, pp. 3294–3302, 2015.\\n[55] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of\\nword representations in vector space,” arXiv preprint arXiv:1301.3781,\\n2013.\\n[56] G. Lebanon et al., Riemannian geometry and statistical machine\\nlearning. LAP LAMBERT Academic Publishing, 2015.\\n[57] J. Leskovec, A. Rajaraman, and J. D. Ullman, Mining of massive\\ndatasets. Cambridge University Press, 2014.\\n[58] Y. Goldberg, “Neural network methods for natural language process-\\ning,” Synthesis Lectures on Human Language Technologies, vol. 10,\\nno. 1, pp. 1–309, 2017.\\n[59] J. Wehrmann, W. Becker, H. E. Cagnini, and R. C. Barros, “A character-\\nbased convolutional neural network for language-agnostic Twitter sen-\\ntiment analysis,” in Neural Networks (IJCNN), 2017 International Joint\\nConference on, pp. 2384–2391, IEEE, 2017.\\n[60] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, “Enriching word\\nvectors with subword information,” arXiv preprint arXiv:1607.04606,\\n2016.\\n[61] J. Botha and P. Blunsom, “Compositional morphology for word rep-\\nresentations and language modelling,” in International Conference on\\nMachine Learning, pp. 1899–1907, 2014.\\n[62] A. See, P. J. Liu, and C. D. Manning, “Get to the point: Summarization\\nwith pointer-generator networks,” in ACL, vol. 1, pp. 1073–1083, 2017.\\n[63] R. Paulus, C. Xiong, and R. Socher, “A deep reinforced model for\\nabstractive summarization,” arXiv preprint arXiv:1705.04304, 2017.\\n[64] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, “Scheduled sampling\\nfor sequence prediction with recurrent neural networks,” in Advances\\nin Neural Information Processing Systems, pp. 1171–1179, 2015.\\n[65] K. Goyal, G. Neubig, C. Dyer, and T. Berg-Kirkpatrick, “A continuous\\nrelaxation of beam search for end-to-end training of neural sequence\\nmodels,” in Thirty-Second AAAI Conference on Artiﬁcial Intelligence,\\n2018.\\n[66] W. Kool, H. Van Hoof, and M. Welling, “Stochastic beams and where\\nto ﬁnd them: The gumbel-top-k trick for sampling sequences with-\\nout replacement,” in International Conference on Machine Learning,\\npp. 3499–3508, 2019.\\n[67] C.-Y. Lin, “Rouge: A package for automatic evaluation of summaries,”\\nin Text summarization branches out, pp. 74–81, 2004.\\n[68] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: a method\\nfor automatic evaluation of machine translation,” in Proceedings of\\nthe 40th annual meeting on Association for Computational Linguistics,\\npp. 311–318, Association for Computational Linguistics, 2002.\\n[69] S. Banerjee and A. Lavie, “METEOR: An automatic metric for\\nMT evaluation with improved correlation with human judgments,” in\\nProceedings of the ACL workshop on intrinsic and extrinsic evaluation\\nmeasures for machine translation and/or summarization, pp. 65–72,\\n2005.\\n[70] Y. Keneshloo, T. Shi, C. K. Reddy, and N. Ramakrishnan, “Deep rein-\\nforcement learning for sequence to sequence models,” arXiv preprint\\narXiv:1805.09461, 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'file_path': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'trapped': '', 'modDate': 'D:20210302011619Z', 'creationDate': 'D:20210302011619Z', 'page': 19}, page_content='TORFI et. al., NLP ADVANCEMENTS BY DEEP LEARNING\\n20\\n[71] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba, “Sequence\\nlevel\\ntraining\\nwith\\nrecurrent\\nneural\\nnetworks,”\\narXiv\\npreprint\\narXiv:1511.06732, 2015.\\n[72] W. Zaremba and I. Sutskever, “Reinforcement learning neural Turing\\nmachines-revised,” arXiv preprint arXiv:1505.00521, 2015.\\n[73] R. J. Williams, “Simple statistical gradient-following algorithms for\\nconnectionist reinforcement learning,” in Reinforcement Learning,\\npp. 5–32, Springer, 1992.\\n[74] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\\nMIT Press, 2018.\\n[75] C. J. Watkins and P. Dayan, “Q-learning,” Machine Learning, vol. 8,\\nno. 3-4, pp. 279–292, 1992.\\n[76] H. Daum´e, J. Langford, and D. Marcu, “Search-based structured\\nprediction,” Machine learning, vol. 75, no. 3, pp. 297–325, 2009.\\n[77] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training of\\ndeep visuomotor policies,” The Journal of Machine Learning Research,\\nvol. 17, no. 1, pp. 1334–1373, 2016.\\n[78] V. Mnih, N. Heess, A. Graves, et al., “Recurrent models of visual\\nattention,” in Advances in neural information processing systems,\\npp. 2204–2212, 2014.\\n[79] C. Sun, L. Huang, and X. Qiu, “Utilizing BERT for aspect-based\\nsentiment analysis via constructing auxiliary sentence,” arXiv preprint\\narXiv:1903.09588, 2019.\\n[80] P. Resnik and J. Lin, “Evaluation of NLP systems,” The handbook\\nof computational linguistics and natural language processing, vol. 57,\\npp. 271–295, 2010.\\n[81] A. Kumar, O. Irsoy, P. Ondruska, M. Iyyer, J. Bradbury, I. Gulrajani,\\nV. Zhong, R. Paulus, and R. Socher, “Ask me anything: Dynamic\\nmemory networks for natural language processing,” in International\\nConference on Machine Learning, pp. 1378–1387, 2016.\\n[82] Z. Huang, W. Xu, and K. Yu, “Bidirectional LSTM-CRF models for\\nsequence tagging,” arXiv preprint arXiv:1508.01991, 2015.\\n[83] D. Andor, C. Alberti, D. Weiss, A. Severyn, A. Presta, K. Ganchev,\\nS. Petrov, and M. Collins, “Globally normalized transition-based neural\\nnetworks,” arXiv preprint arXiv:1603.06042, 2016.\\n[84] X. Xue and J. Zhang, “Part-of-speech tagging of building codes\\nempowered by deep learning and transformational rules,” Advanced\\nEngineering Informatics, vol. 47, p. 101235, 2021.\\n[85] L. Liu, J. Shang, X. Ren, F. F. Xu, H. Gui, J. Peng, and J. Han,\\n“Empower sequence labeling with task-aware neural language model,”\\nin Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.\\n[86] Z. Yang, R. Salakhutdinov, and W. W. Cohen, “Transfer learning for\\nsequence tagging with hierarchical recurrent networks,” arXiv preprint\\narXiv:1703.06345, 2017.\\n[87] X. Ma and E. Hovy, “End-to-end sequence labeling via bi-directional\\nLSTM-CNNs-CRF,” arXiv preprint arXiv:1603.01354, 2016.\\n[88] M.\\nYasunaga,\\nJ.\\nKasai,\\nand\\nD.\\nRadev,\\n“Robust\\nmultilingual\\npart-of-speech\\ntagging\\nvia\\nadversarial\\ntraining,”\\narXiv\\npreprint\\narXiv:1711.04903, 2017.\\n[89] W. Ling, T. Lu´ıs, L. Marujo, R. F. Astudillo, S. Amir, C. Dyer, A. W.\\nBlack, and I. Trancoso, “Finding function in form: Compositional\\ncharacter models for open vocabulary word representation,” arXiv\\npreprint arXiv:1508.02096, 2015.\\n[90] A. Akbik, D. Blythe, and R. Vollgraf, “Contextual string embeddings\\nfor sequence labeling,” in Proceedings of the 27th International Con-\\nference on Computational Linguistics, pp. 1638–1649, 2018.\\n[91] B. Bohnet, R. McDonald, G. Simoes, D. Andor, E. Pitler, and\\nJ. Maynez, “Morphosyntactic tagging with a Meta-BiLSTM model over\\ncontext sensitive token encodings,” arXiv preprint arXiv:1805.08237,\\n2018.\\n[92] J. Legrand and R. Collobert, “Joint RNN-based greedy parsing and\\nword composition,” arXiv preprint arXiv:1412.7028, 2014.\\n[93] J. Legrand and R. Collobert, “Deep neural networks for syntactic\\nparsing of morphologically rich languages,” in Proceedings of the\\n54th Annual Meeting of the Association for Computational Linguistics\\n(Volume 2: Short Papers), pp. 573–578, 2016.\\n[94] A. Kuncoro, M. Ballesteros, L. Kong, C. Dyer, G. Neubig, and N. A.\\nSmith, “What do recurrent neural network grammars learn about\\nsyntax?,” arXiv preprint arXiv:1611.05774, 2016.\\n[95] J. Liu and Y. Zhang, “In-order transition-based constituent parsing,”\\narXiv preprint arXiv:1707.05000, 2017.\\n[96] D. Fried, M. Stern, and D. Klein, “Improving neural parsing by\\ndisentangling model combination and reranking effects,” arXiv preprint\\narXiv:1707.03058, 2017.\\n[97] N. Kitaev and D. Klein, “Constituency parsing with a self-attentive\\nencoder,” arXiv preprint arXiv:1805.01052, 2018.\\n[98] D. Chen and C. Manning, “A fast and accurate dependency parser using\\nneural networks,” in Proceedings of the 2014 conference on empirical\\nmethods in natural language processing (EMNLP), pp. 740–750, 2014.\\n[99] T. Dozat and C. D. Manning, “Deep biafﬁne attention for neural\\ndependency parsing,” arXiv preprint arXiv:1611.01734, 2016.\\n[100] E. Kiperwasser and Y. Goldberg, “Simple and accurate dependency\\nparsing using bidirectional LSTM feature representations,” arXiv\\npreprint arXiv:1603.04351, 2016.\\n[101] C. Dyer, M. Ballesteros, W. Ling, A. Matthews, and N. A. Smith,\\n“Transition-based dependency parsing with stack long short-term mem-\\nory,” arXiv preprint arXiv:1505.08075, 2015.\\n[102] S. Jaf and C. Calder, “Deep learning for natural language parsing,”\\nIEEE Access, vol. 7, pp. 131363–131373, 2019.\\n[103] Y. Zhang, F. Tiryaki, M. Jiang, and H. Xu, “Parsing clinical text\\nusing the state-of-the-art deep learning based parsers: a systematic\\ncomparison,” BMC medical informatics and decision making, vol. 19,\\nno. 3, p. 77, 2019.\\n[104] Y. Zhang, Z. Li, and M. Zhang, “Efﬁcient second-order treecrf for\\nneural dependency parsing,” arXiv preprint arXiv:2005.00975, 2020.\\n[105] T. Dozat and C. D. Manning, “Deep biafﬁne attention for neural\\ndependency parsing,” 2017.\\n[106] Z. Tan, M. Wang, J. Xie, Y. Chen, and X. Shi, “Deep semantic role\\nlabeling with self-attention,” arXiv preprint arXiv:1712.01586, 2017.\\n[107] D. Marcheggiani and I. Titov, “Encoding sentences with graph\\nconvolutional networks for semantic role labeling,” arXiv preprint\\narXiv:1703.04826, 2017.\\n[108] E. Strubell, P. Verga, D. Andor, D. Weiss, and A. McCallum,\\n“Linguistically-informed self-attention for semantic role labeling,”\\narXiv preprint arXiv:1804.08199, 2018.\\n[109] L. He, K. Lee, O. Levy, and L. Zettlemoyer, “Jointly predicting\\npredicates and arguments in neural semantic role labeling,” arXiv\\npreprint arXiv:1805.04787, 2018.\\n[110] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\\nand L. Zettlemoyer, “Deep contextualized word representations,” arXiv\\npreprint arXiv:1802.05365, 2018.\\n[111] Z. Tan, M. Wang, J. Xie, Y. Chen, and X. Shi, “Deep semantic role\\nlabeling with self-attention,” in Thirty-Second AAAI Conference on\\nArtiﬁcial Intelligence, 2018.\\n[112] Z. Li, S. He, H. Zhao, Y. Zhang, Z. Zhang, X. Zhou, and X. Zhou,\\n“Dependency or span, end-to-end uniform semantic role labeling,” in\\nProceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 33,\\npp. 6730–6737, 2019.\\n[113] S. Pradhan, A. Moschitti, N. Xue, H. T. Ng, A. Bj¨orkelund,\\nO. Uryupina, Y. Zhang, and Z. Zhong, “Towards robust linguistic anal-\\nysis using OntoNotes,” in Proceedings of the Seventeenth Conference\\non Computational Natural Language Learning, pp. 143–152, 2013.\\n[114] N. Kalchbrenner, E. Grefenstette, and P. Blunsom, “A convo-\\nlutional neural network for modelling sentences,” arXiv preprint\\narXiv:1404.2188, 2014.\\n[115] H. Palangi, L. Deng, Y. Shen, J. Gao, X. He, J. Chen, X. Song,\\nand R. Ward, “Deep sentence embedding using long short-term\\nmemory networks: Analysis and application to information retrieval,”\\nIEEE/ACM Transactions on Audio, Speech and Language Processing\\n(TASLP), vol. 24, no. 4, pp. 694–707, 2016.\\n[116] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, “Hierar-\\nchical attention networks for document classiﬁcation,” in Proceedings\\nof the 2016 Conference of the North American Chapter of the Associ-\\nation for Computational Linguistics: Human Language Technologies,\\npp. 1480–1489, 2016.\\n[117] S. Lai, L. Xu, K. Liu, and J. Zhao, “Recurrent convolutional neural\\nnetworks for text classiﬁcation.,” in AAAI, vol. 333, pp. 2267–2273,\\n2015.\\n[118] C. Zhou, C. Sun, Z. Liu, and F. Lau, “A C-LSTM neural network for\\ntext classiﬁcation,” arXiv preprint arXiv:1511.08630, 2015.\\n[119] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu,\\nand J. Gao, “Deep learning based text classiﬁcation: A comprehensive\\nreview,” arXiv preprint arXiv:2004.03705, 2020.\\n[120] M. Zulqarnain, R. Ghazali, Y. M. M. Hassim, and M. Rehan, “A\\ncomparative review on deep learning models for text classiﬁcation,”\\nIndones. J. Electr. Eng. Comput. Sci, vol. 19, no. 1, pp. 325–335, 2020.\\n[121] A. Conneau, H. Schwenk, L. Barrault, and Y. LeCun, “Very deep\\nconvolutional networks for text classiﬁcation,” in Proceedings of the\\n15th Conference of the European Chapter of the Association for\\nComputational Linguistics: Volume 1, Long Papers, vol. 1, pp. 1107–\\n1116, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'file_path': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'trapped': '', 'modDate': 'D:20210302011619Z', 'creationDate': 'D:20210302011619Z', 'page': 20}, page_content='TORFI et. al., NLP ADVANCEMENTS BY DEEP LEARNING\\n21\\n[122] R. Johnson and T. Zhang, “Deep pyramid convolutional neural net-\\nworks for text categorization,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics (Volume 1:\\nLong Papers), vol. 1, pp. 562–570, 2017.\\n[123] R. Johnson and T. Zhang, “Supervised and semi-supervised text\\ncategorization using LSTM for region embeddings,” arXiv preprint\\narXiv:1602.02373, 2016.\\n[124] J. Howard and S. Ruder, “Universal language model ﬁne-tuning for\\ntext classiﬁcation,” in Proceedings of the 56th Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers),\\nvol. 1, pp. 328–339, 2018.\\n[125] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and\\nP. Kuksa, “Natural language processing (almost) from scratch,” Journal\\nof Machine Learning Research, vol. 12, no. Aug., pp. 2493–2537, 2011.\\n[126] G. Mesnil, X. He, L. Deng, and Y. Bengio, “Investigation of recurrent-\\nneural-network architectures and learning methods for spoken language\\nunderstanding.,” in Interspeech, pp. 3771–3775, 2013.\\n[127] F. Dernoncourt, J. Y. Lee, and P. Szolovits, “NeuroNER: an easy-to-\\nuse program for named-entity recognition based on neural networks,”\\nConference on Empirical Methods on Natural Language Processing\\n(EMNLP), 2017.\\n[128] A. Baevski, S. Edunov, Y. Liu, L. Zettlemoyer, and M. Auli, “Cloze-\\ndriven pretraining of self-attention networks,” 2019.\\n[129] E. F. Tjong Kim Sang and F. De Meulder, “Introduction to the CoNLL-\\n2003 shared task: Language-independent named entity recognition,” in\\nProceedings of the seventh conference on Natural language learning\\nat HLT-NAACL 2003-Volume 4, pp. 142–147, Association for Compu-\\ntational Linguistics, 2003.\\n[130] K. Clark, M.-T. Luong, C. D. Manning, and Q. V. Le, “Semi-\\nsupervised sequence modeling with cross-view training,” arXiv preprint\\narXiv:1809.08370, 2018.\\n[131] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\\ntraining of deep bidirectional transformers for language understanding,”\\narXiv preprint arXiv:1810.04805, 2018.\\n[132] R. Socher, B. Huval, C. D. Manning, and A. Y. Ng, “Semantic compo-\\nsitionality through recursive matrix-vector spaces,” in Proceedings of\\nthe 2012 joint conference on empirical methods in natural language\\nprocessing and computational natural language learning, pp. 1201–\\n1211, Association for Computational Linguistics, 2012.\\n[133] Z. Geng, G. Chen, Y. Han, G. Lu, and F. Li, “Semantic relation\\nextraction using sequential and tree-structured lstm with attention,”\\nInformation Sciences, vol. 509, pp. 183–192, 2020.\\n[134] X. Han, T. Gao, Y. Lin, H. Peng, Y. Yang, C. Xiao, Z. Liu, P. Li,\\nM. Sun, and J. Zhou, “More data, more relations, more context and\\nmore openness: A review and outlook for relation extraction,” arXiv\\npreprint arXiv:2004.03186, 2020.\\n[135] K.\\nClark\\nand\\nC.\\nD.\\nManning,\\n“Deep\\nreinforcement\\nlearn-\\ning\\nfor\\nmention-ranking\\ncoreference\\nmodels,”\\narXiv\\npreprint\\narXiv:1609.08667, 2016.\\n[136] K. Lee, L. He, and L. Zettlemoyer, “Higher-order coreference resolu-\\ntion with coarse-to-ﬁne inference,” arXiv preprint arXiv:1804.05392,\\n2018.\\n[137] H. Fei, X. Li, D. Li, and P. Li, “End-to-end deep reinforcement learning\\nbased coreference resolution,” in Proceedings of the 57th Annual\\nMeeting of the Association for Computational Linguistics, pp. 660–\\n665, 2019.\\n[138] W. Wu, F. Wang, A. Yuan, F. Wu, and J. Li, “Corefqa: Coreference\\nresolution as query-based span prediction,” in Proceedings of the 58th\\nAnnual Meeting of the Association for Computational Linguistics,\\npp. 6953–6963, 2020.\\n[139] Y. Chen, L. Xu, K. Liu, D. Zeng, and J. Zhao, “Event extraction via\\ndynamic multi-pooling convolutional neural networks,” in Proceedings\\nof the 53rd Annual Meeting of the Association for Computational\\nLinguistics and the 7th International Joint Conference on Natural\\nLanguage Processing (Volume 1: Long Papers), vol. 1, pp. 167–176,\\n2015.\\n[140] T. H. Nguyen and R. Grishman, “Graph convolutional networks with\\nargument-aware pooling for event detection,” in Thirty-Second AAAI\\nConference on Artiﬁcial Intelligence, 2018.\\n[141] T. Zhang, H. Ji, and A. Sil, “Joint entity and event extraction with\\ngenerative adversarial imitation learning,” Data Intelligence, vol. 1,\\nno. 2, pp. 99–120, 2019.\\n[142] W. Zhao, J. Zhang, J. Yang, T. He, H. Ma, and Z. Li, “A novel\\njoint biomedical event extraction framework via two-level modeling\\nof documents,” Information Sciences, vol. 550, pp. 27–40, 2021.\\n[143] T. Nasukawa and J. Yi, “Sentiment analysis: Capturing favorability\\nusing natural language processing,” in Proceedings of the 2nd Interna-\\ntional Conference on Knowledge Capture, pp. 70–77, ACM, 2003.\\n[144] K. Dave, S. Lawrence, and D. M. Pennock, “Mining the peanut gallery:\\nOpinion extraction and semantic classiﬁcation of product reviews,” in\\nProceedings of the 12th international conference on World Wide Web,\\npp. 519–528, ACM, 2003.\\n[145] A. R. Pathak, B. Agarwal, M. Pandey, and S. Rautaray, “Application of\\ndeep learning approaches for sentiment analysis,” in Deep Learning-\\nBased Approaches for Sentiment Analysis, pp. 1–31, Springer, 2020.\\n[146] A. Yadav and D. K. Vishwakarma, “Sentiment analysis using deep\\nlearning architectures: a review,” Artiﬁcial Intelligence Review, vol. 53,\\nno. 6, pp. 4335–4385, 2020.\\n[147] D. Tang, B. Qin, and T. Liu, “Document modeling with gated recurrent\\nneural network for sentiment classiﬁcation,” in Proceedings of the\\n2015 conference on empirical methods in natural language processing,\\npp. 1422–1432, 2015.\\n[148] X. Glorot, A. Bordes, and Y. Bengio, “Domain adaptation for large-\\nscale sentiment classiﬁcation: A deep learning approach,” in Proceed-\\nings of the 28th international conference on machine learning (ICML-\\n11), pp. 513–520, 2011.\\n[149] G. Rao, W. Huang, Z. Feng, and Q. Cong, “Lstm with sentence\\nrepresentations for document-level sentiment classiﬁcation,” Neuro-\\ncomputing, vol. 308, pp. 49–57, 2018.\\n[150] M. Rhanoui, M. Mikram, S. Yousﬁ, and S. Barzali, “A cnn-bilstm\\nmodel for document-level sentiment analysis,” Machine Learning and\\nKnowledge Extraction, vol. 1, no. 3, pp. 832–847, 2019.\\n[151] R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning,\\n“Semi-supervised recursive autoencoders for predicting sentiment dis-\\ntributions,” in Proceedings of the conference on empirical methods in\\nnatural language processing, pp. 151–161, Association for Computa-\\ntional Linguistics, 2011.\\n[152] X. Wang, Y. Liu, S. Chengjie, B. Wang, and X. Wang, “Predicting\\npolarities of tweets by composing word embeddings with long short-\\nterm memory,” in Proceedings of the 53rd Annual Meeting of the\\nAssociation for Computational Linguistics and the 7th International\\nJoint Conference on Natural Language Processing (Volume 1: Long\\nPapers), vol. 1, pp. 1343–1353, 2015.\\n[153] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng,\\nand C. Potts, “Recursive deep models for semantic compositionality\\nover a sentiment treebank,” in Proceedings of the 2013 conference\\non empirical methods in natural language processing, pp. 1631–1642,\\n2013.\\n[154] R. Arulmurugan, K. Sabarmathi, and H. Anandakumar, “Classiﬁcation\\nof sentence level sentiment analysis using cloud machine learning\\ntechniques,” Cluster Computing, vol. 22, no. 1, pp. 1199–1209, 2019.\\n[155] D. Meˇskel˙e and F. Frasincar, “Aldonar: A hybrid solution for sentence-\\nlevel aspect-based sentiment analysis using a lexicalized domain ontol-\\nogy and a regularized neural attention model,” Information Processing\\n& Management, vol. 57, no. 3, p. 102211, 2020.\\n[156] Y. Wang, M. Huang, L. Zhao, et al., “Attention-based LSTM for aspect-\\nlevel sentiment classiﬁcation,” in Proceedings of the 2016 Conference\\non Empirical Methods in Natural Language Processing, pp. 606–615,\\n2016.\\n[157] Y. Ma, H. Peng, T. Khan, E. Cambria, and A. Hussain, “Sentic lstm: a\\nhybrid network for targeted aspect-based sentiment analysis,” Cognitive\\nComputation, vol. 10, no. 4, pp. 639–650, 2018.\\n[158] H. Xu, B. Liu, L. Shu, and P. S. Yu, “BERT post-training for review\\nreading comprehension and aspect-based sentiment analysis,” arXiv\\npreprint arXiv:1904.02232, 2019.\\n[159] H. Xu, B. Liu, L. Shu, and P. S. Yu, “Double embeddings and\\nCNN-based sequence labeling for aspect extraction,” arXiv preprint\\narXiv:1805.04601, 2018.\\n[160] H. H. Do, P. Prasad, A. Maag, and A. Alsadoon, “Deep learning for\\naspect-based sentiment analysis: a comparative review,” Expert Systems\\nwith Applications, vol. 118, pp. 272–299, 2019.\\n[161] S. Rida-E-Fatima, A. Javed, A. Banjar, A. Irtaza, H. Dawood, H. Da-\\nwood, and A. Alamri, “A multi-layer dual attention deep learning model\\nwith reﬁned word embeddings for aspect-based sentiment analysis,”\\nIEEE Access, vol. 7, pp. 114795–114807, 2019.\\n[162] Y. Liang, F. Meng, J. Zhang, J. Xu, Y. Chen, and J. Zhou, “A\\nnovel aspect-guided deep transition model for aspect based sentiment\\nanalysis,” arXiv preprint arXiv:1909.00324, 2019.\\n[163] D. Jurafsky and J. H. Martin, Speech and Language Processing.\\nPrentice Hall, 2008.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'file_path': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'trapped': '', 'modDate': 'D:20210302011619Z', 'creationDate': 'D:20210302011619Z', 'page': 21}, page_content='TORFI et. al., NLP ADVANCEMENTS BY DEEP LEARNING\\n22\\n[164] N. Kalchbrenner and P. Blunsom, “Recurrent continuous translation\\nmodels,” in Proceedings of the 2013 Conference on Empirical Methods\\nin Natural Language Processing, pp. 1700–1709, 2013.\\n[165] S. P. Singh, A. Kumar, H. Darbari, L. Singh, A. Rastogi, and S. Jain,\\n“Machine translation using deep learning: An overview,” in 2017\\ninternational conference on computer, communications and electronics\\n(comptelix), pp. 162–167, IEEE, 2017.\\n[166] S. Yang, Y. Wang, and X. Chu, “A survey of deep learning techniques\\nfor neural machine translation,” arXiv preprint arXiv:2002.07526,\\n2020.\\n[167] L. E. Dostert, “The Georgetown-IBM experiment,” 1955). Machine\\ntranslation of languages. John Wiley & Sons, New York, pp. 124–135,\\n1955.\\n[168] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by\\njointly learning to align and translate,” arXiv preprint arXiv:1409.0473,\\n2014.\\n[169] K. Cho, B. Van Merri¨enboer, D. Bahdanau, and Y. Bengio, “On the\\nproperties of neural machine translation: Encoder-decoder approaches,”\\narXiv preprint arXiv:1409.1259, 2014.\\n[170] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning\\nwith neural networks,” in Advances in neural information processing\\nsystems, pp. 3104–3112, 2014.\\n[171] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey,\\nM. Krikun, Y. Cao, Q. Gao, K. Macherey, et al., “Google’s neural\\nmachine translation system: Bridging the gap between human and\\nmachine translation,” arXiv preprint arXiv:1609.08144, 2016.\\n[172] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin,\\n“Convolutional\\nsequence\\nto\\nsequence\\nlearning,”\\narXiv\\npreprint\\narXiv:1705.03122, 2017.\\n[173] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in\\nAdvances in Neural Information Processing Systems, pp. 5998–6008,\\n2017.\\n[174] K. Ahmed, N. S. Keskar, and R. Socher, “Weighted transformer\\nnetwork for machine translation,” arXiv preprint arXiv:1711.02132,\\n2017.\\n[175] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative\\nposition representations,” arXiv preprint arXiv:1803.02155, 2018.\\n[176] S. Edunov, M. Ott, M. Auli, and D. Grangier, “Understanding back-\\ntranslation at scale,” arXiv preprint arXiv:1808.09381, 2018.\\n[177] R. Aharoni, M. Johnson, and O. Firat, “Massively multilingual neural\\nmachine translation,” 2019.\\n[178] J. Zhu, Y. Xia, L. Wu, D. He, T. Qin, W. Zhou, H. Li, and T.-Y. Liu,\\n“Incorporating bert into neural machine translation,” 2020.\\n[179] Y. Liu, J. Gu, N. Goyal, X. Li, S. Edunov, M. Ghazvininejad,\\nM. Lewis, and L. Zettlemoyer, “Multilingual denoising pre-training\\nfor neural machine translation,” Transactions of the Association for\\nComputational Linguistics, vol. 8, pp. 726–742, 2020.\\n[180] Y. Cheng, L. Jiang, and W. Macherey, “Robust neural machine transla-\\ntion with doubly adversarial inputs,” arXiv preprint arXiv:1906.02443,\\n2019.\\n[181] W. Zhang, Y. Feng, F. Meng, D. You, and Q. Liu, “Bridging the gap\\nbetween training and inference for neural machine translation,” 2019.\\n[182] J. Yang, M. Wang, H. Zhou, C. Zhao, W. Zhang, Y. Yu, and L. Li,\\n“Towards making the most of bert in neural machine translation,” in\\nProceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 34,\\npp. 9378–9385, 2020.\\n[183] A. Bordes, S. Chopra, and J. Weston, “Question answering with\\nsubgraph embeddings,” arXiv preprint arXiv:1406.3676, 2014.\\n[184] B. F. Green Jr, A. K. Wolf, C. Chomsky, and K. Laughery, “Baseball:\\nan automatic question-answerer,” in Papers presented at the May 9-11,\\n1961, Western Joint IRE-AIEE-ACM Computer Conference, pp. 219–\\n224, ACM, 1961.\\n[185] A. Ittycheriah, M. Franz, W.-J. Zhu, A. Ratnaparkhi, and R. J. Mam-\\nmone, “IBM’s statistical question answering system.,” in TREC, 2000.\\n[186] H. Cui, R. Sun, K. Li, M.-Y. Kan, and T.-S. Chua, “Question answering\\npassage retrieval using dependency relations,” in Proceedings of the\\n28th annual international ACM SIGIR conference on Research and\\ndevelopment in information retrieval, pp. 400–407, ACM, 2005.\\n[187] X. Qiu and X. Huang, “Convolutional neural tensor network architec-\\nture for community-based question answering.,” in IJCAI, pp. 1305–\\n1311, 2015.\\n[188] H. T. Ng, L. H. Teo, and J. L. P. Kwan, “A machine learning\\napproach to answering questions for reading comprehension tests,”\\nin Proceedings of the 2000 Joint SIGDAT conference on Empirical\\nmethods in natural language processing and very large corpora: held\\nin conjunction with the 38th Annual Meeting of the Association for\\nComputational Linguistics-Volume 13, pp. 124–132, Association for\\nComputational Linguistics, 2000.\\n[189] C. Xiong, V. Zhong, and R. Socher, “Dynamic coattention networks\\nfor question answering,” arXiv preprint arXiv:1611.01604, 2016.\\n[190] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zit-\\nnick, and D. Parikh, “VQA: Visual question answering,” in Proceedings\\nof the IEEE international conference on computer vision, pp. 2425–\\n2433, 2015.\\n[191] M. Malinowski, M. Rohrbach, and M. Fritz, “Ask your neurons:\\nA neural-based approach to answering questions about images,” in\\nProceedings of the IEEE international conference on computer vision,\\npp. 1–9, 2015.\\n[192] H. Xu and K. Saenko, “Ask, attend and answer: Exploring question-\\nguided spatial attention for visual question answering,” in European\\nConference on Computer Vision, pp. 451–466, Springer, 2016.\\n[193] A. Das, H. Agrawal, L. Zitnick, D. Parikh, and D. Batra, “Human\\nattention in visual question answering: Do humans and deep networks\\nlook at the same regions?,” Computer Vision and Image Understanding,\\nvol. 163, pp. 90–100, 2017.\\n[194] H. Ben-Younes, R. Cadene, N. Thome, and M. Cord, “Block: Bilinear\\nsuperdiagonal fusion for visual question answering and visual relation-\\nship detection,” in Proceedings of the AAAI Conference on Artiﬁcial\\nIntelligence, vol. 33, pp. 8102–8109, 2019.\\n[195] J. Wu and R. J. Mooney, “Self-critical reasoning for robust visual\\nquestion answering,” 2019.\\n[196] R. Nallapati, F. Zhai, and B. Zhou, “SummaRuNNer: A recurrent\\nneural network based sequence model for extractive summarization of\\ndocuments.,” in AAAI, pp. 3075–3081, 2017.\\n[197] S. Narayan, S. B. Cohen, and M. Lapata, “Ranking sentences for ex-\\ntractive summarization with reinforcement learning,” in NAACL:HLT,\\nvol. 1, pp. 1747–1759, 2018.\\n[198] A. M. Rush, S. Chopra, and J. Weston, “A neural attention model for\\nabstractive sentence summarization,” in EMNLP, 2015.\\n[199] J. Tan, X. Wan, and J. Xiao, “Abstractive document summarization with\\na graph-based attentional neural model,” in ACL, vol. 1, pp. 1171–1181,\\n2017.\\n[200] R. Nallapati, B. Zhou, C. dos Santos, C. Gulcehre, and B. Xiang,\\n“Abstractive text summarization using sequence-to-sequence RNNs\\nand beyond,” in Proceedings of The 20th SIGNLL Conference on\\nComputational Natural Language Learning, pp. 280–290, 2016.\\n[201] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay,\\nM. Suleyman, and P. Blunsom, “Teaching machines to read and\\ncomprehend,” in NIPS, pp. 1693–1701, 2015.\\n[202] J. Gu, Z. Lu, H. Li, and V. O. Li, “Incorporating copying mechanism in\\nsequence-to-sequence learning,” in ACL, vol. 1, pp. 1631–1640, 2016.\\n[203] Y.-C. Chen and M. Bansal, “Fast abstractive summarization with\\nreinforce-selected sentence rewriting,” in ACL, 2018.\\n[204] Q. Zhou, N. Yang, F. Wei, S. Huang, M. Zhou, and T. Zhao, “Neu-\\nral document summarization by jointly learning to score and select\\nsentences,” in ACL, pp. 654–663, ACL, 2018.\\n[205] T. Shi, Y. Keneshloo, N. Ramakrishnan, and C. K. Reddy, “Neural\\nabstractive text summarization with sequence-to-sequence models,”\\narXiv preprint arXiv:1812.02303, 2018.\\n[206] C. Ma, W. E. Zhang, M. Guo, H. Wang, and Q. Z. Sheng, “Multi-\\ndocument summarization via deep learning techniques: A survey,”\\narXiv preprint arXiv:2011.04843, 2020.\\n[207] A. Abdi, S. Hasan, S. M. Shamsuddin, N. Idris, and J. Piran, “A hybrid\\ndeep learning architecture for opinion-oriented multi-document sum-\\nmarization based on multi-feature fusion,” Knowledge-Based Systems,\\nvol. 213, p. 106658, 2021.\\n[208] X. Zhang, F. Wei, and M. Zhou, “Hibert: Document level pre-training\\nof hierarchical bidirectional transformers for document summariza-\\ntion,” arXiv preprint arXiv:1905.06566, 2019.\\n[209] E. Merdivan, D. Singh, S. Hanke, and A. Holzinger, “Dialogue sys-\\ntems for intelligent human computer interactions,” Electronic Notes in\\nTheoretical Computer Science, vol. 343, pp. 57–71, 2019.\\n[210] D. Hakkani-T¨ur, G. T¨ur, A. Celikyilmaz, Y.-N. Chen, J. Gao, L. Deng,\\nand Y.-Y. Wang, “Multi-domain joint semantic frame parsing using\\nbi-directional RNN-LSTM,” in Interspeech, pp. 715–719, 2016.\\n[211] C. Toxtli, J. Cranshaw, et al., “Understanding chatbot-mediated task\\nmanagement,” in Proceedings of the 2018 CHI Conference on Human\\nFactors in Computing Systems, p. 58, ACM, 2018.\\n[212] V. Ilievski, C. Musat, A. Hossmann, and M. Baeriswyl, “Goal-oriented\\nchatbot dialog management bootstrapping with transfer learning,” arXiv\\npreprint arXiv:1802.00500, 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'file_path': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'trapped': '', 'modDate': 'D:20210302011619Z', 'creationDate': 'D:20210302011619Z', 'page': 22}, page_content='TORFI et. al., NLP ADVANCEMENTS BY DEEP LEARNING\\n23\\n[213] J. Li, W. Monroe, A. Ritter, D. Jurafsky, M. Galley, and J. Gao, “Deep\\nreinforcement learning for dialogue generation,” in Proceedings of the\\nConference on Empirical Methods in Natural Language Processing,\\npp. 1192–1202, 2016.\\n[214] T.-H. Wen, D. Vandyke, N. Mrksic, M. Gasic, L. M. Rojas-Barahona,\\nP.-H. Su, S. Ultes, and S. Young, “A network-based end-to-end train-\\nable task-oriented dialogue system,” arXiv preprint arXiv:1604.04562,\\n2016.\\n[215] J. D. Williams and G. Zweig, “End-to-end LSTM-based dialog control\\noptimized with supervised and reinforcement learning,” arXiv preprint\\narXiv:1606.01269, 2016.\\n[216] S. Sukhbaatar, J. Weston, R. Fergus, et al., “End-to-end memory\\nnetworks,” in Advances in neural information processing systems,\\npp. 2440–2448, 2015.\\n[217] A. Bordes, Y.-L. Boureau, and J. Weston, “Learning end-to-end goal-\\noriented dialog,” arXiv preprint arXiv:1605.07683, 2016.\\n[218] A. Ritter, C. Cherry, and W. B. Dolan, “Data-driven response generation\\nin social media,” in Proceedings of the conference on empirical\\nmethods in natural language processing, pp. 583–593, Association for\\nComputational Linguistics, 2011.\\n[219] Z. Ji, Z. Lu, and H. Li, “An information retrieval approach to short\\ntext conversation,” arXiv preprint arXiv:1408.6988, 2014.\\n[220] B. Hu, Z. Lu, H. Li, and Q. Chen, “Convolutional neural network\\narchitectures for matching natural language sentences,” in Advances in\\nneural information processing systems, pp. 2042–2050, 2014.\\n[221] R. Lowe, N. Pow, I. Serban, and J. Pineau, “The Ubuntu dialogue\\ncorpus: A large dataset for research in unstructured multi-turn dialogue\\nsystems,” arXiv preprint arXiv:1506.08909, 2015.\\n[222] R. Yan, Y. Song, and H. Wu, “Learning to respond with deep neural\\nnetworks for retrieval-based human-computer conversation system,”\\nin Proceedings of the 39th International ACM SIGIR conference on\\nResearch and Development in Information Retrieval, pp. 55–64, ACM,\\n2016.\\n[223] X. Zhou, L. Li, D. Dong, Y. Liu, Y. Chen, W. X. Zhao, D. Yu, and\\nH. Wu, “Multi-turn response selection for chatbots with deep attention\\nmatching network,” in Proceedings of the 56th Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers),\\nvol. 1, pp. 1118–1127, 2018.\\n[224] S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Jozefowicz, and\\nS. Bengio, “Generating sentences from a continuous space,” arXiv\\npreprint arXiv:1511.06349, 2015.\\n[225] A. Kannan and O. Vinyals, “Adversarial evaluation of dialogue mod-\\nels,” arXiv preprint arXiv:1701.08198, 2017.\\n[226] O. Vinyals and Q. Le, “A neural conversational model,” arXiv preprint\\narXiv:1506.05869, 2015.')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/pdf\",\n",
    "    glob=\"**/*.pdf\", ## Pattern to match files  \n",
    "    loader_cls= PyMuPDFLoader, ##loader class to use\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "pdf_documents=dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0c22287c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 PDF files to process\n",
      "\n",
      "Processing: budget_speech.pdf\n",
      "  ✓ Loaded 58 pages\n",
      "\n",
      "Processing: Conversational Text Extraction with LLM.pdf\n",
      "  ✓ Loaded 6 pages\n",
      "\n",
      "Processing: Multimodal Retrieval-Augmented Generation.pdf\n",
      "  ✓ Loaded 80 pages\n",
      "\n",
      "Processing: NLP Advancements by Deep Learning.pdf\n",
      "  ✓ Loaded 23 pages\n",
      "\n",
      "Total documents loaded: 167\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ebcb65a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 0, 'page_label': '1', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='GOVERNMENT OF INDIA\\nBUDGET 2023-2024\\nSPEECH\\nOF\\nNIRMALA SITHARAMAN\\nMINISTER OF FINANCE\\nFebruary 1,  2023'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 1, 'page_label': '2', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content=''),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 2, 'page_label': '3', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='CONTENTS \\nPART-A \\n Page No. \\n\\uf0b7 Introduction 1 \\n\\uf0b7 Achievements since 2014: Leaving no one behind 2 \\n\\uf0b7 Vision for Amrit Kaal – an empowered and inclusive economy 3 \\n\\uf0b7 Priorities of this Budget 5 \\ni. Inclusive Development  \\nii. Reaching the Last Mile \\niii. Infrastructure and Investment \\niv. Unleashing the Potential \\nv. Green Growth \\nvi. Youth Power  \\nvii. Financial Sector \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\uf0b7 Fiscal Management \\n24 \\nPART B \\n  \\nIndirect Taxes 27 \\n\\uf0b7 Green Mobility  \\n\\uf0b7 Electronics   \\n\\uf0b7 Electrical   \\n\\uf0b7 Chemicals and Petrochemicals   \\n\\uf0b7 Marine products  \\n\\uf0b7 Lab Grown Diamonds  \\n\\uf0b7 Precious Metals  \\n\\uf0b7 Metals  \\n\\uf0b7 Compounded Rubber  \\n\\uf0b7 Cigarettes  \\n  \\nDirect Taxes 30 \\n\\uf0b7 MSMEs and Professionals   \\n\\uf0b7 Cooperation  \\n\\uf0b7 Start-Ups  \\n\\uf0b7 Appeals  \\n\\uf0b7 Better targeting of tax concessions  \\n\\uf0b7 Rationalisation  \\n\\uf0b7 Others  \\n\\uf0b7 Personal Income Tax  \\n  \\nAnnexures 35 \\n\\uf0b7 Annexure to Part B of the Budget Speech 2023-24 \\ni. Amendments relating to Direct Taxes \\nii. Amendments relating to Indirect Taxes'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 3, 'page_label': '4', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content=''),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 4, 'page_label': '5', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='Budget 2023-2024 \\n \\nSpeech of \\nNirmala Sitharaman \\nMinister of Finance \\nFebruary 1, 2023 \\nHon’ble Speaker,  \\n I present the Budget for 2023-24. This is the first Budget in Amrit \\nKaal. \\nIntroduction \\n1. This Budget hopes to build on the foundation laid in the previous \\nBudget, and the blueprint drawn for India@100. We envision a prosperous \\nand inclusive India, in which the fruits of development reach all regions and \\ncitizens, especially our youth, women, farmers, OBCs, Scheduled Castes and \\nScheduled Tribes.  \\n2. In the 75 th year of our Independence, the world has recognised the \\nIndian economy as a ‘bright star’. Our current year’s economic growth is \\nestimated to be at 7 per cent. It is notable that this is the highest among all \\nthe major economies. This is in spite of the massive slowdown globally \\ncaused by Covid-19 and a war. The Indian economy is therefore on the right \\ntrack, and despite a time of challenges, heading towards a bright future.  \\n3. Today as Indians stands with their head held high, and the world \\nappreciates India’s achievements and successes, we are sure that elders \\nwho had fought for India’s independence, will with joy, bless us our \\nendeavors going forward. \\nResilience amidst multiple crises \\n4. Our focus on wide-ranging reforms and sound policies, implemented \\nthrough Sabka Prayas  resulting in Jan Bhagidari  and targeted support to \\nthose in need, helped us perform well in trying times. India’s rising global'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 5, 'page_label': '6', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='2 \\n \\n \\n \\nprofile is because of several accomplishments: unique world class digital \\npublic infrastructure, e.g., Aadhaar, Co-Win and UPI; Covid vaccination drive \\nin unparalleled scale and speed; proactive role in frontier areas such as \\nachieving the climate related goals, mission LiFE, and National Hydrogen \\nMission.  \\n5. During the Covid-19 pandemic, we ensured that no one goes to bed \\nhungry, with a scheme to supply free food grains to over 80 crore persons \\nfor 28 months. Continuing our commitment to ensure food and nutritional \\nsecurity, we are implementing, from 1 st January 2023, a scheme to supply \\nfree food grain to all Antyodaya and priority households for the next one \\nyear, under PM Garib Kalyan Anna Yojana (PMGKAY). The entire \\nexpenditure of about ` 2 lakh crore will be borne by the Central \\nGovernment. \\nG20 Presidency: Steering the global agenda through challenges \\n6. In these times of global challenges, the G20 Presidency gives us a \\nunique opportunity to strengthen India’s role in the world economic order. \\nWith the theme of ‘ Vasudhaiva Kutumbakam’ , we are steering an \\nambitious, people-centric agenda to address global challenges, and to \\nfacilitate sustainable economic development.  \\nAchievements since 2014: Leaving no one behind \\n7. The government’s efforts since 2014 have ensured for all citizens a \\nbetter quality of living and a life of dignity. The per capita income has more \\nthan doubled to ` 1.97 lakh.   \\n8. In these nine years, the Indian economy has increased in size from \\nbeing 10th to 5 th largest in the world. We have significantly improved our \\nposition as a well-governed and innovative country with a conducive \\nenvironment for business as reflected in several global indices. We have \\nmade significant progress in many Sustainable Development Goals.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 6, 'page_label': '7', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='3 \\n \\n \\n \\n9. The economy has become a lot more formalised as reflected in the \\nEPFO membership more than doubling to 27 crore, and 7,400 crore digital \\npayments of ` 126 lakh crore through UPI in 2022.    \\n10. The efficient implementation of many schemes, with \\nuniversalisation of targeted benefits, has resulted in inclusive development. \\nSome of the schemes are: \\ni. 11.7 crore household toilets under Swachh Bharat Mission,  \\nii. 9.6 crore LPG connections under Ujjawala,  \\niii. 220 crore Covid vaccination of 102 crore persons,    \\niv. 47.8 crore PM Jan Dhan bank accounts, \\nv. Insurance cover for 44.6 crore persons under PM Suraksha \\nBima and PM Jeevan Jyoti Yojana, and \\nvi. Cash transfer of ` 2.2 lakh crore to over 11.4 crore farmers \\nunder PM Kisan Samman Nidhi. \\nVision for Amrit Kaal – an empowered and inclusive economy \\n11. Our vision for the Amrit Kaal  includes technology-driven and \\nknowledge-based economy with strong public finances, and a robust \\nfinancial sector. To achieve this, Jan Bhagidari through Sabka Saath Sabka \\nPrayas is essential.   \\n12. The economic agenda for achieving this vision focuses on three \\nthings: first, facilitating ample opportunities for citizens, especially the \\nyouth, to fulfil their aspirations; second, providing strong impetus to growth \\nand job creation; and third, strengthening macro-economic stability.    \\n13. To service these focus areas in our journey to India@100, we believe \\nthat the following four opportunities can be transformative during Amrit \\nKaal.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 7, 'page_label': '8', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='4 \\n \\n \\n \\n1) Economic Empowerment of Women : Deendayal Antyodaya Yojana \\nNational Rural Livelihood Mission has achieved remarkable success \\nby mobilizing rural women into 81 lakh Self Help Groups. We will \\nenable these groups to reach the next stage of economic \\nempowerment through formation of large producer enterprises or \\ncollectives with each having several thousand members and \\nmanaged professionally. They will be helped with supply of raw \\nmaterials and for better design, quality, branding and marketing of \\ntheir products. Through supporting policies, they will be enabled to \\nscale up their operations to serve the large consumer markets, as \\nhas been the case with several start-ups growing into ‘Unicorns’. \\n2) PM VIshwakarma KAushal Samman (PM VIKAS) : For centuries, \\ntraditional artisans and craftspeople, who work with their hands \\nusing tools, have brought renown for India. They are generally \\nreferred to as Vishwakarma. The art and handicraft created by them \\nrepresents the true spirit of Atmanirbhar Bharat. For the first time, a \\npackage of assistance for them has been conceptualized. The new \\nscheme will enable them to improve the quality, scale and reach of \\ntheir products, integrating them with the MSME value chain. The \\ncomponents of the scheme will include not only financial support \\nbut also access to advanced skill training, knowledge of modern \\ndigital techniques and efficient green technologies, brand \\npromotion, linkage with local and global markets, digital payments, \\nand social security. This will greatly benefit the Scheduled Castes, \\nScheduled Tribes, OBCs, women and people belonging to the weaker \\nsections.  \\n3) Tourism: The country offers immense attraction for domestic as well \\nas foreign tourists. There is a large potential to be tapped in tourism. \\nThe sector holds huge opportunities for jobs and entrepreneurship \\nfor youth in particular.  Promotion of tourism will be taken up on \\nmission mode, with active participation of states, convergence of \\ngovernment programmes and public-private partnerships.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 8, 'page_label': '9', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='5 \\n \\n \\n \\n4) Green Growth: We are implementing many programmes for green \\nfuel, green energy, green farming, green mobility, green buildings, \\nand green equipment, and policies for efficient use of energy across \\nvarious economic sectors. These green growth efforts help in \\nreducing carbon intensity of the economy and provides for large-\\nscale green job opportunities.  \\nPriorities of this Budget \\n14. The Budget adopts the following seven priorities. They complement \\neach other and act as the ‘Saptarishi’ guiding us through the Amrit Kaal. \\n1) Inclusive Development  \\n2) Reaching the Last Mile \\n3) Infrastructure and Investment \\n4) Unleashing the Potential \\n5) Green Growth \\n6) Youth Power  \\n7) Financial Sector \\nPriority 1: Inclusive Development  \\n15. The Government’s philosophy of Sabka Saath Sabka Vikas  has \\nfacilitated inclusive development covering in specific, farmers, women, \\nyouth, OBCs, Scheduled Castes, Scheduled Tribes, divyangjan and \\neconomically weaker sections, and overall priority for the underprivileged \\n(vanchiton ko variyata). There has also been a sustained focus on Jammu & \\nKashmir, Ladakh and the North-East. This Budget builds on those efforts.  \\nAgriculture and Cooperation   \\nDigital Public Infrastructure for Agriculture \\n16. Digital public infrastructure for agriculture will be built as an open \\nsource, open standard and inter operable public good. This will enable'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 9, 'page_label': '10', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content=\"6 \\n \\n \\n \\ninclusive, farmer-centric solutions through relevant information services for \\ncrop planning and health, improved access to farm inputs, credit, and \\ninsurance, help for crop estimation, market intelligence, and support for \\ngrowth of agri-tech industry and start-ups.  \\nAgriculture Accelerator Fund \\n17. An Agriculture Accelerator Fund will be set-up to encourage agri-\\nstartups by young entrepreneurs in rural areas. The Fund will aim at \\nbringing innovative and affordable solutions for challenges faced by \\nfarmers. It will also bring in modern technologies to transform agricultural \\npractices, increase productivity and profitability. \\nEnhancing productivity of cotton crop  \\n18. To enhance the productivity of extra-long staple cotton, we will \\nadopt a cluster-based and value chain approach through Public Private \\nPartnerships (PPP). This will mean collaboration between farmers, state and \\nindustry for input supplies, extension services, and market linkages. \\nAtmanirbhar Horticulture Clean Plant Program  \\n19. We will launch an Atmanirbhar Clean Plant Program to boost \\navailability of disease-free, quality planting material for high value \\nhorticultural crops at an outlay of ` 2,200 crore. \\nGlobal Hub for Millets: ‘Shree Anna’ \\n20. “India is at the forefront of popularizing Millets, whose consumption \\nfurthers nutrition, food security and welfare of farmers,” said Hon’ble Prime \\nMinister. \\n21.  We are the largest producer and second largest exporter of ‘Shree \\nAnna’ in the world. We grow several types of ' Shree Anna'  such as  jowar, \\nragi, bajra, kuttu, ramdana, kangni, kutki, kodo, cheena, and sama. These \\nhave a number of health benefits, and have been an integral part of our \\nfood for centuries. I acknowledge with pride the huge service done by small\"),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 10, 'page_label': '11', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content=\"7 \\n \\n \\n \\nfarmers in contributing to the health of fellow citizens by growing these \\n‘Shree Anna’.  \\n22. Now to make India a global hub for ' Shree Anna', the Indian Institute \\nof Millet Research, Hyderabad will be supported as the Centre of Excellence \\nfor sharing best practices, research and technologies at the international \\nlevel.    \\nAgriculture Credit  \\n23. The agriculture credit target will be increased  \\nto ` 20 lakh crore with focus on animal husbandry, dairy and fisheries.  \\nFisheries \\n24. We will launch a new sub-scheme of PM Matsya Sampada Yojana \\nwith targeted investment of ` 6,000 crore to further enable activities of \\nfishermen, fish vendors, and micro & small enterprises, improve value chain \\nefficiencies, and expand the market. \\nCooperation \\n25. For farmers, especially small and marginal farmers, and other \\nmarginalised sections, the government is promoting cooperative-based \\neconomic development model. A new Ministry of Cooperation was formed \\nwith a mandate to realise the vision of ‘Sahakar Se Samriddhi’. To realise \\nthis vision, the government has already initiated computerisation of 63,000 \\nPrimary Agricultural Credit Societies (PACS) with an investment of ` 2,516 \\ncrore. In consultation with all stakeholders and states, model bye-laws for \\nPACS were formulated enabling them to become multipurpose PACS. A \\nnational cooperative database is being prepared for country-wide mapping \\nof cooperative societies.  \\n26. With this backdrop, we will implement a plan to set up massive \\ndecentralised storage capacity. This will help farmers store their produce \\nand realize remunerative prices through sale at appropriate times. The \\ngovernment will also facilitate setting up of a large number of multipurpose\"),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 11, 'page_label': '12', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='8 \\n \\n \\n \\ncooperative societies, primary fishery societies and dairy cooperative \\nsocieties in uncovered panchayats and villages in the next 5 years.  \\nHealth, Education and Skilling \\nNursing Colleges   \\n27\\n. One hundred and fifty-seven new nursing colleges will be \\nestablished in co-location with the existing 157 medical colleges established \\nsince 2014. \\nSickle Cell Anaemia Elimination Mission \\n28. A Mission to eliminate Sickle Cell Anaemia by 2047 will be launched. \\nIt will entail awareness creation, universal screening of 7 crore people in the \\nage group of 0-40 years in affected tribal areas, and counselling through \\ncollaborative efforts of central ministries and state governments.  \\nMedical Research  \\n29. Facilities in select ICMR Labs will be made available for research by \\npublic and private medical college faculty and private sector R&D teams for \\nencouraging collaborative research and innovation. \\nPharma Innovation  \\n30. A new programme to promote research and innovation in \\npharmaceuticals will be taken up through centers of excellence. We shall \\nalso encourage industry to invest in research and development in specific \\npriority areas.  \\nMultidisciplinary courses for medical devices \\n31. Dedicated multidisciplinary courses for medical devices will be \\nsupported in existing institutions to ensure availability of skilled manpower \\nfor futuristic medical technologies, high-end manufacturing and research.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 12, 'page_label': '13', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='9 \\n \\n \\n \\nTeachers’ Training \\n32. Teachers’ training will be re-envisioned through innovative \\npedagogy, curriculum transaction, continuous professional development, \\ndipstick surveys, and ICT implementation. The District Institutes of \\nEducation and Training will be developed as vibrant institutes of excellence \\nfor this purpose.   \\nNational Digital Library for Children and Adolescents  \\n33. A National Digital Library for children and adolescents  will be set-up \\nfor facilitating availability of quality books across geographies, languages, \\ngenres and levels, and device agnostic accessibility. States will be \\nencouraged to set up physical libraries for them at panchayat and ward \\nlevels and provide infrastructure for accessing the National Digital Library \\nresources. \\n34. Additionally, to build a culture of reading, and to make up for \\npandemic-time learning loss, the National Book Trust, Children’s Book Trust \\nand other sources will be encouraged to provide and replenish non-\\ncurricular titles in regional languages and English to these physical libraries. \\nCollaboration with NGOs that work in literacy will also be a part of this \\ninitiative. To inculcate financial literacy, financial sector regulators and \\norganizations will be encouraged to provide age-appropriate reading \\nmaterial to these libraries.  \\nPriority 2: Reaching the Last Mile \\n35. Prime Minister Vajpayee’s government had formed the Ministry of \\nTribal Affairs and the Department of Development of North-Eastern Region. \\nTo provide a sharper focus to the objective of ‘reaching the last mile’, our \\ngovernment has formed the ministries of AYUSH, Fisheries, Animal \\nHusbandry and Dairying, Skill Development, Jal Shakti and Cooperation.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 13, 'page_label': '14', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='10 \\n \\n \\n \\nAspirational Districts and Blocks Programme \\n36. Building on the success of the Aspirational Districts Programme, the \\nGovernment has recently launched the Aspirational Blocks Programme \\ncovering 500 blocks for saturation of essential government services across \\nmultiple domains such as health, nutrition, education, agriculture, water \\nresources, financial inclusion, skill development, and basic infrastructure. \\nPradhan Mantri PVTG Development Mission \\n37. To improve socio-economic conditions of the particularly vulnerable \\ntribal groups (PVTGs), Pradhan Mantri PVTG Development Mission will be \\nlaunched. This will saturate PVTG families and habitations with basic \\nfacilities such as safe housing, clean drinking water and sanitation, \\nimproved access to education, health and nutrition, road and telecom \\nconnectivity, and sustainable livelihood opportunities. An amount  \\nof ` 15,000 crore will be made available to implement the Mission in the \\nnext three years under the Development Action Plan for the Scheduled \\nTribes.  \\nEklavya Model Residential Schools \\n38. In the next three years, centre will recruit 38,800 teachers and \\nsupport staff for the 740 Eklavya Model Residential Schools, serving 3.5 lakh \\ntribal students. \\nWater for Drought Prone Region \\n39. In the drought prone central region of Karnataka, central assistance \\nof ` 5,300 crore will be given to Upper Bhadra Project to provide \\nsustainable micro irrigation and filling up of surface tanks for drinking \\nwater.  \\nPM Awas Yojana \\n40. The outlay for PM Awas Yojana is being enhanced \\n by 66 per cent to over ` 79,000 crore.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 14, 'page_label': '15', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='11 \\n \\n \\n \\nBharat Shared Repository of Inscriptions (Bharat SHRI) \\n41. ‘Bharat Shared Repository of Inscriptions’ will be set up in a digital \\nepigraphy museum, with digitization of one lakh ancient inscriptions in the \\nfirst stage.   \\nSupport for poor prisoners \\n42. For poor persons who are in prisons and unable to afford the \\npenalty or the bail amount, required financial support will be provided.   \\n \\nPriority 3: Infrastructure & Investment \\n43. Investments in Infrastructure and productive capacity have a large \\nmultiplier impact on growth and employment. After the subdued period of \\nthe pandemic, private investments are growing again. The Budget takes the \\nlead once again to ramp up the virtuous cycle of investment and job \\ncreation.    \\n \\nCapital Investment as driver of growth and jobs \\n44. Capital investment outlay is being increased steeply for the third \\nyear in a row by 33 per cent to ` 10 lakh crore, which would be 3.3 per cent \\nof GDP. This will be almost three times the outlay in 2019-20.   \\n45. This substantial increase in recent years is central to the \\ngovernment’s efforts to enhance growth potential and job creation, crowd-\\nin private investments, and provide a cushion against global headwinds. \\nEffective Capital Expenditure  \\n46. The direct capital investment by the Centre is complemented by the \\nprovision made for creation of capital assets through Grants-in-Aid to \\nStates. The ‘Effective Capital Expenditure’ of the Centre is budgeted at  \\n` 13.7 lakh crore, which will be 4.5 per cent of GDP.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 15, 'page_label': '16', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='12 \\n \\n \\n \\nSupport to State Governments for Capital Investment \\n47. I have decided to continue the 50-year interest free loan to state \\ngovernments for one more year to spur investment in infrastructure and to \\nincentivize them for complementary policy actions, with a significantly \\nenhanced outlay of ` 1.3 lakh crore.   \\nEnhancing opportunities for private investment in Infrastructure \\n48. The newly established Infrastructure Finance Secretariat will assist \\nall stakeholders for more private investment in infrastructure, including \\nrailways, roads, urban infrastructure and power, which are predominantly \\ndependent on public resources.  \\nHarmonized Master List of Infrastructure \\n49. The Harmonized Master List of Infrastructure will be reviewed by an \\nexpert committee for recommending the classification and financing \\nframework suitable for Amrit Kaal. \\nRailways \\n50. A capital outlay of ` 2.40 lakh crore has been provided for the \\nRailways. This highest ever outlay is about 9 times the outlay made in 2013-\\n14.  \\nLogistics \\n51. One hundred critical transport infrastructure projects, for last and \\nfirst mile connectivity for ports, coal, steel, fertilizer, and food grains sectors \\nhave been identified. They will be taken up on priority with investment of \\n` 75,000 crore, including ` 15,000 crore from private sources. \\nRegional Connectivity \\n52. Fifty additional airports, heliports, water aerodromes and advance \\nlanding grounds will be revived for improving regional air connectivity.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 16, 'page_label': '17', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='13 \\n \\n \\n \\nSustainable Cities of Tomorrow \\n53. States and cities will be encouraged to undertake urban planning \\nreforms and actions to transform our cities into ‘sustainable cities of \\ntomorrow’. This means efficient use of land resources, adequate resources \\nfor urban infrastructure, transit-oriented development, enhanced \\navailability and affordability of urban land, and opportunities for all.  \\nMaking Cities ready for Municipal Bonds \\n54. Through property tax governance reforms and ring-fencing user \\ncharges on urban infrastructure, cities will be incentivized to improve their \\ncredit worthiness for municipal bonds.   \\nUrban Infrastructure Development Fund  \\n55. Like the RIDF, an Urban Infrastructure Development Fund (UIDF) will \\nbe established through use of priority sector lending shortfall. This will be \\nmanaged by the National Housing Bank, and will be used by public agencies \\nto create urban infrastructure in Tier 2 and Tier 3 cities. States will be \\nencouraged to leverage resources from the grants of the 15 th Finance \\nCommission, as well as existing schemes, to adopt appropriate user charges \\nwhile accessing the UIDF. We expect to make  \\navailable ` 10,000 crore per annum for this purpose. \\nUrban Sanitation \\n56. All cities and towns will be enabled for 100 per cent mechanical \\ndesludging of septic tanks and sewers to transition from manhole to \\nmachine-hole mode. Enhanced focus will be provided for scientific \\nmanagement of dry and wet waste. \\nPriority 4: Unleashing the Potential \\n57. “Good Governance is the key to a nation’s progress. Our government \\nis committed to providing a transparent and accountable administration \\nwhich works for the betterment and welfare of the common citizen,”  said \\nHon’ble Prime Minister.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 17, 'page_label': '18', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='14 \\n \\n \\n \\nMission Karmayogi \\n58. Under Mission Karmayogi, Centre, States and Union Territories are \\nmaking and implementing capacity-building plans for civil servants. The \\ngovernment has also launched an integrated online training platform, iGOT \\nKarmayogi, to provide continuous learning opportunities for lakhs of \\ngovernment employees to upgrade their skills and facilitate people-centric \\napproach.   \\n59. For enhancing ease of doing business, more than  \\n39,000 compliances have been reduced and more than  \\n3,400 legal provisions have been decriminalized. For furthering the trust-\\nbased governance, we have introduced the Jan Vishwas Bill to amend 42 \\nCentral Acts. This Budget proposes a series of measures to unleash the \\npotential of our economy.  \\nCentres of Excellence for Artificial Intelligence \\n60. For realizing the vision of “Make AI in India and Make AI work for \\nIndia”, three centres of excellence for Artificial Intelligence will be set-up in \\ntop educational institutions. Leading industry players will partner in \\nconducting interdisciplinary research, develop cutting-edge applications and \\nscalable problem solutions in the areas of agriculture, health, and \\nsustainable cities. This will galvanize an effective AI ecosystem and nurture \\nquality human resources in the field. \\nNational Data Governance Policy \\n61. To unleash innovation and research by start-ups and academia, a \\nNational Data Governance Policy will be brought out. This will enable access \\nto anonymized data. \\nSimplification of Know Your Customer (KYC) process  \\n62. The KYC process will be simplified adopting a ‘risk-based’ instead of \\n‘one size fits all’ approach. The financial sector regulators will also be'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 18, 'page_label': '19', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='15 \\n \\n \\n \\nencouraged to have a KYC system fully amenable to meet the needs of \\nDigital India. \\nOne stop solution for identity and address updating \\n63. A one stop solution for reconciliation and updating of identity and \\naddress of individuals maintained by various government agencies, \\nregulators and regulated entities will be established using DigiLocker service \\nand Aadhaar as foundational identity.    \\nCommon Business Identifier   \\n64. For the business establishments required to have a Permanent \\nAccount Number (PAN), the PAN will be used as the common identifier for \\nall digital systems of specified government agencies. This will bring ease of \\ndoing business; and it will be facilitated through a legal mandate. \\nUnified Filing Process \\n65. For obviating the need for separate submission of same information \\nto different government agencies, a system of ‘Unified Filing Process’ will be \\nset-up. Such filing of information or return in simplified forms on a common \\nportal, will be shared with other agencies as per filer’s choice.  \\nVivad se Vishwas I – Relief for MSMEs  \\n66. In cases of failure by MSMEs to execute contracts during the Covid \\nperiod, 95 per cent of the forfeited amount relating to bid or performance \\nsecurity, will be returned to them by government and government \\nundertakings.  This will provide relief to MSMEs.  \\nVivad se Vishwas II – Settling Contractual Disputes  \\n67. To settle contractual disputes of government and government \\nundertakings, wherein arbitral award is under challenge in a court, a \\nvoluntary settlement scheme with standardized terms will be introduced. \\nThis will be done by offering graded settlement terms depending on \\npendency level of the dispute.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 19, 'page_label': '20', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='16 \\n \\n \\n \\nState Support Mission  \\n68. The State Support Mission of NITI Aayog will be continued for three \\nyears for our collective efforts towards national priorities. \\nResult Based Financing  \\n69. To better allocate scarce resources for competing development \\nneeds, the financing of select schemes will be changed, on a pilot basis, \\nfrom ‘input-based’ to ‘result-based’. \\nE-Courts  \\n70. For efficient administration of justice, Phase-3 of the \\n E-Courts project will be launched with an outlay  \\nof ` 7,000 crore.  \\nFintech Services  \\n71. Fintech services in India have been facilitated by our digital public \\ninfrastructure including Aadhaar, PM Jan Dhan Yojana, Video KYC, India \\nStack and UPI. To enable more Fintech innovative services, the scope of \\ndocuments available in DigiLocker for individuals will be expanded.  \\nEntity DigiLocker  \\n72. An Entity DigiLocker will be set up for use by MSMEs, large business \\nand charitable trusts. This will be towards storing and sharing documents \\nonline securely, whenever needed, with various authorities, regulators, \\nbanks and other business entities.  \\n5G Services \\n73. One hundred labs for developing applications using  \\n5G services will be set up in engineering institutions to realise a new range \\nof opportunities, business models, and employment potential. The  labs will \\ncover, among others, applications such as smart classrooms, precision \\nfarming, intelligent transport systems, and health care applications.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 20, 'page_label': '21', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='17 \\n \\n \\n \\nLab Grown Diamonds \\n74. Lab Grown Diamonds (LGD) is a technology-and innovation-driven \\nemerging sector with high employment potential. These environment-\\nfriendly diamonds which have optically and chemically the same properties \\nas natural diamonds. To encourage indigenous production of LGD seeds and \\nmachines and to reduce import dependency, a research and development \\ngrant will be provided to one of the IITs for five years.   \\n75. To reduce the cost of production, a proposal to review the custom \\nduty rate on LGD seeds will be indicated in Part B of the speech.   \\nPriority 5: Green Growth \\n76. Hon’ble Prime Minister has given a vision for “LiFE”, or Lifestyle for \\nEnvironment, to spur a movement of environmentally conscious lifestyle. \\nIndia is moving forward firmly for the ‘panchamrit’ and net-zero carbon \\nemission by 2070 to usher in green industrial and economic transition. This \\nBudget builds on our focus on green growth.    \\nGreen Hydrogen Mission \\n77. The recently launched National Green Hydrogen Mission, with an \\noutlay of ` 19,700 crores, will facilitate transition of the economy to low \\ncarbon intensity, reduce dependence on fossil fuel imports, and make the \\ncountry assume technology and market leadership in this sunrise sector. \\nOur target is to reach an annual production of 5 MMT by 2030.  \\nEnergy Transition \\n78. This Budget provides ` 35,000 crore for priority capital investments \\ntowards energy transition and net zero objectives, and energy security by \\nMinistry of Petroleum & Natural Gas.  \\nEnergy Storage Projects \\n79. To steer the economy on the sustainable development path, Battery \\nEnergy Storage Systems with capacity of 4,000 MWH will be supported with'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 21, 'page_label': '22', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='18 \\n \\n \\n \\nViability Gap Funding. A detailed framework for Pumped Storage Projects \\nwill also be formulated.  \\nRenewable Energy Evacuation \\n80. The Inter-state transmission system for evacuation and grid \\nintegration of 13 GW renewable energy from Ladakh will be constructed \\nwith investment of ` 20,700 crore including central support of ` 8,300 crore. \\nGreen Credit Programme \\n81. For encouraging behavioural change, a Green Credit Programme will \\nbe notified under the Environment (Protection) Act. This will incentivize \\nenvironmentally sustainable and responsive actions by companies, \\nindividuals and local bodies, and help mobilize additional resources for such \\nactivities.  \\nPM-PRANAM \\n82. “PM Programme for Restoration, Awareness, Nourishment and \\nAmelioration of Mother Earth” will be launched to incentivize States and \\nUnion Territories to promote alternative fertilizers and balanced use of \\nchemical fertilizers. \\nGOBARdhan scheme \\n83. 500 new ‘waste to wealth’ plants under GOBARdhan (Galvanizing \\nOrganic Bio-Agro Resources Dhan) scheme will be established for promoting \\ncircular economy. These will include 200 compressed biogas (CBG) plants, \\nincluding 75 plants in urban areas, and 300 community or cluster-based \\nplants at total investment of ` 10,000 crore. I will refer to this in Part B. In \\ndue course, a 5 per cent CBG mandate will be introduced for all \\norganizations marketing natural and bio gas. For collection of bio-mass and \\ndistribution of bio-manure, appropriate fiscal support will be provided.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 22, 'page_label': '23', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='19 \\n \\n \\n \\nBhartiya Prakritik Kheti Bio-Input Resource Centres   \\n84. Over the next 3 years, we will facilitate 1 crore farmers to adopt \\nnatural farming. For this, 10,000 Bio-Input Resource Centres will be set-up, \\ncreating a national-level distributed micro-fertilizer and pesticide \\nmanufacturing network.  \\nMISHTI \\n85. Building on India’s success in afforestation, ‘Mangrove Initiative for \\nShoreline Habitats & Tangible Incomes’, MISHTI, will be taken up for \\nmangrove plantation along the coastline and on salt pan lands, wherever \\nfeasible, through convergence between MGNREGS, CAMPA Fund and other \\nsources. \\nAmrit Dharohar \\n86. Wetlands are vital ecosystems which sustain biological diversity. In \\nhis latest Mann Ki Baat, the Prime Minister said, “Now the total number of \\nRamsar sites in our country has increased to 75. Whereas, before 2014, \\nthere were only 26…”  Local communities have always been at the forefront \\nof conservation efforts. The government will promote their unique \\nconservation values through Amrit Dharohar , a scheme that will be \\nimplemented over the next three years to encourage optimal use of \\nwetlands, and enhance bio-diversity, carbon stock,  \\neco-tourism opportunities and income generation for local communities.  \\nCoastal Shipping \\n87. Coastal shipping will be promoted as the energy efficient and lower \\ncost mode of transport, both for passengers and freight, through PPP mode \\nwith viability gap funding.   \\nVehicle Replacement \\n88. Replacing old polluting vehicles is an important part of greening our \\neconomy. In furtherance of the vehicle scrapping policy mentioned in \\nBudget 2021-22, I have allocated adequate funds to scrap old vehicles of'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 23, 'page_label': '24', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='20 \\n \\n \\n \\nthe Central Government. States will also be supported in replacing old \\nvehicles and ambulances.  \\nPriority 6: Youth Power  \\n89. To empower our youth and help the ‘ Amrit Peedhi ’ realize their \\ndreams, we have formulated the National Education Policy, focused on \\nskilling, adopted economic policies that facilitate job creation at scale, and \\nhave supported business opportunities.   \\nPradhan Mantri Kaushal Vikas Yojana 4.0 \\n90. Pradhan Mantri Kaushal Vikas Yojana 4.0 will be launched to skill \\nlakhs of youth within the next three years.  On-job training, industry \\npartnership, and alignment of courses with needs of industry will be \\nemphasized. The scheme will also cover new age courses for Industry 4.0 \\nlike coding, AI, robotics, mechatronics, IOT, 3D printing, drones, and soft \\nskills. To skill youth for international opportunities, 30 Skill India \\nInternational Centres will be set up across different States.  \\n \\nSkill India Digital Platform \\n91. The digital ecosystem for skilling will be further expanded with the \\nlaunch of a unified Skill India Digital platform for: \\n\\uf0b7 enabling demand-based formal skilling,  \\n\\uf0b7 linking with employers including MSMEs, and \\n\\uf0b7 facilitating access to entrepreneurship schemes.  \\nNational Apprenticeship Promotion Scheme \\n92. To provide stipend support to 47 lakh youth in three years, Direct \\nBenefit Transfer under a pan-India National Apprenticeship Promotion \\nScheme will be rolled out.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 24, 'page_label': '25', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='21 \\n \\n \\n \\nTourism \\n93. With an integrated and innovative approach, at  \\nleast 50 destinations will be selected through challenge mode. In addition to \\naspects such as physical connectivity, virtual connectivity, tourist guides, \\nhigh standards for food streets and tourists’ security, all the relevant \\naspects would be made available on an App to enhance tourist experience. \\nEvery destination would be developed as a complete package. The focus of \\ndevelopment of tourism would be on domestic as well as foreign tourists.  \\n94. Sector specific skilling and entrepreneurship development will be \\ndovetailed to achieve the objectives of the ‘Dekho Apna Desh’ initiative. \\nThis was launched as an appeal by the Prime Minister to the middle class to \\nprefer domestic tourism over international tourism. For integrated \\ndevelopment of theme-based tourist circuits, the ‘Swadesh Darshan \\nScheme’ was also launched. Under the Vibrant Villages Programme, tourism \\ninfrastructure and amenities will also be facilitated in border villages.  \\nUnity Mall \\n95. States will be encouraged to set up a Unity Mall in their state capital \\nor most prominent tourism centre or the financial capital for promotion and \\nsale of their own ODOPs (one district, one product), GI products and other \\nhandicraft products, and for providing space for such products of all other \\nStates.   \\nPriority 7: Financial Sector \\n96. Our reforms in the financial sector and innovative use of technology \\nhave led to financial inclusion at scale, better and faster service delivery, \\nease of access to credit and participation in financial markets. This Budget \\nproposes to further these measures.    \\nCredit Guarantee for MSMEs \\n97. Last year, I proposed revamping of the credit guarantee scheme for \\nMSMEs. I am happy to announce that the revamped scheme will take effect'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 25, 'page_label': '26', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='22 \\n \\n \\n \\nfrom 1st April 2023 through infusion of ` 9,000 crore in the corpus. This will \\nenable additional collateral-free guaranteed credit of ` 2 lakh crore. \\nFurther, the cost of the credit will be reduced by about 1 per cent.     \\nNational Financial Information Registry \\n98. A national financial information registry will be set up to serve as the \\ncentral repository of financial and ancillary information. This will facilitate \\nefficient flow of credit, promote financial inclusion, and foster financial \\nstability. A new legislative framework will govern this credit public \\ninfrastructure, and it will be designed in consultation with the RBI. \\nFinancial Sector Regulations  \\n99. To meet the needs of Amrit Kaal  and to facilitate optimum \\nregulation in the financial sector, public consultation, as necessary and \\nfeasible, will be brought to the process of regulation-making and issuing \\nsubsidiary directions. \\n100. To simplify, ease and reduce cost of compliance, financial sector \\nregulators will be requested to carry out a comprehensive review of existing \\nregulations. For this, they will consider suggestions from public and \\nregulated entities. Time limits to decide the applications under various \\nregulations will also be laid down. \\nGIFT IFSC  \\n101. To enhance business activities in GIFT IFSC, the following measures \\nwill be taken: \\n\\uf0b7 Delegating powers under the SEZ Act to IFSCA to avoid dual \\nregulation, \\n\\uf0b7 Setting up a single window IT system for registration and \\napproval from IFSCA, SEZ authorities, GSTN, RBI, SEBI and \\nIRDAI,'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 26, 'page_label': '27', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='23 \\n \\n \\n \\n\\uf0b7 Permitting acquisition financing by IFSC Banking Units of \\nforeign banks,  \\n\\uf0b7 Establishing a subsidiary of EXIM Bank for trade  \\nre-financing, \\n\\uf0b7 Amending IFSCA Act for statutory provisions for arbitration, \\nancillary services, and avoiding dual regulation under SEZ Act, \\nand \\n\\uf0b7 Recognizing offshore derivative instruments as valid contracts.  \\n \\nData Embassy \\n102. For countries looking for digital continuity solutions, we will \\nfacilitate setting up of their Data Embassies in GIFT IFSC.  \\nImproving Governance and Investor Protection in Banking Sector \\n103. To improve bank governance and enhance investors’ protection, \\ncertain amendments to the Banking Regulation Act, the Banking Companies \\nAct and the Reserve Bank of India Act are proposed. \\n \\nCa\\npacity Building in Securities Market \\n104. To build capacity of functionaries and professionals in the securities \\nmarket, SEBI will be empowered to develop, regulate, maintain and enforce \\nnorms and standards for education in the National Institute of Securities \\nMarkets and to recognize award of degrees, diplomas and certificates.  \\nCentral Data Processing Centre  \\n105. A Central Processing Centre will be setup for faster response to \\ncompanies through centralized handling of various forms filed with field \\noffices under the Companies Act.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 27, 'page_label': '28', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='24 \\n \\n \\n \\nReclaiming of shares and dividends  \\n106. For investors to reclaim unclaimed shares and unpaid dividends \\nfrom the Investor Education and Protection Fund Authority with ease, an \\nintegrated IT portal will be established. \\nDigital Payments  \\n107. Digital payments continue to find wide acceptance. In 2022, they \\nshow increase of 76 per cent in transactions  \\nand 91 per cent in value. Fiscal support for this digital public infrastructure \\nwill continue in 2023-24.  \\nAzadi Ka Amrit Mahotsav Mahila Samman Bachat Patra  \\n108. For commemorating Azadi Ka Amrit Mahotsav, a one-time new small \\nsavings scheme, Mahila Samman Savings Certificate, will be made available \\nfor a two-year period up to March 2025. This will offer deposit facility upto \\n` 2 lakh in the name of women or girls for a tenor of 2 years at fixed \\ninterest rate of 7.5 per cent with partial withdrawal option.  \\nSenior Citizens \\n109. The maximum deposit limit for Senior Citizen Savings Scheme will be \\nenhanced from ` 15 lakh to ` 30 lakh. \\n110.  The maximum deposit limit for Monthly Income Account Scheme \\nwill be enhanced from ` 4.5 lakh to ` 9 lakh for single account and from ` 9 \\nlakh to ` 15 lakh for joint account. \\nFiscal Management \\nFifty-year interest free loan to States \\n111. The entire fifty-year loan to states has to be spent on capital \\nexpenditure within 2023-24. Most of this will be at the discretion of states, \\nbut a part will be conditional on states increasing their actual capital'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 28, 'page_label': '29', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='25 \\n \\n \\n \\nexpenditure. Parts of the outlay will also be linked to, or allocated for, the \\nfollowing purposes: \\n\\uf0b7 Scrapping old government vehicles, \\n\\uf0b7 Urban planning reforms and actions, \\n\\uf0b7 Financing reforms in urban local bodies to make them \\ncreditworthy for municipal bonds, \\n\\uf0b7 Housing for police personnel above or as part of police stations,  \\n\\uf0b7 Constructing Unity Malls, \\n\\uf0b7 Children and adolescents’ libraries and digital infrastructure, \\nand \\n\\uf0b7 State share of capital expenditure of central schemes. \\nFiscal Deficit of States \\n112. States will be allowed a fiscal deficit of 3.5 per cent of GSDP of which \\n0.5 per cent will be tied to power sector reforms.  \\nRevised Estimates 2022-23 \\n113. The Revised Estimate of the total receipts other than borrowings is  \\n` 24.3 lakh crore, of which the net tax receipts  \\nare ` 20.9 lakh crore. The Revised Estimate of the total expenditure is  \\n` 41.9 lakh crore, of which the capital expenditure is about ` 7.3 lakh crore.      \\n114. The Revised Estimate of the fiscal deficit is 6.4 per cent of GDP, \\nadhering to the Budget Estimate.        \\nBudget Estimates 2023-24 \\n115. Coming to 2023-24, the total receipts other than borrowings and the \\ntotal expenditure are estimated at ` 27.2 lakh crore and ` 45 lakh crore \\nrespectively. The net tax receipts are estimated at ` 23.3 lakh crore.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 29, 'page_label': '30', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='26 \\n \\n \\n \\n116. The fiscal deficit is estimated to be 5.9 per cent of GDP. In my \\nBudget Speech for 2021-22, I had announced that we plan to continue the \\npath of fiscal consolidation, reaching a fiscal deficit below 4.5 per cent by \\n2025-26 with a fairly steady decline over the period. We have adhered to \\nthis path, and I reiterate my intention to bring the fiscal deficit below 4.5 \\nper cent of GDP by 2025-26.  \\n117.  To finance the fiscal deficit in 2023-24, the net market borrowings \\nfrom dated securities are estimated at ` 11.8 lakh crore. The balance \\nfinancing is expected to come from small savings and other sources. The \\ngross market borrowings are estimated at ` 15.4 lakh crore. \\nI will, now, move to Part B.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 30, 'page_label': '31', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='27 \\n \\n \\n \\nPART B \\nIndirect Taxes \\n118. My indirect tax proposals aim to promote exports, boost domestic \\nmanufacturing, enhance domestic value addition, encourage green energy \\nand mobility.  \\n119. A simplified tax structure with fewer tax rates helps in reducing \\ncompliance burden and improving tax administration. I propose to reduce \\nthe number of basic customs duty rates on goods, other than textiles and \\nagriculture, from 21 to 13. As a result, there are minor changes in the basic \\ncustom duties, cesses and surcharges on some items including toys, \\nbicycles, automobiles and naphtha. \\n \\nGreen Mobility \\n120. To avoid cascading of taxes on blended compressed natural gas, I \\npropose to exempt excise duty on GST-paid compressed bio gas contained \\nin it. To further provide impetus to green mobility, customs duty exemption \\nis being extended to import of capital goods and machinery required for \\nmanufacture of lithium-ion cells for batteries used in electric vehicles. \\nElectronics  \\n121. As a result of various initiatives of the Government, including the \\nPhased Manufacturing programme, mobile phone production in India has \\nincreased from 5.8 crore units valued at about ` 18,900 crore in 2014-15 to \\n31 crore units valued at over ` 2,75,000 crore in the last financial year. To \\nfurther deepen domestic value addition in manufacture of mobile phones, I \\npropose to provide relief in customs duty on import of certain parts and \\ninputs like camera lens and continue the concessional duty on lithium-ion \\ncells for batteries for another year.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 31, 'page_label': '32', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='28 \\n \\n \\n \\n122. Similarly, to promote value addition in manufacture of televisions, I \\npropose to reduce the basic customs duty on parts of open cells of TV \\npanels to 2.5 per cent.  \\nElectrical  \\n123. To rectify inversion of duty structure and encourage manufacturing \\nof electric kitchen chimneys, the basic customs duty on electric kitchen \\nchimney is being increased from 7.5 per cent to 15 per cent and that on \\nheat coils for these is proposed to be reduced from 20 per cent to 15 per \\ncent. \\nChemicals and Petrochemicals  \\n124. Denatured ethyl alcohol is used in chemical industry. \\n I propose to exempt basic customs duty on it. This will also support the \\nEthanol Blending Programme and facilitate our endeavour for energy \\ntransition. Basic customs duty is also being reduced on acid grade fluorspar \\nfrom 5 per cent to 2.5 per cent to make the domestic fluorochemicals \\nindustry competitive. Further, the basic customs duty on crude glycerin for \\nuse in manufacture of epicholorhydrin is proposed to be reduced from 7.5 \\nper cent to 2.5 per cent. \\nMarine products \\n125. In the last financial year, marine products recorded the highest \\nexport growth benefitting farmers in the coastal states of the country. To \\nfurther enhance the export competitiveness of marine products, \\nparticularly shrimps, duty is being reduced on key inputs for domestic \\nmanufacture of shrimp feed. \\nLab Grown Diamonds \\n126. India is a global leader in cutting and polishing of natural diamonds, \\ncontributing about three-fourths of the global turnover by value. With the \\ndepletion in deposits of natural diamonds, the industry is moving towards \\nLab Grown Diamonds (LGDs) and it holds huge promise. To seize this'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 32, 'page_label': '33', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='29 \\n \\n \\n \\nopportunity, I propose to reduce basic customs duty on seeds used in their \\nmanufacture.  \\n \\nPrecious Metals \\n127. Customs Duties on dore and bars of gold and platinum were \\nincreased earlier this fiscal. I now propose to increase the duties on articles \\nmade therefrom to enhance the duty differential. I also propose to increase \\nthe import duty on silver dore, bars and articles to align them with that on \\ngold and platinum. \\nMetals \\n128. To facilitate availability of raw materials for the steel sector, \\nexemption from Basic Customs Duty on raw materials for manufacture of \\nCRGO Steel, ferrous scrap and nickel cathode is being continued. \\n129. Similarly, the concessional BCD of 2.5 per cent on copper scrap is \\nalso being continued to ensure the availability of raw materials for \\nsecondary copper producers who are mainly in the MSME sector. \\nCompounded Rubber \\n130. The basic customs duty rate on compounded rubber is being \\nincreased from 10 per cent to ‘25 per cent or ` 30/kg whichever is lower’, at \\npar with that on natural rubber other than latex, to curb circumvention of \\nduty.  \\nCigarettes \\n131. National Calamity Contingent Duty (NCCD) on specified cigarettes \\nwas last revised three years ago. This is proposed to be revised upwards by \\nabout 16 per cent.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 33, 'page_label': '34', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='30 \\n \\n \\n \\n \\nDirect Taxes \\n132. I now come to my direct tax proposals. These proposals aim to \\nmaintain continuity and stability of taxation, further simplify and rationalise \\nvarious provisions to reduce the compliance burden, promote the \\nentrepreneurial spirit and provide tax relief to citizens. \\n133. It has been the constant endeavour of the Income Tax Department \\nto improve Tax Payers Services by making compliance easy and smooth. Our \\ntax payers’ portal received a maximum of 72 lakh returns in a day; \\nprocessed more than 6.5 crore returns this year; average processing period \\nreduced from 93 days in financial year 13-14 to 16 days now;  \\nand 45 per cent of the returns were processed within 24 hours. We intend \\nto further improve this, roll out a next-generation Common IT Return Form \\nfor tax payer convenience, and also plan to strengthen the grievance \\nredressal mechanism.  \\nMSMEs and Professionals  \\n134. MSMEs are growth engines of our economy.  Micro enterprises with \\nturnover up to ` 2 crore and certain professionals with turnover of up to  \\n` 50 lakh can avail the benefit of presumptive taxation. I propose to provide \\nenhanced limits of ` 3 crore and ` 75 lakh respectively, to the tax payers \\nwhose cash receipts are no more than 5 per cent. Moreover, to support \\nMSMEs in timely receipt of payments, I propose to allow deduction for \\nexpenditure incurred on payments made to them only when payment is \\nactually made.  \\nCooperation \\n135. Cooperation is a value to be cherished. In realizing our Prime \\nMinister’s goal of “Sahkar se Samriddhi ”, and his resolve to “connect the \\nspirit of cooperation with the spirit of Amrit Kaal”, in addition to the \\nmeasures proposed in Part A, I have a slew of proposals for the co-operative \\nsector.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 34, 'page_label': '35', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='31 \\n \\n \\n \\n136. First, new co-operatives that commence manufacturing activities till \\n31.3.2024 shall get the benefit of a lower tax rate of 15 per cent, as is \\npresently available to new manufacturing companies. \\n137. Secondly, I propose to provide an opportunity to sugar co-operatives \\nto claim payments made to sugarcane farmers for the period prior to \\nassessment year 2016-17 as expenditure. This is expected to provide them \\nwith a relief of almost ` 10,000 crore.  \\n138. Thirdly, I am providing a higher limit of ` 2 lakh per member for cash \\ndeposits to and loans in cash by Primary Agricultural Co-operative Societies \\n(PACS) and Primary Co-operative Agriculture and Rural Development Banks \\n(PCARDBs).  \\n139. Similarly, a higher limit of ` 3 crore for TDS on cash withdrawal is \\nbeing provided to co-operative societies. \\nStart-Ups \\n140. Entrepreneurship is vital for a country’s economic development. We \\nhave taken a number of measures for start-ups and they have borne results. \\nIndia is now the third largest ecosystem for start-ups globally, and ranks \\nsecond in innovation quality among middle-income countries. I propose to \\nextend the date of incorporation for income tax benefits to start-ups from \\n31.03.23 to 31.3.24. I further propose to provide the benefit of carry \\nforward of losses on change of shareholding of start-ups from seven years \\nof incorporation to ten years. \\nAppeals \\n141. To reduce the pendency of appeals at Commissioner level, I propose \\nto deploy about 100 Joint Commissioners for disposal of small appeals. We \\nshall also be more selective in taking up cases for scrutiny of returns already \\nreceived this year.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 35, 'page_label': '36', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='32 \\n \\n \\n \\nBetter targeting of tax concessions \\n142. For better targeting of tax concessions and exemptions, \\n I propose to cap deduction from capital gains on investment in residential \\nhouse under sections 54 and 54F to ` 10 crore. Another proposal with \\nsimilar intent is to limit income tax exemption from proceeds of insurance \\npolicies with very high value. \\nRationalisation \\n143. There are a number of proposals relating to rationalisation and \\nsimplification. Income of authorities, boards and commissions set up by \\nstatutes of the Union or State for the purpose of housing, development of \\ncities, towns and villages, and regulating, or regulating and developing an \\nactivity or matter, is proposed to be exempted from income tax. Other \\nmajor measures in this direction are: \\n\\uf0b7 Removing the minimum threshold of ` 10,000/- for TDS and \\nclarifying taxability relating to online gaming; \\n\\uf0b7 Not treating conversion of gold into electronic gold receipt and vice \\nversa as capital gain;  \\n\\uf0b7 Reducing the TDS rate from 30 per cent to 20 per cent on taxable \\nportion of EPF withdrawal in non-PAN cases; and \\n\\uf0b7 Taxation on income from Market Linked Debentures. \\nOthers \\n144. Other major proposals in the Finance Bill relate to the following: \\n\\uf0b7 Extension of period of tax benefits to funds relocating to IFSC, GIFT \\nCity till 31.03.2025; \\n\\uf0b7 Decriminalisation under section 276A of the Income Tax Act; \\n\\uf0b7 Allowing carry forward of losses on strategic disinvestment including \\nthat of IDBI Bank; and \\n\\uf0b7 Providing EEE status to Agniveer Fund.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 36, 'page_label': '37', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='33 \\n \\n \\n \\nPersonal Income Tax \\n145. Now, I come to what everyone is waiting for -- personal income tax. I \\nhave five major announcements to make in this regard. These primarily \\nbenefit our hard-working middle class. \\n146. The first one concerns rebate. Currently, those with income up to  \\n` 5 lakh do not pay any income tax in both old and new tax regimes. I \\npropose to increase the rebate limit to ` 7 lakh in the new tax regime. Thus, \\npersons in the new tax regime, with income up to ` 7 lakh will not have to \\npay any tax.  \\n147. The second proposal relates to middle-class individuals. \\n I had introduced, in the year 2020, the new personal income tax regime \\nwith six income slabs starting from ` 2.5 lakh. I propose to change the tax \\nstructure in this regime by reducing the number of slabs to five and \\nincreasing the tax exemption limit to ` 3 lakh. The new tax rates are: \\n` 0-3 lakh Nil \\n` 3-6 lakh 5 per cent \\n` 6-9 lakh 10 per cent \\n` 9-12 lakh 15 per cent \\n` 12-15 lakh 20 per cent \\nAbove ` 15 lakh 30 per cent \\n \\n148. This will provide major relief to all tax payers in the new regime. An \\nindividual with an annual income of ` 9 lakh will be required to pay only  \\n` 45,000/-. This is only 5 per cent of his or her income. It is a reduction of 25 \\nper cent on what he or she is required to pay now, ie, ` 60,000/-. Similarly, \\nan individual with an income of ` 15 lakh would be required to pay only  \\n` 1.5 lakh or 10 per cent of his or her income, a reduction of 20 per cent \\nfrom the existing liability of ` 1,87,500/.  \\n149. My third proposal is for the salaried class and the pensioners \\nincluding family pensioners, for whom I propose to extend the benefit of'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 37, 'page_label': '38', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='34 \\n \\n \\n \\nstandard deduction to the new tax regime. Each salaried person with an \\nincome of ` 15.5 lakh or more will thus stand to benefit by ` 52,500. \\n150. My fourth announcement in personal income tax is regarding the \\nhighest tax rate which in our country is 42.74 per cent. This is among the \\nhighest in the world. I propose to reduce the highest surcharge rate from 37 \\nper cent to 25 per cent in the new tax regime. This would result in reduction \\nof the maximum tax rate to 39 per cent. \\n151. Lastly, the limit of ` 3 lakh for tax exemption on leave encashment \\non retirement of non-government salaried employees was last fixed in the \\nyear 2002, when the highest basic pay in the government was ` 30,000/- \\npm. In line with the increase in government salaries, I am proposing to \\nincrease this limit to ` 25 lakh. \\n152. We are also making the new income tax regime as the default tax \\nregime. However, citizens will continue to have the option to avail the \\nbenefit of the old tax regime. \\n153. Apart from these, I am also making some other changes as given in \\nthe annexure. \\n154. As a result of these proposals, revenue of about ` 38,000 crore –  \\n` 37,000 crore in direct taxes and ` 1,000 crore in indirect taxes – will be \\nforgone while revenue of about ` 3,000 crore will be additionally mobilized. \\nThus, the total revenue forgone is about ` 35,000 crore annually. \\n155. Mr. Speaker Sir, with these words, I commend the Budget to this \\naugust House. \\n*****'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 38, 'page_label': '39', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='35 \\n \\n \\n \\nAnnexure to Part B of the Budget Speech 2023-24 \\nAmendments relating to Direct Taxes \\nA. PROVIDING TAX RELIEF UNDER NEW PERSONAL TAX REGIME \\nA.1     The new tax regime for Individual and HUF , introduced by the \\nFinance Act 2020, is now proposed to be the default regime.  \\nA.2      This regime would also become the default regime for AOP (other \\nthan co-operative), BOI and AJP.  \\nA.3      Any individual, HUF, AOP (other than co-operative), BOI or AJP not \\nwilling to be taxed under this new regime can opt to be taxed \\nunder the old regime. For those person having income under the \\nhead “profit and gains of business or profession” and having opted \\nfor old regime can revoke that option only once and after that \\nthey will continue to be taxed under the new regime. For those \\nnot having income under the head “profit and gains of business or \\nprofession”, option for old regime may be exercised in each year. \\nA.4      Substantial relief is proposed under the new regime with new slabs \\nand tax rates as under: \\nTotal Income (`) Rate (per cent) \\nUpto 3,00,000 Nil \\nFrom 3,00,001 to 6,00,000 5 \\nFrom 6,00,001 to 9,00,000 10 \\nFrom 9,00,001 to 12,00,000 15 \\nFrom 12,00,001 to 15,00,000 20 \\nAbove 15,00,000 30 \\n \\nA.5      Resident individual with total income up to ` 5,00,000 do not pay \\nany tax due to rebate under both old and new regime. It is \\nproposed to increase the rebate for the resident individual under \\nthe new regime so that they do not pay tax if their total income is \\nup to ` 7,00,000. \\nA.6     Standard deduction of ` 50,000 to salaried individual, and'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 39, 'page_label': '40', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='36 \\n \\n \\n \\ndeduction from family pension up to ` 15,000, is currently allowed \\nonly under the old regime. It is proposed to allow these two \\ndeductions under the new regime also. \\n A.7      Surcharge on income-tax under both old regime and new regime is \\n10 per cent if income is above ` 5 0 lakh and up to ` 1 crore, 15 per \\ncent if income is above  `1 crore and up to ` 2 crore, 25 per cent if \\nincome is above ` 2 crore and up to ` 5 crore, and 37 per cent if \\nincome is above ` 5 crore. It is proposed that the for those \\nindividuals, HUF, AOP (other than co-operative), BOI and AJP \\nunder the new regime, surcharge would be same except that the \\nsurcharge rate of 37 per cent will not apply. Highest surcharge \\nshall be 25 per cent for income above \\n \\n` 2 crore. This would reduce the maximum rate from about 42.7 \\nper cent to about 39 per cent. No change in surcharge is proposed \\nfor those who opt to be under the old regime. \\nA.8      Encashment of earned leave up to 10 months of average salary, at \\nthe time of retirement in case of an employee (other than an \\nemployee of the Central Government or State Government), is \\nexempt under sub-clause (ii) of clause (10AA) of section 10 of the \\nIncome-tax Act (“the Act”) to the extent notified. The maximum \\namount which can be exempted is ` 3 lakh at present. It is \\nproposed to issue notification to extend this limit to ` 25 lakh.  \\nB. SOCIO-ECONOMIC WELFARE MEASURES \\nB.1 Promoting timely payments to Micro and Small Enterprises \\nIn order to promote timely payments to micro and small \\nenterprises, it is proposed to include payments made to such \\nenterprises within the ambit of section 43B of the Act. Thus, \\ndeduction for such payments would be allowed only when actually \\npaid. It will be allowed on accrual basis only if the payment is \\nwithin the time mandated under the Micro, Small and Medium \\nEnterprises Development Act. \\nB.2 Agnipath Scheme, 2022 \\nThe payment received from the Agniveer Corpus Fund by the \\nAgniveers enrolled in Agnipath Scheme, 2022 is proposed to be \\nexempt from taxes. Deduction in the computation of total income \\nis proposed to be allowed to the Agniveer on the contribution'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 40, 'page_label': '41', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='37 \\n \\n \\n \\nmade by him or the Central Government to his Seva Nidhi \\naccount. \\nB.3 Relief to sugar co-operatives from past demand \\nIt is proposed that for sugar co-operatives, for years prior to A.Y. \\n2016-17, if any deduction claimed for expenditure made on \\npurchase of sugar has been disallowed, an application may be \\nmade to the Assessing Officer, who shall recompute the income of \\nthe relevant previous year after allowing such deduction up to the \\nprice fixed or approved by the Government for such previous year.\\n \\nB.4 Increasing threshold limit for Co-operatives to withdraw cash \\nwithout TDS \\nIt is proposed to enable co-operatives to withdraw cash up to ` 3 \\ncrore in a year without being subjected to TDS on such \\nwithdrawal.  \\nB.5 Penalty for cash loan/transactions against primary co-operatives \\nIt is proposed to  amend section 269SS of the Act to provide that \\nwhere a deposit is accepted by a primary agricultural credit \\nsociety or a primary co-operative agricultural and rural \\ndevelopment bank from its member or a loan is taken from a \\nprimary agricultural credit society or a primary co-operative \\nagricultural and rural development bank by its member in cash, no \\npenal consequence would arise, if the amount of such loan or \\ndeposit in cash is less than  ` 2 lakh. Further, section 269T of the \\nAct is proposed to be amended to provide that where a deposit is \\nrepaid by a primary agricultural credit society or a primary co-\\noperative agricultural and rural development bank to its member \\nor such loan is repaid to a primary agricultural credit society or a \\nprimary co-operative agricultural and rural development bank by \\nits member in cash, no penal consequence shall arise, if the \\namount of such loan or deposit in cash is less than ` 2 lakh. \\nB.6 Relief to start-ups in carrying forward and setting off of losses \\nThe condition of continuity of at least 51 per cent shareholding for \\nsetting off of carried forward losses is relaxed for an eligible start \\nup if all the shareholders of the company continue to hold those \\nshares. At present this relaxation applies for losses incurred during \\nthe period of 7 years from incorporation of such start-up. It is'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 41, 'page_label': '42', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='38 \\n \\n \\n \\nproposed to increase this period to 10 years. \\nB.7 Extension of date of incorporation for eligible start up for \\nexemption  \\nCertain start-ups are eligible for some tax benefit if they are \\nincorporated before 1st April, 2023. The period of incorporation of \\nsuch eligible start-ups is proposed to be extended by one year to \\nbefore 1st April, 2024.  \\nB.8 Gold to Electronic Gold Receipt \\nThe conversion of physical gold to Electronic Gold Receipt and vice \\nversa is proposed not to be treated as a transfer and not to attract \\nany capital gains. This would promote investments in electronic \\nequivalent of gold. \\nB.9 Incentives to IFSC \\nRelocation of funds to IFSC has certain tax exemptions, if the \\nrelocation is before 31.03.2023. This date is proposed to be \\nextended to 31.03.2025. Further, any distributed income from the \\noffshore derivative instruments entered into with an offshore \\nbanking unit is also proposed to be exempted subject to certain \\nconditions. \\nB.10 Exemption to development authorities etc. \\nIt is proposed to provide exemption to any income arising to a \\nbody or authority or board or trust or commission, (not being a \\ncompany) which  has been established or constituted by or under \\na Central or State Act with the purposes of satisfying the need for \\nhousing or for planning, development or improvement of cities, \\ntowns and villages or for regulating any activity or matter, \\nirrespective of whether it is carrying out commercial activity. \\nB.11 Facilitating certain strategic disinvestments \\nTo facilitate certain strategic disinvestments, it is proposed to \\nallow carry forward of accumulated losses and unabsorbed \\ndepreciation allowance in the case of amalgamation of one or \\nmore banking company with any other banking institution or a \\ncompany subsequent to a strategic disinvestment, if such \\namalgamation takes place within 5 years of strategic \\ndisinvestment. It is also proposed to modify the definition of \\n‘strategic disinvestment’.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 42, 'page_label': '43', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='39 \\n \\n \\n \\nB.12 15 per cent concessional tax to promote new manufacturing co -\\noperative society \\nIn order to promote the growth of manufacturing in co-operative \\nsector, a new co-operative society formed on or after 01.04.2023, \\nwhich commences manufacturing or production by 31.03.2024 \\nand do not avail of any specified incentive or deduction, is \\nproposed to be allowed an option to pay tax at a concessional rate \\nof 15 per cent similar to what is available to new manufacturing \\ncompanies. \\nC. EASE OF COMPLIANCE \\nC.1 Ease in claiming deduction on amortization of preliminary \\nexpenditure \\nAt present for claiming amortization of certain preliminary \\nexpenses, the activity is to be carried out either by the assessee or \\nby a concern approved by the Board. In order to ease the process \\nof claiming amortization of these expenses it is proposed to \\nremove the condition of activity in connection with these \\nexpenses to be carried out by a concern approved by the Board. \\nFormat for reporting of such expenses by the assessee shall be \\nprescribed. \\nC.2 Increasing threshold limits for presumptive taxation schemes \\nIn order to ease compliance and to promote non-cash \\ntransactions, it is proposed to increase the threshold limits for \\npresumptive scheme of taxation for eligible businesses from ` 2 \\ncrore to ` 3 crore and for specified professions from ` 50 lakh to \\n \\n` 75 lakh. The increased limit will apply only in case the amount or \\naggregate of the amounts received during the year, in cash, does \\nnot exceed five per cent of the total gross receipts/turnover. \\nC.3 Extending the scope for deduction of tax at source at lower or nil \\nrate \\nIt is proposed to allow a taxpayer to obtain certificate of \\ndeduction of tax at source to lower or nil rate on sums on which \\ntax is required to be deducted under section 194LBA of the Act by \\nBusiness Trusts.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 43, 'page_label': '44', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='40 \\n \\n \\n \\nD. WIDENING & DEEPENING OF TAX BASE AND ANTI AVOIDANCE \\nD.1 It is proposed to extend the deemed income accrual provision \\nrelating to sums of money exceeding fifty thousand rupees, \\nreceived from residents without consideration to a not ordinarily \\nresident with effect from 1st April, 2023. \\nD.2 It is proposed to omit the provision to allow tax exemption to \\nnews agencies set up in India solely for collection and distribution \\nof news from the financial year 2023-24.  \\nD.3 It is proposed to tax distributed income by business trusts in the \\nhands of a unit holder (other than dividend, interest or rent which \\nis already taxable) on which tax is currently avoided both in the \\nhands of unit holder as well as in the hands of business trust.   \\nD.4 It is proposed to withdraw the exemption from TDS currently \\navailable on interest payment on listed debentures. \\nD.5 With respect to presumptive schemes for non-residents, it is \\nproposed to disallow carried forward and set off of loss computed \\nas per books of account with presumptive income. \\nD.6 For online games, it is proposed to provide for TDS and taxability \\non net winnings at the time of withdrawal or at the end of the \\nfinancial year. Moreover, TDS would be without the threshold of \\n \\n` 10,000. For lottery, crossword puzzles games, etc threshold limit \\n` 10,000 for TDS shall continue but shall apply to aggregate \\nwinnings during a financial year. \\nD.7     The rate of TCS for foreign remittances for education and for \\nmedical treatment is proposed to continue to be 5 per cent for \\nremittances in excess of ` 7 lakh. Similarly, the rate of TCS on \\nforeign remittances for the purpose of education through loan \\nfrom financial institutions is proposed to continue to be 0.5 per \\ncent in excess of `7 lakh. However, for foreign remittances for \\nother purposes under LRS and purchase of overseas tour program, \\nit is proposed to increase the rates of TCS from 5 per cent to 20 \\nper cent. \\nD.8 Tax on capital gains can be avoided by investing proceeds of such \\ngains in residential property. This is proposed to be capped at ` 10 \\ncrore.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 44, 'page_label': '45', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='41 \\n \\n \\n \\nD.9 The income from market linked debentures is proposed to be \\ntaxed as short-term capital gains at the applicable rates. \\nD.10 It is proposed to provide for some provisions to minimise risk to \\nrevenue due to undervaluation of inventory. \\nD.11 It is proposed to provide that where aggregate of premium for life \\ninsurance policies (other than ULIP) issued on or after 1 st April, \\n2023 is above ` 5 lakh, income from only those policies with \\naggregate premium up to ` 5 lakh shall be exempt. This will not \\naffect the tax exemption provided to the amount received on the \\ndeath of person insured. It will also not affect insurance policies \\nissued till 31st March, 2023. \\nD.12 It is proposed to amend provisions for computing capital gains in \\ncase of joint development of property to include the amount \\nreceived through cheque etc. as consideration.  \\nD.13 While interest paid on borrowed capital for acquiring or improving \\na property can, subject to certain conditions, be claimed as \\ndeduction from income, it can also be included in the cost of \\nacquisition or improvement on transfer, thereby reducing capital \\ngains. It is proposed to provide that the cost of acquisition or \\nimprovement shall not include the amount of interest claimed \\nearlier as deduction. \\nD.14 There are certain assets like intangible assets or rights for which \\nno consideration has been paid for acquisition and the transfer of \\nwhich may result in generation of income. Their cost of acquisition \\nis proposed to be defined to be NIL. \\nE. IMPROVING COMPLIANCE AND TAX ADMINISTRATION \\nE.1 With respect to rectification of orders by the Interim Board of \\nSettlement, it is proposed to provide that where the time-limit for \\namending an order by it or for making an application to it expires \\non or after 01.02.2021 but before 01.02.2022, such time-limit shall \\nstand extended to 30.09.2023. \\nE.2 To expedite the disposal of certain appeals pending with \\nCommissioner (Appeals), it is proposed to introduce a new \\nauthority in the rank of Joint Commissioner/ Additional \\nCommissioner [JCIT(Appeals)], for appeals against certain orders'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 45, 'page_label': '46', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='42 \\n \\n \\n \\npassed by or with the approval of an authority below the rank of \\nJoint Commissioner. Certain related and consequential \\namendments are also proposed in this regard.  \\nE.3 It is proposed to reduce the minimum time period required to be \\nprovided by the transfer pricing officer to assessee for production \\nof documents and information from 30 days to 10 days. \\nE.4 It is proposed to provide for appeal against penalty orders passed \\nby Commissioner (Appeals) under certain sections of the Act \\nbefore the Appellate Tribunal. It is also proposed to provide that \\nan order under section 263 of the Act passed by the Principal \\nChief Commissioner or Chief Commissioner and any rectification \\norder for the same shall also be appealable before the Appellate \\nTribunal. Further, it is proposed to enable filing of memorandum \\nof cross-objections in all classes of cases against which appeal can \\nbe made to the Appellate Tribunal. \\nE.5 It is proposed to amend section 132 of the Act, dealing with \\nsearch and seizure, to allow the authorised officer to take \\nassistance of specific domain experts like digital forensic \\nprofessionals, valuers and services of other professionals like \\nlocksmiths, carpenters etc. during the course of search and also to \\naid in accurate estimation of undisclosed income held in the form \\nof property by the assessee.  \\nE.6 Section 170A of the Act, inserted vide Finance Act, 2022 is \\nproposed to be substituted to clarify that a modified return shall \\nbe furnished by an entity to whom the order of the business \\nreorganisation applies, and to introduce provisions for assessment \\nor reassessment in cases where such modified return is furnished. \\nE.7 It is proposed that an order of assessment may be passed within a \\nperiod of 12 months from the end of the relevant assessment year \\nor the financial year in which updated return is filed, as the case \\nmay be. It is also proposed that in cases where search under \\nsection 132 of the Act or requisition under section 132A of the Act \\nhas been made, the period of limitation of pending assessments \\nshall be extended by twelve months.  \\nE.8 It is proposed to make amendments to empower the Central \\nGovernment to make modifications in the already notified'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 46, 'page_label': '47', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='43 \\n \\n \\n \\nschemes regarding e -Verification, Dispute Resolution, Advance \\nRulings, Appeal and Penalty, at any time to enable better \\nimplementation of such schemes. \\nE.9 It is proposed to limit the time for furnishing of a return for \\nreassessment. Further, it is also proposed to  provide that in cases \\nwhere search related information is available after 15th March of \\nany financial year, an additional period of fifteen days shall be \\nallowed for issuance of notice, for assessment/reassessments etc, \\nunder section 148 of the Act. It is also proposed to clarify that the \\nspecified authority for granting approval shall be Principal Chief \\nCommissioner or Principal Director General or Chief Commissioner \\nor Director General. \\nE.10 It is proposed to provide a penalty of ` 5,000 if there is any \\ninaccuracy in the statement of financial transactions submitted by \\na prescribed reporting financial institution due to false or \\ninaccurate information submitted by the account holder. \\nE.11 It is proposed to amend section 271C and section 276B of the Act \\nto provide for penalty and prosecution where default in TDS \\nrelates to transaction in kind. \\nE.12.   It is proposed to amend the time period for filing of appeal against \\nthe order of the Adjudicating authority under Benami Act within a \\nperiod of 45 days from the date when such order is received by \\nthe Initiating Officer or the aggrieved person. The definition of \\n‘High Court’ is also proposed to be modified to allow \\ndetermination of jurisdiction for filing appeal in the case of non-\\nresidents. \\nF. RATIONALISATION \\nF.1 The restriction on interest deductibility on interest payment to \\noverseas associated enterprise does not apply to those in the \\nbusiness of banking and insurance. It is proposed to extend this \\nbenefit to non-banking financial companies, as may be notified. \\nF.2 TDS on payment of certain income to a non-resident is currently at \\nthe rate of 20 per cent, but the tax rate in treaties may be lower. It \\nis proposed to allow the benefit of tax treaty at the time of TDS on \\nsuch income under section 196A of the Act.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 47, 'page_label': '48', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='44 \\n \\n \\n \\nF.3 At present the TDS rate on withdrawal of taxable component from \\nEmployees’ Provident Fund Scheme in non-PAN cases is 30 per \\ncent. It is proposed to reduce it to 20 per cent, as in other non-\\nPAN cases. \\nF.4 Sometimes, tax for income of an earlier year is deducted later, \\nwhile tax thereon has already been paid in the earlier year. \\nAmendment is proposed to facilitate such taxpayers to claim \\ncredit of this TDS in the earlier year.  \\nF.5 Higher TDS/TCS rate applies, if the recipient is a non-filer i.e. who \\nhas not furnished his return of income of preceding previous year \\nand has aggregate of TDS and TCS of ` 50,000 or more. It is \\nproposed to exclude a person who is not required to furnish the \\nreturn of income for such previous year and who is notified by the \\nCentral Government in the Official Gazette in this behalf. \\nF\\n.6 It is proposed to clarify that the amount of advance tax paid is \\nreduced only once for computing the interest payable u/s 234B in \\nthe case of an updated return. \\nF.7 It is proposed to extend taxability of the consideration (share \\napplication money/ share premium) for shares exceeding the face \\nvalue of such shares to all investors including non-residents. \\nF.8 It is proposed to enable prescription of a uniform methodology for \\ncomputing the value of perquisite with respect to accommodation \\nprovided by employers to their employees. \\nF.9 It is proposed to provide a time limit for an SEZ unit to bring the \\nproceeds from exports of goods or services into India. The filing of \\nincome-tax return is also proposed to be made mandatory for \\nclaiming deduction on export income. \\nF.10 Due to changes in classification of non-banking financial \\ncompanies by the Reserve Bank of India, it is proposed to make \\nnecessary amendments to align such classifications in the Act with \\nthe same. \\nF.11 It is proposed to clarify that for taxability under section 28 of the \\nAct as well for tax deduction at source under section 194R of the \\nAct, the benefit could also be in cash. \\nF.12 It is proposed to make amendments relating to exemption'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 48, 'page_label': '49', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='45 \\n \\n \\n \\nprovided to charitable trusts and institution to \\n\\uf0b7 provide clarity on tax treatment on replenishment of corpus \\nand on repayment of loans/borrowings; \\n\\uf0b7 treat only 85 per cent of donation made to another trust as \\napplication; \\n\\uf0b7 omit the redundant provisions related to rolling back of \\nexemption; \\n\\uf0b7 combine provisional and regular registration in some cases; \\n\\uf0b7 modify the scope of specified violation; \\n\\uf0b7 provide for payment of tax on assets if a trust does not apply \\nfor exemption after getting provisional exemption and for re-\\nexemption after expiry of exemption; \\n\\uf0b7 align of time for furnishing of certain forms; \\n\\uf0b7 clarify that the time provided for furnishing return of income \\nfor claiming exemption shall not include the time provided for \\nfurnishing updated return. \\nF.13 It is proposed to omit certain name-based funds from section 80G \\nof the Act, which provides for deduction of donation to such funds \\nfrom the income of the donor. \\nF.14 It is proposed to provide that where refund is due to a person, \\nsuch refund shall be set off against existing demand, and if \\nproceedings for assessment or reassessment are pending in such \\ncase, the refund due will be withheld by the Assessing Officer till \\nthe date of assessment or reassessment. \\nG. OTHERS \\nG.1 It is proposed to omit section 88 and some of the clauses of \\nsection 10 of the Act which are no longer in force. \\nG.2 It is proposed to extend tax exemption to Specified Undertaking of \\nUnit Trust of India (SUUTI) till 30 th September, 2023. It is also \\nproposed to enable the Central Government to notify the date of \\nvacation of office of administrator of SUUTI. \\nG.3 It is proposed to decriminalize certain acts of omission of \\nliquidators under section 276A of the Act with effect from 1 st \\nApril, 2023.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 49, 'page_label': '50', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='46 \\n \\n \\n \\nAnnexure to Part B of the Budget Speech 2023-24 \\nAmendments relating to Indirect Taxes \\n \\nA. LEGISLATIVE CHANGES IN CUSTOMS LAWS \\nA.1       Amendments in the Customs Act, 1962 \\nSection 25 (4A) is being amended to  exclude certain categories of \\nconditional customs duty exemptions from the validity period of \\ntwo years, such as, notifications issued in relation to multilateral \\nor bilateral trade agreements; obligations under international \\nagreements, treaties, conventions including with respect to UN \\nagencies, diplomats, international organizations; privileges of \\nconstitutional authorities; schemes under Foreign Trade Policy; \\nCentral Government schemes having a validity of more than two \\nyears; re-imports, temporary imports, goods imported as gifts or \\npersonal baggage; any other duties of Customs under any other \\nlaw in force including  IGST levied under section 3(7) of Customs \\nTariff Act, 1975, other than duty of customs levied under section \\n12 of the Customs Act 1962. \\nSection 127C is being amended to specify a time limit of nine \\nmonths from date of filing application for passing final order by \\nSettlement Commission.  \\nA.2  Amendments in the provisions relating to Anti-Dumping Duty \\n(ADD), Countervailing Duty (CVD), and Safeguard Measures \\nSections 9, 9A, 9C of the Customs Tariff Act are being amended to \\nclarify the intent and scope of these provisions. They are also \\nbeing validated retrospectively with effect from 1st January 1995. \\nA.3      Amendments in the First Schedule to the Customs Tariff Act, 1975 \\nThe First Schedule to the Customs Tariff Act, 1975 is being \\namended to increase the rates on certain tariff items with effect \\nfrom 02.02.2023 and also modify the rates on certain other tariff \\nitems as part of rate rationalisation with effect from date of \\nassent. \\nThe First Schedule to the Customs Tariff Act is being proposed to \\nbe amended in accordance with HSN 2022 amendments.  \\nNew tariff lines are also proposed to be created, which will help in \\nbetter identification of millet-based products, mozzarella cheese, \\nmedicinal plants and their parts, certain pesticides, telecom'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 50, 'page_label': '51', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='47 \\n \\n \\n \\nproducts, synthetic diamonds, cotton, fertilizer grade urea etc. \\nThis will also help in trade facilitation by better identification of \\nthe above items, getting clarity on availing concessional import \\nduty through various notifications and thus reducing dwell time.  \\nThese changes shall come into effect from 01.05.2023. \\nA.4     Amendment in the Second Schedule to the Customs Tariff Act, \\n1975 \\nThe Second Schedule (Export Tariff) is being amended to align the \\nentries under heading 1202 with that of the First Schedule (Import \\nTariff) . \\nB. LEGISLATIVE CHANGES IN GST LAWS \\nB.1 Decriminalisation \\nSection 132 and section 138 of CGST Act are being amended, inter \\nalia, to - \\n\\uf0b7 raise the minimum threshold of tax amount for launching \\nprosecution under GST from ` one crore to ` two crore, \\nexcept for the offence of issuance of invoices without supply \\nof goods or services or both; \\n\\uf0b7 reduce the compounding amount from the present range of \\n50 per cent  to 150 per cent of tax amount to the range of 25 \\nper cent to 100 per cent; \\n\\uf0b7 decriminalize certain offences specified under clause (g), (j) \\nand (k) of sub-section (1) of section 132 of CGST Act, 2017, \\nviz.- \\no obstruction or preventing any officer in discharge of his \\nduties;  \\no deliberate tempering of material evidence; \\no failure to supply the information. \\nB.2        Facilitate e-commerce for micro enterprises \\nAmendments are being made in section 10 and section 122 of the \\nCGST Act to enable unregistered suppliers and composition \\ntaxpayers to make intra-state supply of goods through E-\\nCommerce Operators (ECOs), subject to certain conditions.  \\nB.3        Amendment to Schedule III of CGST Act, 2017 \\nParas 7, 8 (a) and 8 (b) were inserted in Schedule III of CGST Act, \\n2017 with effect from 01.02.2019 to keep certain transactions/ \\nactivities, such as supplies of goods from a place outside the \\ntaxable territory to another place outside the taxable territory, \\nhigh sea sales and supply of warehoused goods before their home'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 51, 'page_label': '52', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='48 \\n \\n \\n \\nclearance, outside the purview of GST. In order to remove the \\ndoubts and ambiguities regarding taxability of such transactions/ \\nactivities during the period 01.07.2017 to 31.01.2019, provisions \\nare being incorporated to make the said paras effective from \\n01.07.2017. However, no refund of tax paid shall be available in \\ncases where any tax has already been paid in respect of such \\ntransactions/ activities during the period 01.07.2017 to \\n31.01.2019. \\nB.4        Return filing under GST  \\nSections 37, 39, 44 and 52 of CGST Act, 2017 are being amended \\nto restrict filing of returns/ statements to a maximum period of \\nthree years from the due date of filing of the relevant return / \\nstatement.  \\nB.5        Input Tax Credit for expenditure related to CSR \\nSection 17(5) of CGST Act is being amended to provide that input \\ntax credit shall not be available in respect of goods or services or \\nboth received by a taxable person, which are used or intended to \\nbe used for activities relating to his obligations under corporate \\nsocial responsibility referred to in section 135 of the Companies \\nAct, 2013. \\nB.6        Sharing of information \\nA new section 158A in CGST Act is being inserted to enable sharing \\nof the information furnished by the registered person in his return \\nor application of registration or statement of outward supplies, or \\nthe details uploaded by him for generation of electronic invoice or \\nE-way bill or any other details on the common portal, with other \\nsystems in a manner to be prescribed \\nB.7        Amendments in section 2 clause (16) of IGST Act, 2017 \\nClause (16) of section 2 of IGST Act is amended to revise the \\ndefinition of “non-taxable online recipient” by removing the \\ncondition of receipt of online information and database access or \\nretrieval services for purposes other than commerce, industry or \\nany other business or profession so as to provide for taxability of \\nOIDAR service provided by any person located in non-taxable \\nterritory to an unregistered person receiving the said services and \\nlocated in the taxable territory. Further, it also seeks to clarify that \\nthe persons registered solely in terms of clause (vi) of Section 24 \\nof CGST Act shall be treated as unregistered person for the \\npurpose of the said clause.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 52, 'page_label': '53', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='49 \\n \\n \\n \\nB.8        Online information and database access or retrieval services \\nClause (17) of section 2 of IGST Act is being amended to revise the \\ndefinition of “online information and database access or retrieval \\nservices” to remove the condition of rendering of the said supply \\nbeing essentially automated and involving minimal human \\nintervention.  \\nB.9        Place of supply in certain cases \\nProviso to sub-section (8) of section 12 of the IGST Act is being \\nomitted so as to specify the place of supply, irrespective of \\ndestination of the goods, in cases where the supplier of services \\nand recipient of services are located in India.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 53, 'page_label': '54', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='50 \\n \\n \\n \\n \\nC. CUSTOMS DUTY RATE CHANGES \\n \\nC.1. Reduction in basic customs duty to reduce input costs, deepen value \\naddition, to promote export competitiveness, correct inverted duty \\nstructure so as to boost domestic manufacturing etc [with effect \\nfrom 02.02.2023] \\nS. \\nNo. Commodity \\nFrom \\n(per cent) \\nTo \\n(per cent) \\nI. Agricultural Products \\n1. Pecan Nuts 100  30 \\n2. Fish meal for manufacture of aquatic \\nfeed 15 5 \\n3. Krill meal for manufacture of aquatic \\nfeed 15 5 \\n4. Fish lipid oil for manufacture of aquatic \\nfeed 30 15 \\n5. Algal Prime (flour) for manufacture of \\naquatic feed 30 15 \\n6. Mineral and Vitamin Premixes for \\nmanufacture of aquatic feed 15 5 \\n7 Crude glycerin for use in manufacture \\nof Epichlorohydrin \\n7.5 2.5 \\n8 Denatured ethyl alcohol for use in \\nmanufacture of industrial chemicals. \\n5 Nil \\nII. Minerals \\n1 Acid grade fluorspar (containing by \\nweight more than 97 per cent of \\ncalcium fluoride) \\n5 2.5 \\nIII. Gems and Jewellery Sector \\n1. Seeds for use in manufacturing of \\nrough lab-grown diamonds \\n5 Nil'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 54, 'page_label': '55', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='51 \\n \\n \\n \\nIV. Capital Goods \\n1. Specified capital goods/machinery for \\nmanufacture of lithium-ion cell for use \\nin battery of electrically operated \\nvehicle (EVs) \\nAs \\napplicable \\nNil  \\n(up to \\n31.03.2024)\\n \\nV. IT and Electronics  \\n \\n1. Specified chemicals/items for \\nmanufacture of Pre-calcined Ferrite \\nPowder \\n7.5 Nil \\n(up to \\n31.03.2024) \\n2. Palladium Tetra Amine Sulphate for \\nmanufacture of parts of connectors \\n7.5 Nil \\n(up to \\n31.03.2024) \\n3. Camera lens and its inputs/parts for \\nuse in manufacture of camera module \\nof cellular mobile phone \\n2.5 Nil \\n4. Specified parts for manufacture of \\nopen cell of TV panel \\n5 2.5 \\nVI. Electronic Appliances \\n1. Heat coil for manufacture of electric \\nkitchen chimneys \\n20 15 \\nVII. Others \\n1. Warm blood horse imported by sports \\nperson of outstanding eminence for \\ntraining purpose \\n30 Nil \\n2. Vehicles, specified automobile \\nparts/components, sub-systems and \\ntyres when imported by notified \\ntesting agencies, for the purpose of \\ntesting and/ or certification, subject to \\nconditions. \\nAs \\napplicable \\nNil'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 55, 'page_label': '56', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='52 \\n \\n \\n \\nC.2. Increase in Customs duty [with effect from 02.02.2023]  \\nS. No. Commodity \\n \\nRate of duties \\nFrom \\n(per cent) \\nTo \\n(per cent) \\nI. Chemicals \\n1. Styrene 2 \\n(+0.2 SWS) \\n2.5 \\n(+0.25 \\nSWS) \\n2. Vinyl chloride monomer 2 \\n(+0.2 SWS) \\n2.5 \\n(+0.25 \\nSWS) \\nII Petrochemical \\n1 Naphtha 1 \\n(+ 0.1 SWS) \\n2.5 \\n(+0.25  SWS) \\nIII. Precious Metals \\n1. Silver (including silver plated with gold \\nor platinum), unwrought or in semi-\\nmanufactured forms, or in powder \\nform \\n7.5 \\n(+ 2.5 \\nAIDC+ 0.75 \\nSWS) \\n10 \\n(+ 5 AIDC+ \\nNil SWS) \\n2. Silver dore 6.1 \\n(+ 2.5 \\nAIDC+ 0.61  \\nSWS) \\n10 \\n(+ 4.35 \\nAIDC+ Nil \\nSWS) \\nIV. Gems and Jewellery Sector  \\n1. Articles of Precious Metals such as \\ngold/silver/platinum \\n20 \\n(+Nil AIDC \\n+2 SWS)  \\n25 \\n(+Nil AIDC \\n+Nil SWS) \\n2. Imitation Jewellery 20 or ` \\n400/kg., \\nwhichever is \\nhigher \\n \\n(+Nil AIDC +2 \\nor ` 40 per \\nKg SWS) \\n \\n25 or ` \\n600/kg., \\nwhichever is \\nhigher \\n \\n(+Nil AIDC \\n+Nil SWS)'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 56, 'page_label': '57', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='53 \\n \\n \\n \\nS. No. Commodity \\n \\nRate of duties \\nFrom \\n(per cent) \\nTo \\n(per cent) \\nV. Automobiles  \\n1 Vehicle (including electric vehicles) in \\nSemi-Knocked Down (SKD) form . \\n30 \\n(+3 SWS) \\n35 \\n(+Nil SWS) \\n2 Vehicle in Completely Built Unit (CBU) \\nform, other than with CIF more than \\nUSD 40,000 or with engine capacity \\nmore than 3000 cc for petrol-run \\nvehicle and more than 2500 cc for \\ndiesel-run vehicles, or with both \\n60 \\n(+6  SWS) \\n70 \\n(+Nil SWS) \\n3 Electrically operated Vehicle in \\nCompletely Built Unit (CBU) form, \\nother than with CIF value more than \\nUSD 40,000 \\n60 \\n(+ 6 SWS) \\n70 \\n(+Nil SWS) \\nVI. Others \\n1.  Bicycles 30 \\n \\n(+ Nil AIDC \\n+3 SWS) \\n35 \\n \\n(+ Nil AIDC \\n+Nil SWS) \\n2.  Toys and parts of toys (other than \\nparts of electronic toys) \\n60 \\n \\n(+Nil AIDC+ \\n6 SWS) \\n70 \\n \\n(+Nil AIDC+ \\nNil SWS) \\n3.  Compounded Rubber 10 \\n \\n \\n25 or ` \\n30/kg., \\nwhichever is \\nlower \\n4.  Electric Kitchen Chimney 7.5 \\n \\n15 \\n \\n* AIDC -Agriculture Infrastructure Development Cess; SWS – Social Welfare \\nSurcharge'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 57, 'page_label': '58', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='54 \\n \\n \\n \\nD. CHANGES IN CENTRAL EXCISE \\nD.1. NCCD Duty rate  on Cigarettes [with effect from 02.02.2023]  \\n \\nDescription of goods \\nRate of excise duty \\nFrom \\n(` per 1000 \\nsticks) \\nTo \\n(` per 1000 \\nsticks) \\nOther than filter cigarettes, of length not \\nexceeding 65 mm \\n200 230 \\nOther than filter cigarettes, of length exceeding \\n65 mm but not exceeding 70 mm \\n250 290 \\nFilter cigarettes of length not exceeding 65 mm 440 510 \\nFilter cigarettes of length exceeding 65 mm but \\nnot exceeding 70 mm \\n440 510 \\nFilter cigarettes of length exceeding 70 mm but \\nnot exceeding 75 mm \\n545 630 \\nOther cigarettes 735 850 \\nCigarettes of tobacco substitutes 600 690 \\n \\n \\nD.2. Other changes in Central Excise [with effect from 02.02.2023] \\nIn order to promote green fuel, central excise duty exemption is being \\nprovided to blended Compressed Natural Gas from so much of the amount \\nas is equal to the GST paid on Bio Gas/Compressed Bio Gas contained in the \\nblended CNG. \\nE. OTHERS \\nThere are few other changes of minor nature. For details of the budget \\nproposals, the Explanatory Memorandum and other relevant budget \\ndocuments may be referred to. \\n*****'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content=\"Conversational Text Extraction with Large \\nLanguage Models Using Retrieval-Augmented \\nSystems \\nSoham Roy1, Mitul Goswami1, Nisharg Nargund1, Suneeta Mohanty1 and Prasant Kumar Pattnaik1 \\nSchool of Computer Engineering, Kalinga Institute of Industrial Technology, Patia, Bhubaneswar, 751024, India1, \\nAbstract— This study introduces a system leveraging Large \\nLanguage Models (LLMs) to extract text and enhance user \\ninteraction with PDF documents via a conversational interface. \\nUtilizing Retrieval -Augmented Generation (RAG), the system \\nprovides informative respons es to user inquiries while \\nhighlighting relevant passages within the PDF. Upon user \\nupload, the system processes the PDF, employing sentence \\nembeddings to create a document -specific vector store. This \\nvector store enables efficient retrieval of pertinent s ections in \\nresponse to user queries. The LLM then engages in a \\nconversational exchange, using the retrieved information to \\nextract text and generate comprehensive, contextually aware \\nanswers. While our approach demonstrates competitive \\nROUGE values compared to existing state-of-the-art techniques \\nfor text extraction and summarization, we acknowledge that \\nfurther qualitative evaluation is necessary to fully assess its \\neffectiveness in real -world applications.  The proposed system \\ngives competitive ROUGE values as compared to existing state-\\nof-the-art techniques for text extraction and summarization, \\nthus offering a valuable tool for researchers, students, and \\nanyone seeking to efficiently extract knowledge and gain \\ninsights from documents through an intuitive  question-\\nanswering interface. \\nKeywords—Large Language Model (LLM), Retrieval \\nAugmented Generation, Embeddings, Text Extraction, ROUGE \\nI. INTRODUCTION  \\nThe ever-growing volume of digital documents, particularly \\nPDFs, presents a significant challenge: efficiently extracting \\nknowledge from their text -heavy content. Over the years, \\nvarious tools and techniques have been developed to address \\nthis issue, from basic keyword search functionalities to more \\nadvanced text mining and natural language processing (NLP) \\nalgorithms [1]. Despite these advancements, many solutions \\nstill fall short of providing contextually relevant information \\nquickly and accurately. The e volution of artificial \\nintelligence and machine learning, particularly in the form of \\nlarge language models, has revolutionized this process, \\nenabling more sophisticated and efficient extraction of \\nknowledge from vast repositories of digital documents [2]. \\n \\nLarge language models (LLMs) have undergone significant \\nevolution, transforming the landscape of natural language \\nprocessing (NLP) and information retrieval. While the \\nintegration of Retrieval -Augmented Generation (RAG) with \\nLLMs is noteworthy, it is essential to recognize that similar \\nframeworks have been explored in existing literature. This \\npaper aims to build upon these studies by providing a tailored \\napplication for document interaction. The advent of machine \\nlearning, particularly deep learning, marked a significant leap \\nforward, with models like Word2Vec and GloVe introducing \\nword embeddings that captured semantic relationships \\nbetween words [3]. Furthermore, Transformers utilize self -\\nattention mechanisms to process and understand text in \\nparallel, rather than sequentially, enabling them to capture \\nlong-range dependencies and contextual information more \\neffectively. BERT, with its bidirectional approach, improved \\nthe understanding of context within a text, while GPT, with \\nits autoregressive nature, exc elled in text generation [4][5]. \\nHowever, while the use of these models has become \\nwidespread, the integration of retrieval augmented generation \\nfor targeted PDF interaction remains under -explored. This \\nwork focuses on addressing this niche, aiming to bridge this \\ngap by combining large la nguage models with document \\nretrieval in the conversational interface, providing a more \\ntailored application in the domain of document interaction.  \\nThese advancements in LLMs have significantly enhanced \\ntext extraction and data retrieval capabilities. This capability \\nis particularly useful for handling the ever -growing volume \\nof digital documents, enabling efficient knowledge extraction \\nand insight generation [6].  \\n \\nBuilding on the advancements in LLMs, Retrieval -\\nAugmented Generation (RAG) systems enhance the \\ncapability of these models by integrating a retrieval \\nmechanism. RAG combines information retrieval and \\ngenerative processes to produce highly accurate and \\ncontextually relevant responses [7]. In a RAG framework, the \\nsystem first retrieves relevant passages from a large corpus of \\ndocuments based on the user's query [8][9]. This combination \\nallows the model to generate responses that are both informed \\nby a broad un derstanding of language and enriched with \\nprecise, relevant details from the retrieved content. This \\napproach significantly improves the model's ability to handle \\ncomplex queries and extract pertinent information from large \\ndatasets, making it an invaluabl e tool for efficient and \\naccurate knowledge extraction. In this study, the authors \\nintroduce a novel approach  for text extraction that leverages \\nan LLM system to enhance user interaction with documents \\nvia a conversational interface.  \\nII. RELATED WORK \\nRecent advancements in document understanding and \\ninformation extraction have been driven by deep learning \\ntechniques. Traditional keyword matching and rule -based \\nmethods often struggle with complex documents, while deep \\nlearning models provide more robust  solutions capable of \\naccurately handling intricate structures. For instance, M. Li \\net al. introduced the “BiomedRAG” model, which supervises \\nretrieval in the biomedical domain through varied chunk \\ndatabase creation, enhancing prediction accuracy [10]. \\nSimilarly, M. D. Cyril Zakka et al. developed the “Almanac”\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='framework, which retrieves medical guidelines and treatment \\nadvice, outperforming typical LLMs in factuality, \\ncompleteness, user preference, and safety [11]. Additionally, \\nK. Yang et al. introduced \"LeanDojo,\" a RAG -based LLM \\nthat streamlines theorem provi ng with comprehensive \\ntoolkits and data [12]. P. Lewis et al. explored a fine -tuning \\nrecipe for RAG models, leveraging pre -trained parametric \\nand non -parametric memory to improve language \\ndevelopment [13]. \\n \\nZ. Feng et al. proposed an iterative retrieval -generation \\ncollaborative framework that not only allows for the use of \\nboth parametric and non-parametric knowledge but also aids \\nin the discovery of the correct reasoning path via retrieval -\\ngeneration interactions, which is critical for tasks that require \\nmulti-step reasoning [14]. J. Miao et al. demonstrated the \\ndevelopment of a specialized ChatGPT model connected with \\nan RAG system that is intended to meet the KDIGO 2023 \\ncriteria for chronic kidney disease [15]. In a different domain, \\nH. Li et al. demonstrated the efficacy of leveraging attention \\nprocesses in neural networks to focus on key areas of material \\nfor better question answering in language models [16]. \\nSimilarly, Y. Zhang et al. suggested a unique M ulti-Modal \\nKnowledge-aware Hierarchical Attention Network \\n(MKHAN) to efficiently leverage a multi -modal knowledge \\ngraph (MKG) for explainable medical question answering \\n[17]. However, these approaches are often tailored to specific \\nuse cases, lacking the g eneralizability required for broader \\ndocument interaction tasks. \\nOur work builds upon these advancements by presenting a \\nRAG-inspired system for the interactive exploration of user -\\nuploaded PDF documents. We employ advanced sentence \\nembeddings to ensure efficient retrieval of relevant content. \\nBy integrating this contex t into the response generation \\nprocess of the large language model (LLM), we create a more \\ntailored and contextually rich user experience. This approach \\nallows users to engage in focused conversations that explore \\nthe specific content and nuances of the up loaded PDFs, \\nthereby enhancing the effectiveness and relevance of \\ninformation retrieval and dialogue within the system. \\nIII. RETRIEVAL AUGMENTED GENERATION  \\nThe Retrieval -Augmented Generation (RAG) architecture \\nrepresents a sophisticated integration of information retrieval \\n(IR) and generative modeling techniques, designed to \\nenhance the precision and relevance of generated responses \\nin natural language proces sing tasks. The RAG process \\ncommences with a robust retrieval component that sifts \\nthrough a vast corpus of documents to pinpoint relevant \\npassages aligned with the user\\'s query. Traditional IR \\ntechniques like TF -IDF and BM25 evaluate the statistical \\nrelevance of terms across documents, prioritizing those that \\nclosely match the query [18]. Fig. 1 demonstrates the detailed \\nRAG architecture. Advanced methods such as neural \\nretrievers, exemplified by Dense Passage Retrieval (DPR), \\nemploy deep learning models to encode documents into dense \\nembeddings, capturing semantic relationships and enhancing \\ncontextual understanding. \\n \\n \\n \\nFig. 1. RAG Architecture \\n \\nOnce relevant passages are identified, they undergo encoding \\ninto document embeddings. These embeddings encapsulate \\nthe semantic meaning and context of the retrieved text, \\nemploying techniques like sentence embeddings from models \\nsuch as the Universal Sentence Encoder or BERT [19]. These \\nembeddings serve as enriched inputs to the subsequent stage \\nof the RAG architecture. Integration with a generative model, \\ntypically an LLM such as GPT, marks the next critical phase. \\nThe generative model utilizes the contex tual information \\nembedded in the document embeddings to produce responses \\nthat are not only grammatically accurate and fluent but also \\ncontextually aligned with the user\\'s query [20]. By integrating \\ndetailed context from the embeddings, the generative mode l \\nensures that its responses are informed by both the broad \\nlinguistic knowledge it has learned during training and the \\nspecific details extracted from the retrieved passages. To \\nfurther refine performance, the RAG architecture often \\ninvolves fine -tuning t he generative model on task -specific \\ndatasets [21].  \\nIV. PROPOSED METHODOLOGY \\nA. Data Chunking \\nThe integration of the PyPDF2 library enables efficient text \\nextraction and management of PDF documents within the \\nmodel. Initially, a PdfReader object is created to represent the \\nentire PDF, facilitating seamless interaction with its content \\n[22]. \\n \\n \\nFig. 2. Workflow of the Proposed Model \\n \\nTo extract the raw text, the model iteratively traverses each \\npage using a loop, employing the extract_text() method. The \\nextracted texts are then consolidated into a single string \\nvariable, pdf_text, which captures the entire textual content \\nof the PDF. G iven the potentially large size of pdf_text, the \\nmodel implements text chunking to improve computational'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='efficiency. Equation (1) outlines the mathematical process for \\ntext chunking, where n represents the number of desired \\nchunks, chunk_size indicates the size of each chunk, and  \\nchunk_overlap defines the overlap between consecutive \\nchunks. \\n𝑖𝑛 = (𝑛 − 1) × (𝑐ℎ𝑢𝑛𝑘_𝑠𝑖𝑧𝑒 − 𝑐ℎ𝑢𝑛𝑘_𝑜𝑣𝑒𝑟𝑙𝑎𝑝)           (1) \\n \\nEquation (1) calculates the starting index for each chunk \\nbased on its position n, chunk size, and overlap, ensuring that \\neach chunk overlaps with the previous ones by a specified \\nnumber of characters. To determine the ending index for each \\nchunk, Equation (2) provides a formula where jn indicates the \\nending index of chunk n. \\n𝑗𝑛 = 𝑖𝑛 + 𝑐ℎ𝑢𝑛𝑘_𝑠𝑖𝑧𝑒                                                     (2) \\n \\nThis approach allows for the systematic division of large text \\ninto smaller segments, facilitating easier processing and \\nanalysis in natural language processing tasks such as \\ninformation retrieval, text summarization, and machine \\ntranslations. Each chunk is associated with metadata to enrich \\nthe context and facilitate easier retrieval of specific text \\nsegments. Metadata is organized as a list of dictionaries, with \\neach dictionary corresponding to a chunk in the text list. \\nTypically, metadata includes a key -value pair where the key \\ndenotes the origin or source identifier of the chunk within the \\nPDF signifies the page number, and \"pl\" denotes paragraph \\nlevel). This approach allows for precise tracking and retrieval \\nof information within the PDF document, enhan cing the \\nmodel’s capability to handle and manipulate textual content \\neffectively in various applications and user interactions. \\nB. Vector Embeddings For Efficient Retrieval \\nIn preparing text for efficient retrieval, the model utilizes \\nsentence embedding techniques to convert text chunks into \\nnumerical representations. This step is crucial for enabling \\nfast and accurate retrieval of semantically similar sentences \\nor passages f rom a document. Sentence embedding \\ntechniques are designed to map sentences from their original \\nhigh-dimensional textual space into a lower -dimensional \\nvector space. This transformation allows for efficient \\ncomparison and retrieval of sentences based on their semantic \\ncontent. A widely used approach for generating these \\nembeddings is employing pre -trained sentence transformers. \\nIn the model, the specific embedding used is “ sentence-\\ntransformers/all-MiniLM-L6-v2”. These models are trained \\non extensive text corpora and have learned to encapsulate the \\nsemantic essence of sentences within vector representations \\n[23]. The sentence embedding function is defined in Equation \\n(3) where S is a sentence composed of a sequence of words, \\nW is the embedding matrix for the vocabulary, and  f is the \\nsentence embedding function that maps a sentence S to a \\nvector 𝑠 ∈ 𝑅𝑑 . \\n \\n𝑠 = 𝑓(𝑠)                                                                            (3) \\n \\nThis function f often involves multiple steps, including word \\nembeddings, contextual embeddings using transformer \\nmodels, and aggregation methods. Each word 𝜔𝑖 in the \\nsentence S is mapped to a vector 𝑤i  using an embedding \\nmatrix W in Equation (4) where 𝑊[𝜔𝑖] \\n \\n𝑤𝑖 = 𝑊[𝜔𝑖]                                                                       (4)                                                                                                                                              \\n \\nTo obtain a single fixed -dimensional vector representing the \\nentire sentence, an aggregation method using mean pooling \\nis applied to the contextual embeddings. Equation (5) \\ncomputes the average of the contextual embeddings of all \\nwords in the sentence, res ulting in the final sentence \\nembedding  \\n \\n𝑆 =  \\n1\\n𝑛 ∑ ℎ𝑛\\n1                                                                          (5) \\n \\nThe vector S in Equation (5) is the sentence embedding, \\nwhich captures the meaning of the sentence in a way that \\nallows for efficient comparison and retrieval in natural \\nlanguage processing tasks. The model uses \\nHuggingFaceEmbedings class from langchain -\\ncommunity.embeddings module to work with the pre-trained \\nsentence transformer model. To load the model, the model \\nname is specified along with any extra configuration options. \\nThe embeddings object generates vector representations for \\neach text chunk using t he compute_embeddings method, \\nwhich takes a list of chunks as input and outputs \\ncorresponding embedding vectors that capture their meanings \\nin numerical form. These vectors are then combined with \\nmetadata, which includes information about the source of \\neach chunk within the PDF document. This integration results \\nin a final list of document representations optimized for \\nefficient retrieval. Consequently, the model can quickly and \\naccurately locate relevant passages in response to user \\nqueries, leveraging the meanings captured in the embeddings \\nalong with contextual details from the metadata. \\nC. Building The Conversational Retrieval Chain(CRC) \\n \\n This comprehensive approach involves several key \\ncomponents that synergize to deliver a seamless user \\nexperience. \\n \\n• Large Language Model  \\n \\nAt the heart of the CRC is the LLM, which generates \\nresponses to user queries. The model utilizes the Groq LLM \\n(llm_groq), integrated through the langchain_groq library. \\nThis pre -trained LLM leverages its extensive knowledge \\nbase, derived from vast amounts  of text data, to understand \\nand answer user questions accurately. The LLM\\'s capability \\nto generate coherent and contextually appropriate responses \\nmakes it a crucial component of the CRC. \\n \\n• Retriever \\n \\nThe retriever component is responsible for fetching relevant \\ninformation from the document based on the user\\'s query. \\nThe model employs the faiss library from \\nlangchain_community.vectorstores to create a vector store \\nusing the document embeddings generated  earlier. These \\nembeddings transform text chunks into numerical \\nrepresentations that capture their semantic content. The \\nvector store allows for efficient retrieval of document \\nsections (chunks) that are semantically similar to the user\\'s \\nquery. The as_ret riever method of the vector store object is'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='used to create a retriever object that integrates into the CRC, \\nenabling precise and relevant information retrieval. \\n \\n• Memory \\n \\nMemory management is essential for preserving \\nconversational context. The model employs the \\nConversationBufferMemory class from langchain.memory to \\nstore past user queries and LLM responses. This history is \\ncrucial for the LLM to reference prior interactio ns when \\ngenerating current responses. The memory is set up with \\nkeys: memory_key=\"chat_history\" for conversation history \\nand output_key=\"answer\" for the LLM\\'s responses. This \\nconfiguration facilitates more coherent and contextually \\naware interactions. \\n \\n• Chain Configuration \\n \\n To integrate these components, the \\nConversationalRetrievalChain.from_llm method is employed \\nwith specific parameters. The LLM parameter is configured \\nto utilize thr Groq LLM object(llm_groq). The chain_type is \\ndesignated as “stuff”, indicating a focus on retrieving factual \\ncontent from the document. The retriever parameter is linked \\nto the retriever object generated from the vector store,  \\nensuring efficient retrieval of relevant document section.  The \\nmemory parameter is set to conversation buffer memory \\nobject. Further, return_source_documents is True instructing \\nchains to return chunks along with responses. This ensures \\nthe accurate answers enriched with relevant context from the \\ndocuments.  \\nD. User Interaction And Model Response \\nThe model enables a natural and interactive conversation \\nbetween users and their uploaded PDF documents. The \\nprocess starts when users input their questions through a text \\nfield integrated into the Streamlit interface, ensuring that \\ninitiating queries is s traightforward and accessible. Users \\ntype their questions into the provided input field \\n(st.text_input), and upon submission, the system promptly \\ncaptures the query and processes it using the retrieval chain. \\nThe chain.invoke method efficiently directs the  query to \\nsubsequent stages of the workflow. \\nAt the core of the model\\'s functionality is the Conversational \\nRetrieval Chain. To generate contextually rich responses, the \\nchain first accesses the conversation history via the \\nConversationBufferMemory [25], which retains past user \\nqueries and responses,  ensuring that the current interaction \\nbenefits from previous exchanges. \\nSubsequently, the system utilizes a retriever that operates on \\na pre -constructed document vector store, comprising \\nembeddings of text chunks extracted from the PDF. The \\nretriever searches for document sections that are semantically \\nsimilar to the user\\'s query, using cosine similarity to evaluate \\nhow closely related two vectors are within the embedding \\nspace. Equation (6) illustrates the concept of cosine \\nsimilarity. \\n \\ncos(𝑢, 𝑣) =\\n𝑢.𝑣\\n(‖𝑢‖|‖𝑣‖)\\n                                                      (6) \\n \\nCosine similarity scores range from -1 (completely \\ndissimilar) to 1 (identical). The model retrieves the document \\nsections with the highest cosine similarity scores, indicating \\ntheir relevance to the user\\'s query. These retrieved sections, \\nalong with the c onversation context, are then fed into the \\nGroq LLM. By leveraging its pre -trained knowledge and the \\nspecific context from the retrieved text, the Groq LLM \\ngenerates comprehensive and accurate responses to user \\nquestions. \\n \\nWhen relevant document sections are retrieved, the model \\nenhances responses by referencing these sources. It assigns \\nunique identifiers to each retrieved section and may include \\nthese references in the response text. This method not only \\nensures transparen cy but also allows users to verify the \\ninformation\\'s origin. To improve usability, Streamlit \\nexpanders (st.expander) are used, enabling users to easily \\nview the content of the retrieved document sections. By \\nclicking on the corresponding source names, user s can \\nexpand and read the specific excerpts that informed the \\nLLM\\'s response. This interactive feature allows users to \\nexplore the document content more deeply, enhancing their \\nunderstanding and engagement with the material. \\nV. EXPERIMENTATIONS AND RESULTS \\nTo assess the model\\'s capability to navigate and summarize \\ncomplex academic materials, we employed ROUGE (Recall-\\nOriented Understudy for Gisting Evaluation) scores, a widely \\naccepted metric for evaluating the quality of automatically \\ngenerated summaries ag ainst human -written references. \\nHowever, relying solely on ROUGE metrics may not \\nadequately reflect the system\\'s interactive and conversational \\naspects. Therefore, future studies will include qualitative \\nevaluations to examine user interaction quality and the \\nmodel\\'s effectiveness from an end-user perspective, ensuring \\na comprehensive assessment of its performance. \\nThe evaluation utilized a carefully curated dataset comprising \\ntop-cited research articles, known for their dense information \\ncontent, technical language, and intricate methodologies. \\nThese articles posed significant challenges, making them \\nwell-suited for rigorously testing the model\\'s summarization \\ncapabilities. The article abstracts served as input reference \\nsummaries for calculating the ROUGE scores for each \\ndocument. This analysis provided valuable insights into the \\nmodel\\'s proficiency in accurately capturing and summarizing \\ncritical findings from highly technical and detailed research \\nliterature. ROUGE measures the overlap of n-grams between \\nthe generated text and the reference text, mathematically \\nrepresented in Equation (6), where the maximum number of \\nn-grams co -occurring in both the candidate and reference \\nsummaries is considered. \\n \\n𝑅𝑂𝑈𝐺𝐸 =  \\n∑ ∑ 𝐶𝑜𝑢𝑛𝑡_𝑚𝑎𝑡𝑐ℎ(𝑔𝑟𝑎𝑚𝑛)𝑔𝑟𝑎𝑚𝑛∈𝑆𝑆∈{𝑆𝑢𝑚𝑚𝑎𝑟𝑖𝑒𝑠}\\n∑ ∑ 𝐶𝑜𝑢𝑛𝑡(𝑔𝑟𝑎𝑚𝑛)𝑔𝑟𝑎𝑚𝑛∈𝑆𝑆∈{𝑆𝑢𝑚𝑚𝑎𝑟𝑖𝑒𝑠}\\n       (6) \\n \\nThe study specifically used ROUGE -1 (unigrams) and \\nROUGE-2 (bigrams) in the evaluation. ROUGE -L measures \\nthe longest common subsequence (LCS) between the \\ncandidate and reference summaries. It\\'s calculated using \\nEquations (7), and (8) followed by Equation (9), where X is \\nthe reference summary of length m, Y is the candidate'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content=\"summary of length n, and β is typically set to favor recall ( β \\n>1). \\n \\n𝑅𝑂𝑈𝐺𝐸 − 𝐿𝑅𝑒𝑐𝑎𝑙𝑙 =  \\n𝐿𝐶𝑆(𝑋,𝑌)\\n𝑚                                            (7) \\n𝑅𝑂𝑈𝐺𝐸 − 𝐿𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 =  \\n𝐿𝐶𝑆(𝑋,𝑌)\\n𝑛                                        (8) \\n𝑅𝑂𝑈𝐺𝐸 − 𝐿𝐹 =  \\n(1+𝛽2)∙𝑅∙𝑃\\n𝑅+𝛽2∙𝑃                                                (9) \\n \\nTo evaluate the model performance, the authors tested it with \\na custom dataset of various research papers sourced from top \\nresearch databases and analyzed the ROUGE scores of the \\ngenerated answers. The relatively moderate ROUGE scores \\ncan be attributed to the model’s focus on condensing \\nextensive content into concise responses. This indicates the \\nmodel’s tendency to prioritize brevity and specific ity over \\nword-for-word overlap.  The average representative scores \\nobtained from evaluating upon the dataset, are given in Table. \\nI. \\nTABLE I.  PERFORMANCE METRICS OF THE MODEL \\nPerformance Metric Score Values (Average) \\nROUGE – 1 0.4604 \\nROUGE – 2 0.3576 \\nROUGE - L 0.4283 \\n \\nThe scores indicate that approximately 46% of individual \\nwords (ROUGE -1) and around 35% of bigram phrases \\n(ROUGE-2) in the generated responses matched those found \\nin the original documents. The ROUGE -L score, which lies \\nbetween ROUGE -1 and ROUGE -2, demonst rates some \\npreservation of word order while accommodating gaps and \\nrephrasing. The relatively low ROUGE scores highlight the \\nsystem's capability to distill information into concise answers \\ninstead of replicating large text segments. Moreover, the \\ncomplexity and dense information structure of the input \\nresearch articles create s a high bar for any model aiming to \\nbalance conciseness with informativeness.  Good summaries \\noften rephrase ideas, leading to lower word-for-word matches \\nbut potentially better conveyance of key concepts. \\nFurthermore, the system focuses on providing specific \\nanswers to questions, naturally leading to lower overlap with \\nthe full tex t of the documents. Moreover, the significant \\nlength difference between focused answers and entire articles \\nfurther contributes to the lower ROUGE scores. Table. II \\ncompares the model performance with other SOTA \\napproaches. \\nTABLE II.  COMPARISON OF PROPOSED MODEL PERFORMANCE \\nMETRICS \\nModel ROUGE - 1 ROUGE - 2 ROUGE - L \\nRAG-PDF \\n (Our Model) \\n0.4604 0.3576 0.4283 \\nML + RL \\nROUGE + Novel, \\nWith LM  [26] \\n \\n0.4019 \\n \\n0.1738 \\n \\n0.3752 \\nCOSUM [27] 0.4908 0.2379 0.2834 \\nLatent Semantic \\nAnalysis [28] \\n0.4621 0.2618 0.3479 \\nEdgeSumm [29] 0.5379 0.2858 0.4979 \\nGenerative \\nAdversarial \\nNetwork  [30] \\n \\n0.3992 \\n \\n0.1765 \\n \\n0.3671 \\nTFRSP [31] 0.2483 0.2874 0.2043 \\nTable II presents a comparative analysis of various models \\nbased on ROUGE -1, ROUGE -2, and ROUGE -L scores, \\nwhich evaluate summary quality against reference \\nsummaries. The RAG -PDF model demonstrates strong \\nperformance, achieving a ROUGE -1 score of 0.4604, \\nROUGE-2 score of 0.3576, and ROUGE -L score of 0.4283, \\nindicating its effectiveness in capturing both individual words \\nand longer sequences for coherent summaries. \\nWhile EdgeSumm excels in ROUGE -1 and ROUGE -L, its \\nlower ROUGE -2 score reveals limitations in bigram \\ncoherence. Our model balances coherence, particularly in \\ncomplex technical text. In contrast, the ML + RL ROUGE + \\nNovel model shows poorer performance, espe cially in \\nROUGE-2 (0.1738) and ROUGE -L (0.3752), suggesting \\nchallenges in capturing bi -gram sequences. COSUM \\nperforms well in ROUGE -1 (0.4908) but lacks coherence in \\nlonger sequences with lower ROUGE -2 (0.2379) and \\nROUGE-L (0.2834). \\nLatent Semantic Analysis is comparable to our model in \\nROUGE-1 (0.4621) but falls short in ROUGE-2 (0.2618) and \\nROUGE-L (0.3479). The Generative Adversarial Network \\nmodel exhibits low scores across metrics, particularly in \\nROUGE-2 (0.1765). Lastly, the TFR SP model scores the \\nlowest in ROUGE -1 (0.2483) and ROUGE -L (0.2043), \\nindicating significant challenges in summary generation. \\n \\nWhile ROUGE metrics provide useful insights, they may not \\nfully capture user experience or interaction quality. \\nTherefore, future work will focus on incorporating user -\\ncentered evaluations, including qualitative feedback and \\ninteraction analysis, to align the system’s performance with \\nreal-world needs. \\nVI. CONCLUSION AND FUTURE WORK \\nThe model introduces a unique approach for interacting with \\nPDF documents via a conversational interface, harnessing the \\npower of LLMs and RAG. This system enables users to \\nextract valuable insights from complex and text -heavy \\nmaterials effectively. One of its standout features is its focus \\non the specific content of uploaded PDFs, rather than relying \\non extensive external knowledge bases. By employing \\nsentence embeddings, the model converts text chunks into \\nnumerical vectors and utilizes cosine similarity for efficient \\nretrieval, aligning responses closely with user intent. \\nPerformance evaluations reveal competitive ROUGE \\nscores—0.4604 for ROUGE -1, 0.3576 for ROUGE -2, and \\n0.4283 for ROUGE-L—demonstrating the model's capability \\nto capture essential content a nd structure while \\noutperforming many existing models in summarization and \\nquestion answering. \\nTo enhance the practical application of this system, future \\nwork will aim to generalize its approach for a wider variety \\nof document types. This will include refining the retrieval \\nmechanism to accommodate diverse structures, such as legal, \\nfinancial, and multimodal documents, thereby increasing the \\nsystem's versatility in real -world scenarios. We also plan to \\nincorporate reinforcement learning techniques to improve \\nuser interactions, allowing the model to adapt dynamically \\nbased on feedback. Exploring the incorporation of knowledge \\ngraphs and ontologies may also improve semantic \\nunderstanding and contextualization. Furthermore, refining \\nthe model with user interaction data and reinforcement\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='learning can facilitate more personalized responses, ensuring \\nthat the system continuously evolves to meet user needs \\neffectively. \\nREFERENCES \\n[1] Khurana, D., Koli, A., Khatter, K., & Singh, S. (2023). Natural \\nlanguage processing: State of the art, current trends, and challenges. \\nMultimedia Tools and Applications, 82(4), 3713–3744. \\n[2] Bahl, L. R., Brown, P. F., de Souza, P. V., & Mercer, R. L. (1989). A \\ntree-based statistical language model for natural language speech \\nrecognition. IEEE Transactions on Acoustics, Speech, and Signal \\nProcessing, 37(7), 1001–1008. \\n[3] Curto, G., Jojoa Acosta, M. F., & Comim, F. (2024). Are AI systems \\nbiased against the poor? A machine learning analysis using Word2Vec \\nand GloVe embeddings. AI & Society, 39(3), 617–632. \\n[4] Zheng, X., Zhang, C., & Woodland, P. C. (2021). Adapting GPT, GPT-\\n2, and BERT language models for speech recognition. In 2021 IEEE \\nAutomatic Speech Recognition and Understanding Workshop (ASRU) \\n(pp. 162–168). \\n[5] Qu, Y., Liu, P., Song, W., Liu, L., & Cheng, M. (2020). A text \\ngeneration and prediction system: Pre -training on new corpora using \\nBERT and GPT -2. In 2020 IEEE 10th International Conference on \\nElectronics Information and Emergency Communication (ICEIEC) \\n(pp. 323–326). \\n[6] Wang, L., Ma, C., & Feng, X. (2024). A survey on large language \\nmodel-based autonomous agents. Frontiers of Computer Science, 18, \\n186345.  \\n[7] Xu, L., Lu, L., Liu, M., & others. (2024). Nanjing Yunjin intelligent \\nquestion-answering system based on knowledge graphs and retrieval -\\naugmented generation technology. Heritage Science, 12, 118. \\n[8] Louis, A., van Dijck, G., & Spanakis, G. (2024). Interpretable Long -\\nForm Legal Question Answering with Retrieval -Augmented Large \\nLanguage Models. Proceedings of the AAAI Conference on Artificial \\nIntelligence, 38(20), 22266-22275. \\n[9] Yang, X., Chen, A., PourNejatian, N., & others. (2022). A large \\nlanguage model for electronic health records. npj Digital Medicine, 5, \\n194.  \\n[10] Li, M., Kilicoglu, H., Xu, H., & Zhang, R. (2024). BiomedRAG: A \\nretrieval-augmented large language model for biomedicine. ArXiv: \\nComputation and Language. \\n[11] Hiesinger, W., Zakka, C., Chaurasia, A., Shad, R., Dalal, A., Kim, J., \\nMoor, M., Alexander, K., Ashley, E., Boyd, J., Boyd, K., Hirsch, K., \\nLanglotz, C., & Nelson, J. (2023). Almanac: Retrieval -augmented \\nlanguage models for clinical medicine. Research Square.  \\n[12] Yang, K., Swope, A., Gu, A., Chalamala, R., Song, P., Yu, S., Godil, \\nS., Prenger, R. J., & Anandkumar, A. (2023). LeanDojo: Theorem \\nproving with retrieval -augmented language models. In Advances in \\nNeural Information Processing Systems 36 (NeurIPS 2023). \\n[13] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., \\nKüttler, H., Lewis, M., Yih, W., Rocktäschel, T., Riedel, S., & Kiela, \\nD. (2020). Retrieval -augmented generation for knowledge -intensive \\nNLP tasks. In Advances in Neural Information Processing Systems 33 \\n(NeurIPS 2020). \\n[14] Feng, Z., Feng, X., Zhao, D., Yang, M., & Qin, B. (2024). Retrieval -\\ngeneration synergy augmented large language models. In ICASSP \\n2024 – IEEE International Conference on Acoustics, Speech and Signal \\nProcessing (pp. 11661–11665).  \\n[15] Miao, J., Thongprayoon, C., Suppadungsuk, S., Garcia Valencia, O., & \\nCheungpasitporn, W. (2024). Integrating retrieval -augmented \\ngeneration with large language models in nephrology: Advancing \\npractical applications. Medicina.  \\n[16] Hao, T., Li, X., He, Y., & others. (2022). Recent progress in leveraging \\ndeep learning methods for question answering. Neural Computing & \\nApplications, 34, 2765–2783.  \\n[17] Zhang, Y., Qian, S., Fang, Q., & Xu, C. (2019). Multi -modal \\nknowledge-aware hierarchical attention network for explainable \\nmedical question answering. In Proceedings of the 27th ACM \\nInternational Conference on Multimedia (pp. 1178–1187). \\n[18] Sawarkar, K., Mangal, A., & Solanki, S. R. (2024). Blended RAG: \\nImproving RAG (Retriever -Augmented Generation) accuracy with \\nsemantic search and hybrid query-based retrievers. ArXiv: Information \\nRetrieval.  \\n[19] Arif, N., Latif, S., & Latif, R. (2021). Question classification using \\nUniversal Sentence Encoder and deep contextualized transformer. In \\n2021 14th International Conference on Developments in eSystems \\nEngineering (DeSE) (pp. 206–211).  \\n[20] Goswami, M., Panda, N., Mohanty, S., & Pattnaik, P. K. (2023). \\nMachine learning techniques and routing protocols in 5G and 6G \\nmobile network communication system – An overview. In 2023 7th \\nInternational Conference on Trends in Electronics and Informatics \\n(ICOEI) (pp. 1094–1101).  \\n[21] Li, H., Su, Y., Cai, D., Wang, Y., & Liu, L. (2022). A survey on \\nretrieval-augmented text generation. ArXiv: Computation and \\nLanguage. \\n[22] Bui, D. D. A., Del Fiol, G., & Jonnalagadda, S. (2016). PDF text \\nclassification to leverage information extraction from publication \\nreports. Journal of Biomedical Informatics, 61, 141–148.  \\n[23] Heimerl, F., Kralj, C., Möller, T., & Gleicher, M. (2022). embComp: \\nVisual interactive comparison of vector embeddings. IEEE \\nTransactions on Visualization and Computer Graphics, 28(8), 2953 –\\n2969.  \\n[24] Goswami, M., Mohanty, S., & Pattnaik, P. K. (2024). Optimization of \\nmachine learning models through quantization and data bit reduction \\nin healthcare datasets. Franklin Open, 8. \\n[25] Singh, A., Ehtesham, S., Mahmud, R., & Kim, J. -H. (2024). \\nRevolutionizing mental health care through LangChain: A journey with \\na large language model. In 2024 IEEE 14th Annual Computing and \\nCommunication Workshop and Conference (CCWC) (pp. 73–78).  \\n[26] Kryściński, W., Paulus, R., Xiong, C., & Socher, R. (2018). Improving \\nabstraction in text summarization. ArXiv: Computation and Language.  \\n[27] Alguliyev, R. M., Aliguliyev, R. M., Isazade, N. R., Abdi, A., & Idris, \\nN. B. (2018). COSUM: Text summarization based on clustering and \\noptimization. Expert Systems, 36.  \\n[28] Ozsoy, M. G., Alpaslan, F. N., & Cicekli, I. (2011). Text \\nsummarization using latent semantic analysis. Journal of Information \\nScience, 37(4), 405–417.  \\n[29] El-Kassas, W. S., Salama, C. R., Rafea, A. A., & Mohamed, H. K. \\n(2020). EdgeSumm: Graph -based framework for automatic text \\nsummarization. Information Processing & Management, 57(6).  \\n[30] Liu, L., Lu, Y., Yang, M., Qu, Q., Zhu, J., & Li, H. (2018). Generative \\nadversarial network for abstractive text summarization. Proceedings of \\nthe AAAI Conference on Artificial Intelligence, 32(1). \\n[31] M. S. M., R. M. P., A. R. E., & E. S. G. S. R. (2020). Text \\nsummarization using text frequency ranking sentence prediction. In \\n2020 4th International Conference on Computer, Communication and \\nSignal Processing (ICCCSP) (pp. 1–6).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 0, 'page_label': '1', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\nLANG MEI, Huawei Cloud BU, China\\nSIYU MO, Huawei Cloud BU, China\\nZHIHAN YANG, Huawei Cloud BU, China\\nCHONG CHEN∗, Huawei Cloud BU, China\\nMultimodal Retrieval-Augmented Generation (MRAG) represents a significant advancement in enhancing the\\ncapabilities of large language models (LLMs) by integrating multimodal data, such as text, images, and videos,\\ninto the retrieval and generation processes. Traditional Retrieval-Augmented Generation (RAG) systems, which\\nprimarily rely on textual data, have shown promise in reducing hallucinations and improving response accuracy\\nby dynamically incorporating external knowledge. However, these systems are limited by their reliance on\\ntext-only modalities, which restricts their ability to leverage the rich, contextual information available in\\nmultimodal data. MRAG addresses this limitation by extending the RAG framework to include multimodal\\nretrieval and generation, thereby enabling more comprehensive and contextually relevant responses. In MRAG,\\nthe retrieval step involves locating and integrating relevant knowledge from diverse modalities, while the\\ngeneration step utilizes multimodal large language models (MLLMs) to produce answers that incorporate\\ninformation from multiple data types. This approach not only enhances the quality of question-answering\\nsystems but also significantly reduces the incidence of hallucinations by grounding responses in factual,\\nmultimodal knowledge. Recent research has demonstrated that MRAG outperforms traditional text-modal\\nRAG, particularly in scenarios where visual and textual information are both critical for understanding and\\nresponding to queries. This survey systematically reviews the current state of MRAG research, focusing\\non four key aspects: essential components and technologies, datasets, evaluation methods and metrics, and\\nexisting limitations. By analyzing these dimensions, we aim to provide a comprehensive understanding of\\nhow MRAG can be effectively constructed and improved. Additionally, we highlight current challenges and\\npropose future research directions, encouraging further exploration into this promising paradigm. Our work\\nunderscores the potential of MRAG to revolutionize multimodal information retrieval and generation, offering\\na forward-looking perspective on its development and applications.\\nCCS Concepts: • Information systems →Multimedia and multimodal retrieval ; Language models; •\\nComputing methodologies →Natural language processing .\\nAdditional Key Words and Phrases: Multimodal Retrieval-Augmented Generation, Multimodal Large Language\\nModel, Multimodal Document Parsing and Indexing, Multimodal Search Planning\\nACM Reference Format:\\nLang Mei, Siyu Mo, Zhihan Yang, and Chong Chen. 2018. A Survey on Multimodal Retrieval-Augmented\\nGeneration. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email\\n(Conference acronym ’XX). ACM, New York, NY, USA, 80 pages. https://doi.org/XXXXXXX.XXXXXXX\\n∗Chong Chen is the corresponding author.\\nAuthors’ Contact Information: Lang Mei, Huawei Cloud BU, Beijing, China, meilang1@huawei.com; Siyu Mo, Huawei Cloud\\nBU, Beijing, China, mosiyu@huawei.com; Zhihan Yang, Huawei Cloud BU, Beijing, China, yangzhihan4@huawei.com;\\nChong Chen, Huawei Cloud BU, Beijing, China, chenchong55@huawei.com.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\\nConference acronym ’XX, Woodstock, NY\\n© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 978-1-4503-XXXX-X/2018/06\\nhttps://doi.org/XXXXXXX.XXXXXXX\\n, Vol. 1, No. 1, Article . Publication date: April 2018.\\narXiv:2504.08748v1  [cs.IR]  26 Mar 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 1, 'page_label': '2', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='2 Trovato et al.\\n1 Introduction\\nLarge language models (LLMs), especially the Transformer-based variants, have achieved extraor-\\ndinary success in many language-related tasks. Through pre-training on extensive, high-quality\\ninstruction datasets, LLMs can learn a wide range of language patterns, structures, and factual\\nknowledge. These pre-trained LLMs can generate human-like text with high degrees of fluency and\\ncoherence, and attain strong performance on question-answering tasks, which demonstrates their\\nability to understand and respond to a wide range of queries. However, despite their impressive\\ncapabilities, LLMs still face significant limitations. One of the primary challenges lies in their\\nperformance within specific domains or knowledge-intensive tasks. While these models are often\\ntrained on diverse and extensive datasets, such datasets may not cover the depth of knowledge\\nrequired for highly specialized fields or real-time information updates. This can be particularly\\nproblematic in areas like medicine, law, finance, and other technical fields where precision and\\nup-to-date knowledge are to be prioritized. When handling queries that extend beyond the scope\\nof their training knowledge or require the most current information, LLMs may generate responses\\nthat are speculative or based on patterns they have learned, rather than on verified facts. This\\ncan result in misleading, incorrect, or even entirely fabricated answers, a phenomenon known as\\n\"hallucination\". Minimizing the incidence of hallucinations is important for enhancing the reliability\\nof LLMs in providing accurate and context-relevant information across different domains.\\nRecently, Retrieval-Augmented Generation (RAG) has emerged as an effective solution to mitigate\\nhallucinations, by enhancing the generation capabilities of large language models (LLMs) through\\nthe retrieval of relevant external knowledge. Existing RAG systems typically operate through\\na two-step process: retrieval and generation. In the retrieval step, the goal is to quickly locate\\nrelevant knowledge that is semantically similar to the query from a large-scale document collection.\\nSince the relevant knowledge is often scattered across various parts of documents, each document\\nis pre-processed into multiple chunks. Additional chunks may be created through manual or\\nautomated methods. This process, known as document chunkerization, ensures that fine-grained\\nknowledge can be retrieved more efficiently. In the generation step, the retrieved document chunks\\nare combined with the query to form an augmented input. This augmented input provides the LLM\\nwith context that includes external knowledge. Furthermore, RAG allows LLMs to dynamically\\nintegrate the latest information during the inference stage. This capability ensures that the model’s\\nresponses are not only based on static, pre-trained knowledge but are continuously updated\\nwith current and relevant data. By retrieving and referencing external knowledge, RAG grounds\\nthe generated responses in factual information, thereby significantly reducing the occurrence of\\nhallucinations. However, previous research on RAG systems has primarily focused on knowledge\\nbases built from plain text and LLMs pre-trained on plain text, ignoring other rich sources of\\nknowledge available for query responses in the real world, such as videos and images, referred to\\nas \"multimodal data\".\\nMultimodal data refers to data that comes from multiple sources or formats. This can include text,\\nimages, audio, video, and other types of data. In real-world scenarios, humans naturally interact\\nwith multimodal data, such as browsing web pages that combine text, images, and videos in mixed\\nlayouts. By analyzing images or videos alongside text, the user can better understand the context\\nof the content, and thus improve the satisfaction with the quality of the answers. For example, if\\na passenger inquires about how to store luggage while flying, it will be clearer that the system\\nprovides relevant graphic guides or instructional videos. However, transferring the capabilities\\nof LLMs to the domain of multimodal text and images remains an active area of research, as\\nplain-text LLMs are typically trained only on textual corpora and lack perceptual abilities for visual\\nsignals. How to effectively incorporate multimodal data is important to enhance the capability of\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 2, 'page_label': '3', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 3\\nLLMs. In recent years, the development of multimodal generative models has showcased additional\\napplication possibilities. Apart from textual generative models, multimodal generative models\\nhave been increasingly applied in fields such as human-computer interaction, robot control, image\\nsearch, and speech generation. Similarly, based on multimodal generative models and multimodal\\ndata, how to effectively process Multimodal Retrieval-Augmented Generation (MRAG) is an issue\\nthat needs to be explored.\\nRecently, some research have demonstrated that MRAG with multimodal data outperforms\\ntraditional text-modal RAG. By enhancing the generation capabilities of multimodal large language\\nmodels (MLLMs) through the retrieval of external multimodal knowledge, MRAG system can\\nfurther enhance question answering capabilities and quality, thereby further reducing hallucination\\nissues. The main differences between text-modal RAG and MRAG lie in retrieval and generation. In\\nthe retrieval step, the former only needs to consider retrieving relevant textual knowledge from\\na large document collection, while the latter needs to consider how to retrieve and integrate the\\nrelevant knowledge under different modalities, as well as the relationships between knowledge in\\ndifferent modalities. In the generation step, the former only needs to consider the input text query\\nand relevant textual knowledge, and output a text answer based on the LLM. The latter, however,\\nneeds to consider how to utilize the input query from different modalities and multimodal retrieval\\nknowledge, and output an answer that includes information from different modalities based on the\\nMLLM.\\nConsidering the immense potential of MRAG in this field, this survey aims to systematically\\nreview and analyze the current state and main challenges of MRAG. We discuss existing research\\nfrom several key perspectives: 1) What important components and technologies are involved in\\nMRAG? 2) What types of datasets can be used for the evaluation of MRAG? 3) What methods and\\nmetrics are used to evaluate MRAG? 4) What limitations exist in the different aspects of MRAG? We\\nexplore the main challenges faced by MRAG, and hope to provide clearer guidelines for their future\\ndevelopment. In summary, the main contributions of this paper are as follows:\\n•Comprehensive and Timely Survey: We conducted an extensive survey on the emerging\\nparadigm of multimodal Retrieval-Augmented Generation, systematically reviewing the current\\nstate of research and development in this field.\\n•Systematic Analysis from Four Key Perspectives: Our survey is organized around four key\\naspects: essential components and technologies, datasets, evaluation methods and metrics, and\\nlimitations. This structured approach allows for a detailed understanding of how MRAG can be\\nefficiently constructed, its reliability issues, and how it can be further improved.\\n•Current Challenges and Future Research Directions: We discuss the existing challenges of\\nMRAG, highlight potential research opportunities and directions, and provide a forward-looking\\nperspective on the future development of this paradigm, encouraging researchers to delve deeper\\ninto this exciting field.\\nWe have provided an overall introduction to this survey paper. The section 2 presents a compre-\\nhensive overview of multimodal retrieval-augmented generation, covering multiple developmental\\nstages. The section 3 delves into the technical details of multimodal retrieval-augmented generation,\\nfocusing on key components such as multimodal retrieval, multimodal generation, etc. In section 4,\\nwe discuss how to comprehensively evaluate multimodal retrieval-augmented generation systems\\nusing datasets, including specialized assessments for different competency areas. The section 5\\nintroduces relevant metrics for evaluating multimodal retrieval-augmented generation systems.\\nIn section 6, we outline the current technical challenges associated with multimodal retrieval-\\naugmented generation. In section 7, based on previous investigation of MRAG, we summarize\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 3, 'page_label': '4', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='4 Trovato et al.\\nfuture work in this field, and provide some suggestions. Finally, we give the conclusion of the paper\\nin section 8.\\n2 Overview of MRAG\\nMultimodal Retrieval-Augmented Generation (MRAG) represents a significant evolution of the\\ntraditional Retrieval-Augmented Generation (RAG) framework, building upon its foundational\\nstructure while extending its capabilities to process diverse data modalities. While RAG is limited\\nto processing plain text, MRAG integrates multimodal data, including images, audio, video, and\\ntext, enabling it to address more complex and diverse real-world applications where information\\nspans multiple modalities.\\nIn the early stages of MRAG development, researchers converted multimodal data into unified\\ntextual representations. This approach allowed for a seamless transition from RAG to MRAG\\nby leveraging existing text-based retrieval and generation mechanisms. Although this strategy\\nsimplified multimodal data integration and improved the end-to-end user experience, it introduced\\nsignificant limitations. For instance, the conversion process often resulted in the loss of modality-\\nspecific information, such as visual details in images or tonal nuances in audio, restricting the\\nsystem’s ability to fully exploit the potential of multimodal inputs. Subsequent research has focused\\non addressing these limitations by developing more advanced methods to optimize MRAG systems.\\nThese advancements have substantially enhanced MRAG’s performance and versatility, achieving\\nstate-of-the-art results across various multimodal tasks. This paper categorizes the evolution of\\nMRAG into three distinct stages:\\n2.1 MRAG1.0\\nThe initial stage of the MRAG framework, commonly termed \"pseudo-MRAG\", emerged as a\\nstraightforward extension of the highly successful RAG paradigm. This stage was rapidly adopted\\ndue to its adherence to RAG’s core principles, with modifications to support multimodal data. As\\nillustrated in Figure 1, the MRAG1.0 architecture consists of three key components: Document\\nParsing and Indexing, Retrieval, and Generation.\\n•Document Parsing and Indexing: This component is responsible for processing multimodal\\ndocuments in formats such as Word, Excel, PDF, and HTML. It extracts textual content using\\nOptical Character Recognition (OCR) or format-specific parsing techniques. A document layout\\ndetection model is then utilized to segment the document into structured elements, including\\ntitles, paragraphs, images, videos, tables, and footers. For textual content, a chunking strategy is\\napplied to segment or group semantically coherent passages. For multimodal data, specialized\\nmodels are used to generate captions describing images, videos, and other non-textual elements.\\nThese chunks and captions are encoded into vector representations using an embedding model\\nand stored in a vector database. The choice of embedding model is crucial, as it significantly\\nimpacts the performance and effectiveness of downstream retrieval tasks.\\n•Retrieval: This component processes user queries by encoding them into vector representations\\nusing the same embedding model applied during indexing. The query vectors are then utilized\\nto retrieve the top- 𝑘 most relevant chunks and captions from the vector database, typically\\nemploying cosine similarity as the relevance metric. Duplicate or overlapping information from\\nchunks and captions is merged to create a consolidated set of external knowledge, which is\\nsubsequently integrated into the prompt for the generation phase. This ensures the system\\nretrieves contextually relevant information to deliver accurate and informed responses.\\n•Generation: In the Generation phase, the MRAG system synthesizes the user’s query and\\nretrieved documents into a coherent prompt. A large language model (LLM) generates a response\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 4, 'page_label': '5', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 5\\nDocuments Parsing\\n(Extractive-Based, Plain Text)\\nTable \\nCaptions\\nGeneration Retrieval\\nIndexing\\nDocuments\\n……\\nText\\nText Parsing  \\nModel\\nImage Caption \\nModel\\nMultimodal Data\\nImage\\nTable\\n……\\nTable Parsing \\nModel\\nText Chunks\\nImage \\nCaptions\\nText Embedding \\nModel\\nText \\nVector DB\\nRelevant Text \\nChunks\\nRelevant Image \\nCaptions\\nPrompt\\nLLMs\\nQuery + History\\n（Text Only）\\nOCR-Based \\nModel\\nText Embedding \\nModel\\nAnswer\\n（Text Only）\\nFig. 1. The architecture of MRAG1.0, often termed \"pseudo-MRAG\", closely resembles traditional RAG,\\nconsisting of three modules: Document Parsing and Indexing, Retrieval, and Generation. While the overall\\nprocess remains largely unchanged, the key distinction lies in the Document Parsing stage. In this stage,\\nspecialized models are employed to convert diverse modal data into modality-specific captions. These captions\\nare then stored alongside textual data for utilization in subsequent stages.\\nby integrating its parametric knowledge with the retrieved external information. This approach\\nenhances response accuracy and timeliness, particularly in domain-specific contexts, while\\nreducing the risk of hallucinations common in LLM outputs. In multi-turn dialogues, the system\\nincorporates conversational history into the prompt, enabling contextually aware and seamless\\ninteractions.\\nDespite its initial success, MRAG1.0 exhibited several notable limitations that constrained its\\neffectiveness:\\n•Cumbersome Document Parsing: Converting multimodal data into textual captions introduced\\nsubstantial complexity to the system. This necessitated distinct models for processing different\\ndata modalities, increasing both computational overhead and system intricacy. Additionally,\\nthe conversion process frequently often to multimodal information loss. For instance, image\\ncaptions typically provided only coarse-grained descriptions, failing to capture fine-grained\\ndetails essential for accurate retrieval and generation.\\n•Bottleneck of Retrieval: While text vector retrieval technology is well-established, MRAG1.0\\nencountered challenges in achieving high recall accuracy. Similar to traditional RAG, the chunking\\nstrategy for text segmentation often fragmented keywords, making some content irretrievable.\\nAdditionally, transforming multimodal data into text, while enabling non-textual data retrieval,\\nintroduced additional information loss. These issues collectively created a bottleneck, limiting\\nthe system’s ability to retrieve comprehensive and accurate information.\\n•Challenges in Generation: Unlike traditional RAG, MRAG1.0 required processing not only\\ntext chunks but also image captions and other multimodal data. Effectively organizing these\\ndiverse elements into coherent prompts while minimizing redundancy and preserving relevant\\ninformation posed a significant challenge. Additionally, the \"Garbage In, Garbage Out\" (GIGO)\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 5, 'page_label': '6', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='6 Trovato et al.\\nIndexingDocuments Parsing\\n(Extractive-Based, Multimodal)\\nGeneration Retrieval\\nDocuments\\n……\\nText\\nText Parsing  \\nModel\\nMultimodal Data\\nImage\\nTable\\n……\\nText Chunks\\nText Embedding \\nModel\\nText Vector \\nDB\\nText Only \\nPrompt\\nLMMs\\nQuery + History\\n(Text with Multimodal Data)\\nOCR-Based \\nModel\\nText Embedding \\nModel\\nAnswer\\n(Text Only)\\nMultimodal  \\nEmbedding Model\\nMLLMs\\n Multimoda \\nData Captions\\nMultimodal \\nEmbedding Model\\nRelevant \\nPlain Text  Data\\n……\\nText Chunks\\nImage \\nCaptions\\nRelevant \\nMultimodal Data\\n……\\nTable\\nImage\\nMultimodal \\nVector DB\\nText Only？\\nMultimodal \\nPrompt\\nY\\nN\\nMLLMs\\nFig. 2. The architecture of MRAG2.0 retains multimodal data through document parsing and indexing, while\\nintroducing multimodal retrieval and MLLMs for answer generation, truly entering the multimodal era.\\nprinciple highlighted the sensitivity of LLMs to input quality. Information loss during parsing\\nand retrieval increased the risk of incorporating irrelevant data, compromising the robustness\\nand reliability of the generated responses.\\nThe limitations of MRAG1.0 created a performance ceiling, highlighting the need for more advanced\\ntechnological solutions. The system’s reliance on text-based representations for multimodal data,\\nalong with inherent challenges in retrieval and generation, revealed critical gaps in multimodal\\nunderstanding, retrieval efficiency, and generation robustness. Subsequent iterations of MRAG\\nmust address these issues by adopting more sophisticated models, enhancing information retention\\nduring parsing, and improving the integration of multimodal data into retrieval and generation\\nprocesses.\\n2.2 MRAG2.0\\nWith the rapid evolution of multimodal technologies, MRAG has transitioned into a \"true mul-\\ntimodal\" era, termed MRAG2.0. Unlike its predecessor MRAG1.0, MRAG2.0 not only supports\\nuser queries with multimodal inputs but also preserves the original multimodal data within the\\nknowledge base. By leveraging the capabilities of MLLMs, the generation module can now process\\nmultimodal data directly, minimizing information loss during data conversion. As illustrated in\\nFigure 2, the MRAG2.0 architecture incorporates several key optimizations:\\n•MLLMs Captions: The representational capabilities of MLLMs have significantly advanced, es-\\npecially in captioning tasks. MRAG2.0 leverages a single, unified MLLM—or multiple MLLMs—to\\nextract captions from multimodal documents. This approach replaces the conventional paradigm\\nof using separate models for different modalities, simplifying the document parsing module and\\nreducing its complexity.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 6, 'page_label': '7', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 7\\n•Multimodal Retrieval: MRAG2.0 enhances its retrieval module to support multimodal user\\ninputs by preserving original multimodal data and enabling cross-modal retrieval. This allows text-\\nbased queries to directly retrieve relevant multimodal data, combining caption-based recall with\\ncross-modal search capabilities. The dual retrieval approach enriches data sources for downstream\\ntasks while minimizing data loss, improving accuracy and robustness for downstream tasks.\\n•Multimodal Generation: To fully leverage original multimodal data, the generation module\\nin MRAG2.0 has been enhanced by integrating MLLMs, enabling the synthesis of user queries\\nand retrieval results into a coherent prompt. When retrieval results are accurate and the input\\ncomprises original multimodal data, the generation module mitigates information loss typically\\nassociated with modality conversion. This enhancement has significantly improved the accuracy\\nof question-answering (QA) tasks, especially in scenarios involving interrelated multimodal data.\\nDespite these advancements, MRAG2.0 encounters several emerging challenges: 1) Integrating\\nmultimodal data inputs may reduce the accuracy of traditional textual query descriptions. Further-\\nmore, current multimodal retrieval capabilities remain inferior to text-based retrieval, potentially\\nlimiting the overall accuracy of the retrieval module. 2) The diversity of data formats presents new\\nchallenges for the generation module. Efficiently organizing these diverse data forms and clearly\\ndefining inputs for generation are critical areas requiring further exploration and prioritization.\\n2.3 MRAG3.0\\nAs illustrated in Figure 3, the MRAG3.0 system represents a significant evolution from its predeces-\\nsors, introducing structural and functional innovations that enhance its capabilities across multiple\\ndimensions. This new paradigm shift is characterized by three key advancements: 1) Enhanced\\nDocument Parsing: A novel approach retains document page screenshots during parsing, minimiz-\\ning information loss in database storage. 2) True End-to-End Multimodality: While earlier versions\\nemphasized multimodal capabilities in knowledge base construction and system input, MRAG3.0\\nintroduces multimodal output capabilities, completing the end-to-end multimodal framework. 3)\\nScenario Expansion: Moving beyond traditional focus on understanding capabilities—primarily ap-\\nplied in VQA (Visual Question Answering) scenarios reliant on knowledge bases, the new paradigm\\nintegrates understanding and generation capabilities through module adjustments and additions.\\nThis unification significantly broadens the system’s applicability. In the following sections, we will\\ndetail the scenarios supported by MRAG3.0 and the specific module modifications enabling these\\nadvanced capabilities.\\n2.3.1 Scenario for MRAG.\\n•Retrieval-Augmented Scenario: This scenario addresses cases where LLMs or MLLMs alone\\ncannot adequately answer user queries. MRAG3.0 retrieves relevant content from external\\nknowledge bases to provide accurate answers, leveraging its enhanced retrieval capabilities.\\n•VQA Scenario: This scenario serves as a critical test for evaluating the fundamental capabilities\\nof MLLMs, which generate responses directly from user inputs containing text and multimodal\\nqueries without retrieval. The new MRAG paradigm introduces a search planning module,\\nenabling dynamic routing and retrieval to minimize unnecessary searches and the inclusion of\\nirrelevant information.\\n•Multimodal Generation Scenario: This primarily pertains to multimodal generation tasks,\\nsuch as text-to-image or text-to-video generation. While the original MRAG framework primarily\\naddressed understanding tasks, the new MRAG paradigm extends its capabilities by modifying\\nmultiple generation modules, unifying the solutions for both understanding and generation\\ntasks within a single framework. Following integration, the generation scenarios are further\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 7, 'page_label': '8', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='8 Trovato et al.\\nIndexing\\nDocuments Parsing\\nRepresentation-based Module\\nGeneration\\nRetrieval\\nMultimodal\\nSearch Planning\\nDocuments\\n……\\nExtractive-Based Module\\n（Same with MRAG2.0）\\nDocuments \\nScreenShots \\nEmbedding Model\\nText Embedding \\nModel\\nMultiModal \\nEmbedding Model\\nText \\nVector DB\\nMultimodal \\nVector DB\\nRelevant \\nPlain Text  Data\\n……\\nText Chunks\\nImage Captions\\nRelevant \\nMultimodal Data\\n……\\nTable\\nImage\\nRelevant \\nDocuments \\nScreenshots\\nText Embedding \\nModel\\nMultiModal \\nEmbedding Model\\nDocuments \\nScreenShoots \\nEmbedding Model\\nRetrieval \\nClassification\\nQuery Reformulation\\nNeed Search？\\nQuery + History\\n（Text with Multimodal data）\\nNew Query\\nText Only?\\nText Only?\\nText Only Prompt\\n(No search)\\nMultimodal Prompt\\n(No search)\\nText Only Prompt\\n(With search)\\nMultimodal Prompt\\n(With search)\\nLMMs\\nMLLMs\\nDocuments \\nScreenShots \\nVector DB\\nY\\nN\\nY\\nN\\nMultimodal Answer\\nDocuments \\nScreenshots\\nText Only?\\nAugmented \\nMultimodal Output\\nPosition identification \\nCandidate set Retrieval\\nMatching and Insertion\\nY\\nN\\nN\\nY\\nFig. 3. MRAG3.0 architecture integrates document screenshots during the document parsing and indexing\\nstages to minimize information loss. At the input stage, it incorporates a Multimodal Search Planning module,\\nunifying Visual Question Answering (VQA) and Retrieval-Augmented Generation (RAG) tasks while refining\\nuser query precision. At the output stage, the Multimodal Retrieval-Augmented Composition module enhances\\nanswer generation by transforming plain text into multimodal formats, thereby enriching information delivery.\\nenhanced by Retrieval-Augmentation (RA), which significantly improves the overall performance\\nof generation tasks (see Figure 4).\\n•Fusion Multimodal Output Scenario: This scenario is distinct from those previously men-\\ntioned but represents a significant aspect of the new paradigm, warranting separate discussion. In\\ntraditional settings, the final output is typically a plain text response. However, the new paradigm\\nenhances the generation module to produce outputs that integrate multiple modalities within a\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 8, 'page_label': '9', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 9\\nPlease create a pixel art illustration of Huawei \\nStream Back Slope Village with whimsical fairy tale \\nelements?\\nAccording to the user‘s description, the generated \\npictures are as follows.\\nTraditional Multimodal Generation\\nIt’s not Huawei Stream Back Slope Village\\nPlease create a pixel art illustration of Huawei \\nStream Back Slope Village with whimsical fairy tale \\nelements?\\nI have found these pictures related to Huawei \\nStream Back Slope Village.\\nMultimodal Generation with MRAG\\nGood!\\nThe final generated pictures are as follows.\\nFig. 4. The user aims to generate images depicting \"Huawei Stream Back Slope Village. \" Due to the location’s\\nobscurity and the model’s limited knowledge, it may produce inaccurate representations, such as images of\\nhouses by a stream. By integrating retrieval-augmented capabilities, the model can access relevant information\\nbeforehand, enabling the generation of precise and contextually accurate images.\\nsingle response (e.g., combining text, images, or videos). This can be further categorized into\\nthree sub-scenarios (see Figure 5).\\n– Multimodal Data is Answer: The query can be answered directly through multimodal data\\nwithout any text, as the adage \"a picture is worth a thousand words\" suggests.\\n– Multimodal Data Enhances Accuracy: The integration of multimodal data enhances the\\naccuracy of responses, particularly in instructional contexts such as \"How to register for\\na Gmail account. \". By generating answers that interweave text and image, users can more\\neffectively comprehend and follow the required operations.\\n– Multimodal Data Enhances Richness: While multimodal data is not essential, its inclusion\\ncan significantly enhance user experience. For instance, when responding to a query such\\nas \"Please introduce the Eiffel Tower. \", supplementing the textual explanation with relevant\\nimages or a brief video can offer users a more engaging and visually enriched experience.\\n2.3.2 Modified Modules.\\n•Documents Parsing and Indexing: To minimize information loss and enhance the accuracy\\nof document retrieval, the document parsing and indexing module has been upgraded with\\ninnovative technologies. This new approach preserves document screenshots during parsing,\\naddressing the information loss issues inherent in previous methods. By utilizing fine-tuned\\nMLLMs, the system vectorizes and indexes these document screenshots, enabling efficient\\nretrieval of relevant document screenshots based on user queries. This optimization not only\\nimproves the reliability of the knowledge base but also paves the way for advanced multimodal\\nretrieval capabilities.\\n•Generation: Previously, the generation module relied exclusively on large models with under-\\nstanding capabilities. The new paradigm integrates large models with generation capabilities,\\nunifying reasoning and generation scenarios at the system architecture level. Additionally, by\\nincorporating a multimodal output enhancement submodule, it facilitates a shift from text-based\\nanswers to mixed multimodal answers. The implementation methods can be categorized into\\ntwo types:\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 9, 'page_label': '10', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content=\"10 Trovato et al.\\nYou can learn the route of Columbus's voyage through \\nthe video below.\\n1: Place the eggs and a small amount of salt into a container, \\nand mix well.\\n2: Add warm water and continue to stir, then strain it.\\n3: Steam for 10 minutes, drizzle with sesame oil. It's done!\\nWatch the video below for more detailed instructions.\\nPlease introduce the Egyptian pyramids.\\nThe Egyptian pyramids are among the most iconic and \\nfascinating structures in history.\\nBuilt primarily as tombs for the pharaohs of ancient Egypt, \\nthey served as grand final resting places to ensure their \\nsafe passage into the afterlife. The pyramids are mainly \\nlocated in Giza, near Cairo. The Great Pyramid of Khufu \\nis the largest and most well-known, standing at an \\nastonishing height.\\nConstructed with precision, these pyramids were built \\nusing massive limestone blocks. Thousands of workers, \\nincluding skilled artisans, were involved in their \\nconstruction, which took many years. The architecture is \\nremarkable, with the sides of the pyramids aligned almost \\nperfectly with the four cardinal directions.\\nWhat was the route of Columbus's voyage?\\nScenario IIScenario I\\nMultimodal Data is Answer Multimodal Data Enhances Accuracy\\nScenario III\\nMultimodal Data Enhances Richness\\nWhat do the Alaskan Malamute and the \\nSiberian Husky look like, respectively?\\nThe left is the Alaskan Malamute, and the right is the \\nSiberian Husky .\\nHow to make steamed egg custard?\\nFig. 5. Multimodal output in QA scenarios can be categorized into three distinct types. In sub-scenario I, the\\nuser’s query can be fully addressed using only images or videos, without requiring supplementary textual\\ninformation. Sub-scenario II involves a step-by-step explanation that combines text and images to ensure\\nclarity and precision; omitting the images may lead to user confusion at specific steps. In sub-scenario III,\\nsupplementary images enrich the information conveyed in the answer, but their removal does not compromise\\nthe answer’s accuracy.\\n– Native MLLM-Based Output: In this task, the generation of multimodal data is entirely model-\\ndriven, eliminating the need for external data sources to supplement the model responses.\\nThe most straightforward approach involves using a unified MLLM to produce the desired\\nmultimodal output in a single step, ensuring seamless integration of diverse data types, such\\nas text, images, or audio, within a cohesive framework.\\n– Augmented Multimodal Output: This method utilizes pre-existing multimodal data to\\nenhance textual responses. After generating the text, the system executes three sequential\\nsubtasks to create the final multimodal output: 1) Position Identification: The system deter-\\nmines optimal insertion points within the text where multimodal elements (e.g., images, videos,\\ngraphs) can be integrated to complement or clarify the content. This step ensures that the mul-\\ntimodal data aligns contextually with the text. 2) Candidate Set Retrieval: Relevant multimodal\\ndata is retrieved from external sources, such as the web or a knowledge base, by querying and\\nfiltering potential candidates that best match the text’s context and intent. 3) Matching and\\nInsertion: The system selects the most appropriate multimodal element from the retrieved\\ncandidate set based on relevance, quality, and coherence. The chosen data is then seamlessly\\nintegrated into the identified positions, producing a cohesive and enriched multimodal answer.\\n2.3.3 New Modules.\\n•Multimodal Search Planning: This module tackles key decision-making challenges in MRAG\\nsystems by focusing on two core tasks: retrieval classification and query reformulation. Given a\\nmultimodal query Q= (𝑞,𝑣), where 𝑞is the textual component and 𝑣 is the visual component,\\nthe module is designed to optimize information acquisition. Specifically, retrieval classification\\ninvolves determining the relevance and category of the multimodal query to guide the search\\ntoward the most appropriate data sources. Query reformulation, on the other hand, refines the\\n, Vol. 1, No. 1, Article . Publication date: April 2018.\"),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 10, 'page_label': '11', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 11\\nquery by integrating textual and visual cues to improve retrieval accuracy and comprehensive-\\nness. By combining these tasks, the module strengthens the system’s ability to handle complex\\nmultimodal inputs, ensuring more effective and contextually relevant information retrieval.\\n– Retrieval Classification: This task determines the optimal retrieval strategy 𝑎∗from the\\naction space A= {𝑎𝑛𝑜𝑛𝑒,𝑎𝑡𝑒𝑥𝑡,𝑎𝑖𝑚𝑎𝑔𝑒}based on the current query and optionally the retrieved\\nhistorical documents. The decision process is formulated as:\\n𝑎∗= 𝑎𝑟𝑔𝑚𝑎𝑥\\n𝑎∈A\\nF𝑅𝐶 (𝑎 |Q,D) (1)\\nwhere the retrieval control module F𝑅𝐶 evaluates the utility of retrieval actions by considering\\nquery characteristics, the MLLM’s inherent capabilities, and, when available, the retrieved\\ndocuments Dfrom previous iterations. For example, in multi-hop scenarios, after retrieving\\nvisual information in the initial round, the module may leverage accumulated knowledge\\nto determine subsequent actions, such as text-based retrieval or direct generation. Existing\\nMRAG frameworks typically follow a rigid pipeline with predetermined retrieval actions, which\\nposes significant limitations. Recent studies [125] have shown that compulsive image-to-image\\nretrieval can be counterproductive, as retrieved images may introduce misleading information,\\ndegrading MLLM performance. This highlights the necessity of dynamic retrieval strategy\\nselection.\\n– Query Reformulation: In scenarios where external information is required ( 𝑎∗ ≠ 𝑎𝑛𝑜𝑛𝑒)\\nfor queries, the task of query reformulation involves generating an enhanced query Q∗by\\nintegrating visual information and, when applicable, retrieved documents from previous\\niterations. This process can be formulated as:\\nQ∗= F𝑄𝑅 (Q,D) (2)\\nwhere F𝑄𝑅 denotes the query enhancement function, which utilizes visual cues and, if available,\\nhistorical retrieval results to refine the query’s precision. This task is particularly critical in real-\\nworld human interactions, where queries often rely heavily on visual context and frequently\\nemploy anaphoric references. The inherent challenges of visual incompleteness and textual\\nambiguity pose significant obstacles to retrieving relevant information through straightforward\\nsearch mechanisms. For complex queries that necessitate multi-hop reasoning, the enhanced\\nquery Q∗may be further decomposed into a series of atomic sub-queries {𝑞∗\\n1,...,𝑞 ∗\\n𝑛}. Each\\nsub-query is meticulously formulated by considering both textual and visual contexts, as well\\nas the accumulated knowledge from previous iterations, when relevant. This decomposition\\nallows for a more granular and precise retrieval process, addressing the nuanced dependencies\\nand ambiguities present in real-world queries.\\nThis dual-task approach optimizes information acquisition by minimizing unnecessary retrievals\\nwhile maximizing the relevance of retrieved content. The structured planning framework signifi-\\ncantly enhances the MRAG system’s ability to gather comprehensive and accurate information,\\nensuring computational efficiency.\\n3 Components & Technologies of MRAG\\nIn this section, we will sequentially introduce the details of the five key technical components\\nof MRAG: Multimodal Document Parsing and Indexing (section3.1), Multimodal Search Planning\\n(section 3.2), Multimodal Retrieval (section 3.3), Multimodal Generation (section 3.4)\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 11, 'page_label': '12', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='12 Trovato et al.\\n3.1 Multimodal Document Parsing and Indexing\\nMRAG systems significantly enhance the reliability and quality of generated answers, by integrating\\ntarget multimodal knowledge from external multimodal knowledge bases. Target multimodal knowl-\\nedge can be derived from various granularity in knowledge bases, including localized segments\\nwithin a single document, cross-segment references within a document, or even cross-document\\nknowledge collections. Thus, how to effectively parse, index, and organize the multimodal docu-\\nments in external knowledge bases, can largely affect the model’s utilization of target multimodal\\nknowledge, thereby determining end-to-end performance. In this section, we first classify docu-\\nments in multimodal knowledge bases according to their structure, then we provide a detailed\\nintroduction to the parsing methods and the evolution of these methods for different types of\\nmultimodal documents. Specifically, multimodal documents can be categorized into the following\\nthree types:\\n•Unstructured Multimodal Data: refers to various multimodal information that does not have\\na specific format or schema, such as text, images, videos, and audio. Among the unstructured\\ndata, documents with images are widely studied in MRAG. For example, SlideVQA [347] is a\\ntypical dataset for visual question-answering task, where all documents are input as images.\\n•Semi-structured Multimodal Data: mainly refers to multimodal information that lacks the\\nrigid schema of traditional relational databases but retains some organizational features, such\\nas PDFs, HTML, XML, and JSON. In such documents, rule-based methods can directly extract\\nstructural characteristics. For instance, in HTML, the title can be identified using the <title>\\ntag. A common challenge in processing these documents is that their inherent structure, easily\\ninterpretable by humans, is often lost during parsing, resulting in information loss.\\n•Structured Multimodal Data: refers to multimodal information arranged in a predefined format,\\ntypically following a fixed schema, such as relational databases and knowledge graphs. The\\nprimary challenge in handling such data is formulating an accurate structured query language\\ncorresponding to natural language.\\nIn MRAG scenarios, the primary focus is on processing and leveraging unstructured and semi-\\nstructured documents. Document parsing methods in MRAG are broadly categorized into two\\napproaches: extraction-based and representation-based. Each approach has distinct advantages\\nand limitations, with the choice depending on task-specific requirements such as scalability or\\ncomputational efficiency. Extraction-based methods involve a two-step process: first, multimodal\\ninformation is extracted from documents, and second, the extracted data is parsed and structured\\nfor storage and downstream use. In contrast, representation-based methods do not require explicit\\nextraction of multimodal information. Instead, these methods focus on storing document content\\nholistically, often employing representation techniques for document segments. This approach\\nenables a more comprehensive processing of document content.\\n3.1.1 Extraction-based. Early document parsing solutions were entirely extractive. They evolved\\ngradually from plain text extraction to multimodal data extraction, depending on the type of content\\nbeing extracted. This subsection will present the process in this sequence.\\n•Plain Text Extraction. In this phase, only textual information from all modal data in the docu-\\nment was extracted. For example, for tables and images, only their textual content was captured.\\nSemi-structured documents, such as PDFs, XML, and HTML, can be parsed directly according to\\ntheir structural rules. Numerous open-source tools support such capabilities, including pymupdf\\n[3] and pdfminer [2] for PDF parsing, and jsoup [1] for HTML extraction. While this approach\\nenables simple and efficient document parsing, it has limitations: it cannot extract multimodal\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 12, 'page_label': '13', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 13\\ninformation (e.g., text within images) and struggles with complex document formats. Additionally,\\nthe parsed results often suffer from significant loss of document structure information.\\nTo enhance document parsing accuracy and address the limitations of rule-based methods\\nin handling complex real-world documents, such as in Visual Document Understanding (VDU)\\ntasks, OCR (Optical Character Recognition)-based approaches have become widely adopted. The\\ntraditional OCR-based document parsing pipeline consists of three main stages: text detection,\\ntext recognition, and text parsing. Text Detection involves locating and extracting text regions\\nfrom documents. Early methods primarily relied on Connected Component Analysis (CCA) [26,\\n363] and Edge Detection algorithms [261]. For more complex layouts, techniques such as Contour\\nAnalysis and Stroke Width Transform (SWT) [73, 344] were employed to handle multi-oriented\\ntext. With advancements in machine learning, hybrid models combining regression-based object\\ndetection frameworks (e.g., Faster R-CNN) with semantic segmentation networks were developed\\nto address arbitrary-shaped text instances [15, 206, 481]. This stage outputs precise bounding\\nboxes or polygon coordinates around text elements, serving as the basis for subsequent processing.\\nText Recognition converts visual text representations into machine-readable text, playing a critical\\nrole in digitizing unstructured data. Its evolution can be divided into three phases: The classical\\nphase relied on handcrafted features [ 290] and statistical models [ 22], but faced challenges\\nwith fragmented processing and limited robustness. The deep learning phase introduced CNNs\\n(Convolutional Neural Networks) for feature extraction and CTC (Connectionist Temporal\\nClassification)/RNNs (Recurrent Neural Networks) for sequence modeling, with breakthroughs\\nlike CRNN enabling unified pipelines and improved accuracy on irregular text. The modern phase\\nleverages transformer architectures [191], achieving global context awareness and robustness\\nto arbitrary-shaped text. Text Parsing reconstructs semantic relationships through three key\\nsteps: Layout Analysis segments documents into logical components using rule-based heuristics\\nand graph models based on spatial and typographic cues. Syntactic Parsing extracts structured\\ndata from unstructured text using regular expressions and finite-state machines. Post-processing\\ncorrects recognition errors through contextual algorithms like language model interpolation (e.g.,\\nn-gram models and dictionary lookups). This comprehensive process ensures accurate semantic\\nreconstruction from complex documents.\\nHowever, the OCR-dependent approach has critical problems: It is not conducive to paralleliza-\\ntion and occupies a large amount of computing resources, besides, errors in the pipeline will\\npropagate downward through the system, affecting the overall performance. In recent years, with\\nthe development of Transformer architectures, the aforementioned issues have been effectively\\naddressed. It enhances global context modeling through the self-attention mechanism, signifi-\\ncantly improves processing efficiency by leveraging parallel computing, and directly maps images\\nto structured text in an end-to-end training mode. This effectively eliminates the cumulative\\nerror issues associated with the multi-stage cascading of traditional OCR systems. LayoutLM\\n[412] uses the BERT architecture as the backbone and adds two new input embeddings: a 2-D\\nposition embedding and an image embedding to jointly model interactions between text and\\nlayout information across scanned document images. LayoutLMv2 [413] and LayoutLMv3 [133]\\nfurther propose a new single multimodal framework to model the interaction among text, layout,\\nand image. DocFormer [12] based on the multimodal transformer architecture proposes a novel\\nmultimodal attention layer to fuse text, vision, and spatial features in a document, thereby\\nachieving end-to-end document parsing.\\n•Multimodal Extraction. In this phase, the original format of multimodal data is preserved\\nduring extraction, allowing downstream tasks to autonomously determine subsequent operations.\\nFor semi-structured documents, extraction can be performed similarly using rule-based methods.\\nRelevant multimodal data is identified through specific tags, such as extracting images from\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 13, 'page_label': '14', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='14 Trovato et al.\\nHTML files using the \"<img>\" tag. However, this approach faces similar challenges to plain text\\nextraction.\\nThe pipeline for multimodal document parsing based on OCR consists of three steps: page\\nsegmentation, text recognition, and text parsing. Page segmentation, similar to text detection in\\nplain text extraction, locates and extracts target regions while annotating them with semantic\\nlabels (e.g., title, table, footnote). This subtask of semantic segmentation commonly employs CNN-\\nbased methods, categorized into region-based, FCN-based, and weakly supervised approaches\\n[111]. Text recognition, similar to plain text extraction, focuses on parsing text data such as\\ntitles and page text. Text parsing involves layout analysis and other operations, processing\\nmultimodal data according to downstream task requirements. In the era of LLMs, multimodal\\ndata is often converted into text for utilization, as seen in models like TableNet [286] for tables and\\nUniChart [265] for charts. This necessitates distinct models for extracting captions from different\\nmodalities. With the advancement of MLLMs, there is a trend toward unifying these models\\ninto a single MLLM framework, leveraging their robust representation capabilities [173, 227].\\nFurther developments in MLLMs enable the direct retention and input of original multimodal\\ndata during generation [312, 437, 474].\\n3.1.2 Representation-based. Although extractive-based methods have been widely adopted, they\\nsuffer from several inherent limitations: (1) The parsing process is time-consuming, involves\\nmultiple steps, and requires different models for different document types; (2) Critical information,\\nsuch as document structure, may be lost during extraction; and (3) Parsing errors can propagate\\nto downstream tasks. Recent advancements in MLLMs [6, 20, 218] have enabled a novel approach\\nthat directly uses document screenshots as primary data for metadata indexing, addressing these\\nissues [78, 170, 253, 342, 463]. To capture both global and local information, DSE [253] processes the\\ndocument screenshot along with its sub-images through a unified encoding framework. Additionally,\\na late interaction mechanism, inspired by ColBERT [163], has been introduced to improve recall\\nefficiency [78]. However, page-level document splitting may hinder the model’s ability to capture\\nfull context and inter-part relationships. To address this problem, a holistic document representation\\nmethod has been proposed [170], which segments large documents into passages within the token\\nlimit of MLLMs. Empirical studies reveal a performance gap between multimodal and text-only\\nretrieval, highlighting differences in effectiveness when using raw multimodal data versus text or\\ncombined modalities [312, 463]. Consequently, a new paradigm has emerged that leverages OCR\\nfor text indexing, document screenshots for multimodal indexing, and executes textual and visual\\nRAG in parallel. The results from both streams are then fused through modality integration to\\nproduce the final answer [342].\\n3.2 Multimodal Search Planning\\nMultimodal search planning refers to the strategies employed by MRAG systems, to effectively\\nretrieve and integrate information from multiple modalities to address complex queries. The\\nplanning can be broadly categorized into two main approaches: fixed planning and adaptive\\nplanning.\\n3.2.1 Fixed Planning. Early MRAG systems typically adopt fixed planning strategies for handling\\nmultimodal queries, characterized by predetermined processing pipelines that lack flexibility in\\nadapting to diverse query requirements. These approaches can be broadly categorized based on\\ntheir retrieval modality choices:\\n•Planning for Single-modal Retrieval. Early fixed planning strategies usually focus on a single\\nmodality for retrieval, despite the multimodal nature of input queries. These approaches can be\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 14, 'page_label': '15', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 15\\nbroadly classified into text-centric and image-centric paradigms, reflecting initial efforts to adapt\\ntraditional IR query processing techniques [171, 184] to the multimodal domain.\\n– Text-centric planning approaches prioritize textual retrieval by transforming multimodal\\nqueries into text-only formats. For instance, Plug-and-Play [ 357] employs vision-language\\nmodels to convert the visual component of a query into textual descriptions, followed by text-\\nbased retrieval planning. This strategy simplifies the multimodal problem into a traditional\\ntext-based RAG pipeline, leveraging established multi-stage query processing techniques from\\nconventional IR systems. However, this approach often introduces a semantic gap between\\nthe user’s original intent and the generated textual descriptions. The conversion of visual\\nqueries to text may fail to precisely capture the user’s specific information needs, leading to\\nthe retrieval of irrelevant or noisy documents that diverge from the query’s focus.\\n– Image-centric planning strategies rely solely on image-based retrieval regardless of the query\\ncharacteristics. Systems such as Wiki-LLaVA [24] demonstrate this paradigm by consistently\\ntriggering image retrieval from knowledge bases for multimodal queries. While this approach\\nensures visual information preservation, it presents practical limitations. Recent empirical\\nstudies [125] highlight that compulsive image retrieval can be counterproductive, particularly\\nwhen textual information suffices or when retrieved images introduce misleading visual\\ncontexts, impairing MLLM performance.\\nThe inflexibility of single-modality planning strategies highlights their inherent limitations: they\\ncannot adapt to the diverse information needs of real-world scenarios. For example, while a\\ntext-centric approach may be suitable for queries referencing visual content but focused on\\nfactual information, an image-centric strategy is more effective for queries requiring detailed\\nvisual comparisons.\\n•Planning for Multimodal Retrieval. Recent studies have begun investigating the use of\\nmultimodal information retrieval to enhance the performance of MRAG systems. Unlike single-\\nmodality approaches, these methods integrate both textual and visual knowledge sources, albeit\\nthrough fixed processing pipelines. For instance, MMSearch [147] employs a rigid multimodal\\nplanning pipeline, mandating Google Lens image searches for all image-containing queries.\\nThis is followed by a \"Requery\" phase, where LLMs reformulate the search query using the\\noriginal query, image, and Google Lens results. While this structured approach ensures systematic\\ninformation retrieval, its inflexible design often leads to unnecessary image searches, increasing\\ncomputational overhead when visual information is irrelevant to the query.\\nFixed pipeline approaches, whether single-modality or multimodality, exhibit several critical limi-\\ntations. First, their rigid retrieval strategies struggle to adapt to the diverse nature of real-world\\nqueries, where the optimal retrieval modality depends on specific information needs. Second,\\nmandatory retrieval operations often introduce redundant or irrelevant information, particularly\\nwhen certain knowledge types are unnecessary for addressing the query. Third, these approaches\\nincur significant computational overhead, especially in multimodal pipelines handling large-scale\\nknowledge bases. As highlighted by mR2AG [474], a more fundamental issue is that not all queries\\nrequire external knowledge retrieval. Current MRAG systems frequently perform retrieval indiscrim-\\ninately, resulting in unnecessary computational costs and potential noise in response generation.\\nThese limitations emphasize the need to transition from predetermined pipelines to adaptive plan-\\nning mechanisms that dynamically adjust retrieval strategies based on query characteristics and\\nintermediate results.\\n3.2.2 Adaptive Planning. Recent studies have highlighted two key limitations in fixed pipeline\\napproaches [197]: 1) Non-adaptive Retrieval Queries: inflexible retrieval strategies that fail to adjust\\nto evolving contexts or intermediate results; and 2) Overloaded Retrieval Queries: concatenating\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 15, 'page_label': '16', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='16 Trovato et al.\\nvisual content descriptions with input questions into a single query, leading to ambiguous retrievals\\nand irrelevant knowledge. To address these issues, OmniSearch [197] introduces a self-adaptive\\nplanning agent for multimodal retrieval, mimicking human problem-solving behavior. Instead of\\nrelying on a fixed pipeline, the system dynamically breaks down complex multimodal questions\\ninto sub-question chains with retrieval actions. At each step, the agent adapts its next action based\\non the problem-solving state and retrieved content, enabling deeper understanding of retrieved\\ninformation and adaptive refinement of retrieval strategies. CogPlanner [440] iteratively refines\\nqueries and selects retrieval strategies, enabling both parallel and sequential modeling approaches.\\n3.3 Multimodal Retrieval\\nIn this section, we present a comprehensive overview of the three critical components of multimodal\\nretrieval in the MRAG system: retriever (section 3.3.1), reranker, and refiner. Each component plays\\na distinct yet interconnected role in enhancing the quality and relevance of information retrieval\\nand utilization for LLMs. We summarize the taxonomy of multimodal retrieval research in Figure 6.\\n3.3.1 RETRIEVER. The retriever is a core component that sources relevant documents from a large\\nexternal knowledge base using advanced indexing and search algorithms. It retrieves candidate\\ninformation aligned with user queries, aiming to provide a broad yet relevant set of documents to\\nsupport high-quality LLM responses. Its performance is crucial, as it directly influences the quality\\nof the downstream retrieval pipeline. As shown in Figure 7, existing retrieval methods fall into\\ntwo categories based on architecture: Single/Dual-stream Structure and Generative Structure, each\\ninvolves single-modal (e.g., text, images) and cross-modal information retrieval.\\n•Single/Dual-stream Structure : The Single-stream Structure integrates multimodal fusion\\nmodules to model image-text relationships in a unified semantic space, capturing fine-grained\\ninteractions but incurring higher computational costs and slower inference, limiting scalability\\nfor large-scale multimodal retrieval tasks in real-world applications. In contrast, the Dual-stream\\nStructure uses separate vision and language streams, leveraging contrastive learning to align\\nglobal features in a shared semantic space efficiently. However, it lacks explicit multimodal\\ninteraction and struggles with feature alignment due to information imbalance, exacerbated by\\nthe brevity of dataset captions.\\n– Retrieval for Single-Modal. In MRAG systems, single-modal retrieval focuses on text and\\nimage retrieval. Text retrieval uses NLP techniques to extract relevant information from\\ndatasets, identifying contextually aligned documents. Image retrieval employs computer vision\\nalgorithms and feature extraction methods to encode visual data into high-dimensional vectors\\nfor similarity matching. Both modalities are essential for enhancing MRAG system performance.\\n∗Text-centric. Text retrieval, a core component of information retrieval (IR), identifies\\nrelevant textual information from large corpora or web resources in response to user queries.\\nIt is widely used in downstream applications such as question answering [161, 304], dialogue\\nsystems [334, 436, 456], web search [ 92, 269, 270], and retrieval-augmented generation\\nsystems [33, 45, 102, 330]. Recent advancements categorize text retrieval methods into two\\ntypes: sparse retrieval and dense retrieval.\\n·Sparse Text Retrieval. Early research in text retrieval focused on extracting representative\\nterms from documents, leading to the development of vector space models [320] based\\non the \"bag-of-words\" assumption, which represents documents and queries as sparse\\nterm vectors, ignoring term order. Term weighting methods like tf-idf [9, 314, 319] and\\nBM25 models [315, 316] were introduced to assign weights based on term importance\\nwithin and across corpora, while inverted indexes [ 513] improved retrieval efficiency\\nby organizing corpora into term-document ID pairs. Statistical language modeling [452]\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 16, 'page_label': '17', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 17\\nMultimodal\\nRetrieval\\nRefiner\\nSoft\\nCross-\\nModal PromptMM [393], RACC [395], VTC-CLS [364], VisToG [128]\\nSingle-\\nModal\\nAskell et al . [13], PI [ 53], Context Distillation [ 331], Wingate et al . [396], Sun et al .\\n[339], Distilling step-by-step [ 122], Gist [ 279], AutoCompressor [49], ICAE [ 99],\\nPOD [190], xRAG [48], COCOM [ 308], Gist-COCO [ 193], UltraGist [ 467], LLoCO\\n[346], 500xCompressor [203], RDRec [379], SelfCP [90], QGC [25], UniICL [91]\\nHard\\nCross-\\nModal\\nLLaVolta [34], PyramidDrop [ 407], DeCo [ 425], MustDrop [ 226], G-Search [ 484],\\nG-Prune [151]\\nSingle-\\nModal\\nDynaICL [499], FILCO [ 385], Selective Context [ 196], CoT-Influx [132], RECOMP\\n[410], MEMWALKER [32], TCRA-LLM [220], LLMLingua [ 148], LongLLMLingua\\n[149], CPC [ 213], AdaComp [ 469], Prompt-SAW [11], PCRL [ 159], LLMLingua-2\\n[288], Nano-Capsulator [ 56], CompAct [432], Style-Compress [ 297], TACO-RL [323],\\nFaviComp [158]\\nReranker\\nPrompt\\nCross-\\nModal TIGeR [303], Lin et al. [210]\\nSingle-\\nModal\\nZhuang et al . [509], MCRanker [ 109], UPR [ 318], Zhuang et al . [511], Co-Prompt\\n[52], PaRaDe [ 70], DemoRank [ 228], PRP-AllPair [302], PRP-Graph [ 248], Yan et al .\\n[416], RankGPT [ 341], LRL [ 256], Tang et al . [349], TourRank [43], TDPart [ 292],\\nFIRST [309]\\nFine-tune\\nCross-\\nModal Wen et al. [394], RagVL [311]\\nSingle-\\nModal\\nNogueira and Cho [282], monoBERT [ 285], Nogueira et al . [283], Ju et al . [157],\\nDuoT5 [296], RankT5 [ 510], ListT5 [ 433], RankLLaMA [ 255], TSARankLLM [ 464],\\nQ-PEFT [294], Zhang et al. [475], PE-Rank [222]\\nRetriever\\nGenerative\\nCross-\\nModal IRGen [479], GeMKR [238], GRACE [199], ACE [76], AVG [195]\\nSingle-\\nModal\\nGENRE [61], DSI [ 352], DynamicRetriever [ 502], SEAL [ 19], DSI-QG [ 512], NCI\\n[383], Ultron [ 501], LTRGR [ 201], GenRRL [ 500], DGR [ 202], GenRet [ 340], MINDER\\n[200], NOVO [389], LMIndexer [153], ASI [420], RIPOR [449], GLEN [175]\\nSingle/Dual-\\nStream\\nCross-\\nModal\\n•Text/Image–Image:\\nMSDS [370], VSE++ [ 75], Liu et al . [231], Wehrmann and Barros [390], Guo et al .\\n[110], DSCMR [ 489], DRCE [ 384], ESSE [ 386], SDCMR [ 388], DSVEL [ 72], CRAN\\n[299], CAAN [ 468], RANet [ 408], PVSE [ 333], PCME [ 57], RLCMR [ 422], DREN\\n[419], TGDT [ 215], HREM [ 87], TransTPS [ 18], IEFT [ 350], TEAM [ 405], CSIC\\n[234], LAPS [ 88], COTS [ 240], AGREE [380], IRRA [ 146], USER [ 477], EI-CLIP [ 250],\\nMAKE [492]\\n•Text–Video:\\nLLVE [361], Mithun et al . [276], Miech et al . [274], MMT [ 89], HiT [ 224], CLIP4Clip\\n[247], VoP [131], Cap4Video [402], DGL [421], TeachCLIP [356], T-MASS [371]\\n•Text–Audio:\\nATR [239], OML [271], MGRL [485], TAP-PMR [406], CMRF [494], TTMR++ [63]\\n•Unified:\\nFLAVA [326], UniVL-DR [ 237], MARVEL [ 498], FLMR [ 211], UniIR [ 391], VISTA\\n[495], E5-V [150], VLM2VEC [ 152], GME [ 476], Ovis [ 244], ColPali [ 79], CREAM\\n[462], DSE [254]\\nSingle-\\nModal\\nTf-Idf [319], BM25 [ 316], statistical [ 452], DeepCT [ 60], HDCT [ 59], COIL [ 94],\\nuniCOIL [208], DocTTTTTquery [ 284], SPLADE [ 84], SPLADE v2 [ 83], Poly [ 137],\\nME-BERT [246], ColBERT [ 163], ColBERTer [120], MVR [ 471], MADRM [ 166],\\nrandom [127], in-batch [ 118], hard [ 161], STAR [454], ANCE [ 409], ADORE [ 454],\\nRocketQA [304], AR2 [ 459], SimANS [ 497], Lee et al . [172], Chang et al . [27], Prop\\n[251], B-PROP [ 252], Condenser [ 92], co-Condenser [ 93], Contriever [ 139], SimLM\\n[373], RetroMAE [491], Liu and Yang [214]\\nFig. 6. Taxonomy of recent advancements in multimodal retrieval research.\\nfurther advanced retrieval by estimating term probability distributions for probabilistic\\nranking. However, early sparse retrieval methods face limitations, such as assuming term\\nindependence and relying on lexical matching, which hinders their ability to capture\\ncontextual term importance or semantic relationships between terms. Consequently, these\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 17, 'page_label': '18', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='18 Trovato et al.\\nSingle/Dual-Stream Generative\\nSingle-Modal Cross-Modal\\nText Query\\n------------------\\nWhat is a new \\nenergy vehicle?\\nText Candidate\\n------------------------\\nA new energy \\nvehicle (NEV) is a \\ntype of vehicle that \\nutilizes alternative \\nenergy sources \\ninstead of ......\\nText Query\\n------------------------------------\\nWhat is a new energy vehicle?\\nVisual Candidate\\n(Image, Video, Audio, …)\\n------------------------------------\\nSingle-Modal Cross-Modal\\nText Query\\n------------------\\nWhat is a new \\nenergy vehicle?\\nText Candidate\\n---------------------------\\nA new energy vehicle \\n(NEV) is a type of \\nvehicle that utilizes \\nalternative energy \\nsources instead of \\ntraditional internal \\ncombustion engines \\nthat run on fossil fuels \\nlike gasoline or diesel.\\nLLM\\nTextID\\nIdentifier\\nTextID\\nEncoder\\nText Query\\n------------------\\nWhat is a new \\nenergy vehicle?\\nVisual Candidate\\n(Image, Video, Audio, …)\\n------------------------------------\\nMLLM\\nVisualID\\nIdentifier\\nVisualID\\nEncoder\\nText \\nRetriever\\nCross-Modal \\nRetriever\\nVisual Query\\n------------------------\\nVisual Candidate\\n------------------------\\nVisual\\nRetriever\\nFig. 7. The architectures of retriever in multimodal retrieval.\\nmethods struggle to understand deeper textual meanings and contextual relevance between\\nqueries and documents.\\nRecent advancements in sparse retrieval models have been driven by the integration of\\npre-trained language models (PLMs). While these approaches leverage PLMs, they remain\\nfundamentally rooted in lexical matching, enabling the reuse of traditional sparse index\\nstructures by incorporating auxiliary information such as contextualized embeddings\\n[94, 208] and extended tokens [83, 84, 284]. This research domain focuses on two main\\napproaches: term weighting and term expansion. Term weighting enhances relevance\\nestimation by leveraging context-specific token representations. DeepCT [60] and HDCT\\n[59] use learned token representations to estimate the context-specific importance of terms\\nwithin passages, while COIL [94] and uniCOIL [208] employ contextualized token repre-\\nsentations of exact matching terms to compute relevance via dot products and summed\\nsimilarity scores. Term expansion mitigates vocabulary mismatch by expanding queries or\\ndocuments using PLMs. For instance, DocTTTTTquery [ 284] predicts relevant queries\\nfor documents to enrich the document’s content, while SPLADE [84] and SPLADEv2 [83]\\nproject terms onto vocabulary-sized weight vectors derived from masked language model\\nlogits. These vectors, aggregated via methods like summing or max pooling, effectively\\nexpand content by incorporating absent terms. Sparsity regularization ensures efficient\\nsparse representations for inverted index usage.\\nIn summary, sparse retrieval models achieve an optimal balance in cross-domain transfer,\\nretrieval efficiency, and overall effectiveness.\\n·Dense Text Retrieval. Recent advancements in deep learning [5, 50, 51, 105, 119, 167,\\n169], particularly pre-trained language models (PLMs) [23, 62, 229] based on the Trans-\\nformer architecture [ 80, 362], have increasingly adopted dense vector embeddings in\\nlow-dimensional Euclidean spaces for modeling semantic relationships between queries\\nand documents. These embeddings enable relevance measurement through Euclidean dis-\\ntances or inner products. Dense retrieval methods have demonstrated strong performance\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 18, 'page_label': '19', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 19\\nacross various information retrieval tasks [161, 163, 270]. Additionally, Approximate Near-\\nest Neighbor Search (ANNS) algorithms [98, 142, 156], particularly quantization-based\\nmethods [98, 142] and their retrieval-oriented variants [415, 453, 455, 461], enable efficient\\nretrieval of top-ranked documents from large collections using precomputed ANNS indices.\\nDense retrieval techniques primarily focus on two key aspects: model architecture and\\ntraining methods.\\nFor model architecture, dense retrieval methods employ a two-tower architecture to\\nbalance retrieval efficiency and effectiveness by modeling semantic interactions between\\nqueries and documents through their representations. These methods vary in represen-\\ntation granularity, primarily falling into two categories: single-vector and multi-vector\\nrepresentations. Then, the relevance scores are computed using similarity functions (e.g.,\\ncosine similarity, inner product) between these embeddings. A common technique in-\\nvolves placing a special token (e.g., “[CLS]”) at the beginning of a text sequence, with\\nits learned representation capturing the overall semantics. The existing dense retrieval\\nmodels learn the query and document representations by fine-tuning PLMs like BERT [62],\\nRoBERTa [229], or Mamba Gu and Dao [106], Zhang et al. [458], or large language models\\n(LLMs) like RepLLaMA [255] on annotated datasets (e.g., MSMARCO [281], BEIR [354]).\\nHowever, single-vector bi-encoders struggle to model fine-grained semantic interactions\\nbetween queries and documents. To address this limitation, multi-vector representation en-\\nhance text representation and semantic interaction by employing multiple-representation\\nbi-encoders. The Poly-encoder [137] generates multiple context codes to capture text se-\\nmantics from multiple views. ME-BERT [246] produces 𝑚representations for a candidate\\ntext using the contextualized embeddings of the first 𝑚tokens. ColBERT [163] maintains\\nper-token contextualized embeddings with a late interaction mechanism. ColBERTer [120]\\nextends ColBERT by combining single- (“[CLS]”) and multi-representation (per-token)\\nmechanisms for better performance. MVR [471] introduces multiple “[VIEW]” tokens to\\nlearn diverse representations, with a local loss to identify the best-matched view. MADRM\\n[166] learns multiple aspect embeddings for queries and texts, supervised by explicit aspect\\nannotations.\\nFor training method, to achieve optimal retrieval performance, dense retrieval mod-\\nels are typically trained using two key techniques: negative sampling and pretraining.\\nNegative sampling focuses on selecting high-quality negatives to compute the negative\\nlog-likelihood loss used for training dense retrieval models. Basic methods include ran-\\ndom sampling [127] and in-batch negatives [118, 161, 304], which increase the number\\nof negatives within memory limits but do not guarantee the inclusion of hard negatives,\\ni.e., irrelevant texts with high semantic similarity to the query. Hard negatives are critical\\nfor improving the model’s ability to distinguish relevant from irrelevant texts. Various\\napproaches have been proposed to incorporate hard negatives. BM25-retrieved documents\\nare used as static hard negatives [ 95, 161]. STAR [454] combines static hard negatives\\nwith random negatives, while ANCE [ 409] retrieves hard negatives using a warm-up\\ndense retrieval model and refreshes the document index during training. ADORE [454]\\nemploys an adaptive query encoder to retrieve top-ranked texts as hard negatives, keeping\\nthe text encoder and document index fixed. However, hard negatives may include false\\nnegatives, introducing noise that can degrade performance. RocketQA [ 304] addresses\\nthis by using a cross-encoder to filter out likely false negatives. AR2 [ 459] integrates a\\ndual-encoder retriever with a cross-encoder ranker, jointly optimized through a minimax\\nadversarial objective to produce harder negatives and improve the retriever. SimANS [497]\\nintroduces the concept of sampling ambiguous negatives, i.e., texts ranked near positives\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 19, 'page_label': '20', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='20 Trovato et al.\\nwith moderate similarity to the query. These negatives are more informative and less likely\\nto be false negatives, further enhancing model performance.\\nPretraining aims to learn universal semantic representations that generalize to down-\\nstream dense retrieval tasks. To enhance the modeling capacity of PLMs, self-supervised\\npretraining tasks, such as those proposed by Lee et al. [172] (selecting random sentences\\nas queries) and Chang et al. [27] (leveraging hyperlinks for constructing query-passage\\npairs), mimic retrieval objectives. Prop [251] and B-PROP [252] use document language\\nmodels (e.g., unigram, BERT) to sample word sets, training PLMs to predict pairwise\\npreferences. To enhance dense retrieval models, studies focus on improving the “[CLS]”\\ntoken embedding. Condenser [92] aggregates global text information for masked token\\nrecovery, while co-Condenser [93] adds a query-agnostic contrastive loss to cluster related\\ntext segments while distancing unrelated ones. Contriever [139] generates positive pairs\\nby sampling two spans from the same text and negatives using in-batch and cross-batch\\ntexts. Following with an unbalanced architecture (strong encoder, simple decoder), SimLM\\n[373] pretrains the encoder and decoder with replaced language modeling, recovering\\noriginal tokens after replacement. It further optimizes the retriever through hard negative\\ntraining and cross-encoder distillation. RetroMAE [491] utilizes a high masking ratio for\\nthe decoder and a standard ratio for the encoder, incorporating an enhanced decoding\\nmechanism with two-stream and position-specific attention masks. Liu and Yang [214]\\nintroduces a two-stage pretraining approach, combining general-corpus pretraining with\\ndomain-specific continual pretraining, achieving strong benchmark performance.\\nHowever, single-modal retrieval is inherently limited by its inability to capture cross-modal\\nrelationships, which underscores the importance of integrating multimodal retrieval strategies\\nto bridge textual and visual semantics for more comprehensive information retrieval and\\ngeneration.\\n– Retrieval for Cross-modal. Cross-modal retrieval enables the identification of relevant data\\nin one modality (e.g., images) using a query from another (e.g., text). It enhances MRAG systems\\nby facilitating the retrieval and generation of information across diverse modalities, including\\ntext, images, audio, and video.\\n∗Text–Image Retrieval. Text–Image Retrieval aims to match images with corresponding\\ntextual queries by leveraging multimodal data co-occurrence, such as paired text-image\\ninstances or manual annotations, to capture semantic correlations. Existing methods can be\\ncategorized into three groups: CNN/RNN-based approaches, Transformer-based techniques,\\nand Vision-Language Pretraining (VLP) model-based methods.\\nEarly CNN/RNN-based methods [75, 110, 174, 231, 370, 390] extract features from each\\nmodality separately using MLP, CNN, and RNN, enforcing cross-modal constraints through\\npositive/negative sample construction. MSDS [370] uses CNN with a maximum likelihood-\\nbased scheme for image-text relevance. VSE++ [ 75] combines CNN and RNN with hard\\nsample mining in ranking loss. Advances include residual learning [ 231], character-level\\nconvolution [390], and disentangled representation [110] for improved feature mapping and\\nretrieval. DSCMR [489] maps multimodal data into a shared space using modality-specific\\nnetworks and fully connected layers, leveraging label constraints and pairwise loss for\\ndiscriminant learning. Recent CNN/RNN-based methods improve image-text matching by\\naddressing key challenges. DRCE [384] enhances rare content representation and association\\nusing a dual-path structure, adaptive fusion, and reranking to mitigate long-tail issues. ESSE\\n[386] tackles one-to-many correspondence by projecting data as sectors with uncertainty\\napertures. SDCMR [388] employs diverse CNNs for multimodal feature extraction and a dual\\nadversarial mechanism to isolate semantic-shared features, ensuring retrieval consistency.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 20, 'page_label': '21', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 21\\nThese methods collectively advance cross-modal retrieval robustness and accuracy. Spatial\\nattention [72, 135, 299, 408, 468] is widely used in CNN/RNN-based cross-modal retrieval to\\nuncover fine-grained associations by generating weighted masks for local regions, enhancing\\nkey features while suppressing irrelevant ones. DSVEL [72] employs spatial-aware pooling\\nto align image regions with text, while CRAN [299] and CAAN [468] improve global-local\\nalignment through relation alignment and context-aware selection. RANet [ 408] refines\\nattention mechanisms with reference attention to reduce incorrect scores and adaptive\\naggregation to amplify relevant information and minimize redundancy.\\nTransformer-based methods [18, 57, 87, 215, 333, 350, 419, 422, 451] leverage multi-head\\nself-attention to encode multimodal relationships and optimize modality-specific encoders,\\ndemonstrating superior performance in multimodal modeling and cross-modal retrieval tasks.\\nRecent advancements in multimodal representation learning have focused on enhancing\\nTransformer architectures and feature alignment. PVSE [333] integrates self-attention and\\nresidual learning, while PCME [57] uses probabilistic embeddings to model one-to-many\\nand many-to-many correlations. RLCMR [422] tokenizes multimodal data and trains with\\na unified Transformer encoder for cross-modal semantic correlation. DREN [419] refines\\nfeature representation through character-level and context-driven augmentation. TGDT\\n[215] unifies coarse- and fine-grained learning with multimodal contrastive loss for feature\\nalignment. HREM [87] improves image-text matching by capturing multi-level intra- and\\ninter-modal relationships. TransTPS [ 18] extends Transformers with cross-modal multi-\\ngranularity matching and contrastive loss for better feature distinction. IEFT [350] models\\ntext-image pairs as unified entities to model their intrinsic correlation.\\nWith the rapid advancement of pretraining paradigms, Vision-Language Pretraining (VLP)\\nmodels [46, 62, 68, 107, 136, 144, 183, 194, 305], including both single- and dual-stream\\narchitectures, have leveraged large-scale visual-linguistic datasets for joint pretraining. Re-\\nsearchers have utilized the strong representational capabilities [88, 146, 234, 240, 250, 380,\\n405, 477, 492] of VLP models to significantly enhance cross-modal retrieval performance.\\nSingle-stream models like TEAM [405] align multimodal token embeddings for token-level\\nmatching, while dual-stream approaches such as COTS [240] integrate contrastive learning\\nwith token- and task-level interactions. Methods like CSIC [234] and LAPS [88] improve mul-\\ntimodal alignment by quantifying semantic significance and associating patch features with\\nwords, respectively. AGREE [380] fine-tunes and reranks cross-modal entities to harmonize\\ntheir alignment. IRRA [146] employs text-specific mask mechanism to capture fine-grained\\nintra- and inter-modal relationships. USER [477], EI-CLIP [250], and MAKE [492] leverage\\nCLIP [305] or ALIGN [144] to integrate contrastive learning and keyword enhancement\\nfor enriching representations. Overall, VLP models, through strategies such as fine-tuning,\\nreranking, and follow-up training, have become essential for improving cross-modal align-\\nment and interaction.\\n∗Text–Video Retrieval. Text-video retrieval involves matching textual descriptions with\\ncorresponding videos, requiring spatiotemporal representations to address temporal dy-\\nnamics, scene transitions, and precise text-video alignment. This task is more complex\\nthan text-image retrieval due to the need to model both visual and sequential information\\neffectively.\\nEarly CNN/RNN-based methods [64, 273, 274, 276, 361, 441] encode videos and texts into\\na shared latent space for similarity measurement. LLVE [361] employs CNNs and LSTMs\\nto extract latent features from images and texts, with LSTMs further capturing temporal\\nrelationships between video frames. Subsequent studies [274, 276] apply mean/max pooling\\nto frame sequences to generate compact video-level representations, prioritizing efficiency\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 21, 'page_label': '22', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='22 Trovato et al.\\nover granularity. Later advancements incorporate additional modalities, such as audio and\\nmotion, to enhance video semantics [273]. For text encoding, simpler methods like Word2Vec,\\nLSTMs, or GRUs are commonly used [274, 276, 441], with evidence suggesting that combining\\nmultiple text encoding strategies improves retrieval performance [64].\\nTransformer-based methods [89, 224] utilize self-attention mechanisms to jointly encode\\nvideos and texts, enabling cross-modal interaction. MMT [ 89] employs mutual attention\\nbetween video and text modalities, integrating temporal information to enhance feature\\nrepresentation. Inspired by MoCo [ 116], HiT [ 224] introduces hierarchical cross-modal\\ncontrastive matching at both feature and semantic levels. Additionally, these methods [89,\\n224] encode diverse modalities in video data, such as audio and motion, further enriching\\nvideo representations.\\nRecently, VLP-based models [131, 247, 356, 371, 402, 421] utilize pretrained models like\\nCLIP [305] to enhance text-video tasks in text-video retrieval tasks by capturing cross-modal\\nand temporal dependencies. CLIP4Clip [247] adapt CLIP for text-video retrieval and caption-\\ning, analyzing temporal dependencies. VoP [131] introduces prompt tuning and fine-tunes\\nCLIP to model spatiotemporal video aspects, while Cap4Video [ 402] leverages zero-shot\\ncaptioning with CLIP and GPT-2 [306] for auxiliary captions. DGL [421] proposes dynamic\\nglobal-local prompt tuning, emphasizing intermodal interaction and global video informa-\\ntion through shared latent spaces and attention mechanisms. TeachCLIP [ 356] improves\\nCLIP4Clip by integrating fine-grained cross-modal knowledge from advanced models, and re-\\nfining text-video similarity with an frame-feature aggregation block. T-MASS [371] addresses\\ndataset limitations by enriching text embeddings with stochastic text modeling.\\n∗Text–Audio Retrieval. Text-audio retrieval involves matching textual queries with corre-\\nsponding audio content, requiring alignment of semantic text information with dynamic\\nacoustic patterns in speech, music, or environmental sounds. The challenge lies in bridging\\nthe gap between discrete text and continuous audio signals.\\nEarly CNN/RNN-based approaches [ 239, 271, 485] focus on encoding text and audio\\nseparately and aligning them in a shared space for similarity measurement. ATR [239] uses\\npretrained CNN-based audio networks with NetRVLAD pooling [143] to aggregate features\\ninto a unified representation. OML [271] employs CNNs for robust audio feature extraction\\nand metric learning to enhance audio-text alignment. MGRL [ 485] leverages CNNs for\\nlocalized audio features and introduces adaptive aggregation to handle varying text–audio\\ngranularities.\\nFurthermore, Transformer-based methods [63, 406, 494] utilize multi-head attention mech-\\nanisms and fine-tuning to enhance cross-modal interactions. TAP-PMR [406] employs scaled\\ndot-product attention to enable text to focus on relevant audio frames, reducing mislead-\\ning information, while its prior matrix revised loss optimizes dual matching by addressing\\nsimilarity inconsistencies. CMRF [494] enhances audio-lyrics retrieval through directional\\ncross-modal attention and reinforcement learning to refine multimodal embeddings and\\ninteractions. TTMR++ [63] integrates fine-tuned LLMs and rich metadata to generate detailed\\ntext descriptions, improving retrieval by addressing musical attributes and user preferences.\\n∗Unified-Modal Retrieval. Unified-Modal Retrieval aims to process diverse hybrid-modal\\ndata (e.g., text, images, videos) within a unified model architecture, such as transformer-based\\nPLMs, to encode all modalities into a shared feature space. This enables efficient cross-modal\\nretrieval between any pairwise combination of hybrid-modal data. With the growing demand\\nfor multimodal applications, there is an increasing need for unified multimodal retrieval\\nmodels tailored to complex scenarios. Current approaches leverage pre-trained models like\\nCLIP [305], BLIP [186], and ALIGN [144] for multimodal embedding. For instance, FLAVA\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 22, 'page_label': '23', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 23\\n[326] integrates multiple modalities into a unified framework, leveraging joint pretraining\\non multimodal data with cross-modal alignment and fusion objectives. Similarly, UniVL-DR\\n[237] encodes queries and multimodal resources into a shared embedding space, employing\\na universal embedding optimization strategy with modality-balanced hard negatives and an\\nimage verbalization method to bridge the gap between images and texts. MARVEL [ 498]\\naddresses the modality gap between images and texts by incorporating visual features\\ninto the encoding process. FLMR [211] enhances image representations by using a visual\\nmodel aligned with existing text-based retrievers to supplement the image representation of\\nimage-to-text transforms. UniIR [391] introduces a unified instruction-guided multimodal\\nretriever, achieving robust generalization through instruction tuning on diverse multimodal-\\nIR tasks. VISTA [495] extends image understanding capability by integrating visual token\\nembeddings into a text encoder, supported by high-quality composed image-text data and\\na multi-stage training algorithm. E5-V [ 150] fine-tunes MLLMs on single-text or vision-\\ncentric relevance data, outperforming traditional image-text pair training. VLM2VEC [152]\\nproposes a contrastive training framework to convert vision-language models into embedding\\nmodels using the MMEB dataset [152]. To address modality imbalance, GME [476] trains\\nan MLLM-based dense retriever on the large-scale UMRB dataset[ 476]. Ovis [244] aligns\\nvisual and textual embeddings by integrating a learnable visual embedding table, enabling\\nprobabilistic combinations of indexed embeddings for rich visual semantics. ColPali [ 79]\\nleverages Vision Language Models and the ViDoRe benchmark [79] to index documents from\\ntheir visual features, facilitating efficient query matching with late interaction mechanisms.\\nCREAM [462] employs a coarse-to-fine retrieval and ranking approach, combining similarity\\ncalculations with large language model-based grouping and attention pooling for MLLM-\\nbased multi-page document processing. DSE [254] fine-tunes a large vision-language model\\non 1.3 million Wikipedia web page screenshots, enabling direct encoding of document\\nscreenshots into dense representations.\\n•Generative Structure: Traditional information retrieval (IR) methods, which rely on similarity\\nmatching to return ranked lists of documents, have long been a cornerstone of information\\nacquisition, dominating the field for decades. However, with the advent of pre-trained language\\nmodels, generative retrieval (GR) has emerged as a novel paradigm, garnering increasing attention\\nin recent years. GR primarily consists of two fundamental components: model training and\\ndocument identifier. Model Training aims to train generative models to effectively index and\\nretrieve documents, while enhancing the model’s capacity to memorize information from the\\ndocument corpus. This is typically achieved through sequence-to-sequence (seq2seq) training,\\nwhere the model learns to map queries to their corresponding Document Identifiers (DocIDs).\\nThe training process emphasizes optimizing the model’s understanding of semantic relationships\\nbetween queries and documents, thereby improving retrieval accuracy. Document Identifiers\\n(DocIDs) serve as the target output for the generative retrieval model, and unique representations\\nof each document in the corpus. The quality of these identifiers is crucial, as they directly impact\\nthe model’s ability to memorize and retrieve document information. Effective DocIDs are often\\ngenerated using dense, low-dimensional embeddings or structured representations that capture\\nthe essential content and context of documents, enabling the model to distinguish between\\ndocuments more accurately and enhancing retrieval performance. By overcoming the limitations\\nof traditional IR in terms of content granularity and relevance matching, GR offers enhanced\\nflexibility, efficiency, and creativity, better aligning with practical demands.\\n– Retrieval for Text-modal. The recent advancements in generative language models have\\ndemonstrated their ability to memorize knowledge from documents and recall knowledge to\\nrespond to user queries effectively, which focuses on the use of document identifiers (DocIDs)\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 23, 'page_label': '24', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='24 Trovato et al.\\nand their optimization for retrieval tasks. The approaches can be categorized into static DocID-\\nbased methods and learnable DocID-based methods.\\nStatic DocID-based methods rely on pre-defined, fixed document identifiers. They often\\nuse unique names, numeric formats, or structured identifiers to represent documents. GENRE\\n[61] generates entity names via constrained beam search using a prefix tree, with document\\ntitles serving as DocIDs. DSI [352] introduces numeric DocID formats, including unstructured,\\nnaively structured, and semantically structured identifiers, trained through indexing and\\nretrieval strategies. DynamicRetriever [502] uses unstructured atomic DocIDs and enhances\\nmemorization with pseudo queries. SEAL [19] representing documents with N-gram sub-string\\nidentifiers, leveraging FM-Index [82] for retrieval. DSI-QG [512] represents documents with\\ngenerated queries, re-ranked by a cross-encoder. NCI [383] generates document identifiers using\\na seq2seq network with a prefix-aware decoder. It is trained on both labeled and augmented\\npseudo query-document pairs. Ultron [501] combines URLs and titles as DocIDs to uniquely\\nidentify web documents. It encodes documents into a latent semantic space using BERT [62]\\nand compresses vectors via Product Quantization (PQ) [ 98, 142], with PQ codes serving as\\nsemantic identifiers. Additional digits ensure DocID uniqueness. LTRGR [ 201] focuses on\\nlearning to rank passages directly using generative retrieval models, optimizing autoregressive\\nmodels via rank loss. GenRRL [500] integrates reinforcement learning for aligning token-level\\nDocID generation with document-level relevance estimation. DGR [202] enhances generative\\nretrieval through knowledge distillation, using a cross-encoder as a teacher model to provide\\nfine-grained ranking supervision. Despite these innovations, most approaches rely on static\\nDocIDs, which are not optimized for retrieval tasks, limiting their ability to capture document\\nsemantics and relationships, thereby hindering retrieval performance.\\nTo address this limitation, Learnable DocID-based methods introduce learnable document\\nrepresentations, where DocIDs are optimized during training to better capture document\\nsemantics and improve retrieval performance. GenRet [340] employs a discrete autoencoder\\nto encode documents into compact DocIDs, minimizing reconstruction error. MINDER [200]\\nenhances document representations using multi-view identifiers, including pseudo-queries,\\ntitles, and sub-strings. NOVO [389] introduces learnable continuous N-gram DocIDs, refining\\nembeddings through query denoising and retrieval tasks. LMIndexer [153] generates neural\\nsequential discrete IDs via progressive training and contrastive learning, addressing semantic\\nmismatches. ASI [420] automates DocID learning, assigning similar IDs to semantically close\\ndocuments and optimizing end-to-end retrieval using an generative model. RIPOR [ 449]\\nimproves relevance scoring during sequential DocID generation using dense encoding and\\nResidual Quantization [264]. GLEN [175] employs a dynamic lexical identifier with a two-phase\\nindex learning strategy. Firstly, the keyword-based DocID are defined by extracting keywords\\nfrom documents using self-supervised signals. Secondly, dynamic DocIDs are refined by\\nintegrating query-document relevance, enabling efficient inference. The field of generative text\\nretrieval is evolving from static, pre-defined DocIDs to dynamic, learnable DocIDs that better\\ncapture document semantics and relationships. Learnable DocIDs, combined with advanced\\ntechniques like reinforcement learning, knowledge distillation, and contrastive learning, are\\ndriving improvements in retrieval performance.\\n– Retrieval for Cross-modal. Similarly, MLLMs are considered to memorize and retrieve\\nmultimodal content, such as images and videos, within their parameters. When presented\\nwith a user query for visual content, the MLLM is expected to \"recall\" the relevant image\\nfrom its parameters as a response. Achieving this capability presents significant challenges,\\nparticularly in developing effective visual memory and recall mechanisms within MLLMs.\\nIRGen [479] employs a seq2seq model to predict discrete visual tokens (image identifiers) from\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 24, 'page_label': '25', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 25\\nquery images. Its key innovation is a semantic image tokenizer that encodes global features\\ninto discrete visual tokens, enabling end-to-end differentiable search for improved accuracy\\nand efficiency. GeMKR [238] integrates LLMs with visual-text features through a generative\\nmultimodal knowledge retrieval framework. It first guides multi-granularity visual learning\\nusing object-aware prefix tuning techniques to align visual features with LLMs’ text feature\\nspace, then adopts a two-step retrieval process: generating knowledge clues relevant to the\\nquery and retrieving documents based on these clues. GRACE [199] assigns unique identifier\\nstrings to represent images, training MLLMs to memorize and retrieve image identifiers from\\ntextual queries. ACE [76] combines K-Means and RQ-VAE to construct coarse and fine tokens\\nas multimodal data identifiers, aligning natural language queries with candidate identifiers.\\nAVG [195] introduces autoregressive voken (i.e., visual token) generation, tokenizing images\\ninto vokens that serve as image identifiers while preserving visual and semantic alignment.\\nBy framing text-to-image retrieval as a token-to-voken generation task, AVG bridges the gap\\nbetween generative training and retrieval objectives through discriminative training, refining\\nthe learning direction during token-to-voken generation.\\n3.3.2 RERANKER. Reranker, as a critical second-stage component in multimodal retrieval, is\\ndesigned to re-rank a multimodal document list initially retrieved by a first-stage retriever. It\\nachieves this by employing advanced relevance scoring mechanisms, such as cross-attention\\nmodels, which enable more contextual interactions between queries and documents. Based on\\nthe utilization of large models, including LLMs and MLLMs, existing reranking methods can be\\ncategorized into two primary paradigms: fine-tuning-as-reranker and prompting-as-reranker.\\n•Fine-tuning-as-Reranker: The fine-tuning-as-reranker paradigm adapts PLMs to domain-\\nspecific reranking tasks through supervised fine-tuning on domain-specific datasets, addressing\\ntheir inherent lack of ranking awareness and inability to effectively measure query-document\\nrelevance.\\n– Reranking for Text-Modal : According the development of large models’ architecture,\\nreranker can be divided to three categories: encoder-only, encoder-decoder, and decoder-\\nonly.\\nEncoder-only rerankers have advanced document ranking by fine-tuning PLMs (e.g., BERT\\n[62]) to achieve precise relevance estimation. Key examples include Nogueira and Cho [282]\\nand monoBERT [285], which format query-document pairs as query-document sequences.\\nThe relevance score is derived from the “[CLS]” token’s representation via a linear layer, with\\noptimization achieved through negative sampling and cross-entropy loss.\\nExisting research on encoder-decoder rerankers primarily formulates document ranking as a\\ngeneration task [157, 283, 296, 510], fine-tuning models like T5 to generate classification tokens\\n(e.g., “true” or “false”) for query-document pairs, with relevance scores derived from token logits\\n[283]. Extensions include multi-view learning approaches [157] that simultaneously generate\\nclassification tokens for query-document pairs and queries conditioned on documents, and\\nDuoT5 [296], which compares the classification tokens of document pairs to determine relative\\nrelevance. Beyond these approaches, studies have explored alternative training losses and\\narchitectures. Contrast with previous methods that rely on text generation losses, RankT5 [510]\\ndirectly produces numerical relevance scores for each query-document pair, optimizing with\\nranking losses instead of generation losses. ListT5 [433] further advances this by processing\\nmultiple documents simultaneously, directly generating reranked lists using the Fusion-in-\\nDecoder architecture.\\nRecent studies [ 222, 255, 294, 464, 475] have explored fine-tuning decoder-only models\\nlike LLaMA for document reranking. RankLLaMA [255] formats query-document pairs into\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 25, 'page_label': '26', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='26 Trovato et al.\\nprompts and uses the last token representation for relevance scoring. TSARankLLM [ 464]\\nemploys a two-stage training approach: continuous pretraining on web-sourced relevant text\\npairs to align LLMs with ranking tasks, followed by fine-tuning with supervised data and\\ntailored loss functions. Q-PEFT [294] introduces query-dependent parameter-efficient fine-\\ntuning to generate accurate queries from documents. In contrast, listwise approaches like\\nthose in [475] and PE-Rank [222] focus on directly outputting reranked document lists. Zhang\\net al. [475] highlight the limitations of point-wise datasets with binary labels, and instead use\\nranking outputs from existing systems as gold standards to train a listwise reranker. PE-Rank\\n[222] compresses documents into single embeddings, reducing input length and improving\\nreranking efficiency.\\n– Reranking for Cross-Model : The multi-modal reranking uses the multi-modal question\\nand multi-modal knowledge items to obtain the relevance score, as reranking have already\\nshown its importance in various knowledge-intensive tasks. Wen et al . [394] fine-tunes a\\npretrained MLLM to facilitate cross-item interaction between questions and knowledge items.\\nThe reranker is trained on the same dataset as the answer generator, using distant supervision\\nby checking whether answer candidates appear in the knowledge text. RagVL RETRIEVAL\\net al. [311] introduces a novel framework featuring knowledge-enhanced reranking and noise-\\ninjected training. The approach involves instruction-tuning the MLLM with a simple yet\\neffective template to enhance its ranking capability, enabling it to serve as a reranker for\\naccurately filtering the top-𝑘 retrieved images.\\nIn summary, these approaches leverages the representational capacity of large models while\\noptimizing them for task-specific relevance signals, often achieving high reranking accuracy.\\nHowever, it requires substantial computational resources and labeled training data, resulting in\\nincreased costs.\\n•Prompting-as-Reranker: In contrast, the prompting-as-reranker paradigm leverages large\\nmodels in a zero-shot or few-shot manner by designing prompts that direct the model to gen-\\nerate relevance scores or rankings directly. This approach exploits the inherent knowledge\\nand reasoning capabilities of large models, eliminating the need for extensive fine-tuning and\\noffering greater flexibility and resource efficiency. Researchers have explored prompting LLMs\\nand MLLMs to perform ranking tasks on multimodal documents, with prompting strategies\\ngenerally categorized into three types: point-wise, pair-wise, and list-wise methods.\\n– Reranking for Text-Model : LLMs are increasingly employed in text-modal reranking tasks,\\nleveraging their advanced capabilities to optimize the ranking of textual documents.\\nPoint-wise methods evaluate the relevance between a query and individual documents,\\nreranking them based on relevance scores.. Zhuang et al. [509] integrates fine-grained rele-\\nvance labels into prompts for better document distinction. MCRanker [109] addresses biases\\nin existing point-wise rerankers by generating relevance scores based on multi-perspective\\ncriteria. UPR [318] re-scores retrieved passages using a zero-shot question generation model.\\nZhuang et al. [511] show that LLMs pre-trained without supervised instruction fine-tuning\\n(e.g., LLaMA) also exhibit strong zero-shot ranking capabilities. Despite their effectiveness,\\nthese methods often rely on suboptimal handcrafted prompts. To improve prompts for rank-\\ning tasks, Co-Prompt [52] introduces a discrete prompt optimization method for improving\\nprompt generation in reranking tasks. PaRaDe [70] proposes a difficulty-based approach to\\nselect the most challenging in-context demonstrations for prompts, though experiments reveal\\nthat this method does not significantly outperform random selection. To improve demonstra-\\ntion selection, DemoRank [228] advances demonstration selection with a dependency-aware\\ndemonstration reranker, optimizing top-ranked examples through efficient training sample\\nconstruction and a novel list-pairwise loss.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 26, 'page_label': '27', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 27\\nPair-wise methods involve presenting LLMs with a query and a document pair, instructing\\nthem to identify the more relevant document. PRP-AllPair [302] generates all possible pairs,\\nassigns discrete relevance judgments, and aggregates these into a final relevance score per\\ndocument. PRP-Graph [248] improves this by using judgment generation probabilities and a\\ngraph-based aggregation for scoring relevance. Additionally, a post-processing technique [416]\\nrefines LLM-generated labels by aligning them with pairwise preferences while minimizing\\ndeviations from original values.\\nListwise methods directly rank document lists by incorporating queries and documents into\\nprompts, instructing LLMs to output reranked document identifiers. RankGPT [341] introduces\\ninstructional permutation generation and a sliding window strategy to address context length\\nlimits, while LRL [256] reorders document identifiers for candidate documents. However, these\\nmethods face challenges: (1) performance is highly sensitive to document order, revealing\\npositional bias, and (2) the sliding window strategy limits the number of documents ranked\\nper iteration. Recent advancements have attempted to address these issues: Tang et al. [349]\\npropose permutation self-consistency to mitigate bias. TourRank [43] introduces a tournament\\nmechanism, parallelizing reranking to minimize the impact of initial document order. TDPart\\n[292] employs a top-down partitioning algorithm, which processes documents to depth using\\na pivot element. FIRST [ 309] leverages the output logits of the first generated identifier to\\ndirectly obtain a ranked ordering of candidates.\\n– Reranking for Cross-Model : Prompt-Based Multimodal Reranker uses prompts to guide a\\nMLLM in reranking items. TIGeR [303] proposes a framework leveraging multimodal LLMs\\nfor zero-shot reranking via a generative retrieval approach. However, their method is limited\\nto text-only query retrieval tasks. In contrast, Lin et al. [210] extends this scope by utilizing\\nmultimodal LLMs to address diverse multimodal reranking tasks, supporting queries and\\ndocuments in text, image, or interleaved text-image formats.\\nIn summary, these approaches leverages the pre-existing knowledge and reasoning capabilities\\nof LLMs, reducing the need for extensive task-specific fine-tuning. Consequently, it provides\\ngreater flexibility and resource efficiency, particularly in scenarios with limited labeled data or\\ncomputational resources. However, its effectiveness depends heavily on the quality and design of\\nthe prompts, as well as the model’s ability to generalize its pre-trained knowledge to the specific\\ndemands of the target task.\\n3.3.3 REFINER. Theoretically, LLMs improves with more comprehensive task-relevant knowledge\\nin the retrieved and reranked context. However, unlimited input length poses practical deployment\\nchallenges: (1) Limited Context Window: LLMs have a fixed input length determined during pre-\\ntraining, and any text exceeding this limit is truncated, leading to loss of contextual semantics.\\n(2) Catastrophic Forgetting: Insufficient cache space can cause LLMs to forget previously learned\\nknowledge when processing long sequences. (3) Slow Inference Speed. Consequently, refined\\nprompts are crucial for optimizing LLM performance.\\nThe refiner is an optional yet highly impactful component that optimizes retrieved and reranked\\ninformation before its utilization by the LLM. It performs advanced processing tasks, such as\\nsummarization, distillation, or contextualization, to condense and refine content into a more\\ndigestible and actionable format. By extracting key insights, eliminating redundancies, and aligning\\ninformation with the query’s context, the refiner enhances the utility of the retrieved data, enabling\\nthe LLM to generate more coherent, accurate, and contextually relevant responses.\\nPrompt refinement can be achieved through two primary approaches: hard prompt methods and\\nsoft prompt methods. Hard prompt methods involve filtering out unnecessary or low-information\\ncontent, still using natural language tokens and resulting in less fluent but generalizable prompts\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 27, 'page_label': '28', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='28 Trovato et al.\\nthat can be used across LLMs with different embedding configurations. Soft prompt methods, in\\ncontrast, encode prompt information into continuous representations, producing latent vectors\\n(special tokens) that are not human-readable but optimized for model performance.\\n•Hard Prompt Refiner : Hard prompts consist of natural language tokens from the LLM/MLLM’s\\nvocabulary, representing specific words or sub-words, and can be generated by humans or\\nmodels.\\n– Refining for Text-Model : Recent advancements in prompt compression and context distil-\\nlation aim to optimize the efficiency of LLMs. DynaICL [ 499] employs a meta controller to\\ndynamically allocate in-context demonstrations based on input complexity and computational\\nconstraints. FILCO [385] distills retrieved documents using lexical and information-theoretic\\nmethods—String Inclusion, Lexical Overlap, and CXMI—training both context filtering and\\ngeneration models for RAG tasks. CPC [213] preserves semantic integrity by using a context-\\naware encoder to remove irrelevant sentences, while AdaComp [ 469] dynamically selects\\noptimal documents via a compression-rate predictor. LLMLingua [148] introduces a coarse-\\nto-fine approach, compressing prompt components (instructions, questions, demonstrations)\\nusing a small language model (SLM) to measure token informativeness via perplexity (PPL).\\nLongLLMLingua [149] extends this to long documents, employing a linear scheduler, reorder-\\ning mechanism, and contrastive perplexity to retain question-relevant tokens while ensuring\\nkey information integrity. CoT-Influx [132] compresses GPT-4-generated Chain-of-Thought\\n(CoT) prompts using a shot-pruner and token-pruner, both implemented as MLPs trained\\nvia reinforcement learning. These methods collectively improve performance while reducing\\nuseless CoT examples and redundant tokens. Selective Context [196] evaluates lexical unit in-\\nformativeness using a causal language model and a percentile-based filtering method to remove\\nredundancy. It calculates token self-information by predicting next-token probabilities, aggre-\\ngating these at phrase and sentence levels. Prompt-SAW [11] preserves syntactic and semantic\\nstructures by extracting key tokens via relation-aware graphs, integrating them into com-\\npressed prompts. PCRL [159] treats prompt compression as a binary classification task, using a\\nfrozen pre-trained policy language model with trainable MLP layers. The compression policy\\nlabels tokens as include or exclude, optimizing a reward function that balances faithfulness and\\nprompt length reduction. LLMLingua-2 [288] employs a bidirectional encoder-only model with\\na linear classification layer for compression, determining token retention or removal. RECOMP\\n[410] employs extractive and abstractive compressors to generate query-focused summaries,\\nleveraging contrastive learning and knowledge distillation. Nano-Capsulator [56] optimizes\\ncompression using reward feedback from response differences and enforces strict length con-\\nstraints. MEMWALKER [32] uses interactive prompting to build and navigate a memory tree\\nfor context summarization. CompAct [432] sequentially compresses document segments for\\nlong-context question-answering, achieving high compression rates. Style-Compress [297]\\niteratively refines prompts using diverse styles and task-specific examples, evaluated by larger\\nLLMs. TCRA-LLM [220] combines summarization and semantic compression to reduce token\\nsize. TACO-RL [323] employs reinforcement learning for task-aware prompt compression,\\nensuring low latency. FaviComp [ 158] enhances evidence familiarity by combining token\\nprobabilities from compression and target models, reducing perplexity.\\n– Refining for Cross-Model : Recent advancements in visual token compression for MLLMs\\nfocus on enhancing efficiency without significant performance loss. LLaVolta [34] introduces\\na method to reduce the number of visual tokens, enhancing training and inference efficiency\\nwithout compromising performance. To minimize information loss during compression while\\nmaintaining training efficiency, it employs a lightweight, staged training scheme. This scheme\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 28, 'page_label': '29', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 29\\nprogressively compresses visual tokens from heavy to light compression during training, en-\\nsuring no information loss during testing. PyramidDrop [407] is a visual redundancy reduction\\nstrategy for MLLMs, designed to improve efficiency in both inference and training with negli-\\ngible performance loss. It partitions the MLLM into several stages and drops a predefined ratio\\nof image tokens at the end of each stage. DeCo [425] proposes the principle of \"Decouple Com-\\npression from Abstraction, \" which involves compressing visual tokens at the patch level using\\nprojectors while allowing the LLM to handle visual semantic abstraction entirely. MustDrop\\n[226] measures the importance of each token throughout its lifecycle, including the vision\\nencoding, prefilling, and decoding stages. During vision encoding, it merges spatially adjacent\\ntokens with high similarity and establishes a key token set to retain vision-critical tokens.\\nIn the prefilling stage, it further compresses vision tokens guided by text semantics using a\\ndual-attention filtering strategy. In the decoding stage, an output-aware cache policy reduces\\nthe size of the KV cache. By employing tailored strategies across these stages, MustDrop\\nachieves an optimal balance between performance and efficiency. G-Search [484] proposes a\\ngreedy search algorithm to determine the minimum number of vision tokens to retain at each\\nlayer, from shallow to deep. Based on this strategy, a parametric sigmoid function (P-Sigmoid)\\nis designed to guide token reduction at each layer of the MLLM, with parameters optimized\\nusing Bayesian Optimization. G-Prune [151] introduces a graph-based method for training-free\\nvisual token pruning. It treats visual tokens as nodes and constructs connections based on\\nsemantic similarities. Information flow is propagated through weighted links, and the most\\nimportant tokens are retained for MLLMs after iterations.\\nAlthough interpretable and transparent, the inherent ambiguity of hard prompts often hinders\\nthe precise expression of intent, limiting their effectiveness in diverse or complex scenarios.\\nCrafting accurate and impactful hard prompts demands significant human effort and may require\\nmodel-based refinement or optimization. Moreover, even minor variations in hard prompts can\\nlead to inconsistent LLM performance for identical tasks.\\n•Soft Prompt Refiner : Soft prompts are trainable, continuous vectors that match the dimension-\\nality of token embeddings in LLM’s vocabulary. Unlike hard prompts, which rely on discrete\\ntokens from a predefined vocabulary, soft prompts are optimized through training to capture\\nnuanced meanings that discrete tokens cannot express. When fine-tuned on diverse datasets,\\nsoft prompts enhance the LLM’s performance across various tasks.\\n– Refining for Text-Model : Language models convert text prompts into vectors for denser\\nrepresentation, enabling compression of discrete text into continuous vectors within the model.\\nThese vectors can serve as internal parameters (internalization) or additional soft prompts\\n(encoding). Such compression extends the context window and enhances inference speed,\\nparticularly with repeated prompt usage.\\nEarly work focused on system prompt internalization. Askell et al. [13] used Knowledge\\nDistillation to align models with human values, while Choi et al. [53] introduced Pseudo-Input\\nGeneration, generating pseudo-inputs from prompts and distilling knowledge between teacher\\nand student models to avoid redundant inference computations. Later research compressed user\\nprompt contexts. Snell et al. [331] distilled abstract instructions, reasoning, and examples into\\nprompts with distinct distribution differences, enabling task execution without explicit prompts.\\nSun et al. [339] internalized ranking techniques for zero-shot relevance tasks, while Distilling\\nStep-by-Step [122] improved reasoning tasks by distilling rationales as additional supervision.\\nIn retrieval-augmented generation, xRAG [48] integrated compressed document embeddings\\nvia a plug-and-play projector, using self-distillation for robustness. For context compression,\\nCOCOM [308] reduced long contexts to few embeddings, balancing trade-offs between decoding\\ntime and answer quality. LLoCO [346] learned offline compressed representations for efficient\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 29, 'page_label': '30', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='30 Trovato et al.\\nQA retrieval. QGC [25] retained key information under high compression using query-guided\\ndynamic strategies. UniICL [91] unified demonstration selection, compression, and generation\\nwithin a single frozen LLM, projecting demonstrations and inputs into virtual tokens for\\nsemantic-based processing.\\nRecent advancements in prompt compression for LLMs focus on encoding hard prompts\\ninto reusable soft prompts to enhance efficiency and generalization across tasks. Early work by\\nWingate et al. [396] distilled complex hard prompts into concise soft prompts by minimizing\\noutput distribution differences, reducing inference costs. A series of works aim to enhance\\ngeneralization across diverse prompts. Gist [279] used meta-learning to encode multi-task in-\\nstructions into gist tokens, while Gist-COCO [193] employed an encoder-decoder architecture\\nto compresses original prompts into shorter gist prompts, via the Minimum Description Length\\nprinciple. UltraGist [467] optimized cross-attention for compressing ultra-long contexts into\\nnear-lossless UltraGist tokens. AutoCompressor [49] iteratively compressed contexts segments\\ninto summary vectors using a Recurrent Memory Transformer, reducing computational load.\\nOther approaches, like ICAE [99] and 500xCompressor [203], fine-tuned LoRA-adapted LLMs\\nfor context encoding and prompt compression. For LLM-based recommendations, POD [190]\\ndistilled discrete prompt templates into continuous prompt vectors with an whole-word em-\\nbedding to integrate the item ID, while RDRec [379] synthesizes training data and internalizes\\nrationales into a smaller model. SelfCP [90] balances training cost, inference efficiency, and\\ngeneration quality by compressing over-limit prompts asynchronously using frozen LLMs\\nas the compressor and generator and trainable linear layers to project hidden states into\\nLLM-acceptable memory tokens.\\n– Refining for Cross-Model : PromptMM [393] tackles overfitting and side information inac-\\ncuracies in multi-modal recommenders by using Multi-modal Knowledge Distillation with\\nprompt-tuning. It compresses models by distilling user-item relationships and multi-modal\\ncontent from complex teacher models to lightweight student models, eliminating extra pa-\\nrameters. Soft prompt-tuning bridges the semantic gap between multi-modal context and\\ncollaborative signals, enhancing robustness. Additionally, a disentangled multi-modal list-wise\\ndistillation with modality-aware re-weighting addresses multimedia data inaccuracies. RACC\\n[395] compresses and aggregates retrieved knowledge for image-question pairs, generating a\\ncompact Key-Value (KV) cache modulation to adapt downstream frozen MLLMs for efficient\\ninference. VTC-CLS [364] uses the prior knowledge of the association between the [CLS] token\\nand visual tokens in the visual encoder to evaluate visual token importance, enabling Visual\\nToken Compression and shortening visual context. VisToG [128] introduces a grouping mecha-\\nnism using pretrained vision encoders to group similar image segments without segmentation\\nmasks. Semantic tokens represent image segments after linear projection and before input\\ninto the vision encoder. Isolated attention identifies and eliminates redundant visual tokens,\\nreducing computational demands.\\nHowever, as dataset size increases, so do the computational resource requirements. Additionally,\\nsoft prompts are less interpretable than hard prompts, as their continuous vectors are not directly\\nreadable or explainable by humans.\\n3.4 Multimodal Generation\\nMultimodal generation based on Multimodal Large Language Models (MLLMs) represents a sig-\\nnificant advancement, enabling the generation of content across multiple modalities such as text,\\nimages, audio, and video. These models leverage the strengths of large language models (LLMs) and\\nextend them to handle and integrate diverse data types, creating rich, coherent, and contextually\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 30, 'page_label': '31', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 31\\nMultimodal\\nGeneration\\nModality\\nAugmentation\\nInternLM-XComposer [465], InternLM-XComposer2 [ 67], InternLM-XComposer-2.5 [ 466], MuRAR [ 508],\\n𝑀2𝑅𝐴𝐺 [259]\\nMLLM\\n•Image ⊕Text →Text\\nBLIP-2 [185], ChatSpot [ 483], OpenFlamingo [ 14], ASM [ 378], Qwen-VL [16], Kosmos-2.5 [ 249], InternLM-\\nXComposer [465], JAM [ 8], Kosmos-1 [ 130], PaLM-E [ 69], ViperGPT [ 343], PandaGPT [ 335], PaLI-X [ 40], LLaVA-\\nMed [182], LLaVAR [478], mPLUG-DocOwl [ 426], P-Former [ 145], MiniGPT-v2 [35], LLaVA [219], MiniGPT-4\\n[504], mPLUG-Owl [ 427], Otter [ 181], MultiModal-GPT [ 103], CogVLM [ 377], mPLUG-Owl2 [ 428], Monkey\\n[205], DocPedia [ 81], ShareGPT4V [ 37], mPLUG-PaperOwl [ 123], RLHF-V [438], Silkie [ 189], Lyrics [ 241], VILA\\n[209], CogAgent [ 121], Volcano [176], DRESS [ 44], LION [ 31], Osprey [ 443], LLaVA-MoLE [39], VLGuard [ 514],\\nMobileVLM V2 [55], ViGoR [ 417], V* [ 399], MobileVLM [54], TinyGPT-V [444], DocLLM [ 367], LLaVA-Phi [507],\\nKAM-CoT [277], InternLM-XComposer2 [ 67], InternLM-XComposer-2.5 [ 466], MoE-LLaVA [207], VisLingInstruct\\n[505]\\n•Image ⊕Text →Image ⊕Text\\nVisual ChatGPT [ 397], DetGPT [ 295], FROMAGe [165], Shikra [ 36], GPT4RoI [ 472], SEED [ 100], LISA [ 168],\\nGILL [164], Kosmos-2 [ 293], DreamLLM [ 65], MiniGPT-5 [490], Kosmos-G [ 287], VisCPM [ 124], CM3Leon [ 434],\\nLaVIT [155], GLaMM [ 307], RPG [ 418], Vary-toy [392], CogCoM [ 298], SPHINX-X [ 216], LLaVA-Plus [223],\\nPixelLM [310], VL-GPT [506], CLOVA [96], Emu-2 [337], MM-Interleaved [355], DiffusionGPT [301]\\n•Video ⊕Text →Text\\nVideo-ChatGPT [260], VideoChat [187], Dolphins [257]\\n•Video ⊕Text →Video ⊕Text\\nVideo-LaVIT [154]\\n•Unified →Text\\nFlamingo [10], X-LLM [ 30], LanguageBind [ 503], InstructBLIP [ 219], MM-REACT [423], X-InstructBLIP [ 289],\\nEmbodiedGPT [280], Video-LLaMA [460], Lynx [450], LLaMA-VID [198], InternVL [47], AnyMAL [278]\\n•Unified →Image ⊕Text\\nBuboGPT [487], Emu [338], GroundingGPT [204]\\n•Unified →Unified\\nTEAL [424], GPT-4 [7], Gemini [ 353], HuggingGPT [ 325], CoDi-2 [ 351], AudioGPT [129], ModaVerse [382],\\nMLLM-Tool [366], ControlLLM [236], NExT-GPT [400]\\nFig. 8. Taxonomy of recent advancements in multimodal generation research.\\nrelevant outputs. We classify MLLMs from generative perspectives of inputs and outputs, and\\nsummarize the related researches in Figure 8.\\n3.4.1 MODALITY INPUT. With the rapid advancement of large language models in the domain\\nof textual knowledge comprehension and question-answering, researchers try to explore how\\nto enable these models to understand and process inputs from a broader range of modalities,\\nthereby facilitating more extensive multimodal question-answering tasks. Initial efforts focused on\\nincorporating image modality into the input of large models. For instance, Blip-2 [185] proposes\\na generic and efficient pre-training strategy that bootstraps vision-language pre-training from\\noff-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges\\nthe modality gap with a lightweight Querying Transformer, which is pre-trained in two stages.\\nThe first stage bootstraps vision-language representation learning from a frozen image encoder.\\nThe second stage bootstraps vision-to-language generative learning from a frozen language model.\\nInternlm-xcomposer2 [66] proposes a vision-language model excelling in free-form text-image com-\\nposition and comprehension. This model goes beyond conventional vision-language understanding,\\nadeptly crafting interleaved text-image content from diverse inputs like outlines, detailed textual\\nspecifications, and reference images, enabling highly customizable content creation. DiffusionGPT\\n[301] leverages Large Language Models (LLM) to offer a unified generation system capable of\\nseamlessly accommodating various types of prompts and integrating domain-expert models. Diffu-\\nsionGPT constructs domain-specific Trees for various generative models based on prior knowledge.\\nWhen provided with an input, the LLM parses the prompt and employs the Trees-of-Thought to\\nguide the selection of an appropriate model.\\nAs the variety of modal data continues to expand, more complex modalities, such as video, have\\nbeen integrated into the inputs of large models. For instance, Video-ChatGPT [ 260] proposes a\\nmultimodal model that merges a video-adapted visual encoder with an LLM. The resulting model is\\ncapable of understanding and generating detailed conversations about videos. Video-LaVIT [154]\\naddress spatiotemporal dynamics limitations in video-language pretraining with an efficient video\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 31, 'page_label': '32', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='32 Trovato et al.\\ndecomposition that represents each video as keyframes and temporal motions. These are then\\nadapted to an LLM using well-designed tokenizers that discretize visual and temporal information as\\na few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference,\\nthe generated tokens from the LLM are carefully recovered to the original continuous pixel space\\nto create various video content. The proposed framework is both capable of comprehending and\\ngenerating image and video content.\\nRecently, the input for multimodal large models has evolved from specialized modal data to a\\nunified input that can handle arbitrary modal data. For instance, InstructBLIP [289] conduct a vision-\\nlanguage instruction tuning based on the pretrained BLIP-2 models. Additionally, we introduce an\\ninstruction-aware Query Transformer, which extracts informative features tailored to the given\\ninstruction. InternVL [47] design a large-scale vision-language foundation model (InternVL) which\\nscales up the vision foundation model to 6 billion parameters and progressively aligns it with\\nthe LLM using web-scale image-text data from various sources. GPT-4 [7] proposes a large-scale,\\nmultimodal model which can accept image and text inputs and produce text outputs. While less\\ncapable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on\\nvarious professional and academic benchmarks. HuggingGPT [325] proposes an LLM-powered agent\\nthat leverages LLMs (eg, ChatGPT) to connect various AI models in machine learning communities\\n(eg, Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when\\nreceiving a user request, select models according to their function descriptions available in Hugging\\nFace, execute each subtask with the selected AI model, and summarize the response according to\\nthe execution results. By leveraging the strong language capability of ChatGPT and abundant AI\\nmodels in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning\\ndifferent modalities and domains. NExT-GPT [400] present an end-to-end general-purpose any-to-\\nany MM-LLM system. NExT-GPT connect an LLM with multimodal adaptors and different diffusion\\ndecoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations\\nof text, image, video, and audio. By leveraging the existing well-trained high-performing encoders\\nand decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection\\nlayers, which not only benefits low-cost training but also facilitates convenient expansion to more\\npotential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and\\nmanually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with\\ncomplex cross-modal semantic understanding and content generation.\\n3.4.2 MODALITY OUTPUT. With the explosive growth in the capabilities of MLLMs, the ability\\nto answer questions based on multimodal inputs and generate multimodal outputs has also seen a\\nqualitative improvement. There is also increasing attention from researchers on VQA scenarios that\\nshift from generating text results to generating multimodal results that include text.In this section,\\nwe are discussing multimodal outputs that are not scenarios like text-to-image or text-to-video,\\nwhich only generate a single modality, but rather scenarios where the answers includes text and at\\nleast one other modality of data, such as text-image output, or image-video output.In the basic VQA\\ntask, MIMOQA[328] was the first to propose the concept of multimodal output, which achieved\\nthe capability of multimodal output by transforming questions into an image-text matching task.\\nIt constructed a dual-tower model called MExBERT. The text stream, based on BERT, takes in\\nthe query and related documents to output the final text answer. The visual stream, based on\\nVGG-19, receives images related to the query and documents, outputting a relevance score between\\nthe image and text. The final insertion of the image is determined by this relevance score. Its\\ngroundbreaking introduction to multimodal output research has, however, certain limitations: 1) It\\nis necessary to screen out images related to the question. The model only needs to select and output\\nimages from the small number of screened ones. The task is relatively simple. 2) Multimodality is\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 32, 'page_label': '33', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 33\\nstill limited to the image modality. 3) To simplify the issue, it is still limited to scenarios where the\\ninput images must include at least one relevant image. Based on the aforementioned limitations,\\nthe latest research has made corresponding improvements[67, 259, 401, 465, 466, 508].\\nA common workflow paradigm for implementing multimodal output is to first conduct position\\nidentification after generating a text answer to determine where to insert multimodal data. Subse-\\nquently, based on the surrounding context of the corresponding positions, candidate multimodal\\ndata is retrieved. Finally, a relevance matching model is utilized to determine the final data to\\nbe inserted. InternLM-XComposer [ 465] achieves multimodal output of text and images. After\\ngenerating each paragraph of text, it calls a model to determine whether to insert an image. If\\nit is determined that an image needs to be inserted, it will generate a caption of the image to be\\ninserted and search the web for candidate images, eventually allowing the model to select the most\\nrelevant image from candidate set for insertion. InternLM-XComposer2 and 2.5 [ 67, 466] allow\\nusers to directly input a set of candidate images on the basis of the above. MuRAR [508] has also\\nimplemented multi - modal output in RAG scenarios based on this paradigm, but it has innovated\\nthe methods of position identification and candidate set recall in RAG scenarios. It uses source\\nattribution to confirm the correspondence between the generated snippet and the retrieved snippet\\nfrom the large model input, thereby determining the insertion point, and the candidate set directly\\nuses the multimodal data associated with the retrieved snippet, simplifying the recall operation. In\\naddition, it has expanded the multimodal data from images to include tables and videos. 𝑀2𝑅𝐴𝐺\\n[259] employs an alternative paradigm to achieve multimodal output in the RAG scenario. It uses\\nthe user’s query to simultaneously recall associated text elements and images. Then, based on the\\nassociations of the images and text elements in the original document, they are refined. Subse-\\nquently, MLLMs are employed to vectorize the images or convert them into descriptions, which are\\ninput into the generative model in the form of placeholders. The output generates answer text and\\na simple description placeholder for the associated image. Finally, through a chain-of-thought(COT)\\nprocess, the placeholders are converted into actual images. NExT-GPT [401] employs an entirely\\ndifferent and novel paradigm. It directly trains a unified multimodal large model, unifying the\\nreasoning and generation process, and directly generates multimodal data including text, images,\\nvideos, etc., through the model [401].\\n4 Dataset for MRAG\\nTo evaluate the general capabilities of MRAG systems in real-world multimodal understanding and\\nknowledge-based question-answering tasks, we curated a collection of existing datasets designed to\\ncomprehensively evaluate the MRAG pipeline. These datasets are categorized into two classes: (1)\\nRetrieval & Generation-Joint Components, which evaluate the synergy of retrieval and generation\\nby requiring systems to retrieve external knowledge and generate accurate responses; and (2)\\nGeneration, focusing solely on the model’s ability to produce contextually accurate outputs without\\nexternal retrieval. This categorization enables a detailed evaluation of MRAG systems’ strengths\\nand limitations in diverse scenarios.\\n4.1 Dataset for Retrieval & Generation\\nDatasets for Retrieval & Generation in MRAG are designed to evaluate end-to-end systems capable\\nof retrieving relevant knowledge from multimodal sources (e.g., text, images, videos) and generating\\naccurate responses. These datasets evaluate the synergistic integration of retrieval and generation,\\nfocusing on the system’s ability to dynamically utilize external knowledge to improve response\\nquality and relevance. In this section, we introduce key benchmarks designed for diverse evaluation\\nof Retrieval & Generation tasks. Figure 9 provides an overview of existing benchmarks, while Table\\n1 summarizes the statistics of selected representative datasets.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 33, 'page_label': '34', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='34 Trovato et al.\\nDataset for\\nRetrieval & Generation\\nVisual Advertisement Ads [138]\\nVideo Understanding\\n& Reasoning KnowIT VQA [97], SOK-Bench [365]\\nVisual Commonsense\\n& Reasoning VCR [448], VisualCOMET [291]\\nComprehensive\\nKB-VQA [375], FVQA [374], KVQA [322], OK-VQA [263], S3VQA [140], ManyModalQA\\n[114], MultiModalQA [ 345], MIMOQA [ 329], A-OKVQA [321], WebQA [28], ViQuAE [ 177],\\nInfoSeek [42], Encyclopedic-VQA [272], MMSearch [ 147], MRAG-bench [ 126], MRAMG-\\nBench [435]\\nFig. 9. Categories of MRAG dataset for retrieval & generation.\\nTable 1. Summary of dataset for retrieval & generation.\\nDataset Time Statistics\\nComprehensive\\nKB-VQA [375] 2015 2,402 questions, 700 images, 1 knowledge bases.\\nFVQA [374] 2017 5,826 questions, 2,190 images, 3 knowledge bases.\\nKVQA [322] 2019 183,007 question-answer pairs about 18,880 unique entities contained within 24,602 images.\\nOK-VQA [263] 2019 14,055 questions, 10 scenarios.\\nS3VQA [140] 2021 6,765 question-image pairs.\\nManyModalQA [114] 2020 10,190 questions with 2,873 image, 3,789 text, and 3,528 table.\\nMultiModalQA [345] 2021 29,918 questions that requires knowledge from text, tables, and images (35.7% require cross-modality\\nreasoning).\\nMIMOQA [329] 2021 56,693 QA pairs, with 401,182 images.\\nA-OKVQA [321] 2022 24,903 multiple-choice questions.\\nWebQA [28] 2022 24,929 image-based and 24,343 text-based questions.\\nViQuAE [177] 2022 3.7K questions paired with images. A Knowledge base composed of 1.5M Wikipedia articles paired with\\nimages.\\nInfoSeek [42] 2023 8.9K human-written and 1.3M semi-automated questions, 9 image classification and retrieval datasets.\\nEncyclopedic-VQA\\n[272]\\n2023 1M Image-Question-Answer triplets derived from 221k textual QA pairs from 16.7k different categories.\\nEach QA pair is combined with (up to) 5 images. 514k unique images. 15k textual single-hop questions,\\n25k multi-answer questions, and 22k two-hop questions.\\nMMSearch [147] 2024 2,901 unique images, 300 manually collected queries spanning 14 subfields.\\nMRAG-bench [126] 2024 1,353 multiple-choice questions, 16,130 images, 9 scenarios.\\nMRAMG-Bench\\n[435]\\n2025 4,800 QA pairs across three distinct domains, containing 4,346 documents and 14,190 images, with tasks\\ncategorized into three difficulty levels.\\nVisual Commonsense Reasoning\\nVCR [448] 2019 290k multiple choice QA problems derived from 110k movie scenes.\\nVisualCOMET [291] 2020 1,465,704 commonsense inferences over 59,356 images, and 139,377 distinct events.\\nVideo Understanding & Commonsense Reasoning\\nKnowIT VQA [97] 2020 24,282 human-generated QA pairs about a popular sitcom.\\nSOK-Bench [365] 2024 44K QA pairs covers over 12 types of questions, sourcing from about 10K situations. Each question is\\naccompanied by two types of answers: a direct answer and a set of four multiple-choice options.\\nVisual Advertisement\\nAds [138] 2017 202,090 questions from 64,832 image ads and 3,477 video ads.\\nEarly knowledge-based datasets include KB-VQA [375] and FVQA [374], which rely on closed\\nknowledge. FVQA, for instance, uses a fixed knowledge graph, making questions straightforward\\nonce the knowledge is known, with minimal reasoning required. KVQA [322] focuses on images in\\nWikipedia articles, primarily testing named entity recognition and Wikipedia knowledge retrieval\\nrather than commonsense reasoning. OK-VQA [263] and A-OKVQA [321] evaluate multimodal\\nreasoning using external knowledge, with A-OKVQA introducing \"rationale\" annotations to better\\nevaluate knowledge acquisition and reasoning. S3VQA [140] extends OK-VQA by requiring object\\ndetection and web queries, but like OK-VQA, it often reduces to single retrieval tasks rather than\\ncomplex reasoning. MultiModalQA [345] pioneers complex questions requiring reasoning across\\nsnippets, tables, and images, focusing on cross-modal knowledge extraction. However, its template-\\nbased questions simplify the task to filling in blanks with modality-specific answering mechanisms.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 34, 'page_label': '35', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 35\\nManyModalQA [114] also uses snippets, images, and tables but emphasizes answer modality\\nchoice over knowledge aggregation. MIMOQA [329] introduces “Multimodal Input Multimodal\\nOutput”, requiring both text and image selections to enhance understanding. WebQA [ 28] is a\\nmanually crafted, multi-hop multimodal QA dataset that retrieves visual content but provides\\nonly textual answers, relying solely on MLLMs for reasoning, making it unsuitable for models\\ndependent on linguistic context. ViQuAE [ 177] focuses on answering questions about named\\nentities grounded in a visual context using a Knowledge Base. InfoSeek [42] and Encyclopedic-VQA\\n[272] target knowledge-based questions beyond common sense knowledge, with Encyclopedic-\\nVQA using model-generated annotations. MMSearch [147] evaluates MLLMs as multimodal search\\nengines, focusing on image-to-image retrieval. Compared with previous works, MRAG-bench\\n[126] evaluates MLLMs in utilizing vision-centric retrieval-augmented knowledge, identifying\\nscenarios where visual knowledge outperforms textual knowledge. MRAMG-Bench [435] evaluates\\nanswers combining text and images, leveraging multimodal data within a corpus. Additionally,\\nVCR [448] and VisualCOMET [291], derived from movie scenes, evaluate Visual Commonsense\\nReasoning. KnowIT VQA [97] and SOK-Bench [365] focus on video understanding and reasoning\\ntask, combining visual, textual, and temporal reasoning with knowledge-based questions. Ads [138]\\nproposes an automatic advertisement understanding task, featuring rich annotations on topics,\\nsentiments, and persuasive reasoning.\\n4.2 Dataset for Generation\\nThe Generation category evaluates a model’s intrinsic capacity to generate contextually accurate\\noutputs based solely on its pre-trained knowledge and internal reasoning, without external retrieval.\\nThis evaluation isolates the generation component, providing insights into the model’s foundational\\nlanguage understanding capabilities. It enables a detailed analysis of MRAG systems’ strengths\\nand limitations across diverse scenarios. In this section, we provide an overview of representative\\nbenchmarks developed for various evaluation of Generation tasks. The existing benchmarks are\\nsystematically organized in Figure 10, and the statistics of selected representative benchmarks are\\nsummarized in Table 2.\\n4.2.1 Comprehensive. To rigorously evaluate the capabilities of MLLMs, a diverse range of evalua-\\ntion benchmarks has been developed. These benchmarks are designed to test various dimensions\\nof model performance. By utilizing these benchmarks, researchers can systematically quantify\\nthe strengths and limitations of MLLMs, ensuring their alignment with real-world applications\\nand user expectations. This evaluation framework not only supports the iterative improvement of\\nMLLMs but also provides a standardized basis for comparing models in terms of perceptual and\\nreasoning abilities.\\nVQA v2 [ 104], an early benchmark with 453K annotated QA pairs, focuses on open-ended\\nquestions with concise answers. VizWiz [ 112], introduced around the same time, includes 8K\\nQA pairs from visually impaired individuals’ daily lives, addressing real-world needs of disabled\\nusers. NLVR2 [336] explores multi-image vision capabilities by evaluating captions against image\\npairs. However, these benchmarks often fail to assess modern MLLMs’ emergent capabilities, such\\nas advanced reasoning. Recent efforts like LVLM-eHub [ 411], MDVP [ 212], and LAMM [ 430]\\ncompile extensive datasets for comprehensive evaluation, revealing that while MLLMs excel in\\ncommonsense tasks, they lag in image classification, OCR, VQA, large-scale counting, fine-grained\\nattribute differentiation, and precise object localization. Fine-tuning can mitigate some of these\\nlimitations.\\nResearchers are developing specialized benchmarks to address the limitations of traditional\\nevaluations for MLLMs. Notable examples include MME [ 29], which covers 14 perception and\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 35, 'page_label': '36', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='36 Trovato et al.\\nDataset for\\nGeneration\\nMultidisciplinary ScienceQA [243], MMMU [445], CMMU [117], CMMMU [457], MMMU-Pro [446]\\nConversational QA SparklesDialogue [134], SciGraphQA [192], ConvBench [225], MMDU [235]\\nIndustry MME-Industry [429]\\nVideo Understanding\\nTGIF-QA [141], ActivityNet-QA [442], EgoSchema [ 262], Video-MME [ 85], MVBench [ 188],\\nMMBench-Video [77], MLVU [ 496], LVBench [ 376], Event-Bench [ 71], VNBench [ 488], TempCompass\\n[232], MovieChat [332]\\nMathematics MathVista [242], We-Math [300], Math-Vision [372], Olympiadbench [115], MathVerse [470]\\nStructural Document\\nFigureQA [160], DocVQA [268], VisualMRC [ 348], ChartQA [ 266], InfographicVQA [267], ChartBench\\n[414], SciGraphQA [ 192], MMC-Benchmark [ 217], MP-DocVQA [358], ChartX [ 404], DocGenome\\n[403], CharXiv [ 387], MMLongBench-Doc [ 258], ComTQA [486], Web2Code [447], VisualWebBench\\n[221], SciFIBench [313]\\nOptical Character\\nRecognition\\nTextVQA [327], OCR-VQA [275], WebSRC [41], OCRBench [ 233], VCR [473], SEED-Bench-2-Plus\\n[178]\\nComprehensive\\nVQA v2 [104], NLVR2 [ 336], VizWiz [ 112], MME [ 29], Visit-Bench [ 21], Touchstone [17], MM-Vet\\n[439], InfiMM-Eval [ 113], Q-Bench [ 398], Seed-Bench [ 180], SEED-Bench-2 [ 179], SEED-Bench-2-Plus\\n[178], LVLM-eHub [ 411], LAMM [ 430], MMT-Bench [431], RealWorldQA [4], WV-Bench [ 245], MME-\\nRealWorld [480], MMStar [ 38], CV-Bench [359], MDVP [ 212], FOCI [ 101], MMVP [ 360], V*-Bench\\n[399], MME-RealWorld [480], Visual COT [ 324], Mementos [ 381], MIRB [ 482], ReMI [ 162], MuirBench\\n[368], VEGA [493], MMBench [230], BLINK [86]\\nFig. 10. Categories of MRAG dataset for generation.\\ncognition tasks; MMBench [ 230], featuring 20 ability dimensions, including object localization\\nand social reasoning; and SEED-Bench [180], which focuses on multiple-choice questions. SEED-\\nBench-2 [179] expanded the scope to 24K QA pairs, including the evaluation of both text and image\\ngeneration. MMT-Bench [431] further scaled up to 31K QA pairs across diverse scenarios. Common\\nfindings reveal that model performance improves with scale, but challenges persist in fine-grained\\nperception tasks (e.g., spatial localization), chart and visual mathematics comprehension, and\\ninterleaved image-text understanding. Open-source MLLMs have shown rapid progress, often\\nmatching or surpassing closed-source models.\\nReal-world usage scenarios are critical for evaluating model performance in practical applications.\\nBenchmarks like RealWorldQA [4] evaluates spatial understanding capabilities sourced from real-\\nlife scenarios, while BLINK [86] highlights tasks such as visual correspondence and multi-view\\nreasoning that challenge current MLLMs despite being intuitive for humans. WV-Bench [245] and\\nVisit-Bench [21] emphasize human preferences and instruction-following capabilities, whereas\\nV*-Bench [399] evaluates high-resolution image processing and correct visual details through\\nattribute recognition and spatial reasoning tasks. MME-RealWorld [ 480] enhances quality and\\ndifficulty with extensive annotated QA pairs and high-resolution images. These benchmarks reveal\\nthat fine-grained perception tasks remain challenging for models, while artistic style recognition\\nand relative depth perception are relatively stronger. Although closed-source models like GPT-4o\\noutperform others, human performance still surpasses general models significantly.\\nMany studies simplify evaluation into binary or multi-choice problems for easier quantification,\\nbut this approach overlooks the importance of the reasoning process, which is critical for under-\\nstanding model capabilities. To address this, some works use open-ended generation and LLM-based\\nevaluators, though these face challenges with inaccurate LLM scoring. For instance, MM-Vet [439]\\nemploys diverse question formats to assess integrated vision-language capabilities, while Touch-\\nstone [17] emphasizes real-world dialogue evaluation, arguing that multiple-choice questions are\\ninsufficient for evaluating multimodal dialogue capabilities. InfiMM-Eval [113] evaluates models on\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 36, 'page_label': '37', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 37\\ndeductive, abductive, and analogical reasoning across tasks, including intermediate reasoning steps,\\naligning with practical scenarios like mathematical problem-solving. These benchmarks highlight\\nthe strengths and limitations of MLLMs in complex tasks. Closed-source models excel in reasoning\\nbut struggle with complex localization, structural relationships, charts, and visual mathematics.\\nHigh-resolution data improves recognition of small objects, dense text, and fine-grained details.\\nWhile Chain-of-Thought (CoT) strategies significantly boost reasoning in closed-source models,\\ntheir impact on open-source models remains limited.\\nThe development of multimodal benchmarks emphasizes continuous refinement to accurately as-\\nsess model capabilities. MMStar [38] addresses data leakage by curating 1.5K visually-dependent QA\\npairs, while CV-Bench [359] tackles the scarcity of vision-centric benchmarks with 2.6K manually-\\ninspected samples for 2D/3D understanding. FOCI [101] evaluates MLLMs using domain-specific\\nsubsets and supplementary classification datasets, revealing challenges in fine-grained perception.\\nMMVP [360] identifies 9 distinct patterns in CLIP-based models, showing MLLMs’ struggles with\\nvisual details, with only Gemini and GPT-4V performing above random guessing. Q-Bench [398]\\nevaluates low-level attribute perception, highlighting GPT-4V’s near-human performance. Visual-\\nCOT [324] introduces visual chain-of-thought prompts to enhance MLLMs’ focus on specific image\\nregions. To further upgrading vision capabilities on multiple image understanding, Mementos [381]\\nevaluates sequential image understanding, while MIRB [482] focuses on multi-image reasoning\\nacross perception, visual knowledge, and multi-hop reasoning tasks. ReMI [162] designs 13 tasks\\nwith diverse image relationships and input formats, and MuirBench [368] includes 12 multi-image\\nunderstanding tasks with unanswerable variants for robust assessment. VEGA [ 493] is specifi-\\ncally designed to evaluate interleaved image-text comprehension. The task requires models to\\nidentify relevant images and text while filtering out irrelevant information to arrive at the correct\\nanswer. Evaluation results reveal that even advanced proprietary MLLMs, such as GPT-4V and\\nGemini 1.5 Pro, achieve only modest performance, highlighting significant room for improvement\\nin interleaved information processing capabilities.\\n4.2.2 Optical Character Recognition (OCR). Multimodal benchmarks are increasingly focusing\\non the evaluation of Optical Character Recognition (OCR) tasks, driving progress in document\\nunderstanding. Early benchmarks like TextVQA [327] and OCR-VQA [275] evaluated standard\\ntext recognition, while WebSRC [ 41] introduces advanced structural reasoning tasks like web\\npage layout interpretation. SEED-Bench-2-Plus [178] and OCRBench [233] expanded evaluation to\\ndiverse data types, including charts, maps, and web pages, showing models achieving near state-of-\\nthe-art performance in recognizing various OCR text. VCR [473] addresses OCR task with partially\\nobscured text embedded in images, requiring content reconstruction. Despite advancements, many\\nMLLMs struggle with fine-grained OCR tasks. While models like GPT-4V perform well, they lag\\nbehind specialized OCR models. Performance varies significantly by data type, with knowledge\\ngraphs and maps posing greater challenges than simpler formats like charts, suggesting potential\\nimprovements through data-specific optimization or dedicated OCR integration.\\n4.2.3 Structural Document. Structural documents, including charts, HTML web content, and vari-\\nous document formats, play a critical role in practical applications due to their ability to efficiently\\nconvey complex information. These data types are characterized by their highly structured nature\\nand information density, distinguishing them from natural images. Unlike images, which rely on\\nvisual patterns and textures, structural documents require models to comprehend intricate layouts,\\nspatial relationships, and semantic connections between embedded elements such as text, tables,\\nand graphical components.\\nTo advance models capable of understanding and reasoning with such data, several benchmarks\\nhave been proposed for different types of structural documents. Early dataset FigureQA [ 160]\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 37, 'page_label': '38', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='38 Trovato et al.\\nintroduces a visual reasoning corpus with synthetic images and scientific-style figures, focusing on\\nrelationships between plot elements. ChartQA [266] emphasizes VQA with charts, ranging from\\ntasks that require both data extraction and math reasoning. ChartX [404] collects a comprehensive\\ndataset with 22 topics, 18 chart types, and 7 tasks, incorporating multiple modalities. VisualMRC\\n[348] targets visual machine reading comprehension, emphasizing natural language understanding\\nand generation. ChartBench [ 414] evaluates chart comprehension and data reliability through\\ncomplex reasoning. MMC-Benchmark [217] provides a human-annotated benchmark to assess\\nMLLMs on visual chart understanding tasks like chart information extraction, reasoning, and\\nclassification. Web2Code [447] introduces a webpage-to-code dataset for instruction tuning and\\nan evaluation framework to assess MLLMs’ webpage understanding and HTML code translation\\ncapabilities. VisualWebBench [221] evaluates MLLMs on various web tasks at website, element,\\nand action levels. Many charts lack data point annotations, necessitating MLLMs to infer values\\nusing chart elements. ComTQA [ 486] introduces a table VQA benchmark for perception and\\ncomprehension tasks, while DocVQA [268] focuses on document image QA with an emphasis on\\ninformation extraction tasks. InfographicVQA [267] targets understanding infographics images,\\nwhich are designed to present information concisely. Infographics exhibit diverse layouts and\\nstructures, requiring basic reasoning and arithmetic skills. As MLLMs advance, benchmarks now\\nfocus on complex chart and document understanding. For instance, DocGenome [403] analyzes\\nscientific papers, covering tasks like information extraction, layout detection, VQA, and code\\ngeneration. CharXiv [387] targets challenging charts from scientific papers, while MP-DocVQA\\n[358] extends DocVQA to multi-page scenario, where questions are constructed based on multi-\\npage documents instead of single page. MMLongBench-Doc [ 258] focuses on long document\\nunderstanding, averaging 47.5 pages. SciGraphQA [ 192] is a synthetic dataset with 295K QA\\ndialogues about academic graphs, generated using Palm-2 from CS/ML ArXiv papers. SciFIBench\\n[313] benchmarks scientific figure interpretation, using adversarial filtering for negative examples\\nand human verification for quality assurance.\\nDespite advancements, a performance gap persists between proprietary and open-source models\\non conventional benchmarks. Current MLLMs continue to face challenges in reasoning tasks and\\nlong-context document comprehension, particularly in interpreting extended multimodal contexts,\\nwhich remains a critical limitation.\\n4.2.4 Mathematics. Visual math problem-solving is key to evaluating MLLMs, leading to the\\ndevelopment of specialized benchmarks. MathVista [242] pioneered this effort by aggregating 28\\nexisting datasets and introducing 3 new ones, featuring diverse tasks like logical, algebraic, and\\nscientific reasoning with various visual inputs. Subsequent benchmarks, such as Math-Vision [372]\\nand OlympiadBench [115], introduced more complex tasks and fine-grained evaluation methods.\\nWe-Math [300] decomposes problems into sub-problems to assess fundamental understanding,\\nwhile MathVerse [470] further evaluates MLLMs’ comprehension of math diagrams by transforming\\nproblems into six versions with varying proportions of visual and textual content.\\nDespite promising results from MLLMs, significant challenges remain. existing MLLMs often\\nstruggle with interpreting complex diagrams, rely heavily on textual cues, and address composite\\nproblems through memorization rather than underlying reasoning. These limitations highlight the\\nneed for further development in MLLM capabilities.\\n4.2.5 Video Understanding. Traditional video-QA benchmarks like TGIF-QA [141] and ActivityNet-\\nQA [442] are domain-specific, focusing on tasks related to human activities. With advancements in\\nMLLMs, new benchmarks have emerged to address more complex video understanding challenges.\\nVideo-MME [85] explores diverse video domains with multimodal inputs and manual annotations,\\nwhile MVBench [188] reannotates existing datasets using ChatGPT. MMBench-Video [77] features\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 38, 'page_label': '39', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 39\\nfree-form questions for short to medium-length videos. Benchmarks like MLVU [496], LVBench\\n[376], Event-Bench [71], and VNBench [488] emphasize long-video understanding, testing models\\non extended multimodal contexts. VNBench [488] introduces a synthetic framework for evaluating\\ntasks like retrieval and ordering, by inserting irrelevant images or text into videos. Specialized\\nbenchmarks like EgoSchema [ 262] focus on egocentric videos. TempCompass [ 232] evaluates\\nfine-grained temporal perception, and MovieChat [ 332] targets long videos but often reduces\\ntasks to short-video problems. Current MLLMs, especially open-source ones, face challenges with\\nlong-context processing and temporal perception, underscoring the need for improved capabilities\\nin these areas.\\n4.2.6 Industry. The absence of a comprehensive benchmark for evaluating MLLMs across diverse\\nindustry verticals has limited understanding of their applicability in specialized real-world scenarios.\\nTo address this gap, MME-Industry [429] was developed specifically for industrial applications,\\ncovering over 21 industrial sectors such as power generation, electronics manufacturing, textile\\nproduction, steel, and chemical processing. Domain experts from each sector meticulously annotated\\nand validated test cases, ensuring the benchmark’s reliability, accuracy, and practical relevance.\\nMME-Industry thus serves as a robust tool for assessing MLLMs in industrial contexts.\\n4.2.7 Conversational QA. Current MLLMs are primarily designed for multi-round chatbot inter-\\nactions, yet most benchmarks focus on single-round QA tasks. To better align with real-world\\nconversational scenarios, multi-round QA benchmarks have been developed to simulate human-AI\\ninteractions with extended contextual histories. SparklesDialogue [134] evaluates conversational\\nproficiency across multiple images and dialogue turns, featuring flexible text-image interleaving\\nwith two rounds and four images per instance. SciGraphQA [192] constructs multi-turn QA con-\\nversations based on scientific graphs from Arxiv papers, emphasizing complex scientific discourse.\\nConvBench [225] assesses perception, reasoning, and creation capabilities across individual rounds\\nand overall conversations, revealing that MLLMs’ reasoning and creation failures often stem from\\ninadequate fine-grained perception. MMDU [235] engages models in multi-turn, multi-image con-\\nversations, with up to 20 images and 27 turns, highlighting that the performance gap between\\nopen-source and closed-source models is largely due to limited conversational instruction tuning\\ndata. These benchmarks collectively enhance the evaluation of MLLMs in complex, real-world\\ninteraction scenarios.\\n4.2.8 Multidisciplinary. The mastery of multidisciplinary knowledge is a key indicator of a model’s\\nexpertise, and several benchmarks have been developed to evaluate this capability. ScienceQA\\n[243] comprises scientific questions annotated with lectures and explanations, designed to facilitate\\nchain-of-thought evaluation. It spans grade-level knowledge across diverse domains. MMMU [445]\\npresents a more challenging college-level benchmark across diverse subjects, including engineering,\\nart and design, business, science, humanities, social science, and medicine. Its question format\\nextends beyond single image-text pairs to include interleaved text and images. Similarly, CMMU\\n[117] and CMMMU [457] provide Chinese domain-specific benchmarks for grade-level and college-\\nlevel knowledge, respectively. MMMU-Pro [446] enhances MMMU with a more robust version for\\nadvanced evaluation.\\nTable 2. Summary of dataset for generation.\\nDataset Time Statistics\\nComprehensive\\nVQA v2 [104] 2017 contains more than 443K training, 214K validation and 453K test image-question pairs.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 39, 'page_label': '40', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='40 Trovato et al.\\nNLVR2 [336] 2018 contains 107,292 examples of English sentences paired with web photographs, including 29,680 unique\\nsentences and 127,502 images. The task is to determine whether a natural language caption is true about\\na pair of photographs.\\nVizWiz [112] 2018 contains 20,000 training, 3,173 validation, and 8,000 test sets of visual questions originating from blind\\npeople.\\nMME [29] 2023 measures both perception and cognition abilities on a total of 14 subtasks\\nVisit-Bench [21] 2023 comprising 592 instances and 1,159 public images. The instances are either from 45 newly assembled\\ninstruction families or reformatted from 25 existing datasets. 10 instruction families cater to multi-image\\nquery scenarios.\\nTouchstone [17] 2023 908 questions covering 27 subtasks. The highest proportion of questions pertains to recognition, ac-\\ncounting for about 44.1%, followed by comprehension questions at 29.6%. The proportions of the\\nother categories are 15.3% for basic descriptive ability, 7.4% for visual storytelling ability, and 3.6% for\\nmulti-image analysis ability.\\nMM-Vet [439] 2023 defines 6 core vision-language capabilities and examines the 16 integrations of interest derived from\\ntheir combinations. It contains 200 images, and 218 questions (samples), all paired with their respective\\nground truths.\\nInfiMM-Eval [113] 2023 It consists of 279 manually curated reasoning questions, associated with a total of 342 images. The\\ndataset is categorized into three reasoning paradigms: deductive, abductive, and analogical reasoning.\\n49 questions pertain to abductive reasoning, 181 require deductive reasoning, and 49 involve analogical\\nreasoning. Furthermore, the dataset is divided into two folds based on reasoning complexity, with 108\\nclassified as “High” reasoning complexity and 171 as “Moderate” reasoning complexity.\\nQ-Bench [398] 2023 consists of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its\\nlow-level attributes.\\nSeed-Bench [180] 2023 consists of 19K multiple-choice questions with accurate human annotations, which spans 12 evaluation\\ndimensions including the comprehension of both the image and video modality.\\nSEED-Bench-2 [179] 2024 comprises 24K multiple-choice questions with accurate human annotations, which span 27 dimensions,\\nincluding the evaluation of both text and image generation.\\nLVLM-eHub [411] 2024 contains 42 datasets in our LVLM-eHub. The sizes of specific datasets are 109.8K, 29.5K, 177.2K, 67.3K, and\\n8.9K for visual perception, knowledge acquisition, reasoning, commonsense, and object hallucination,\\nrespectively.\\nLAMM [430] 2024 evaluate 9 common image tasks, using a total of 11 datasets with over 62,439 samples, and 3 common\\npoint cloud tasks, by utilizing 3 datasets with over 12,788 data samples.\\nMMT-Bench [431] 2024 comprises 31,325 meticulously curated multi-choice visual questions from various multimodal scenarios,\\ncovering 32 core meta-tasks and 162 subtasks in multimodal understanding.\\nRealWorldQA [4] 2024 consists of 765 images, with a question and easily verifiable answer for each image.\\nWV-Bench [245] 2024 constructed by selecting 500 high-quality samples from 8,000 user submissions in WV-ARENA.\\nMME-RealWorld\\n[480]\\n2024 constructed by collecting more than 300K images from public datasets and the Internet, filtering 13,366\\nhigh-quality images for annotation and contributing to 29,429 question-answer pairs that cover 43\\nsubtasks across 5 real-world scenarios.\\nMMStar [38] 2024 contains 1,500 challenging samples, each rigorously validated by humans. It identify 6 core capabili-\\nties (i.e., coarse perception, fine-grained perception, instance reasoning, logical reasoning, science &\\ntechnology, mathematics) along with 18 specific dimensions.\\nCV-Bench [359] 2024 provides 2,638 manually-inspected examples, and formulate natural language questions that evaluates\\n2D understanding via spatial relationships & object counting, and 3D understanding via depth order &\\nrelative distance.\\nMDVP [212] 2024 contains 1.6M unique image-visual prompt-text instruction-following samples, including natural images,\\ndocument images, OCR images, mobile screenshots, web screenshots, and multi-panel images.\\nFOCI [101] 2024 constructed from 5 popular classification datasets for different domains: 1) aircraft contains images of\\n100 different aircraft types; 2) flowers contains images of 102 different flower species; 3) food covers 101\\ndishes; 4) pets contains images of 37 cat and dog breeds. 5) cars covers 196 car models. Additionally,\\nFOCI create 4 domain subsets for animals (1322 classes), plants (957 classes), food (563 classes), and\\nman-made objects (2631 classes).\\nMMVP [360] 2024 summarizes 9 prevalent patterns of the CLIP-blind pairs, such as “orientation”, “counting”, and “view-\\npoint”. Utilizing the collected CLIP-blind pairs, MMVP design 150 pairs with 300 questions.\\nV*-Bench [399] 2024 It is built based on 191 high-resolution images with an average image resolution of 2246 ×1582. V*-\\nBench contains two sub-tasks: attribute recognition and spatial relationship reasoning. The attribute\\nrecognition task has 115 samples. The spatial relationship reasoning task has 76 samples.\\nMME-RealWorld\\n[480]\\n2024 contains 29,429 question-answer pairs that cover 43 subtasks across 5 real-world scenarios, where each\\none has at least 100 questions.\\nVisual COT [324] 2024 438k visual chain-of-thought question-answer pairs spans across five distinct domains, each consisting\\nof a question, an answer, and an intermediate bounding box as CoT contexts. About 98k question-answer\\npairs include extra detailed reasoning steps.\\nMementos [381] 2024 It consists of 4,761 image sequences with varying episode lengths, encompassing diverse scenarios from\\ndailylife, robotics tasks, and comic-style storyboards. Each sequence is paired with a human-annotated\\ndescription of the primary objects and their behaviors within the sequence.\\nMIRB [482] 2024 comprises 925 samples with average image number of 3.78, constructed from four distinct categories of\\nmulti-image understanding: perception, visual world knowledge, reasoning, and multi-hop reasoning.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 40, 'page_label': '41', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 41\\nReMI [162] 2024 It consists of 13 tasks that span a range of domains and properties. The tasks require reasoning over\\nup to six images, with all tasks requiring reasoning over at least two images. The images comprise a\\nvariety of heterogeneous image types.\\nMuirBench [368] 2024 It consists of 12 distinctive multi-image understanding tasks that involve 10 categories of multi-image\\nrelations, comprising 11,264 images and 2,600 multiple-choice questions, with an average of 4.3 images\\nper instance.\\nVEGA [493] 2024 contains 50k scientific literature entries, over 200k question-and-answer pairs, and a rich trove of\\n400k images. It includes the Interleaved Image-Text Comprehension subset, which is segmented into\\ntwo categories based on token length: one supports up to 4,000 tokens, while the other extends to\\n8,000 tokens. Here, images are equated to 256 tokens each. Both categories offer roughly 200k training\\ninstances and approximately 700 test samples.\\nMMBench [230] 2025 contains 3,217 multiple-choice questions covering a diverse spectrum of 20 fine-grained skills.\\nBLINK [86] 2025 reformats 14 classic computer vision tasks, and contains 3,807 multiple-choice questions across 7.3K\\nimages, paired with single or multiple images and visual prompting.\\nOptical Character Recognition\\nTextVQA [327] 2019 contains 45,336 questions asked by humans on 28,408 images that require reasoning about text to answer.\\nEach question-image pair has 10 ground truth answers provided by humans.\\nOCR-VQA [275] 2019 comprises of 207,572 images of book covers and contains more than 1 million question-answer pairs\\nabout visual question answering by reading text in images.\\nWebSRC [41] 2021 It consists of 400K question-answer pairs, which are collected from 6.4K web pages. Along with the QA\\npairs, corresponding HTML source code, screenshots, and metadata are also provided in the dataset.\\nEach question in WebSRC requires a certain structural understanding of a web page to answer.\\nOCRBench [233] 2024 includes 1000 question-answer pairs, which is consist of five components: text recognition, scene\\ntext-centric VQA, document-oriented VQA, key information extraction, and handwritten mathematical\\nexpression recognition.\\nVCR [473] 2024 It comprise 2.11M English and 346K Chinese entities, featuring captions in both languages across ‘easy’\\nand ‘hard’ difficulty levels.\\nSEED-Bench-2-Plus\\n[178]\\n2024 comprises 2.3K multiple-choice questions with precise human annotations, spanning three broad\\ncategories: Charts, Maps, and Webs.\\nStructural Document\\nFigureQA [160] 2017 its training set contains 100,000 images with 1.3 million questions; the validation and test sets each\\ncontain 20,000 images with over 250, 000 questions. The images are synthetic, scientific-style figures\\nfrom five classes: line plots, dot-line plots, vertical and horizontal bar graphs, and pie charts.\\nDocVQA [268] 2021 contains 50,000 question-answer pairs with 12,767 document images sourced from documents in UCSF\\nIndustry Documents Library.\\nVisualMRC [348] 2021 It contains 30562 pairs of a question and an abstractive answer for 10,197 document images sourced\\nfrom multiple domains of webpages.\\nChartQA [266] 2022 consists of 20,882 charts curated from four different online sources. It covers 9,608 human-written\\nquestions focusing on logical and visual reasoning questions, and generates another 23,111 questions\\nautomatically from human-written chart summaries.\\nInfographicVQA\\n[267]\\n2022 comprises 30,035 questions over 5,485 images. Questions in the dataset include questions grounded on\\ntables, figures and visualizations and questions that require combining multiple cues.\\nChartBench [414] 2023 includes over 68k charts and more than 600k high-quality instruction data, covering 9 major categories\\nand 42 subcategories of charts. 5 chart question-answering tasks to assess the models’ cognitive and\\nperceptual abilities.\\nSciGraphQA [192] 2023 generate 295K samples of open-vocabulary multi-turn question-answering dialogues about the graphs.\\nAs context, it provided the text-only Palm-2 with paper title, abstract, paragraph mentioning the\\ngraph, and rich text contextual data from the graph itself, obtaining dialogues with an average 2.23\\nquestion-answer turns for each graph.\\nMMC-Benchmark\\n[217]\\n2023 consists of 2k QA pairs with 1,063 unique images, accompanied by 1,275 multiple-choice questions and\\n851 free-form questions. The average length of the questions is 15.6.\\nMP-DocVQA [358] 2023 contains 46K questions and 6K documents, with a total of 48K pages (images). On average, each question\\nis associated with 8.27 pages.\\nChartX [404] 2024 collected 48K multi-modal chart data covering 22 topics, 18 chart types, and 7 tasks. Each chart data\\nwithin this dataset includes 4 modalities, including image, Comma-Separated Values (CSV), python\\ncode, and text description. 7 chart tasks is classified into perception tasks and cognition tasks.\\nDocGenome [403] 2024 constructed by annotating 500K scientific documents from 153 disciplines in the arXiv open-access\\ncommunity. It contains structure data from all modalities including 13 layout attributes along with their\\nLATEX source codes. It provides 6 logical relationships between different entities within each scientific\\ndocument. It covers various document-oriented tasks.\\nCharXiv [387] 2024 involves 2,323 real-world charts handpicked from scientific papers spanning 8 major subjects published\\non arXiv, and produces more than 10K questions.\\nMMLongBench-Doc\\n[258]\\n2024 comprising 1,082 expert-annotated questions. It is constructed upon 135 lengthy PDF-formatted docu-\\nments with an average of 47.5 pages and 21,214 textual tokens. 494 questions are single-page questions\\n(with one evidence page). 365 questions are cross-page questions requiring evidence across multiple\\npages. 223 questions are designed to be unanswerable for detecting potential hallucinations.\\nComTQA [486] 2024 comprises a total of 9,070 QA pairs across 1,591 images. It contains challenging questions, such as\\nmultiple answers, mathematical calculations, and logical reasoning.\\nWeb2Code [447] 2024 contains a total of 1179.7k webpage based instruction-response pairs.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 41, 'page_label': '42', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='42 Trovato et al.\\nVisualWebBench\\n[221]\\n2024 consists of 7 tasks, and comprises 1.5K human-curated instances from 139 real websites, covering 87\\nsub-domains.\\nSciFIBench [313] 2024 consists of 2000 multiple-choice scientific figure interpretation questions split between two tasks across\\n8 categories. The questions are curated from arXiv paper figures and captions.\\nMathematics\\nMathVista [242] 2023 incorporates 28 existing multimodal datasets, including 9 math-targeted question answering (MathQA)\\ndatasets and 19 VQA datasets. In addition, it creates three new datasets (i.e., IQTest, FunctionQA,\\nPaperQA) which are tailored to evaluating logical reasoning on puzzle test figures, algebraic reasoning\\nover functional plots, and scientific reasoning with academic paper figures, respectively. It consists of\\n6,141 examples, with 736 of them being newly curated.\\nWe-Math [300] 2024 It collect and categorize 6.5K visual math problems, spanning 67 hierarchical knowledge concepts and 5\\nlayers of knowledge granularity.\\nMath-Vision [372] 2024 comprises 3,040 mathematical problems within visual contexts across 12 grades, selected from 19 math\\ncompetitions. It contains 1,532 problems in an open-ended format and 1,508 in a multiple-choice format.\\nAll problems encompass 16 subjects over 5 levels of difficulty.\\nOlympiadbench\\n[115]\\n2024 an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,476 problems from Olympiad-\\nlevel mathematics and physics competitions, including the Chinese college entrance exam. Each problem\\nis detailed with expert-level annotations for step-by-step reasoning.\\nMathVerse [470] 2025 It contains 2,612 math problems from three fundamental math subjects, i.e., plane geometry (1,746), solid\\ngeometry (332), and functions (534). Each problem is then transformed by human annotators into six\\ndistinct versions, each offering varying degrees of information content in multimodality, contributing\\nto 15K test samples in total.\\nVideo Understanding\\nTGIF-QA [141] 2017 consists of 103,919 QA pairs collected from 56,720 animated GIFs. TGIF-QA includes four task types:\\nrepetition count, repeating action, state transition, frame QA.\\nActivityNet-QA\\n[442]\\n2019 It exploits 5,800 videos from the ActivityNet dataset, which contains about 20,000 untrimmed web\\nvideos representing 200 action classes. Each video is annotated with ten question-answer pairs using\\ncrowdsourcing to finally obtain 58,000 question-answer pairs. The maximum question length is 20 and\\nthe maximum answer length is 5. The average question length is 8.67 and average answer length is 1.85.\\nEgoSchema [262] 2023 consists of over 5000 human curated multiple choice question answer pairs, spanning over 250 hours\\nof real video data. For each question, it requires the correct answer to be selected between five given\\noptions based on a three-minute-long video clip.\\nVideo-MME [85] 2024 It contains a annotated set of 2,700 high-quality multiple-choice questions (3 per video) from 900 videos,\\n744 subtitles and 900 audio files across various scenarios. For diversity in video types, it spans 6 visual\\ndomains, with 30 subfields. For duration in temporal dimension, it encompasses both short-, medium-,\\nand long-term videos, ranging from 11 seconds to 1 hour.\\nMVBench [188] 2024 covers 20 video temporal understanding tasks that cannot be effectively solved with a single frame.\\nEach task produces 200 multiple-choice QA pairs by leveraging ChatGPT to automatically reannotate\\nexisting video datasets with their original annotations.\\nMMBench-Video [77] 2024 incorporates approximately 600 web videos from YouTube, spanning 16 major categories. Each video\\nranges in duration from 30 seconds to 6 minutes. The benchmark includes roughly 2,000 original\\nquestion-answer pairs, contributed by volunteers, covering a total of 26 fine-grained capabilities.\\nMLVU [496] 2024 consists of 3,102 questions across 9 categories with 2,593 questions for dev set and 509 questions for test\\nset. It is made up of videos of diversified lengths, spanning from 3 min to more than 2 hours. Besides,\\neach video is further partitioned as incremental segments, e.g., the first 3 min, the first 6 min, and the\\nentire video.\\nLVBench [376] 2024 gathers an initial collection of 500 videos, each with a minimum duration of 30 minutes. Finally, these\\nvideos is annotated to select a subset of 103 videos.\\nEvent-Bench [71] 2024 includes 6 event-related tasks and 2,190 test instances.\\nVNBench [488] 2024 1,350 samples with 9 sub-tasks.\\nTempCompass [232] 2024 collects a total of 410 videos and 500 pieces of meta-information, with 9 content categories.\\nMovieChat [332] 2024 1K long videos and 13K manual question-answering pairs.\\nIndustry\\nMME-Industry [429] 2025 encompasses 21 distinct domain, comprising 1050 question-answer pairs with 50 questions per domain.\\nConversational QA\\nSparklesDialogue\\n[134]\\n2023 SparklesDialogueCC comprises 4.5K dialogues, each consisting of at least two images spanning two\\nconversational turns. SparklesDialogueVG includes 2K dialogues, each with at least three distinct images\\nacross two turns.\\nSciGraphQA [192] 2023 selected 290,000 Computer Science or Machine Learning ArXiv papers, and then used Palm-2 to generate\\n295K samples of open-vocabulary multi-turn question-answering dialogues about the graphs. As context,\\nit provided the text-only Palm-2 with paper title, abstract, paragraph mentioning the graph, and rich\\ntext contextual data from the graph itself, obtaining dialogues with an average 2.23 question-answer\\nturns for each graph.\\nConvBench [225] 2024 comprises 577 image-instruction pairs tailored for multi-round conversations. Each pair is structured\\naround three sequential instructions, each targeting a distinct cognitive skill—beginning with perception,\\nfollowed by reasoning, and culminating in creation. Encompassing 215 tasks, the benchmark is divided\\ninto 71 tasks focused on perception, 65 on reasoning, and 79 on creation.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 42, 'page_label': '43', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 43\\nMMDU [235] 2024 comprises 110 multi-image multi-turn dialogues with more than 1600 questions, each accompanied by\\ndetailed long-form answers. The questions in MMDU involve 2 to 20 images, with an average image&text\\ntoken length of 8.2k tokens, a maximum turn length of 27, and a maximum image&text length reaching\\n18K tokens.\\nMultidisciplinary\\nScienceQA [243] 2022 multiple-choice science question dataset containing 21,208 examples. It covers diverse topics across\\nthree subjects: natural science, social science, and language science.\\nMMMU [445] 2024 includes 11.5K multimodal questions from college exams, quizzes, and textbooks, covering 6 core\\ndisciplines. These questions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous\\nimage types.\\nCMMU [117] 2024 It consists of 3,603 questions in 7 subjects, covering knowledge from primary to high school. The\\nquestions can be categorized into 3 types: multiple-choice, multiple-response, and fill-in-the-blank.\\nCMMMU [457] 2024 A Chinese Multi-discipline multimodal Understanding, including 12k manually collected multimodal\\nquestions from college exams, quizzes, and textbooks, covering 6 core disciplines. These questions span\\n30 subjects and comprise 39 highly heterogeneous image typesbenchmark.\\nMMMU-Pro [446] 2024 3460 questions in total (1730 samples are in the standard format and the other 1730 are in the screenshot\\nor photo form)\\n5 Evaluation Metrics of MRAG\\nMultimodal RAG systems generally consist of four core components: document parsing, search\\nplanning, retrieval, and generation, which collectively influence their end-to-end performance.\\nAccurate and comprehensive evaluation of these components is essential, leveraging available\\nmultimodal benchmarks. In practice, three common evaluation strategies are typically employed:\\nhuman evaluation, rule-based evaluation, and LLM/MLLM-based evaluation. Each strategy offers\\ndistinct advantages and disadvantages in calculating evaluation metrics.\\n•Human evaluation: Human evaluation is widely regarded as the gold standard for assessing\\nMRAG systems, as their effectiveness is ultimately determined by human users. This method is\\nextensively used in research to ensure the reliability and relevance of model outputs. For instance,\\nBingo [58] employs human annotators to assess the accuracy of GPT-4V’s responses, with a focus\\non identifying and analyzing model biases. In hallucination detection, M-HalDetect [108] demon-\\nstrates that human evaluation outperforms model-based methods in detecting subtle inaccuracies,\\nhighlighting its precision. Additionally, WV-Arena [245] uses a human voting system combined\\nwith Elo ratings to rank and compare multiple models, providing a robust benchmarking frame-\\nwork. However, human evaluation presents challenges, including increased time and labor costs,\\nwhich limit its scalability for large-scale assessments. The reliability of results can also be affected\\nby the limited number of evaluators, as individual biases may influence outcomes. To address\\nthese issues, some studies employ diverse evaluator pools and cross-validation techniques to\\nenhance the balance and representativeness of assessments. Nonetheless, the trade-off between\\nevaluation accuracy and resource expenditure remains a critical consideration in designing RAG\\nmodel evaluation methodologies.\\n•Rule-based evaluation: Rule-based evaluation metrics [41, 430, 473] are essential for assessing\\nthe performance of MRAG systems. These metrics rely on standardized evaluation tools, enabling\\nobjective, reproducible assessments with minimal human intervention. Compared to subjective\\nhuman evaluations, deterministic metrics offer significant advantages, including reduced time\\nconsumption, lower susceptibility to bias, and greater consistency across multiple assessments.\\nSuch consistency is particularly crucial for large-scale evaluations or when comparing different\\nsystems or model iterations.\\n•LLM/MLLM-based evaluation: For evaluation of MRAG systems, LLMs/MLLMs are employed\\nto compare reference answers with generated outputs or to directly score responses. For example,\\nMM-Vet [439] uses GPT-4 to automate evaluation, generating scores for each sample based on\\nthe input question, ground truth, and model output. Similarly, TouchStone [17] and LLaVA-bench\\n[219] leverage GPT-4 to directly compare generated answers with reference answers, simplifying\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 43, 'page_label': '44', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='44 Trovato et al.\\nthe evaluation process. While integrating LLMs/MLLMs in evaluation reduces human effort, it\\nhas limitations. This approach is prone to systematic biases, such as sensitivity to the order of\\nresponse presentation. Additionally, evaluation outcomes are heavily influenced by the inherent\\ncapabilities and limitations of the LLMs/MLLMs themselves, leading to potential inconsistencies,\\nas different models may produce divergent results for the same task. These challenges underscore\\nthe need for careful model selection and evaluation design to mitigate biases and ensure reliable\\nassessments.\\n5.1 Metrics of Retrieval & Generation\\nThe evaluation of MRAG systems is essential for ensuring their effectiveness and reliability in\\nprocessing complex, multimodal data. Evaluation metrics can be broadly classified into rule-based\\nand LLM/MLLM-based approaches.\\n•Rule-based Metrics : Rule-based metrics evaluate the performance of MRAG systems using\\npredefined criteria and heuristics. These metrics are generally interpretable, transparent, and\\ncomputationally efficient, making them well-suited for tasks with well-defined benchmarks.\\nExamples of common rule-based metrics include:\\n– Exact Match (EM) : This metric evaluates whether the model’s output exactly matches the\\nground truth, offering a clear and unambiguous performance measure. It is especially valuable\\nin tasks requiring high accuracy and fidelity to reference data, such as question answering,\\nfact verification, and information retrieval. While exact match (EM) provides a straightforward\\nand interpretable evaluation, it may fall short in scenarios where semantically equivalent but\\nlexically divergent responses are acceptable.\\n– ROUGE-N (N-gram Recall) : The ROUGE metric is a widely used framework for evaluating\\ntext summarization and generation tasks. ROUGE-N measures the overlap of N-grams (con-\\ntiguous sequences of N words) between generated text and one or more reference texts, with\\na strong emphasis on recall. This metric assesses how well the generated text captures the\\nessential content of the reference. For example, ROUGE-1 evaluates unigram overlap, ROUGE-2\\nfocuses on bigrams, and higher-order N-grams (e.g., ROUGE-3) capture more complex linguis-\\ntic structures. While ROUGE-N provides a quantitative measure of lexical similarity, it is often\\nsupplemented by other metrics to account for semantic coherence, fluency, and relevance,\\nparticularly in multimodal contexts where textual and non-textual data interact.\\n– BLEU: BLEU is a widely used metric in NLP for evaluating the quality of machine-generated\\ntext by assessing its similarity to one or more reference texts. Initially designed for machine\\ntranslation, BLEU has been adapted to various NLP tasks, including multimodal generation. In\\nmultimodal settings, BLEU can evaluate the alignment between generated text and associated\\nmodalities (e.g., images, videos) by comparing the output to reference descriptions or captions.\\nHowever, while BLEU offers a quantitative measure of n-gram overlap, it has limitations\\nin capturing semantic depth, contextual coherence, and multimodal consistency, which are\\nessential for comprehensive evaluation in MRAG systems.\\n– Mean Reciprocal Rank (MRR) : MRR is a widely used metric for evaluating the performance\\nof systems that produce ranked lists of results, such as search engines, recommendation\\nsystems, or retrieval-augmented models. MRR measures the rank position of the first relevant\\nitem in the returned list, reflecting the system’s ability to surface correct or useful information\\nquickly. It is calculated as the average of the reciprocal ranks of the first relevant result across\\nmultiple queries or tasks. A higher MRR indicates better performance, as it demonstrates the\\nsystem’s effectiveness in prioritizing relevant results at the top of the list.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 44, 'page_label': '45', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 45\\n– CIDEr (Consensus-based Image Description Evaluation) : CIDEr is specifically designed to\\nmeasure the agreement between machine-generated captions and human-authored reference\\ncaptions. It utilizes a TF-IDF weighting mechanism to quantify the similarity between generated\\nand reference texts.\\n– SPICE (Semantic Propositional Image Caption Evaluation) : The evaluation of MRAG\\nsystems frequently utilizes the SPICE metric to assess the quality of generated captions.\\nSPICE prioritizes semantic fidelity by parsing captions into structured scene graphs, which\\ndepict objects, attributes, and relationships within the text. These generated scene graphs are\\nsubsequently compared to reference graphs derived from ground-truth captions. By emphasiz-\\ning semantic similarity over lexical overlap, SPICE offers a robust measure of how well the\\ngenerated content aligns with the intended meaning. This makes it particularly well-suited\\nfor evaluating multimodal systems that integrate visual and textual information, ensuring a\\nnuanced and contextually accurate assessment of MRAG outputs.\\n– BERTScore : Evaluation of MRAG focuses on assessing the quality and relevance of outputs\\nin contexts integrating both textual and non-textual data (e.g., images, audio). A key metric\\nfor evaluating textual components is BERTScore, which utilizes contextual embeddings from\\nBERT to measure semantic similarity between generated and reference texts. Unlike traditional\\nmetrics such as BLEU or ROUGE, which depend on exact word matches or n-gram overlap,\\nBERTScore captures deeper semantic relationships by aligning tokens based on their contextual\\nembeddings.\\n– Perplexity: It measures the model’s ability to predict the next word in a sequence, with lower\\nperplexity values indicating greater confidence and accuracy in predictions. This reflects a\\nstronger understanding of the underlying data distribution.\\nRule-based metrics offer objective and reproducible outcomes but frequently lack the adaptability\\nneeded to capture nuanced semantic or contextual understanding, especially in multimodal\\nenvironments where text, images, and other data types interact.\\n•LLM/MLLM-based Metrics : The emergence of LLMs and MLLMs has transformed evalua-\\ntion paradigms, enabling the use of their advanced reasoning and comprehension capabilities.\\nLLM/MLLM-based metrics now provide more holistic and context-aware assessments of MRAG\\nsystems, with key approaches including:\\n– Answer Precision: This metric measures the degree to which the knowledge in a model-\\ngenerated answer is supported or entailed by the ground truth. It assesses the accuracy and\\nrelevance of retrieved information by evaluating the overlap between the model’s output and\\nthe factual or contextual basis provided by the ground truth. High answer precision indicates\\nthat the model effectively utilizes retrieved multimodal data to produce responses aligned\\nwith the expected factual content. This metric is crucial for evaluating the reliability and\\nfactual consistency of multimodal RAG systems, ensuring that generated outputs are both\\ncontextually appropriate and informationally accurate.\\n– Ground Truth Recall : This metric measures the degree to which the knowledge in the\\nground truth is accurately captured and reflected in the model-generated response. It assesses\\nthe model’s ability to retrieve and integrate relevant information from the provided knowledge\\nbase or multimodal sources, ensuring the output aligns with the factual or contextual details\\nin the reference data. It is particularly crucial for evaluating retrieval-augmented systems,\\nas it directly quantifies the fidelity of the model’s output to the intended knowledge. Higher\\nscores indicate stronger alignment with the ground truth, reflecting enhanced retrieval and\\ngeneration capabilities.\\n– Retrieved Context Precision : This metric measures the alignment between the knowledge\\nin the retrieved context and the information in the ground truth response. It evaluates the\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 45, 'page_label': '46', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='46 Trovato et al.\\nproportion of relevant and accurate information in the retrieved context that is directly\\nsupported or entailed by the ground truth, assessing the retrieval system’s precision and\\ncontextual appropriateness in generating accurate responses. This metric is especially vital\\nin multimodal RAG systems, where integrating diverse data types (e.g., text, images, audio)\\nrequires robust evaluation of relevance and precision across modalities.\\n– Retrieved Context Recall : This metric measures the degree to which the retrieved context\\naligns with and encompasses the knowledge necessary to generate ground truth responses.\\nIt evaluates the proportion of relevant information from the ground truth captured within\\nthe retrieved context, serving as a key indicator of the retrieval system’s effectiveness in\\nsupporting accurate and comprehensive response generation. High values indicate that the\\nretrieval mechanism effectively identifies and incorporates essential knowledge, thereby\\nenhancing the overall performance of the MRAG system.\\n– Faithfulness: This metric evaluates the extent to which generated text maintains factual\\nconsistency with the information in the retrieved documents, ensuring the output accurately\\nreflects the source material and minimizes hallucinations or deviations from the evidence.\\nIn MRAG systems, it also ensures alignment with multimodal retrieved content, including\\ntextual, visual, and auditory elements, maintaining consistency across modalities.\\n– Hallucination: This metric measures the proportion of generated outputs containing hal-\\nlucinated content, such as unsupported claims, fabricated information, or inaccuracies not\\nsubstantiated by the retrieved data. It is essential for evaluating the reliability and factual\\nconsistency of the model’s responses.\\nLLM/MLLM-based metrics are highly effective at capturing complex semantic relationships and\\ncontextual nuances, making them particularly suitable for multimodal RAG systems. However,\\nthey may inherit biases from the underlying models and demand substantial computational\\nresources.\\n•Metric Calculation: When evaluating multimodal retrieval-augmented generation systems,\\nimplementation methods for the same metric can vary significantly, primarily categorized into\\ncoarse-grained and fine-grained approaches. These methodologies differ in their granularity\\nand the depth of analysis applied to assess the quality of model-generated responses against\\nreference answers.\\n– Coarse-Grained Evaluation: Coarse-grained evaluation utilizes LLMs or MLLMs to compare\\nmodel-generated responses with reference answers. This method involves inputting both\\nthe generated output and the reference into the LLM/MLLM, which evaluates the overall\\nsemantic alignment, coherence, and relevance between the two. The model assesses whether\\nthe generated content captures the core meaning and intent of the reference, providing a holistic\\nscore or qualitative feedback. This approach is computationally efficient and scalable, making\\nit suitable for rapid benchmarking and high-level quality checks in large-scale applications.\\nHowever, its broad focus may overlook fine-grained inaccuracies, such as subtle factual\\nerrors, logical inconsistencies, or nuanced contextual mismatches. Consequently, coarse-\\ngrained evaluation is best used as an initial screening tool or in scenarios where high-level\\nsemantic fidelity is prioritized over detailed precision. For more rigorous evaluation, it is\\noften supplemented by fine-grained metrics that address specific aspects of content quality.\\nIn summary, coarse-grained evaluation offers a pragmatic balance between efficiency and\\neffectiveness, particularly in applications requiring quick assessments or large-scale model\\ncomparisons.\\n– Fine-Grained Evaluation: Fine-grained evaluation, such as RAGChecker [317] and RAGAS\\n[74], offers a nuanced and detailed approach to assessing MRAG systems, surpassing the limita-\\ntions of coarse-grained methods. This approach involves decomposing both model-generated\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 46, 'page_label': '47', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 47\\nresponses and reference answers into granular knowledge points or semantic units, which are\\nindividually evaluated based on criteria such as accuracy, relevance, and alignment with the\\nreference. By analyzing responses at this level of detail, the method enables precise identifica-\\ntion of a model’s strengths and weaknesses, particularly in capturing and reproducing intricate\\ninformation. The fine-grained approach is especially valuable for diagnosing performance\\nissues in handling complex or nuanced content. However, it is computationally intensive,\\nrequiring robust mechanisms for extracting, matching, and evaluating multiple semantic units.\\nCareful design of these mechanisms is essential to ensure evaluation consistency and reliability.\\nDespite its challenges, this method provides a rigorous and comprehensive framework for\\nadvancing the development and refinement of MRAG systems, making it a critical tool in the\\nfield.\\nThe choice between coarse-grained and fine-grained evaluation depends on the assessment\\nobjectives. Coarse-grained methods are ideal for obtaining quick, high-level insights, whereas\\nfine-grained approaches are better suited for detailed analysis and iterative model refinement.\\nIntegrating both strategies can provide a balanced perspective, combining the efficiency of\\ncoarse-grained evaluation with the precision of fine-grained analysis to comprehensively assess\\nMRAG systems.\\n6 Challenges of MRAG\\nIn this section, we delineate the challenges associated with various modules in a MRAG system.\\nThese challenges span multiple critical components, including document parsing and indexing,\\nsearch planning, retrieval, generation, dataset, and evaluation. Each module presents unique\\ncomplexities that must be addressed to ensure the system’s effectiveness and robustness.\\n6.1 Document Parsing and Indexing\\nDocument parsing and indexing has established the data foundation based on MRAG, which plays\\na crucial role in the entire system. The relevant technologies extensively studied even before the\\nadvent of LLMs, have seen significant advancements in the LLM era. However, they continue to\\nface several challenges that necessitate further exploration and refinement.\\n•Challenges in Data Accuracy and Completeness: As the primary input source, the accuracy\\nand completeness of upstream data are critical. Errors or omissions in the upstream data can\\npropagate and amplify downstream, significantly degrading system performance. For example,\\nwhile MRAG systems have enhanced document information preservation—such as capturing\\nper-page screenshots—they still face challenges in maintaining inter-page relationships. This\\nlimitation is particularly problematic in long documents with associated segments. Preserving\\nthese relationships is essential for ensuring contextually accurate outputs.\\n•Balancing Multimodal and Textual Data: The document parsing module has grown increas-\\ningly complex as modern MRAG systems must handle multimodal data, including images, tables,\\nand text. To address this, contemporary approaches preserve the original multimodal inputs\\nto minimize information loss, while also converting them into textual captions or descriptions.\\nAlthough retaining the original data reduces information loss, relying solely on it has proven\\nsuboptimal. Recent studies highlight the benefits of leveraging textual representations derived\\nfrom multimodal data. For example, Riedler and Langer [312] showed that models generate\\nhigher-quality responses using textual captions from images rather than processing raw images\\ndirectly. Similarly, Ma et al. [259] found that LLMs outperform MLLMs in text generation tasks,\\nrevealing a performance gap between multimodal and text-focused systems. This gap highlights\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 47, 'page_label': '48', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='48 Trovato et al.\\nthe limitations of current multimodal systems in effectively integrating diverse data types, ne-\\ncessitating additional components in document parsing pipelines. These enhancements, while\\nimproving functionality, increase system complexity and expand the volume of data requiring\\nprocessing, storage, and management.\\n6.2 Multimodal Search Planning\\nThe challenges in multimodal search planning can be more effectively understood through a\\nhierarchical framework similar to leveled RAG systems, where queries span a spectrum from simple\\nfactual retrievals to complex, creative tasks. This framework highlights three critical challenges\\nthat must be addressed to advance the field.\\n•Intelligent Adaptive Planning Mechanisms: The primary challenge is developing intelligent\\nadaptive planning mechanisms that can dynamically adjust to the diversity and complexity of\\nqueries. Current systems often rely on predetermined pipelines, which fail to accommodate\\nvariations in query characteristics or computational constraints, leading to inefficient resource\\nallocation and suboptimal performance [125, 474]. While fixed strategies may suffice for homo-\\ngeneous query types, real-world applications handle heterogeneous query patterns that demand\\ndynamic adjustment of retrieval strategies. For example, complex queries involving multi-hop\\nreasoning or creative problem-solving could greatly benefit from a multi-agent collaborative\\napproach [369]. In such a framework, specialized agents could explore parallel reasoning paths,\\npropose complementary retrieval strategies, and collaboratively synthesize findings to construct\\ncomprehensive search plans. This collaborative paradigm not only simulates diverse perspec-\\ntives but also facilitates intricate interactions between knowledge sources and reasoning steps.\\nBy evaluating search plans from multiple angles, such systems can balance effectiveness and\\nefficiency, ensuring robust performance across diverse query types.\\n•Query Reformulation and Semantic Alignment: A second major challenge is query reformu-\\nlation, particularly in maintaining semantic alignment between the original multimodal query\\nintent and the reformulated queries [197]. As queries become more sophisticated, accurately\\ncapturing and maintaining their intent grows increasingly complex. This challenge is amplified\\nin multimodal contexts, where queries may integrate text, images, audio, or other data types,\\neach requiring precise interpretation. To address this, multi-perspective reformulation strategies\\ncould be employed, leveraging diverse interpretations of the query to generate reformulations\\nthat better align with the original intent. Such strategies might integrate contextual under-\\nstanding, domain-specific knowledge, and cross-modal alignment techniques to ensure semantic\\nconsistency with the user’s intent.\\n•Comprehensive Evaluation Benchmarks: The third critical challenge is the absence of com-\\nprehensive evaluation benchmarks capable of assessing planning mechanisms across diverse\\nquery complexities and scenarios. Existing benchmarks often focus on narrow performance\\naspects, failing to capture the full spectrum of real-world applications. To address this gap, future\\nbenchmarks should evaluate systems across multiple dimensions, including adaptability to query\\ndiversity, robustness in handling complex queries, and efficiency in resource utilization. These\\nbenchmarks should incorporate a wide range of query types, from simple factual retrievals to\\nmulti-hop reasoning and creative tasks, ensuring rigorous testing under realistic conditions. Addi-\\ntionally, benchmarks should incorporate metrics for semantic alignment in query reformulation,\\ncomputational efficiency, and scalability.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 48, 'page_label': '49', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 49\\nThese interconnected challenges highlight the need for future research to develop adaptive planning\\nmechanisms capable of addressing both query diversity and complexity. This could involve multi-\\nagent coordination for advanced cases, alongside robust query reformulation and comprehensive\\nevaluation frameworks.\\n6.3 Retrieval\\nMultimodal retrieval has made significant progress but continues to face challenges that can be\\ncategorized into methodological and practical issues. These challenges arise from the inherent\\ncomplexity of integrating and retrieving information across diverse data modalities such as text,\\nimages, audio, and video. Below, we outline the key challenges in this field:\\n•Heterogeneity of Cross-Modal Data : The heterogeneity of data across modalities poses a\\nsignificant challenge in multimodal retrieval and representation learning. Text, being sequential\\nand discrete, relies on syntactic and semantic structures best captured by language models, while\\nimages, being spatial and continuous, require convolutional or transformer-based architectures to\\nextract hierarchical visual features. This structural divergence complicates cross-modal alignment\\nand comparison, as each modality demands specialized feature extraction techniques tailored to\\nits unique characteristics. Extracting meaningful and comparable features from each modality is\\nnon-trivial, requiring domain-specific expertise and sophisticated models capable of capturing\\nnuanced data properties. For instance, while transformers excel in processing sequential data like\\ntext, their adaptation to spatial data like images often necessitates architectural modifications,\\nsuch as vision transformers (ViTs), to handle pixel arrays. Aligning these features into a unified\\nrepresentation space that preserves cross-modal semantic relationships remains a major challenge.\\nCurrent approaches, including cross-modal transformers and MLLMs, often fail to create a\\ncommon embedding space that adequately captures the semantic richness of each modality while\\nensuring inter-modal consistency.\\n•Cross-modal components (reranker, refiner) : While the dual-tower architecture has made\\nsignificant strides in first-stage retrieval by efficiently encoding and aligning multimodal data (e.g.,\\ntext and images), developing advanced reranking models that enable fine-grained multimodal\\ninteraction remains a challenge. Additionally, refining external multimodal knowledge post-\\nretrieval and reranking remains underexplored, despite its potential to enhance result accuracy\\nand relevance. Addressing these gaps requires innovative methodologies that leverage MLLMs\\nand LLMs to enable sophisticated cross-modal understanding and reasoning.\\n6.4 Generation\\nThe multimodal module in MRAG achieves human-aligned sensory representation through diver-\\nsified modality integration, which significantly enhances user experience and system usability.\\nHowever, achieving these enhancement objectives entails addressing shared challenges across both\\nQA systems and multimodal generation:\\n•Multimodal Input: Multimodal systems face the challenge of integrating diverse data structures\\nand representations across modalities such as text, images, audio, and video. As multimodal\\nmodels evolve, they are increasingly required to process arbitrary combinations of modalities (e.g.,\\ntext+image, text+video, image+audio). This necessitates a highly flexible and adaptive framework\\ncapable of dynamically accommodating diverse input configurations. Such frameworks must be\\nmodality-agnostic, enabling seamless integration of any input combination without predefined\\nstructures or extensive retraining. Achieving this flexibility involves designing architectures that\\ngeneralize across modalities, extract relevant features, and fuse them meaningfully, regardless of\\ninput composition.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 49, 'page_label': '50', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='50 Trovato et al.\\n•Multimodal Output:\\n– Coherent and Contextually Relevant Generation : Ensuring consistency across different\\nmodalities in the output presents a significant challenge. For instance, in a text-image pair, the\\nimage must accurately reflect the scene or object described in the text, while the text should\\nprecisely convey the visual content.\\n– Positioning and Integration of Multimodal Elements : In multimodal outputs, such as\\ntext with embedded images or videos, the model must intelligently determine where to\\nintegrate non-textual elements. This requires an understanding of the narrative flow and the\\nidentification of optimal insertion points to enhance coherence and readability. Additionally,\\nthe model should dynamically generate or retrieve relevant multimodal content based on\\ncontext. For instance, when creating a text-image pair, the model may need to generate an\\nimage caption, search for relevant images, and select the most appropriate one. This process\\nmust be efficient and seamless to ensure the final output is both relevant and high-quality.\\n– Diversity of Outputs : In some applications, generating diverse outputs—such as multiple\\nimages or videos corresponding to a given text description—is essential. However, balancing\\ndiversity with relevance and quality poses a significant challenge. The model must explore a\\nbroad range of possibilities while ensuring each output remains contextually appropriate and\\nadheres to high-quality standards.\\n6.5 Dataset & Evaluation\\nThe advancement of MLLMs has heightened the need for comprehensive evaluation. Despite\\nthe introduction of over a hundred benchmarks by both academic and industrial communities,\\nseveral challenges remain in the current evaluation landscape. First, there is a lack of a universally\\naccepted, standardized capability taxonomy, with existing benchmarks often defining their own\\ndisparate ability dimensions. Second, current benchmarks exhibit significant gaps in critical areas\\nsuch as instruction following, complex multimodal reasoning, multi-turn dialogue, and creativity\\nassessment. Third, task-specific evaluations for MLLMs are insufficient, particularly in commercially\\nrelevant domains like invoice recognition, multimodal knowledge base comprehension, and UI\\nunderstanding and industry. Finally, while existing multimodal benchmarks primarily focus on\\nimage and video modalities, there is a notable deficit in assessing capabilities related to audio\\nand 3D representations. Addressing these challenges is essential for developing more robust and\\ncomprehensive evaluation methodologies for MLLMs in the future.\\nDespite rapid advancements, current evaluations of MLLMs remain insufficiently comprehensive,\\nprimarily focusing on perception and reasoning abilities through objective questions. This creates a\\nsignificant gap between evaluation methodologies and real-world applications. Moreover, optimiz-\\ning models based on objective assessments often leads developers to prioritize objective question\\ncorpora during instruction tuning, potentially degrading the quality of dialogue experiences. Al-\\nthough subjective multimodal evaluation platforms like WildVision and OpenCompass MultiModal\\nArena have emerged, further research is needed to develop assessment methods that better align\\nwith practical usage scenarios. Current evaluation strategies predominantly rely on curated or\\ncrafted questions to assess specific capabilities, yet complex multimodal tasks typically require\\nthe integration of multiple skills. For instance, a chart-related question may involve OCR, spatial\\nrelationship recognition, reasoning, and calculations. The absence of decoupled assessments for\\nthese distinct capabilities represents a major limitation in existing frameworks. Additionally, crucial\\nabilities such as instruction following remain under-evaluated. Multiturn dialogue, the primary\\nmode of human interaction with multimodal models, remains a weakness for most models, and\\ncorresponding evaluations, are still in their infancy. In the realm of complex multimodal reasoning,\\ncurrent evaluations predominantly focus on mathematical and examination problems, necessitating\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 50, 'page_label': '51', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 51\\nimprovements in both difficulty and relevance to everyday use cases. Notably, the evaluation of\\nmultimodal creative tasks, a key application area for these models—such as text generation based on\\nimage and textual prompts—remains largely unexplored, highlighting a critical gap in the current\\nevaluation landscape.\\nMLLMs are still in the early stages of development, with limited business applications to date.\\nAs a result, current evaluations primarily focus on assessing foundational capabilities rather than\\nreal-world performance. Moving forward, it is critical to develop evaluation frameworks that\\nmeasure MLLM performance on specific tasks with commercial value, such as large-scale document\\nprocessing, multimodal knowledge base comprehension, anomaly detection, and industrial visual\\ninspection. When designing task-specific evaluations, it is essential to consider not only performance\\nmetrics but also computational costs and inference speeds, benchmarking them against traditional\\ncomputer vision methods like OCR, object detection, and action recognition to determine practical\\napplicability. Additionally, a key potential of MLLMs lies in their ability to plan and interact with\\nenvironments as agents to solve complex problems. Developing diverse virtual environments\\nfor MLLMs to demonstrate agent-based problem-solving capabilities will likely become a critical\\ncomponent of future evaluations. Current efforts in this domain remain nascent, highlighting a\\npromising area for future research in multimodal AI assessment.\\n7 Future Directions\\nIn this chapter, we propose several suggestions to the future development of multimodal Retrieval-\\nAugmented Generation (MRAG) systems, informed by related research and identified challenges.\\nThese recommendations collectively aim to overcome existing limitations and unlock the full\\npotential of MRAG in complex, real-world scenarios.\\n7.1 Documents Parsing\\nMultimodal document parsing has become a crucial element in MRAG systems, particularly with\\nthe emergence of large language models (LLMs) and multimodal large models (MLLMs). The fusion\\nof text, images, and other data types into a cohesive framework presents both transformative\\nopportunities and notable challenges. This paper provides a detailed analysis of future directions\\nin this evolving field.\\n•Enhancing Data Accuracy and Completeness:\\n– Contextual Relationship Preservation: To improve the accuracy and coherence of multi-\\nmodal document parsing, especially for long and complex documents, advanced algorithms are\\nneeded to capture and preserve both inter-page and intra-document relationships. Techniques\\nsuch as graph-based representations and hierarchical document modeling can help maintain\\ncontextual coherence across the document. These methods enable the system to understand\\nstructural and semantic dependencies between sections, tables, figures, and other elements,\\nensuring the preservation of the document’s logical flow. Additionally, cross-referencing mech-\\nanisms are essential for linking related content across pages. These mechanisms dynamically\\nconnect sections, tables, and figures, facilitating seamless retrieval and utilization of contextual\\nrelationships in downstream tasks like information extraction, summarization, or question\\nanswering. By integrating these approaches, the system can better handle the complexities of\\nlong documents, ensuring accurate maintenance and leveraging of contextual relationships\\nfor enhanced performance in multimodal document understanding tasks. This is particularly\\nrelevant when combining Optical Character Recognition (OCR), LLMs, and MLLMs to process\\nand interpret documents with diverse content types.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 51, 'page_label': '52', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='52 Trovato et al.\\n– Error Detection and Correction: To improve the accuracy and reliability of multimodal\\ndocument parsing, integrating advanced error detection and correction mechanisms is crucial.\\nLeveraging LLMs and MLLMs, systems can validate extracted text against the original docu-\\nment, identifying and correcting inaccuracies or omissions. These models can be enhanced\\nwith consistency-checking algorithms to ensure coherence and accuracy across multimodal\\ndata, including text, images, and tables. For critical documents, a human-in-the-loop (HITL)\\napproach is advisable. This involves human reviewers verifying and refining parsed data,\\nespecially in cases where systems may struggle with complex layouts, ambiguous content,\\nor domain-specific nuances. By combining the strengths of LLMs, MLLMs, and human ex-\\npertise, this hybrid approach ensures high accuracy and reliability, making it suitable for\\nprecision-demanding applications such as legal, medical, or financial document processing.\\n•Improving Multimodal Data Integration:\\n– Unified Multimodal Representation: Advancing multimodal document parsing requires\\nthe development of unified representation frameworks that seamlessly integrate diverse data\\ntypes, such as text, images, and tables, into a cohesive structure. Such frameworks enable\\nrobust, context-aware analysis by leveraging multimodal transformers—like CLIP, Flamingo,\\nor other state-of-the-art models—to encode disparate modalities into a shared embedding\\nspace. This interoperability enhances downstream tasks, including information extraction,\\nquestion answering, and summarization. A promising approach involves hybrid strategies\\nthat combine raw multimodal data with textual representations. For instance, raw images\\ncan support visual tasks (e.g., object detection or layout analysis), while textual captions\\nor OCR-derived text can improve text generation tasks (e.g., summarization or translation).\\nThis dual methodology leverages the strengths of each modality, ensuring both accuracy and\\nefficiency in processing complex documents, as demonstrated by recent research. Additionally,\\nintegrating LLMs and MLLMs with Optical Character Recognition (OCR) systems can enhance\\nthe parsing of scanned or image-based documents. By aligning OCR outputs with multimodal\\nembeddings, these systems improve the handling of noisy or unstructured data, enabling more\\naccurate interpretation and contextual understanding.\\n– Advanced Captioning and Description Generation: To improve multimodal data inte-\\ngration, particularly in document parsing, enhancing automated captioning and description\\ngeneration for non-textual elements like images, tables, and charts is critical. Leveraging state-\\nof-the-art vision-language models (VLMs) and MLLMs can boost the accuracy and contextual\\nrelevance of textual descriptions. These models bridge the gap between visual and textual\\ndata, enabling more comprehensive document understanding. Integrating domain-specific\\nknowledge into captioning models is essential for generating accurate and contextually tai-\\nlored descriptions. This can be achieved by fine-tuning pre-trained models on domain-specific\\ndatasets or incorporating external knowledge bases. Such an approach ensures that descrip-\\ntions align with the document’s content, enhancing the utility of multimodal data integration.\\n•Leveraging LLMs and MLLMs for Enhanced Parsing:\\n– LLM/MLLM-Driven Parsing and Indexing: LLMs and MLLMs can be fine-tuned on domain-\\nspecific corpora to improve their ability to parse and interpret complex document structures.\\nLeveraging their advanced multimodal understanding, these models can accurately identify\\nand extract key information—such as legal clauses, scientific hypotheses, or technical speci-\\nfications—even from dense or unstructured text. Fine-tuning enhances their proficiency in\\nrecognizing domain-specific terminology, relationships, and contextual nuances. Furthermore,\\nLLMs and MLLMs can generate metadata, tags, and summaries. By automatically annotat-\\ning documents with relevant keywords, classifications, or concise summaries, these models\\nstreamline the organization and accessibility of large document repositories. This capability\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 52, 'page_label': '53', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 53\\nis particularly valuable in applications like legal case management, academic research, and\\nenterprise knowledge bases. In multimodal contexts, MLLMs extend these capabilities by\\nintegrating and interpreting data from diverse sources, such as text, images, tables, and dia-\\ngrams. This enables a more comprehensive parsing process, where visual and textual elements\\nare jointly analyzed to extract richer, more accurate information. For example, in scientific\\ndocuments, MLLMs can parse and correlate data from textual descriptions and accompanying\\ncharts, facilitating a deeper understanding of the content.\\n– Bridging the Gap Between LLMs and MLLMs: A promising approach involves hybrid\\narchitectures that combine the strengths of MLLMs and LLMs. MLLMs process raw multimodal\\ninputs (e.g., images, audio, video) to extract meaningful representations, while LLMs generate\\ncoherent and contextually accurate textual outputs. This division of labor optimizes perfor-\\nmance, as MLLMs excel in multimodal feature extraction and LLMs in linguistic precision. For\\nexample, in document parsing, MLLMs analyze visual layouts, tables, or embedded graphics,\\nwhile LLMs synthesize this information into structured textual formats.\\n7.2 Multimodal Search Planning\\nThe future of multimodal search planning should focus on addressing three key challenges within\\nthe hierarchical framework: intelligent adaptive planning mechanisms, query reformulation and\\nsemantic alignment, and comprehensive evaluation benchmarks. Below are targeted suggestions\\nfor advancing each area.\\n•Intelligent Adaptive Planning Mechanisms:\\n– Multi-Agent Collaborative Systems: To address the challenges of multimodal search and\\ncomplex query resolution, multi-agent collaborative systems can be designed to leverage\\nspecialized agents working in tandem. These systems enhance efficiency, adaptability, and\\nrobustness in handling multi-hop, creative, or cross-modal queries. Key mechanisms include:\\n1) Parallel Reasoning Paths: Specialized agents can simultaneously explore multiple reasoning\\ntrajectories, enabling faster and more comprehensive solutions. This approach is particularly\\neffective for multi-hop queries, where intermediate reasoning steps are critical, or for creative\\ntasks requiring diverse perspectives. By evaluating multiple pathways in parallel, the system\\ncan identify optimal solutions while mitigating the risk of local optima. 2) Complementary Re-\\ntrieval Strategies: Agents can employ diverse retrieval methodologies, such as keyword-based,\\nsemantic, or cross-modal retrieval, to address different aspects of a query. For instance, one\\nagent might focus on extracting structured data, while another leverages semantic embeddings\\nor visual-textual alignment for multimodal contexts. The synthesis of these strategies ensures\\nrobust and contextually relevant search outcomes, enhancing the system’s ability to handle\\nheterogeneous data sources. 3) Dynamic Resource Allocation: Agents can monitor computa-\\ntional resources and system constraints in real-time, dynamically adjusting retrieval strategies\\nto optimize performance. For example, under limited computational bandwidth, agents might\\nprioritize lightweight retrieval methods or redistribute tasks to balance load. This adaptive\\nmechanism ensures efficient resource utilization while maintaining high-quality query resolu-\\ntion. 4) Integration with MLLMs and LLMs: The collaborative multi-agent framework can be\\nseamlessly integrated with MLLMs and LLMs to enhance their capabilities. MLLMs can serve\\nas central orchestrators, interpreting multimodal inputs and coordinating agent tasks, while\\nLLMs provide deep contextual understanding and reasoning support. This integration enables\\nthe system to handle complex, multimodal queries with greater precision and adaptability.\\n– Hierarchical Planning Frameworks: To address the challenges of ambiguous or highly\\ncreative queries in multimodal search and planning, integrating human feedback into the\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 53, 'page_label': '54', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='54 Trovato et al.\\ndecision-making process is essential. Human-in-the-loop (HITL) systems facilitate iterative\\nrefinement by leveraging user expertise to guide and validate intermediate results. These\\ninteractive systems enable users to dynamically adjust search parameters, prioritize modalities,\\nor correct misinterpretations, ensuring more accurate and contextually relevant outcomes. By\\ncombining the strengths of multimodal large language models (MLLMs) with human intuition,\\nHITL systems enhance adaptability, build trust, and improve the robustness of intelligent\\nplanning frameworks. This collaborative approach is particularly valuable in domains requiring\\nnuanced understanding, creativity, or domain-specific knowledge.\\n– Reinforcement Learning for Adaptation: Intelligent adaptive planning mechanisms can\\nbe developed using reinforcement learning (RL) to enable dynamic, context-aware decision-\\nmaking. By modeling the search process as a sequential decision problem, RL agents can\\nbe trained to optimize resource allocation and retrieval accuracy. These agents adapt their\\nstrategies by receiving rewards for minimizing computational overhead, reducing latency, and\\ndelivering precise results tailored to query characteristics, such as modality, complexity, and\\nuser intent.\\n•Query Reformulation and Semantic Alignment:\\n– Multi-Perspective Reformulation: To address the complexity of multimodal search, query\\nreformulation strategies must generate diverse interpretations while preserving the original\\nintent across modalities. This involves: 1) Contextual Understanding: Leveraging contextual\\nembeddings (e.g., from transformer-based models) to capture semantic nuances and contex-\\ntual dependencies, ensuring reformulated queries retain the richness of the original input. 2)\\nCross-Modal Alignment: Employing advanced techniques like contrastive learning to align rep-\\nresentations across text, images, and audio modalities. By embedding queries and multimodal\\ndata into a shared latent space, this ensures consistent interpretation and retrieval across\\ndiverse data types. 3) Domain-Specific Knowledge Integration: Incorporating domain-specific\\nontologies or knowledge graphs to enhance reformulation accuracy, particularly in specialized\\nfields. This leverages structured domain knowledge to improve the relevance and precision of\\nreformulated queries.\\n– Interactive Query Refinement: A pivotal strategy is interactive query refinement, which\\nallows users to iteratively adjust queries based on intermediate results. Intelligent systems can\\nfacilitate this process by suggesting alternative query formulations, identifying ambiguities,\\nand providing contextual feedback to better align queries with user intent. By integrating user\\nfeedback loops and real-time semantic analysis, these systems dynamically bridge the gap\\nbetween user input and multimodal data, ensuring more precise and contextually relevant\\nsearch outcomes.\\n– Explainable Reformulation: A critical aspect of query reformulation is its explainability.\\nBy offering clear and concise explanations for how queries are transformed, users gain insight\\ninto the system’s reasoning and decision-making processes. For example, when a user submits\\na vague or ambiguous query, the system can generate a reformulated version while detailing\\nthe rationale behind the changes, such as term disambiguation, incorporation of contextual\\ncues, or alignment with multimodal data (e.g., text, images, or audio). This transparency fosters\\nuser trust, enables validation of the system’s interpretation, and enhances user control and\\nsatisfaction. Furthermore, explainable reformulation underscores the importance of semantic\\nalignment, where the system bridges the gap between user intent and the underlying data\\nrepresentation, ensuring the reformulated query accurately reflects the user’s needs.\\n•Comprehensive Evaluation Benchmarks:\\n– Diverse Query Datasets: It is crucial to establish robust benchmarks. These benchmarks\\nshould incorporate diverse query datasets spanning a wide range of query types, from simple\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 54, 'page_label': '55', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 55\\nfactual retrievals to complex multi-hop reasoning and creative tasks. The datasets must\\nreflect real-world heterogeneity in query patterns and modalities, capturing the intricacies of\\nuser interactions across text, image, audio, and video inputs. By integrating such diversity,\\nbenchmarks can more accurately assess model performance, generalization capabilities, and\\nadaptability to varied real-world applications.\\n– Multi-Dimensional Metrics: To ensure the effectiveness and reliability of Multimodal Search\\nPlanning systems, robust evaluation frameworks must be established. These frameworks\\nshould employ multi-dimensional metrics to comprehensively assess system performance\\nacross diverse operational scenarios. Key dimensions include: 1) Adaptability: The system’s\\nability to handle a broad spectrum of query types, from simple to highly complex, while\\nintegrating multiple modalities (e.g., text, images, audio). This metric evaluates the model’s\\nflexibility in addressing varied user needs and its capacity to generalize across domains. 2)\\nRobustness: The system’s resilience under challenging conditions, such as computational con-\\nstraints, noisy or incomplete inputs, and adversarial scenarios. Robustness ensures consistent\\nperformance in real-world applications, where ideal conditions are seldom present. 3) Effi-\\nciency: The optimization of resource utilization (e.g., memory, processing power) and response\\ntime. This metric is critical for scalability and user satisfaction, especially in time-sensitive\\nor resource-constrained environments. 4) Semantic Alignment: The system’s accuracy in\\npreserving and interpreting the intent of user queries during reformulation or multimodal\\nintegration. This ensures that outputs remain contextually and semantically aligned with the\\nuser’s original request.\\n7.3 Retrieval\\nThe challenges in multimodal retrieval underscore the complexity of integrating and retrieving\\ninformation across diverse data types. To address these issues and advance the field, future research\\nshould prioritize the following directions:\\n•Unified Cross-Modal Representation Learning : The primary objective is to develop robust\\nand unified representation learning frameworks that effectively align and compare data across\\ndiverse modalities, including text, images, audio, and video. A key element of this framework is\\nthe enhancement of cross-modal attention mechanisms to better model complex interactions\\nbetween modalities. Cross-modal attention layers, inspired by transformer architectures, are\\ncentral to capturing fine-grained relationships. These mechanisms enable one modality to focus\\non relevant features in another, allowing the model to dynamically prioritize the most informative\\naspects of the data. For example, text can guide attention over visual regions, or audio cues can\\nemphasize relevant temporal segments in video data. Techniques such as multi-head cross-modal\\nattention and hierarchical attention further refine this process, ensuring robust and context-aware\\nrepresentations.\\n•Cross-Modal Context: The primary objective is to improve the ability of models to perform\\nfine-grained interactions between modalities, particularly in the reranking and refinement stages.\\n– Reranker: Multi-Modal Reranking Models rerank retrieved document list by incorporating\\ndetailed cross-modal interactions, such as text-image, text-audio, or video-text relationships.\\nBy integrating LLMs and MLLMs, reranking models enhance their ability to capture nuanced\\nsemantic alignments between modalities.\\n– Refiner: Leveraging their cross-modal reasoning abilities, MLLMs refine retrieval results\\nthrough knowledge-enhanced refinement, yielding more accurate, contextually relevant, and\\nsemantically rich outputs. This refinement process utilizes MLLMs’ contextual understanding\\nand multimodal alignment to re-rank, filter, or augment retrieved content.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 55, 'page_label': '56', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='56 Trovato et al.\\n7.4 Generation\\nThe future of multimodal generation should focus on overcoming existing challenges on multimodal\\ninput and output. Below are key suggestions for advancing multimodal generation systems.\\n•Flexible and Adaptive Multimodal Input Frameworks: To address the growing complexity\\nof multimodal data, it is crucial to develop modality-agnostic architectures that dynamically\\nadapt to diverse and arbitrary input modality combinations, such as text+image, text+video, or\\nimage+audio. These frameworks should process inputs without relying on predefined structures\\nor extensive retraining.\\n•Coherent and Contextually Relevant Multimodal Output: Achieving coherent and con-\\ntextually relevant outputs in multimodal generation necessitates the development of advanced\\nmodels capable of maintaining consistency across modalities. For example, in text-image gen-\\neration tasks, the generated image must precisely align with the textual description, and the\\ntext should accurately reflect the visual content of the image. This cross-modal consistency is\\nessential for ensuring the reliability and usability of multimodal systems.\\n•Intelligent Positioning and Integration of Multimodal Elements: To seamlessly integrate\\nnon-textual elements (e.g., images, videos, audio) into a narrative, models must be trained to\\nidentify optimal insertion points. This requires a deep understanding of the content’s structure,\\nflow, and contextual nuances to ensure coherence, readability, and enhanced user engagement.\\nAdvanced techniques, such as attention mechanisms, can analyze the narrative’s semantic and\\nsyntactic structure, enabling the model to determine where multimodal elements can comple-\\nment or enrich the text. Modern multimodal systems must dynamically retrieve or generate\\ncontextually relevant non-textual content. For example, when generating a text-image pair,\\nthe model should use cross-modal alignment techniques to either retrieve an existing image\\nfrom a database or synthesize a new one that aligns with the textual context. This relies on\\nrobust multimodal representation learning, where embeddings from different modalities (text,\\nimage, video) are mapped into a shared latent space, enabling precise cross-modal retrieval or\\ngeneration.\\n•Diversity in Multimodal Outputs: Achieving a balance between diversity, relevance, and\\nquality in multimodal generation requires controlled mechanisms. For instance, in text-to-image\\ngeneration, models should produce diverse yet faithful representations of textual descriptions.\\nTechniques like conditional sampling can guide models to explore varied latent spaces while\\nadhering to input constraints.\\n7.5 Dataset & Evaluation\\nThe future direction of datasets and evaluation in MRAG should focus on addressing current gaps\\nand challenges while harnessing the unique capabilities of MLLMs. Below are refined suggestions\\nfor advancing datasets and evaluation methodologies in this field.\\n•Comprehensive Benchmark Development : To enhance the evaluation of MLLMs and LLMs\\nin retrieval-augmented generation, it is crucial to develop comprehensive benchmarks that\\naddress key limitations in current assessment frameworks. These benchmarks should focus on\\nthe following areas: 1) Instruction Following: Create tasks to evaluate the model’s ability to\\ncomprehend and execute complex, multi-step instructions across diverse modalities. This includes\\nassessing precision in adhering to nuanced directives and handling ambiguous or incomplete\\ninputs. 2) Multiturn Dialogue: Develop datasets that simulate real-world conversational dynamics,\\nemphasizing the model’s capacity for context retention, coherence, and adaptability over extended\\ninteractions. Scenarios should include cross-modal references and long-term memory challenges.\\n3) Complex Multimodal Reasoning: Design tasks requiring the integration of multiple modalities\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 56, 'page_label': '57', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 57\\n(e.g., text, images, audio) to solve real-world problems, such as interpreting charts, maps, or\\ncombining visual and textual data for decision-making. 4) Creativity Evaluation: Introduce\\nbenchmarks to assess generative capabilities in creative tasks, such as composing stories, poems,\\nor designing visual artifacts from multimodal inputs. These tasks should measure originality,\\nrelevance, and the ability to synthesize diverse inputs into coherent outputs. 5) Diverse Modalities:\\nExpand evaluation frameworks to include emerging modalities like audio, 3D models, and sensor\\ndata, ensuring robustness and versatility in handling a wide range of input types.\\n•Multimodal Retrieval-Augmented Generation : The development of robust metrics for eval-\\nuating retrieval and generation in multimodal systems requires assessing relevance, precision,\\ndiversity, and cross-modal alignment to ensure semantic consistency and contextual appropri-\\nateness. Metrics should quantify the system’s ability to filter noise and redundancy, delivering\\nconcise and meaningful outputs. For generation quality, coherence, fluency, creativity, and adapt-\\nability are essential, alongside factual accuracy and consistency with retrieved data and external\\nknowledge. Effective multimodal integration is crucial to unify diverse inputs into contextually\\nrich outputs. Comprehensive benchmarks must simulate real-world scenarios, incorporating\\nvaried queries, multimodal sources, and differing complexity levels to evaluate the end-to-end\\nperformance of retrieval-augmented generation (RAG) pipelines.\\n8 Conclusion\\nIn conclusion, this survey comprehensively examines the emerging field of Multimodal Retrieval-\\nAugmented Generation (MRAG), highlighting its potential to enhance the capabilities of large\\nlanguage models (LLMs) by integrating multimodal data such as text, images, and videos. Unlike\\ntraditional text-based RAG systems, MRAG addresses the challenges of retrieving and generat-\\ning information across different modalities, thereby improving the accuracy and relevance of\\nresponses while reducing hallucinations. The survey systematically analyzes MRAG from four key\\nperspectives: essential components and technologies, datasets, evaluation methods and metrics,\\nand existing limitations. It identifies current challenges, such as effectively integrating multimodal\\nknowledge and ensuring the reliability of generated outputs, while also proposing future research\\ndirections. By providing a structured overview and forward-looking insights, this survey aims to\\nguide researchers in advancing MRAG, ultimately contributing to the development of more robust\\nand versatile Multimodal Retrieval-Augmented Generation.\\nReferences\\n[1] [n. d.]. jsoup. https://jsoup.org/\\n[2] [n. d.]. pdfminer. https://github.com/pdfminer/pdfminer.six\\n[3] [n. d.]. PyMuPDF. https://github.com/pymupdf/PyMuPDF\\n[4] [n. d.]. realworldQA. https://huggingface.co/datasets/visheratin/realworldqa\\n[5] Ossama Abdel-Hamid, Abdel-rahman Mohamed, Hui Jiang, Li Deng, Gerald Penn, and Dong Yu. 2014. Convolutional\\nneural networks for speech recognition. IEEE/ACM Transactions on audio, speech, and language processing 22, 10\\n(2014), 1533–1545.\\n[6] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree,\\nArash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin\\nCai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao\\nCheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao,\\nAmit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett,\\nWenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero\\nKauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li,\\nYunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong\\nLiu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio César Teodoro Mendes,\\nArindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid\\nPryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase,\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 57, 'page_label': '58', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='58 Trovato et al.\\nOlli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen,\\nSwadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua\\nWang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu,\\nXiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei\\nYang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang,\\nYi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. 2024. Phi-3 Technical Report: A Highly Capable Language\\nModel Locally on Your Phone. arXiv:2404.14219 [cs.CL] https://arxiv.org/abs/2404.14219\\n[7] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774\\n(2023).\\n[8] Emanuele Aiello, Lili Yu, Yixin Nie, Armen Aghajanyan, and Barlas Oguz. 2023. Jointly training large autoregressive\\nmultimodal models. arXiv preprint arXiv:2309.15564 (2023).\\n[9] Akiko Aizawa. 2003. An information-theoretic perspective of tf–idf measures. Information Processing & Management\\n39, 1 (2003), 45–65.\\n[10] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\\nKatherine Millican, Malcolm Reynolds, et al. 2022. Flamingo: a visual language model for few-shot learning. Advances\\nin neural information processing systems 35 (2022), 23716–23736.\\n[11] Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Guimin Hu, Weimin Lyu,\\nLijie Hu, Lu Yu, et al. 2024. Prompt-saw: Leveraging relation-aware graphs for textual prompt compression. arXiv\\npreprint arXiv:2404.00489 (2024).\\n[12] Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and R Manmatha. 2021. Docformer: End-to-end\\ntransformer for document understanding. In Proceedings of the IEEE/CVF international conference on computer vision .\\n993–1003.\\n[13] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph,\\nBen Mann, Nova DasSarma, et al. 2021. A general language assistant as a laboratory for alignment. arXiv preprint\\narXiv:2112.00861 (2021).\\n[14] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton,\\nSamir Gadre, Shiori Sagawa, et al. 2023. Openflamingo: An open-source framework for training large autoregressive\\nvision-language models. arXiv preprint arXiv:2308.01390 (2023).\\n[15] Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk Lee. 2019. Character Region Awareness for\\nText Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) .\\n[16] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\\nZhou. 2023. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966\\n1, 2 (2023), 3.\\n[17] Shuai Bai, Shusheng Yang, Jinze Bai, Peng Wang, Xingxuan Zhang, Junyang Lin, Xinggang Wang, Chang Zhou,\\nand Jingren Zhou. 2023. Touchstone: Evaluating vision-language models by language models. arXiv preprint\\narXiv:2308.16890 (2023).\\n[18] Liping Bao, Longhui Wei, Wengang Zhou, Lin Liu, Lingxi Xie, Houqiang Li, and Qi Tian. 2023. Multi-Granularity\\nMatching Transformer for Text-Based Person Search. IEEE Transactions on Multimedia (2023).\\n[19] Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Scott Yih, Sebastian Riedel, and Fabio Petroni. 2022. Autore-\\ngressive search engines: Generating substrings as document identifiers. Advances in Neural Information Processing\\nSystems 35 (2022), 31668–31683.\\n[20] Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann,\\nIbrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda\\nKoppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos,\\nRishabh Kabra, Matthias Bauer, Matko Bošnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana\\nBalazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and\\nXiaohua Zhai. 2024. PaliGemma: A versatile 3B VLM for transfer. arXiv:2407.07726 [cs.CV] https://arxiv.org/abs/\\n2407.07726\\n[21] Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, and\\nLudwig Schmidt. 2023. Visit-bench: A benchmark for vision-language instruction following inspired by real-world\\nuse. arXiv preprint arXiv:2308.06595 (2023).\\n[22] Chinmoy B Bose and Shyh-Shiaw Kuo. 1994. Connected and degraded text recognition using hidden Markov model.\\nPattern Recognition 27, 10 (1994), 1345–1363.\\n[23] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\\nPranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural\\ninformation processing systems 33 (2020), 1877–1901.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 58, 'page_label': '59', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 59\\n[24] Davide Caffagni, Federico Cocchi, Nicholas Moratelli, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara.\\n2024. Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs. InProceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition . 1818–1826.\\n[25] Zhiwei Cao, Qian Cao, Yu Lu, Ningxin Peng, Luyang Huang, Shanbo Cheng, and Jinsong Su. 2024. Retaining key\\ninformation under high compression ratios: Query-guided compressor for llms. arXiv preprint arXiv:2406.02376\\n(2024).\\n[26] Bing-Bing Chai, Jozsef Vass, and Xinhua Zhuang. 1999. Significance-linked connected component analysis for wavelet\\nimage coding. IEEE Transactions on Image processing 8, 6 (1999), 774–784.\\n[27] Wei-Cheng Chang, Felix X Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. 2020. Pre-training tasks for\\nembedding-based large-scale retrieval. arXiv preprint arXiv:2002.03932 (2020).\\n[28] Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk. 2022. Webqa: Multihop\\nand multimodal qa. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 16495–16504.\\n[29] Yunhang Shen Yulei Qin Mengdan Zhang Xu Lin Jinrui Yang Xiawu Zheng Ke Li Xing Sun Yunsheng Wu Rongrong Ji\\nChaoyou Fu, Peixian Chen. 2023. Mme: A comprehensive evaluation benchmark for multimodal large language\\nmodels. arXiv preprint arXiv:2306.13394 (2023).\\n[30] Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu. 2023. X-llm: Bootstrap-\\nping advanced large language models by treating multi-modalities as foreign languages.arXiv preprint arXiv:2305.04160\\n(2023).\\n[31] Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, and Liqiang Nie. 2024. Lion: Empowering multimodal large\\nlanguage model with dual-level visual knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition . 26540–26550.\\n[32] Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. 2023. Walking down the memory maze:\\nBeyond context limit through interactive reading. arXiv preprint arXiv:2310.05029 (2023).\\n[33] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking large language models in retrieval-augmented\\ngeneration. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 38. 17754–17762.\\n[34] Jieneng Chen, Luoxin Ye, Ju He, Zhaoyang Wang, Daniel Khashabi, and Alan L Yuille. 2024. Efficient large multi-modal\\nmodels via visual context compression. Advances in Neural Information Processing Systems 37 (2024), 73986–74007.\\n[35] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas\\nChandra, Yunyang Xiong, and Mohamed Elhoseiny. 2023. Minigpt-v2: large language model as a unified interface for\\nvision-language multi-task learning. arXiv preprint arXiv:2310.09478 (2023).\\n[36] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. 2023. Shikra: Unleashing multimodal\\nllm’s referential dialogue magic. arXiv preprint arXiv:2306.15195 (2023).\\n[37] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2024. Sharegpt4v:\\nImproving large multi-modal models with better captions. In European Conference on Computer Vision . Springer,\\n370–387.\\n[38] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao,\\nDahua Lin, et al. 2024. Are We on the Right Way for Evaluating Large Vision-Language Models? arXiv preprint\\narXiv:2403.20330 (2024).\\n[39] Shaoxiang Chen, Zequn Jie, and Lin Ma. 2024. Llava-mole: Sparse mixture of lora experts for mitigating data conflicts\\nin instruction finetuning mllms. arXiv preprint arXiv:2401.16160 (2024).\\n[40] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz,\\nSebastian Goodman, Xiao Wang, Yi Tay, et al. 2023. Pali-x: On scaling up a multilingual vision and language model.\\narXiv preprint arXiv:2305.18565 (2023).\\n[41] Xingyu Chen, Zihan Zhao, Lu Chen, Danyang Zhang, Jiabao Ji, Ao Luo, Yuxuan Xiong, and Kai Yu. 2021. Websrc: A\\ndataset for web-based structural reading comprehension. arXiv preprint arXiv:2101.09465 (2021).\\n[42] Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-Wei Chang. 2023. Can\\npre-trained vision and language models answer visual information-seeking questions? arXiv preprint arXiv:2302.11713\\n(2023).\\n[43] Yiqun Chen, Qi Liu, Yi Zhang, Weiwei Sun, Xinyu Ma, Wei Yang, Daiting Shi, Jiaxin Mao, and Dawei Yin. 2024.\\nTourrank: Utilizing large language models for documents ranking with a tournament-inspired strategy.arXiv preprint\\narXiv:2406.11678 (2024).\\n[44] Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran. 2024. Dress: Instructing large vision-\\nlanguage models to align and interact with humans via natural language feedback. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition . 14239–14250.\\n[45] Yiqun Chen, Lingyong Yan, Weiwei Sun, Xinyu Ma, Yi Zhang, Shuaiqiang Wang, Dawei Yin, Yiming Yang, and Jiaxin\\nMao. 2025. Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning. arXiv preprint\\narXiv:2501.15228 (2025).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 59, 'page_label': '60', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='60 Trovato et al.\\n[46] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020.\\nUniter: Universal image-text representation learning. In European conference on computer vision . Springer, 104–120.\\n[47] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu,\\nLewei Lu, et al. 2024. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In\\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition . 24185–24198.\\n[48] Xin Cheng, Xun Wang, Xingxing Zhang, Tao Ge, Si-Qing Chen, Furu Wei, Huishuai Zhang, and Dongyan Zhao. 2024.\\nxrag: Extreme context compression for retrieval-augmented generation with one token.arXiv preprint arXiv:2405.13792\\n(2024).\\n[49] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. 2023. Adapting language models to compress\\ncontexts. arXiv preprint arXiv:2305.14788 (2023).\\n[50] Kyunghyun Cho. 2014. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint\\narXiv:1409.1259 (2014).\\n[51] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and\\nYoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation.\\narXiv preprint arXiv:1406.1078 (2014).\\n[52] Sukmin Cho, Soyeong Jeong, Jeongyeon Seo, and Jong C Park. 2023. Discrete prompt optimization via constrained\\ngeneration for zero-shot re-ranker. arXiv preprint arXiv:2305.13729 (2023).\\n[53] Eunbi Choi, Yongrae Jo, Joel Jang, and Minjoon Seo. 2022. Prompt injection: Parameterization of fixed inputs. arXiv\\npreprint arXiv:2206.11349 (2022).\\n[54] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang,\\nXiaolin Wei, et al. 2023. Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices.\\narXiv preprint arXiv:2312.16886 1, 2 (2023), 3.\\n[55] Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang\\nLin, Bo Zhang, et al. 2024. Mobilevlm v2: Faster and stronger baseline for vision language model. arXiv preprint\\narXiv:2402.03766 (2024).\\n[56] Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, and Xia Hu. 2024. Learning to compress\\nprompt in natural language formats. arXiv preprint arXiv:2402.18700 (2024).\\n[57] Sanghyuk Chun, Seong Joon Oh, Rafael Sampaio De Rezende, Yannis Kalantidis, and Diane Larlus. 2021. Probabilistic\\nembeddings for cross-modal retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition. 8415–8424.\\n[58] Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. 2023. Holistic\\nanalysis of hallucination in gpt-4v (ision): Bias and interference challenges. arXiv preprint arXiv:2311.03287 (2023).\\n[59] Zhuyun Dai and Jamie Callan. 2020. Context-aware document term weighting for ad-hoc search. In Proceedings of\\nThe Web Conference 2020 . 1897–1907.\\n[60] Zhuyun Dai and Jamie Callan. 2020. Context-aware term weighting for first stage passage retrieval. In Proceedings of\\nthe 43rd International ACM SIGIR conference on research and development in Information Retrieval . 1533–1536.\\n[61] Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2020. Autoregressive entity retrieval. arXiv\\npreprint arXiv:2010.00904 (2020).\\n[62] Jacob Devlin. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint\\narXiv:1810.04805 (2018).\\n[63] SeungHeon Doh, Minhee Lee, Dasaem Jeong, and Juhan Nam. 2024. Enriching Music Descriptions with A Finetuned-\\nLLM and Metadata for Text-to-Music Retrieval. InICASSP 2024-2024 IEEE International Conference on Acoustics, Speech\\nand Signal Processing (ICASSP) . IEEE, 826–830.\\n[64] Jianfeng Dong, Xirong Li, and Cees GM Snoek. 2018. Predicting visual features from text for image and video caption\\nretrieval. IEEE Transactions on Multimedia 20, 12 (2018), 3377–3388.\\n[65] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu\\nZhou, Haoran Wei, et al . 2023. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint\\narXiv:2309.11499 (2023).\\n[66] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong\\nDuan, Maosong Cao, et al. 2024. Internlm-xcomposer2: Mastering free-form text-image composition and comprehen-\\nsion in vision-language large model. arXiv preprint arXiv:2401.16420 (2024).\\n[67] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong\\nDuan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen,\\nConghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. 2024. InternLM-XComposer2: Mastering\\nFree-form Text-Image Composition and Comprehension in Vision-Language Large Model. arXiv:2401.16420 [cs.CV]\\nhttps://arxiv.org/abs/2401.16420\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 60, 'page_label': '61', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 61\\n[68] Alexey Dosovitskiy. 2020. An image is worth 16x16 words: Transformers for image recognition at scale.arXiv preprint\\narXiv:2010.11929 (2020).\\n[69] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson,\\nQuan Vuong, Tianhe Yu, Wenlong Huang, et al. 2023. Palm-e: An embodied multimodal language model. (2023).\\n[70] Andrew Drozdov, Honglei Zhuang, Zhuyun Dai, Zhen Qin, Razieh Rahimi, Xuanhui Wang, Dana Alon, Mohit Iyyer,\\nAndrew McCallum, Donald Metzler, et al . 2023. PaRaDe: Passage ranking using demonstrations with LLMs. In\\nFindings of the Association for Computational Linguistics: EMNLP 2023 . 14242–14252.\\n[71] Yifan Du, Kun Zhou, Yuqi Huo, Yifan Li, Wayne Xin Zhao, Haoyu Lu, Zijia Zhao, Bingning Wang, Weipeng Chen,\\nand Ji-Rong Wen. 2024. Towards Event-oriented Long Video Understanding. arXiv preprint arXiv:2406.14129 (2024).\\n[72] Martin Engilberge, Louis Chevallier, Patrick Pérez, and Matthieu Cord. 2018. Finding beans in burgers: Deep semantic-\\nvisual embedding with localization. In Proceedings of the IEEE conference on computer vision and pattern recognition .\\n3984–3993.\\n[73] Boris Epshtein, Eyal Ofek, and Yonatan Wexler. 2010. Detecting text in natural scenes with stroke width transform.\\nIn 2010 IEEE computer society conference on computer vision and pattern recognition . IEEE, 2963–2970.\\n[74] Shahul Es, Jithin James, Luis Espinosa Anke, and Steven Schockaert. 2024. Ragas: Automated evaluation of re-\\ntrieval augmented generation. In Proceedings of the 18th Conference of the European Chapter of the Association for\\nComputational Linguistics: System Demonstrations . 150–158.\\n[75] Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. 2017. Vse++: Improving visual-semantic embeddings\\nwith hard negatives. arXiv preprint arXiv:1707.05612 (2017).\\n[76] Minghui Fang, Shengpeng Ji, Jialong Zuo, Hai Huang, Yan Xia, Jieming Zhu, Xize Cheng, Xiaoda Yang, Wenrui Liu,\\nGang Wang, et al. 2024. Ace: A generative cross-modal retrieval framework with coarse-to-fine semantic modeling.\\narXiv preprint arXiv:2406.17507 (2024).\\n[77] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. 2024. MMBench-Video:\\nA Long-Form Multi-Shot Benchmark for Holistic Video Understanding. arXiv preprint arXiv:2406.14515 (2024).\\n[78] Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, and Pierre Colombo. 2024.\\nColPali: Efficient Document Retrieval with Vision Language Models. arXiv:2407.01449 [cs.IR] https://arxiv.org/abs/\\n2407.01449\\n[79] Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, and Pierre Colombo. 2024.\\nColpali: Efficient document retrieval with vision language models. In The Thirteenth International Conference on\\nLearning Representations .\\n[80] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with\\nsimple and efficient sparsity. Journal of Machine Learning Research 23, 120 (2022), 1–39.\\n[81] Hao Feng, Qi Liu, Hao Liu, Jingqun Tang, Wengang Zhou, Houqiang Li, and Can Huang. 2024. Docpedia: Unleashing\\nthe power of large multimodal model in the frequency domain for versatile document understanding. Science China\\nInformation Sciences 67, 12 (2024), 1–14.\\n[82] Paolo Ferragina and Giovanni Manzini. 2000. Opportunistic data structures with applications. In Proceedings 41st\\nannual symposium on foundations of computer science . IEEE, 390–398.\\n[83] Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and Stéphane Clinchant. 2021. SPLADE v2: Sparse lexical\\nand expansion model for information retrieval. arXiv preprint arXiv:2109.10086 (2021).\\n[84] Thibault Formal, Benjamin Piwowarski, and Stéphane Clinchant. 2021. SPLADE: Sparse lexical and expansion model\\nfor first stage ranking. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in\\nInformation Retrieval . 2288–2292.\\n[85] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang\\nShen, Mengdan Zhang, et al. 2024. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal\\nllms in video analysis. arXiv preprint arXiv:2405.21075 (2024).\\n[86] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu Ma,\\nand Ranjay Krishna. 2025. Blink: Multimodal large language models can see but not perceive. In European Conference\\non Computer Vision . Springer, 148–166.\\n[87] Zheren Fu, Zhendong Mao, Yan Song, and Yongdong Zhang. 2023. Learning semantic relationship among instances\\nfor image-text matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\\n15159–15168.\\n[88] Zheren Fu, Lei Zhang, Hou Xia, and Zhendong Mao. 2024. Linguistic-Aware Patch Slimming Framework for Fine-\\ngrained Cross-Modal Alignment. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\\n26307–26316.\\n[89] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. 2020. Multi-modal transformer for video retrieval.\\nIn Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IV 16 .\\nSpringer, 214–229.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 61, 'page_label': '62', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='62 Trovato et al.\\n[90] Jun Gao, Ziqiang Cao, and Wenjie Li. 2024. SelfCP: Compressing over-limit prompt via the frozen large language\\nmodel itself. Information Processing & Management 61, 6 (2024), 103873.\\n[91] Jun Gao, Ziqiang Cao, and Wenjie Li. 2024. Unifying demonstration selection and compression for in-context learning.\\narXiv preprint arXiv:2405.17062 (2024).\\n[92] Luyu Gao and Jamie Callan. 2021. Condenser: a pre-training architecture for dense retrieval. arXiv preprint\\narXiv:2104.08253 (2021).\\n[93] Luyu Gao and Jamie Callan. 2021. Unsupervised corpus aware language model pre-training for dense passage retrieval.\\narXiv preprint arXiv:2108.05540 (2021).\\n[94] Luyu Gao, Zhuyun Dai, and Jamie Callan. 2021. COIL: Revisit exact lexical match in information retrieval with\\ncontextualized inverted list. arXiv preprint arXiv:2104.07186 (2021).\\n[95] Luyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Benjamin Van Durme, and Jamie Callan. 2021. Complement\\nlexical retrieval model with semantic residual embeddings. In European Conference on Information Retrieval . Springer,\\n146–160.\\n[96] Zhi Gao, Yuntao Du, Xintong Zhang, Xiaojian Ma, Wenjuan Han, Song-Chun Zhu, and Qing Li. 2024. Clova: A\\nclosed-loop visual assistant with tool usage and update. In Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition . 13258–13268.\\n[97] Noa Garcia, Mayu Otani, Chenhui Chu, and Yuta Nakashima. 2020. KnowIT VQA: Answering knowledge-based\\nquestions about videos. In Proceedings of the AAAI conference on artificial intelligence , Vol. 34. 10826–10834.\\n[98] Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. 2013. Optimized product quantization. IEEE transactions on pattern\\nanalysis and machine intelligence 36, 4 (2013), 744–755.\\n[99] Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. 2023. In-context autoencoder for context\\ncompression in a large language model. arXiv preprint arXiv:2307.06945 (2023).\\n[100] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. 2023. Planting a seed of vision in large language\\nmodel. arXiv preprint arXiv:2307.08041 (2023).\\n[101] Gregor Geigle, Radu Timofte, and Goran Glavaš. 2024. African or European Swallow? Benchmarking Large Vision-\\nLanguage Models for Fine-Grained Object Classification. arXiv preprint arXiv:2406.14496 (2024).\\n[102] Peiyuan Gong, Jiamian Li, and Jiaxin Mao. 2024. Cosearchagent: a lightweight collaborative search agent with large\\nlanguage models. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in\\nInformation Retrieval . 2729–2733.\\n[103] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping\\nLuo, and Kai Chen. 2023. Multimodal-gpt: A vision and language model for dialogue with humans. arXiv preprint\\narXiv:2305.04790 (2023).\\n[104] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the v in vqa matter:\\nElevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition . 6904–6913.\\n[105] Alex Graves and Alex Graves. 2012. Long short-term memory. Supervised sequence labelling with recurrent neural\\nnetworks (2012), 37–45.\\n[106] Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint\\narXiv:2312.00752 (2023).\\n[107] Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu Minzhe, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang,\\nXin Jiang, et al. 2022. Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark. Advances in\\nNeural Information Processing Systems 35 (2022), 26418–26431.\\n[108] Anisha Gunjal, Jihan Yin, and Erhan Bas. 2024. Detecting and preventing hallucinations in large vision language\\nmodels. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 38. 18135–18143.\\n[109] Fang Guo, Wenyu Li, Honglei Zhuang, Yun Luo, Yafu Li, Qi Zhu, Le Yan, and Yue Zhang. 2024. Generating diverse\\ncriteria on-the-fly to improve point-wise LLM rankers. arXiv preprint arXiv:2404.11960 (2024).\\n[110] Weikuo Guo, Huaibo Huang, Xiangwei Kong, and Ran He. 2019. Learning disentangled representation for cross-\\nmodal retrieval with deep mutual information estimation. In Proceedings of the 27th ACM International Conference on\\nMultimedia. 1712–1720.\\n[111] Yanming Guo, Yu Liu, Theodoros Georgiou, and Michael S Lew. 2018. A review of semantic segmentation using deep\\nneural networks. International journal of multimedia information retrieval 7 (2018), 87–93.\\n[112] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.\\n2018. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition . 3608–3617.\\n[113] Xiaotian Han, Quanzeng You, Yongfei Liu, Wentao Chen, Huangjie Zheng, Khalil Mrini, Xudong Lin, Yiqi Wang,\\nBohan Zhai, Jianbo Yuan, et al. 2023. InfiMM-Eval: Complex Open-Ended Reasoning Evaluation For Multi-Modal\\nLarge Language Models. arXiv e-prints (2023), arXiv–2311.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 62, 'page_label': '63', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 63\\n[114] Darryl Hannan, Akshay Jain, and Mohit Bansal. 2020. Manymodalqa: Modality disambiguation and qa over diverse\\ninputs. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 7879–7886.\\n[115] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang,\\nYuxiang Zhang, et al. 2024. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual\\nmultimodal scientific problems. arXiv preprint arXiv:2402.14008 (2024).\\n[116] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momentum contrast for unsupervised\\nvisual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition .\\n9729–9738.\\n[117] Zheqi He, Xinya Wu, Pengfei Zhou, Richeng Xuan, Guang Liu, Xi Yang, Qiannan Zhu, and Hua Huang. 2024.\\nCMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning. arXiv preprint\\narXiv:2401.14011 (2024).\\n[118] Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun-Hsuan Sung, László Lukács, Ruiqi Guo, Sanjiv Kumar, Balint\\nMiklos, and Ray Kurzweil. 2017. Efficient natural language response suggestion for smart reply. arXiv preprint\\narXiv:1705.00652 (2017).\\n[119] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent\\nVanhoucke, Patrick Nguyen, Tara N Sainath, et al. 2012. Deep neural networks for acoustic modeling in speech\\nrecognition: The shared views of four research groups. IEEE Signal processing magazine 29, 6 (2012), 82–97.\\n[120] Sebastian Hofstätter, Omar Khattab, Sophia Althammer, Mete Sertkan, and Allan Hanbury. 2022. Introducing neural\\nbag of whole-words with colberter: Contextualized late interactions using enhanced reduction. In Proceedings of the\\n31st ACM International Conference on Information & Knowledge Management . 737–747.\\n[121] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao\\nDong, Ming Ding, et al. 2024. Cogagent: A visual language model for gui agents. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition . 14281–14290.\\n[122] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna,\\nChen-Yu Lee, and Tomas Pfister. 2023. Distilling step-by-step! outperforming larger language models with less\\ntraining data and smaller model sizes. arXiv preprint arXiv:2305.02301 (2023).\\n[123] Anwen Hu, Yaya Shi, Haiyang Xu, Jiabo Ye, Qinghao Ye, Ming Yan, Chenliang Li, Qi Qian, Ji Zhang, and Fei Huang.\\n2024. mplug-paperowl: Scientific diagram analysis with the multimodal large language model. In Proceedings of the\\n32nd ACM International Conference on Multimedia . 6929–6938.\\n[124] Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye\\nZhang, et al. 2023. Large multilingual models pivot zero-shot multimodal learning across languages. arXiv preprint\\narXiv:2308.12038 (2023).\\n[125] Wenbo Hu, Jia-Chen Gu, Zi-Yi Dou, Mohsen Fayyaz, Pan Lu, Kai-Wei Chang, and Nanyun Peng. 2024. MRAG-Bench:\\nVision-Centric Evaluation for Retrieval-Augmented Multimodal Models. arXiv preprint arXiv:2410.08182 (2024).\\n[126] Wenbo Hu, Jia-Chen Gu, Zi-Yi Dou, Mohsen Fayyaz, Pan Lu, Kai-Wei Chang, and Nanyun Peng. 2024. MRAG-Bench:\\nVision-Centric Evaluation for Retrieval-Augmented Multimodal Models. arXiv preprint arXiv:2410.08182 (2024).\\n[127] Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanabhan, Giuseppe\\nOttaviano, and Linjun Yang. 2020. Embedding-based retrieval in facebook search. In Proceedings of the 26th ACM\\nSIGKDD International Conference on Knowledge Discovery & Data Mining . 2553–2561.\\n[128] Minbin Huang, Runhui Huang, Han Shi, Yimeng Chen, Chuanyang Zheng, Xiangguo Sun, Xin Jiang, Zhenguo Li,\\nand Hong Cheng. 2024. Efficient Multi-modal Large Language Models via Visual Token Grouping. arXiv preprint\\narXiv:2411.17773 (2024).\\n[129] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong,\\nJiawei Huang, Jinglin Liu, et al. 2024. Audiogpt: Understanding and generating speech, music, sound, and talking\\nhead. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 38. 23802–23804.\\n[130] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan\\nMohammed, Barun Patra, et al . 2023. Language is not all you need: Aligning perception with language models.\\nAdvances in Neural Information Processing Systems 36 (2023), 72096–72109.\\n[131] Siteng Huang, Biao Gong, Yulin Pan, Jianwen Jiang, Yiliang Lv, Yuyuan Li, and Donglin Wang. 2023. Vop: Text-video\\nco-operative prompt tuning for cross-modal retrieval. In Proceedings of the IEEE/CVF conference on computer vision\\nand pattern recognition . 6565–6574.\\n[132] Xijie Huang, Li Lyna Zhang, Kwang-Ting Cheng, Fan Yang, and Mao Yang. 2023. Fewer is more: Boosting LLM\\nreasoning with reinforced context pruning. arXiv preprint arXiv:2312.08901 (2023).\\n[133] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. 2022. Layoutlmv3: Pre-training for document ai with\\nunified text and image masking. In Proceedings of the 30th ACM International Conference on Multimedia . 4083–4091.\\n[134] Yupan Huang, Zaiqiao Meng, Fangyu Liu, Yixuan Su, Nigel Collier, and Yutong Lu. 2023. Sparkles: Unlocking chats\\nacross multiple images for multimodal instruction-following models. arXiv preprint arXiv:2308.16463 (2023).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 63, 'page_label': '64', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='64 Trovato et al.\\n[135] Yan Huang, Wei Wang, and Liang Wang. 2017. Instance-aware image and sentence matching with selective multimodal\\nlstm. In Proceedings of the IEEE conference on computer vision and pattern recognition . 2310–2318.\\n[136] Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, and Jianlong Fu. 2021. Seeing out of the box:\\nEnd-to-end pre-training for vision-language representation learning. In Proceedings of the IEEE/CVF conference on\\ncomputer vision and pattern recognition . 12976–12985.\\n[137] S Humeau. 2019. Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-\\nsentence scoring. arXiv preprint arXiv:1905.01969 (2019).\\n[138] Zaeem Hussain, Mingda Zhang, Xiaozhong Zhang, Keren Ye, Christopher Thomas, Zuha Agha, Nathan Ong, and\\nAdriana Kovashka. 2017. Automatic understanding of image and video advertisements. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition . 1705–1715.\\n[139] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard\\nGrave. 2021. Towards unsupervised dense information retrieval with contrastive learning. arXiv preprint\\narXiv:2112.09118 2, 3 (2021).\\n[140] Aman Jain, Mayank Kothyari, Vishwajeet Kumar, Preethi Jyothi, Ganesh Ramakrishnan, and Soumen Chakrabarti.\\n2021. Select, substitute, search: A new benchmark for knowledge-augmented visual question answering. InProceedings\\nof the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 2491–2498.\\n[141] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. 2017. Tgif-qa: Toward spatio-temporal\\nreasoning in visual question answering. InProceedings of the IEEE conference on computer vision and pattern recognition .\\n2758–2766.\\n[142] Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010. Product quantization for nearest neighbor search. IEEE\\ntransactions on pattern analysis and machine intelligence 33, 1 (2010), 117–128.\\n[143] Hervé Jégou, Matthijs Douze, Cordelia Schmid, and Patrick Pérez. 2010. Aggregating local descriptors into a compact\\nimage representation. In 2010 IEEE computer society conference on computer vision and pattern recognition . IEEE,\\n3304–3311.\\n[144] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and\\nTom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. In\\nInternational conference on machine learning . PMLR, 4904–4916.\\n[145] Yiren Jian, Chongyang Gao, and Soroush Vosoughi. 2023. Bootstrapping vision-language learning with decoupled\\nlanguage pre-training. Advances in Neural Information Processing Systems 36 (2023), 57–72.\\n[146] Ding Jiang and Mang Ye. 2023. Cross-modal implicit relation reasoning and aligning for text-to-image person retrieval.\\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2787–2797.\\n[147] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Guanglu Song,\\nPeng Gao, et al. 2024. Mmsearch: Benchmarking the potential of large models as multi-modal search engines. arXiv\\npreprint arXiv:2409.12959 (2024).\\n[148] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. Llmlingua: Compressing prompts for\\naccelerated inference of large language models. arXiv preprint arXiv:2310.05736 (2023).\\n[149] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. Longllmlingua:\\nAccelerating and enhancing llms in long context scenarios via prompt compression. arXiv preprint arXiv:2310.06839\\n(2023).\\n[150] Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen\\nZhuang. 2024. E5-v: Universal embeddings with multimodal large language models. arXiv preprint arXiv:2407.12580\\n(2024).\\n[151] Yutao Jiang, Qiong Wu, Wenhao Lin, Wei Yu, and Yiyi Zhou. 2025. What Kind of Visual Tokens Do We Need?\\nTraining-free Visual Token Pruning for Multi-modal Large Language Models from the Perspective of Graph. arXiv\\npreprint arXiv:2501.02268 (2025).\\n[152] Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu Chen. 2024. Vlm2vec: Training vision-\\nlanguage models for massive multimodal embedding tasks. arXiv preprint arXiv:2410.05160 (2024).\\n[153] Bowen Jin, Hansi Zeng, Guoyin Wang, Xiusi Chen, Tianxin Wei, Ruirui Li, Zhengyang Wang, Zheng Li, Yang Li,\\nHanqing Lu, et al. 2023. Language models as semantic indexers. arXiv preprint arXiv:2310.07815 (2023).\\n[154] Yang Jin, Zhicheng Sun, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang\\nSong, et al. 2024. Video-lavit: Unified video-language pre-training with decoupled visual-motional tokenization.\\narXiv preprint arXiv:2402.03161 (2024).\\n[155] Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Quzhe Huang, Bin Chen, Chenyi Lei, An Liu, Chengru Song,\\net al. 2023. Unified language-vision pretraining in llm with dynamic discrete visual tokenization. arXiv preprint\\narXiv:2309.04669 (2023).\\n[156] Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with gpus. IEEE Transactions on\\nBig Data 7, 3 (2019), 535–547.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 64, 'page_label': '65', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 65\\n[157] Jia-Huei Ju, Jheng-Hong Yang, and Chuan-Ju Wang. 2021. Text-to-text multi-view learning for passage re-ranking.\\nIn Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval .\\n1803–1807.\\n[158] Dongwon Jung, Qin Liu, Tenghao Huang, Ben Zhou, and Muhao Chen. 2024. Familiarity-aware evidence compression\\nfor retrieval augmented generation. arXiv preprint arXiv:2409.12468 (2024).\\n[159] Hoyoun Jung and Kyung-Joong Kim. 2024. Discrete prompt compression with reinforcement learning. IEEE Access\\n(2024).\\n[160] Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Ákos Kádár, Adam Trischler, and Yoshua Bengio. 2017.\\nFigureqa: An annotated figure dataset for visual reasoning. arXiv preprint arXiv:1710.07300 (2017).\\n[161] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau\\nYih. 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906 (2020).\\n[162] Mehran Kazemi, Nishanth Dikkala, Ankit Anand, Petar Devic, Ishita Dasgupta, Fangyu Liu, Bahare Fatemi, Pranjal\\nAwasthi, Dee Guo, Sreenivas Gollapudi, et al. 2024. ReMI: A Dataset for Reasoning with Multiple Images. arXiv\\npreprint arXiv:2406.09175 (2024).\\n[163] Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage search via contextualized late\\ninteraction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in\\nInformation Retrieval . 39–48.\\n[164] Jing Yu Koh, Daniel Fried, and Russ R Salakhutdinov. 2023. Generating images with multimodal language models.\\nAdvances in Neural Information Processing Systems 36 (2023), 21487–21506.\\n[165] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. 2023. Grounding language models to images for multimodal\\ninputs and outputs. In International Conference on Machine Learning . PMLR, 17283–17300.\\n[166] Weize Kong, Swaraj Khadanga, Cheng Li, Shaleen Kumar Gupta, Mingyang Zhang, Wensong Xu, and Michael\\nBendersky. 2022. Multi-aspect dense retrieval. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge\\nDiscovery and Data Mining . 3178–3186.\\n[167] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural\\nnetworks. Advances in neural information processing systems 25 (2012).\\n[168] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. 2024. Lisa: Reasoning segmentation\\nvia large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\\n9579–9589.\\n[169] Yann LeCun, Yoshua Bengio, et al. 1995. Convolutional networks for images, speech, and time series. The handbook\\nof brain theory and neural networks 3361, 10 (1995), 1995.\\n[170] Jaewoo Lee, Joonho Ko, Jinheon Baek, Soyeong Jeong, and Sung Ju Hwang. 2024. Unified Multimodal Interleaved\\nDocument Representation for Retrieval. arXiv:2410.02729 [cs.CL] https://arxiv.org/abs/2410.02729\\n[171] Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi Chen. 2020. Learning dense representations of phrases at scale.\\narXiv preprint arXiv:2012.12624 (2020).\\n[172] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain\\nquestion answering. arXiv preprint arXiv:1906.00300 (2019).\\n[173] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal,\\nPeter Shaw, Ming-Wei Chang, and Kristina Toutanova. 2023. Pix2struct: Screenshot parsing as pretraining for visual\\nlanguage understanding. In International Conference on Machine Learning . PMLR, 18893–18912.\\n[174] Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He. 2018. Stacked cross attention for image-text\\nmatching. In Proceedings of the European conference on computer vision (ECCV) . 201–216.\\n[175] Sunkyung Lee, Minjin Choi, and Jongwuk Lee. 2023. GLEN: Generative retrieval via lexical index learning. arXiv\\npreprint arXiv:2311.03057 (2023).\\n[176] Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Minjoon Seo. 2023. Volcano: mitigating multimodal hallucination\\nthrough self-feedback guided revision. arXiv preprint arXiv:2311.07362 (2023).\\n[177] Paul Lerner, Olivier Ferret, Camille Guinaudeau, Hervé Le Borgne, Romaric Besançon, José G Moreno, and Jesús\\nLovón Melgarejo. 2022. ViQuAE, a dataset for knowledge-based visual question answering about named entities. In\\nProceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval .\\n3108–3120.\\n[178] Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. 2024. Seed-bench-2-plus: Benchmarking\\nmultimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790 (2024).\\n[179] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. 2024. SEED-Bench:\\nBenchmarking Multimodal Large Language Models. In Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition . 13299–13308.\\n[180] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. 2023. Seed-bench: Benchmarking\\nmultimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125 (2023).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 65, 'page_label': '66', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='66 Trovato et al.\\n[181] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. 2023.\\nMimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425 (2023).\\n[182] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung\\nPoon, and Jianfeng Gao. 2023. Llava-med: Training a large language-and-vision assistant for biomedicine in one day.\\nAdvances in Neural Information Processing Systems 36 (2023), 28541–28564.\\n[183] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang. 2020. Unicoder-vl: A universal encoder for vision\\nand language by cross-modal pre-training. In Proceedings of the AAAI conference on artificial intelligence , Vol. 34.\\n11336–11344.\\n[184] Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. 2022. A survey on retrieval-augmented text generation.\\narXiv preprint arXiv:2202.01110 (2022).\\n[185] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pre-training\\nwith frozen image encoders and large language models. In International conference on machine learning . PMLR,\\n19730–19742.\\n[186] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training\\nfor unified vision-language understanding and generation. In International conference on machine learning . PMLR,\\n12888–12900.\\n[187] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. 2023.\\nVideochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355 (2023).\\n[188] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. 2024.\\nMvbench: A comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition . 22195–22206.\\n[189] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng\\nKong. 2023. Silkie: Preference distillation for large visual language models. arXiv preprint arXiv:2312.10665 (2023).\\n[190] Lei Li, Yongfeng Zhang, and Li Chen. 2023. Prompt distillation for efficient llm-based recommendation. InProceedings\\nof the 32nd ACM International Conference on Information and Knowledge Management . 1348–1357.\\n[191] Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, and Furu Wei.\\n2023. Trocr: Transformer-based optical character recognition with pre-trained models. In Proceedings of the AAAI\\nconference on artificial intelligence , Vol. 37. 13094–13102.\\n[192] Shengzhi Li and Nima Tajbakhsh. 2023. Scigraphqa: A large-scale synthetic multi-turn question-answering dataset\\nfor scientific graphs. arXiv preprint arXiv:2308.03349 (2023).\\n[193] Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yukun Yan, Shuo Wang, and Ge Yu. 2024. Say more with less:\\nUnderstanding prompt learning behaviors through gist compression. arXiv preprint arXiv:2402.16058 (2024).\\n[194] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu\\nWei, et al. 2020. Oscar: Object-semantics aligned pre-training for vision-language tasks. In Computer Vision–ECCV\\n2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX 16 . Springer, 121–137.\\n[195] Yongqi Li, Hongru Cai, Wenjie Wang, Leigang Qu, Yinwei Wei, Wenjie Li, Liqiang Nie, and Tat-Seng Chua. 2024. Rev-\\nolutionizing Text-to-Image Retrieval as Autoregressive Token-to-Voken Generation.arXiv preprint arXiv:2407.17274\\n(2024).\\n[196] Yucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. 2023. Compressing context to enhance inference efficiency\\nof large language models. arXiv preprint arXiv:2310.06201 (2023).\\n[197] Yangning Li, Yinghui Li, Xinyu Wang, Yong Jiang, Zhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao Zheng, Pengjun\\nXie, Philip S. Yu, Fei Huang, and Jingren Zhou. 2024. Benchmarking Multimodal Retrieval Augmented Generation\\nwith Dynamic VQA Dataset and Self-adaptive Planning Agent. (2024). arXiv:2411.02937 [cs.CL] https://arxiv.org/\\nabs/2411.02937\\n[198] Yanwei Li, Chengyao Wang, and Jiaya Jia. 2024. Llama-vid: An image is worth 2 tokens in large language models. In\\nEuropean Conference on Computer Vision . Springer, 323–340.\\n[199] Yongqi Li, Wenjie Wang, Leigang Qu, Liqiang Nie, Wenjie Li, and Tat-Seng Chua. 2024. Generative cross-modal\\nretrieval: Memorizing images in multimodal language models for retrieval and beyond.arXiv preprint arXiv:2402.10805\\n(2024).\\n[200] Yongqi Li, Nan Yang, Liang Wang, Furu Wei, and Wenjie Li. 2023. Multiview identifiers enhanced generative retrieval.\\narXiv preprint arXiv:2305.16675 (2023).\\n[201] Yongqi Li, Nan Yang, Liang Wang, Furu Wei, and Wenjie Li. 2024. Learning to rank in generative retrieval. In\\nProceedings of the AAAI Conference on Artificial Intelligence , Vol. 38. 8716–8723.\\n[202] Yongqi Li, Zhen Zhang, Wenjie Wang, Liqiang Nie, Wenjie Li, and Tat-Seng Chua. 2024. Distillation Enhanced\\nGenerative Retrieval. arXiv preprint arXiv:2402.10769 (2024).\\n[203] Zongqian Li, Yixuan Su, and Nigel Collier. 2024. 500xCompressor: Generalized Prompt Compression for Large\\nLanguage Models. arXiv preprint arXiv:2408.03094 (2024).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 66, 'page_label': '67', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 67\\n[204] Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li, Van Tu Vu, et al.\\n2024. LEGO: language enhanced multi-modal grounding model. arXiv e-prints (2024), arXiv–2401.\\n[205] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. 2024.\\nMonkey: Image resolution and text label are important things for large multi-modal models. In proceedings of the\\nIEEE/CVF conference on computer vision and pattern recognition . 26763–26773.\\n[206] Minghui Liao, Baoguang Shi, Xiang Bai, Xinggang Wang, and Wenyu Liu. 2016. TextBoxes: A Fast Text Detector with\\na Single Deep Neural Network. arXiv:1611.06779 [cs.CV] https://arxiv.org/abs/1611.06779\\n[207] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Jinfa Huang, Junwu Zhang, Yatian Pang, Munan Ning,\\net al. 2024. Moe-llava: Mixture of experts for large vision-language models. arXiv preprint arXiv:2401.15947 (2024).\\n[208] Jimmy Lin and Xueguang Ma. 2021. A few brief notes on deepimpact, coil, and a conceptual framework for information\\nretrieval techniques. arXiv preprint arXiv:2106.14807 (2021).\\n[209] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. 2024. Vila: On pre-training\\nfor visual language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition .\\n26689–26699.\\n[210] Sheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, and Wei Ping. 2024. Mm-embed:\\nUniversal multimodal retrieval with multimodal llms. arXiv preprint arXiv:2411.02571 (2024).\\n[211] Weizhe Lin, Jinghong Chen, Jingbiao Mei, Alexandru Coca, and Bill Byrne. 2023. Fine-grained late-interaction\\nmulti-modal retrieval for retrieval augmented visual question answering. Advances in Neural Information Processing\\nSystems 36 (2023), 22820–22840.\\n[212] Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, and\\nHongsheng Li. 2024. Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you\\nwant. arXiv preprint arXiv:2403.20271 (2024).\\n[213] Barys Liskavets, Maxim Ushakov, Shuvendu Roy, Mark Klibanov, Ali Etemad, and Shane Luke. 2024. Prompt com-\\npression with context-aware sentence encoding for fast and improved llm inference. arXiv preprint arXiv:2409.01227\\n(2024).\\n[214] Alexander Liu and Samuel Yang. 2022. Masked autoencoders as the unified learners for pre-trained sentence\\nrepresentation. arXiv preprint arXiv:2208.00231 (2022).\\n[215] Chong Liu, Yuqi Zhang, Hongsong Wang, Weihua Chen, Fan Wang, Yan Huang, Yi-Dong Shen, and Liang Wang.\\n2023. Efficient token-guided image-text retrieval with consistent multimodal contrastive training. IEEE Transactions\\non Image Processing 32 (2023), 3622–3633.\\n[216] Dongyang Liu, Renrui Zhang, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng\\nJin, Kaipeng Zhang, et al. 2024. Sphinx-x: Scaling data and parameters for a family of multi-modal large language\\nmodels. arXiv preprint arXiv:2402.05935 (2024).\\n[217] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong\\nYu. 2023. Mmc: Advancing multimodal chart understanding with large-scale instruction tuning. arXiv preprint\\narXiv:2311.10774 (2023).\\n[218] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual Instruction Tuning. arXiv:2304.08485 [cs.CV]\\nhttps://arxiv.org/abs/2304.08485\\n[219] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. Advances in neural\\ninformation processing systems 36 (2023), 34892–34916.\\n[220] Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, and Yiming Qian. 2023. Tcra-llm: Token compression retrieval\\naugmented large language model for inference cost reduction. arXiv preprint arXiv:2310.15556 (2023).\\n[221] Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. 2024. Visualwebbench:\\nHow far have multimodal llms evolved in web page understanding and grounding? arXiv preprint arXiv:2404.05955\\n(2024).\\n[222] Qi Liu, Bo Wang, Nan Wang, and Jiaxin Mao. 2024. Leveraging passage embeddings for efficient listwise reranking\\nwith large language models. In THE WEB CONFERENCE 2025 .\\n[223] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu,\\net al. 2024. Llava-plus: Learning to use tools for creating multimodal agents. In European Conference on Computer\\nVision. Springer, 126–142.\\n[224] Song Liu, Haoqi Fan, Shengsheng Qian, Yiru Chen, Wenkui Ding, and Zhongyuan Wang. 2021. Hit: Hierarchical\\ntransformer with momentum contrast for video-text retrieval. In Proceedings of the IEEE/CVF international conference\\non computer vision . 11915–11925.\\n[225] Shuo Liu, Kaining Ying, Hao Zhang, Yue Yang, Yuqi Lin, Tianle Zhang, Chuanhao Li, Yu Qiao, Ping Luo, Wenqi\\nShao, et al. 2024. Convbench: A multi-turn conversation evaluation benchmark with hierarchical capability for large\\nvision-language models. arXiv preprint arXiv:2403.20194 (2024).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 67, 'page_label': '68', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='68 Trovato et al.\\n[226] Ting Liu, Liangtao Shi, Richang Hong, Yue Hu, Quanjun Yin, and Linfeng Zhang. 2024. Multi-Stage Vision Token\\nDropping: Towards Efficient Multimodal Large Language Model. arXiv preprint arXiv:2411.10803 (2024).\\n[227] Weihao Liu, Fangyu Lei, Tongxu Luo, Jiahe Lei, Shizhu He, Jun Zhao, and Kang Liu. 2023. MMHQA-ICL: Multimodal\\nIn-context Learning for Hybrid Question Answering over Text, Tables and Images. arXiv preprint arXiv:2309.04790\\n(2023).\\n[228] Wenhan Liu, Yutao Zhu, and Zhicheng Dou. 2024. Demorank: Selecting effective demonstrations for large language\\nmodels in ranking task. arXiv preprint arXiv:2406.16332 (2024).\\n[229] Yinhan Liu. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 364\\n(2019).\\n[230] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui\\nHe, Ziwei Liu, et al. 2025. Mmbench: Is your multi-modal model an all-around player?. In European conference on\\ncomputer vision . Springer, 216–233.\\n[231] Yu Liu, Yanming Guo, Erwin M Bakker, and Michael S Lew. 2017. Learning a recurrent residual fusion network for\\nmultimodal matching. In Proceedings of the IEEE international conference on computer vision . 4107–4116.\\n[232] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. [n. d.].\\nTempcompass: Do video llms really understand videos?, 2024c. URL https://arxiv. org/abs/2403.00476 ([n. d.]).\\n[233] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen\\nJin, and Xiang Bai. 2024. OCRBench: on the hidden mystery of OCR in large multimodal models. Science China\\nInformation Sciences 67, 12 (2024), 220102.\\n[234] Zejun Liu, Fanglin Chen, Jun Xu, Wenjie Pei, and Guangming Lu. 2022. Image-text retrieval with cross-modal semantic\\nimportance consistency. IEEE Transactions on Circuits and Systems for Video Technology 33, 5 (2022), 2465–2476.\\n[235] Ziyu Liu, Tao Chu, Yuhang Zang, Xilin Wei, Xiaoyi Dong, Pan Zhang, Zijian Liang, Yuanjun Xiong, Yu Qiao, Dahua\\nLin, et al. 2024. MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset\\nfor LVLMs. arXiv preprint arXiv:2406.11833 (2024).\\n[236] Zhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Ziheng Li, Xizhou Zhu, Lewei Lu, Qifeng Chen, Yu Qiao, Jifeng\\nDai, et al. 2024. Controlllm: Augment language models with tools by searching on graphs. In European Conference on\\nComputer Vision . Springer, 89–105.\\n[237] Zhenghao Liu, Chenyan Xiong, Yuanhuiyi Lv, Zhiyuan Liu, and Ge Yu. 2022. Universal vision-language dense\\nretrieval: Learning a unified representation space for multi-modal retrieval. arXiv preprint arXiv:2209.00179 (2022).\\n[238] Xinwei Long, Jiali Zeng, Fandong Meng, Zhiyuan Ma, Kaiyan Zhang, Bowen Zhou, and Jie Zhou. 2024. Generative\\nmulti-modal knowledge retrieval with large language models. In Proceedings of the AAAI Conference on Artificial\\nIntelligence, Vol. 38. 18733–18741.\\n[239] Siyu Lou, Xuenan Xu, Mengyue Wu, and Kai Yu. 2022. Audio-text retrieval in context. In ICASSP 2022-2022 IEEE\\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 4793–4797.\\n[240] Haoyu Lu, Nanyi Fei, Yuqi Huo, Yizhao Gao, Zhiwu Lu, and Ji-Rong Wen. 2022. Cots: Collaborative two-stream\\nvision-language pre-training model for cross-modal retrieval. In Proceedings of the IEEE/CVF conference on computer\\nVision and pattern recognition . 15692–15701.\\n[241] Junyu Lu, Dixiang Zhang, Songxin Zhang, Zejian Xie, Zhuoyang Song, Cong Lin, Jiaxing Zhang, Bingyi Jing, and\\nPingjian Zhang. 2023. Lyrics: Boosting fine-grained language-vision alignment and comprehension via semantic-aware\\nvisual objects. arXiv preprint arXiv:2312.05278 (2023).\\n[242] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang,\\nMichel Galley, and Jianfeng Gao. 2023. Mathvista: Evaluating mathematical reasoning of foundation models in visual\\ncontexts. arXiv preprint arXiv:2310.02255 (2023).\\n[243] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and\\nAshwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering.\\nAdvances in Neural Information Processing Systems 35 (2022), 2507–2521.\\n[244] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. 2024. Ovis: Structural\\nembedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797 (2024).\\n[245] Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin. 2024. WildVision:\\nEvaluating Vision-Language Models in the Wild with Human Preferences. arXiv preprint arXiv:2406.11069 (2024).\\n[246] Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2021. Sparse, dense, and attentional representations\\nfor text retrieval. Transactions of the Association for Computational Linguistics 9 (2021), 329–345.\\n[247] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. 2022. Clip4clip: An empirical\\nstudy of clip for end to end video clip retrieval and captioning. Neurocomputing 508 (2022), 293–304.\\n[248] Jian Luo, Xuanang Chen, Ben He, and Le Sun. 2024. Prp-graph: Pairwise ranking prompting to llms with graph\\naggregation for effective text re-ranking. InProceedings of the 62nd Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers) . 5766–5776.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 68, 'page_label': '69', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 69\\n[249] Tengchao Lv, Yupan Huang, Jingye Chen, Yuzhong Zhao, Yilin Jia, Lei Cui, Shuming Ma, Yaoyao Chang, Shaohan\\nHuang, Wenhui Wang, et al. 2023. Kosmos-2.5: A multimodal literate model. arXiv preprint arXiv:2309.11419 (2023).\\n[250] Haoyu Ma, Handong Zhao, Zhe Lin, Ajinkya Kale, Zhangyang Wang, Tong Yu, Jiuxiang Gu, Sunav Choudhary, and\\nXiaohui Xie. 2022. Ei-clip: Entity-aware interventional contrastive learning for e-commerce cross-modal retrieval. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 18051–18061.\\n[251] Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Xiang Ji, and Xueqi Cheng. 2021. Prop: Pre-training with\\nrepresentative words prediction for ad-hoc retrieval. In Proceedings of the 14th ACM International Conference on Web\\nSearch and Data Mining . 283–291.\\n[252] Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Yingyan Li, and Xueqi Cheng. 2021. B-PROP: bootstrapped\\npre-training with representative words prediction for ad-hoc retrieval. In Proceedings of the 44th International ACM\\nSIGIR Conference on Research and Development in Information Retrieval . 1513–1522.\\n[253] Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin. 2024. Unifying Multimodal Retrieval via\\nDocument Screenshot Embedding. arXiv:2406.11251 [cs.IR] https://arxiv.org/abs/2406.11251\\n[254] Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin. 2024. Unifying multimodal retrieval via\\ndocument screenshot embedding. arXiv preprint arXiv:2406.11251 (2024).\\n[255] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2024. Fine-tuning llama for multi-stage text retrieval.\\nIn Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval .\\n2421–2425.\\n[256] Xueguang Ma, Xinyu Zhang, Ronak Pradeep, and Jimmy Lin. 2023. Zero-shot listwise document reranking with a\\nlarge language model. arXiv preprint arXiv:2305.02156 (2023).\\n[257] Yingzi Ma, Yulong Cao, Jiachen Sun, Marco Pavone, and Chaowei Xiao. 2024. Dolphins: Multimodal language model\\nfor driving. In European Conference on Computer Vision . Springer, 403–420.\\n[258] Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong,\\net al. 2024. Mmlongbench-doc: Benchmarking long-context document understanding with visualizations. arXiv\\npreprint arXiv:2407.01523 (2024).\\n[259] Zi-Ao Ma, Tian Lan, Rong-Cheng Tu, Yong Hu, Heyan Huang, and Xian-Ling Mao. 2024. Multi-modal Retrieval\\nAugmented Multi-modal Generation: A Benchmark, Evaluate Metrics and Strong Baselines. arXiv:2411.16365 [cs.CL]\\nhttps://arxiv.org/abs/2411.16365\\n[260] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. 2023. Video-chatgpt: Towards detailed\\nvideo understanding via large vision and language models. arXiv preprint arXiv:2306.05424 (2023).\\n[261] Raman Maini and Himanshu Aggarwal. 2009. Study and comparison of various image edge detection techniques.\\nInternational journal of image processing (IJIP) 3, 1 (2009), 1–11.\\n[262] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. 2023. Egoschema: A diagnostic benchmark for very\\nlong-form video language understanding. Advances in Neural Information Processing Systems 36 (2023), 46212–46244.\\n[263] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019. Ok-vqa: A visual question answering\\nbenchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern\\nrecognition. 3195–3204.\\n[264] Julieta Martinez, Holger H Hoos, and James J Little. 2014. Stacked quantizers for compositional vector compression.\\narXiv preprint arXiv:1411.2173 (2014).\\n[265] Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, and Shafiq Joty. 2023. Unichart: A universal\\nvision-language pretrained model for chart comprehension and reasoning. arXiv preprint arXiv:2305.14761 (2023).\\n[266] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. 2022. Chartqa: A benchmark for question\\nanswering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244 (2022).\\n[267] Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. 2022. Infographicvqa.\\nIn Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision . 1697–1706.\\n[268] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. 2021. Docvqa: A dataset for vqa on document images. In\\nProceedings of the IEEE/CVF winter conference on applications of computer vision . 2200–2209.\\n[269] Lang Mei, Jiaxin Mao, Gang Guo, and Ji-Rong Wen. 2022. Learning Probabilistic Box Embeddings for Effective and\\nEfficient Ranking. In Proceedings of the ACM Web Conference 2022 . 473–482.\\n[270] Lang Mei, Jiaxin Mao, Juan Hu, Naiqiang Tan, Hua Chai, and Ji-Rong Wen. 2023. Improving first-stage retrieval of\\npoint-of-interest search by pre-training models. ACM Transactions on Information Systems 42, 3 (2023), 1–27.\\n[271] Xinhao Mei, Xubo Liu, Jianyuan Sun, Mark D Plumbley, and Wenwu Wang. 2022. On metric learning for audio-text\\ncross-modal retrieval. arXiv preprint arXiv:2203.15537 (2022).\\n[272] Thomas Mensink, Jasper Uijlings, Lluis Castrejon, Arushi Goel, Felipe Cadar, Howard Zhou, Fei Sha, André Araujo,\\nand Vittorio Ferrari. 2023. Encyclopedic VQA: Visual questions about detailed properties of fine-grained categories.\\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision . 3113–3124.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 69, 'page_label': '70', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='70 Trovato et al.\\n[273] Antoine Miech, Ivan Laptev, and Josef Sivic. 2018. Learning a text-video embedding from incomplete and heteroge-\\nneous data. arXiv preprint arXiv:1804.02516 (2018).\\n[274] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019.\\nHowto100m: Learning a text-video embedding by watching hundred million narrated video clips. In Proceedings of\\nthe IEEE/CVF international conference on computer vision . 2630–2640.\\n[275] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. 2019. Ocr-vqa: Visual question\\nanswering by reading text in images. In 2019 international conference on document analysis and recognition (ICDAR) .\\nIEEE, 947–952.\\n[276] Niluthpol Chowdhury Mithun, Juncheng Li, Florian Metze, and Amit K Roy-Chowdhury. 2018. Learning joint\\nembedding with multimodal cues for cross-modal video-text retrieval. In Proceedings of the 2018 ACM on international\\nconference on multimedia retrieval . 19–27.\\n[277] Debjyoti Mondal, Suraj Modi, Subhadarshi Panda, Rituraj Singh, and Godawari Sudhakar Rao. 2024. Kam-cot:\\nKnowledge augmented multimodal chain-of-thoughts reasoning. In Proceedings of the AAAI conference on artificial\\nintelligence, Vol. 38. 18798–18806.\\n[278] Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh,\\nPrakash Murugesan, Peyman Heidari, Yue Liu, et al. 2024. Anymal: An efficient and scalable any-modality augmented\\nlanguage model. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry\\nTrack. 1314–1332.\\n[279] Jesse Mu, Xiang Li, and Noah Goodman. 2023. Learning to compress prompts with gist tokens. Advances in Neural\\nInformation Processing Systems 36 (2023), 19327–19352.\\n[280] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and\\nPing Luo. 2023. Embodiedgpt: Vision-language pre-training via embodied chain of thought. Advances in Neural\\nInformation Processing Systems 36 (2023), 25081–25094.\\n[281] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. Ms\\nmarco: A human-generated machine reading comprehension dataset. (2016).\\n[282] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT. arXiv preprint arXiv:1901.04085 (2019).\\n[283] Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020. Document ranking with a pretrained sequence-to-sequence\\nmodel. arXiv preprint arXiv:2003.06713 (2020).\\n[284] Rodrigo Nogueira, Jimmy Lin, and AI Epistemic. 2019. From doc2query to docTTTTTquery. Online preprint 6, 2\\n(2019).\\n[285] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-stage document ranking with BERT.arXiv\\npreprint arXiv:1910.14424 (2019).\\n[286] Shubham Singh Paliwal, D Vishwanath, Rohit Rahul, Monika Sharma, and Lovekesh Vig. 2019. Tablenet: Deep\\nlearning model for end-to-end table detection and tabular data extraction from scanned document images. In 2019\\nInternational Conference on Document Analysis and Recognition (ICDAR) . IEEE, 128–133.\\n[287] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. 2023. Kosmos-g: Generating\\nimages in context with multimodal large language models. arXiv preprint arXiv:2310.02992 (2023).\\n[288] Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing\\nYang, Chin-Yew Lin, et al . 2024. Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt\\ncompression. arXiv preprint arXiv:2403.12968 (2024).\\n[289] Artemis Panagopoulou, Le Xue, Ning Yu, Junnan Li, Dongxu Li, Shafiq Joty, Ran Xu, Silvio Savarese, Caiming Xiong,\\nand Juan Carlos Niebles. 2023. X-instructblip: A framework for aligning x-modal instruction-aware representations\\nto llms and emergent cross-modal reasoning. arXiv preprint arXiv:2311.18799 (2023).\\n[290] Yanwei Pang, Yuan Yuan, Xuelong Li, and Jing Pan. 2011. Efficient HOG human detection. Signal processing 91, 4\\n(2011), 773–781.\\n[291] Jae Sung Park, Chandra Bhagavatula, Roozbeh Mottaghi, Ali Farhadi, and Yejin Choi. 2020. Visualcomet: Reasoning\\nabout the dynamic context of a still image. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK,\\nAugust 23–28, 2020, Proceedings, Part V 16 . Springer, 508–524.\\n[292] Andrew Parry, Sean MacAvaney, and Debasis Ganguly. 2024. Top-down partitioning for efficient list-wise ranking.\\narXiv preprint arXiv:2405.14589 (2024).\\n[293] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. 2023. Kosmos-2:\\nGrounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824 (2023).\\n[294] Zhiyuan Peng, Xuyang Wu, Qifan Wang, Sravanthi Rajanala, and Yi Fang. 2024. Q-peft: Query-dependent parameter\\nefficient fine-tuning for text reranking with large language models. arXiv preprint arXiv:2404.04522 (2024).\\n[295] Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong, Jipeng Zhang, Lewei Yao, Jianhua Han, Hang Xu, Lingpeng\\nKong, et al. 2023. Detgpt: Detect what you need via reasoning. arXiv preprint arXiv:2305.14167 (2023).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 70, 'page_label': '71', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 71\\n[296] Ronak Pradeep, Rodrigo Nogueira, and Jimmy Lin. 2021. The expando-mono-duo design pattern for text ranking\\nwith pretrained sequence-to-sequence models. arXiv preprint arXiv:2101.05667 (2021).\\n[297] Xiao Pu, Tianxing He, and Xiaojun Wan. 2024. Style-Compress: An LLM-Based Prompt Compression Framework\\nConsidering Task-Specific Styles. arXiv preprint arXiv:2410.14042 (2024).\\n[298] Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, et al.\\n2024. Cogcom: Train large vision-language models diving into details through chain of manipulations. arXiv preprint\\narXiv:2402.04236 (2024).\\n[299] Jinwei Qi, Yuxin Peng, and Yuxin Yuan. 2018. Cross-media multi-level alignment with relation attention network.\\narXiv preprint arXiv:1804.09539 (2018).\\n[300] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei,\\nZhe Wei, Miaoxuan Zhang, et al. 2024. We-math: Does your large multimodal model achieve human-like mathematical\\nreasoning? arXiv preprint arXiv:2407.01284 (2024).\\n[301] Jie Qin, Jie Wu, Weifeng Chen, Yuxi Ren, Huixia Li, Hefeng Wu, Xuefeng Xiao, Rui Wang, and Shilei Wen. 2024.\\nDiffusionGPT: LLM-driven text-to-image generation system. arXiv preprint arXiv:2401.10061 (2024).\\n[302] Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald\\nMetzler, et al. 2023. Large language models are effective text rankers with pairwise ranking prompting. arXiv preprint\\narXiv:2306.17563 (2023).\\n[303] Leigang Qu, Haochuan Li, Tan Wang, Wenjie Wang, Yongqi Li, Liqiang Nie, and Tat-Seng Chua. 2024. Unified\\ntext-to-image generation and retrieval. arXiv preprint arXiv:2406.05814 (2024).\\n[304] Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang.\\n2020. RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering.\\narXiv preprint arXiv:2010.08191 (2020).\\n[305] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda\\nAskell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision.\\nIn International conference on machine learning . PMLR, 8748–8763.\\n[306] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are\\nunsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.\\n[307] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M\\nAnwer, Eric Xing, Ming-Hsuan Yang, and Fahad S Khan. 2024. Glamm: Pixel grounding large multimodal model. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 13009–13018.\\n[308] David Rau, Shuai Wang, Hervé Déjean, and Stéphane Clinchant. 2024. Context embeddings for efficient answer\\ngeneration in rag. arXiv preprint arXiv:2407.09252 (2024).\\n[309] Revanth Gangi Reddy, JaeHyeok Doo, Yifei Xu, Md Arafat Sultan, Deevya Swain, Avirup Sil, and Heng Ji. 2024. FIRST:\\nFaster Improved Listwise Reranking with Single Token Decoding. arXiv preprint arXiv:2406.15657 (2024).\\n[310] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin. 2024. Pixellm:\\nPixel reasoning with large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition . 26374–26383.\\n[311] TIMODAL RETRIEVAL, KNOWLEDGE-ENHANCED RERANKING, and NOISE-INJECTED TRAINING. [n. d.]. MLLM\\nIS A STRONG RERANKER: ADVANCING MUL. ([n. d.]).\\n[312] Monica Riedler and Stefan Langer. 2024. Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial\\nApplications. arXiv:2410.21943 [cs.CL] https://arxiv.org/abs/2410.21943\\n[313] Jonathan Roberts, Kai Han, Neil Houlsby, and Samuel Albanie. 2024. Scifibench: Benchmarking large multimodal\\nmodels for scientific figure interpretation. arXiv preprint arXiv:2405.08807 (2024).\\n[314] Stephen Robertson. 2004. Understanding inverse document frequency: on theoretical arguments for IDF. Journal of\\ndocumentation 60, 5 (2004), 503–520.\\n[315] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond.Foundations\\nand Trends® in Information Retrieval 3, 4 (2009), 333–389.\\n[316] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. 1995. Okapi at\\nTREC-3. Nist Special Publication Sp 109 (1995), 109.\\n[317] Dongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Cheng Jiayang, Cunxiang Wang,\\nShichao Sun, Huanyu Li, et al. 2024. Ragchecker: A fine-grained framework for diagnosing retrieval-augmented\\ngeneration. Advances in Neural Information Processing Systems 37 (2024), 21999–22027.\\n[318] Devendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke\\nZettlemoyer. 2022. Improving passage retrieval with zero-shot question generation. arXiv preprint arXiv:2204.07496\\n(2022).\\n[319] Gerard Salton and Christopher Buckley. 1988. Term-weighting approaches in automatic text retrieval. Information\\nprocessing & management 24, 5 (1988), 513–523.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 71, 'page_label': '72', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='72 Trovato et al.\\n[320] Gerard Salton, Anita Wong, and Chung-Shu Yang. 1975. A vector space model for automatic indexing. Commun.\\nACM 18, 11 (1975), 613–620.\\n[321] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. 2022. A-okvqa: A\\nbenchmark for visual question answering using world knowledge. InEuropean conference on computer vision . Springer,\\n146–162.\\n[322] Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar. 2019. Kvqa: Knowledge-aware visual\\nquestion answering. In Proceedings of the AAAI conference on artificial intelligence , Vol. 33. 8876–8884.\\n[323] Shivam Shandilya, Menglin Xia, Supriyo Ghosh, Huiqiang Jiang, Jue Zhang, Qianhui Wu, and Victor Rühle.\\n2024. TACO-RL: Task Aware Prompt Compression Optimization with Reinforcement Learning. arXiv preprint\\narXiv:2409.13035 (2024).\\n[324] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. 2024. Visual\\ncot: Advancing multi-modal language models with a comprehensive dataset and benchmark for chain-of-thought\\nreasoning. Advances in Neural Information Processing Systems 37 (2024), 8612–8642.\\n[325] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving\\nai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems 36 (2023),\\n38154–38180.\\n[326] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and\\nDouwe Kiela. 2022. Flava: A foundational language and vision alignment model. In Proceedings of the IEEE/CVF\\nconference on computer vision and pattern recognition . 15638–15650.\\n[327] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.\\n2019. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition. 8317–8326.\\n[328] Hrituraj Singh, Anshul Nasery, Denil Mehta, Aishwarya Agarwal, Jatin Lamba, and Balaji Vasan Srinivasan. 2021.\\nMIMOQA: Multimodal Input Multimodal Output Question Answering. In Proceedings of the 2021 Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language Technologies , Kristina\\nToutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell,\\nTanmoy Chakraborty, and Yichao Zhou (Eds.). Association for Computational Linguistics, Online, 5317–5332. doi:10.\\n18653/v1/2021.naacl-main.418\\n[329] Hrituraj Singh, Anshul Nasery, Denil Mehta, Aishwarya Agarwal, Jatin Lamba, and Balaji Vasan Srinivasan. 2021.\\nMimoqa: Multimodal input multimodal output question answering. In Proceedings of the 2021 conference of the north\\namerican chapter of the association for computational linguistics: Human language technologies . 5317–5332.\\n[330] Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana, and Suranga\\nNanayakkara. 2023. Improving the domain adaptation of retrieval augmented generation (RAG) models for open\\ndomain question answering. Transactions of the Association for Computational Linguistics 11 (2023), 1–17.\\n[331] Charlie Snell, Dan Klein, and Ruiqi Zhong. 2022. Learning by distilling context. arXiv preprint arXiv:2209.15189\\n(2022).\\n[332] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo,\\nTian Ye, Yanting Zhang, et al. 2024. Moviechat: From dense token to sparse memory for long video understanding. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 18221–18232.\\n[333] Yale Song and Mohammad Soleymani. 2019. Polysemous visual-semantic embedding for cross-modal retrieval. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 1979–1988.\\n[334] Yiping Song, Rui Yan, Cheng-Te Li, Jian-Yun Nie, Ming Zhang, and Dongyan Zhao. 2018. An Ensemble of Retrieval-\\nBased and Generation-Based Human-Computer Conversation Systems. (2018).\\n[335] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. 2023. Pandagpt: One model to instruction-follow\\nthem all. arXiv preprint arXiv:2305.16355 (2023).\\n[336] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. 2018. A corpus for reasoning about\\nnatural language grounded in photographs. arXiv preprint arXiv:1811.00491 (2018).\\n[337] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun\\nHuang, and Xinlong Wang. 2024. Generative multimodal models are in-context learners. InProceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition . 14398–14409.\\n[338] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\\nHuang, and Xinlong Wang. 2023. Emu: Generative pretraining in multimodality. arXiv preprint arXiv:2307.05222\\n(2023).\\n[339] Weiwei Sun, Zheng Chen, Xinyu Ma, Lingyong Yan, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and\\nZhaochun Ren. 2023. Instruction distillation makes large language models efficient zero-shot rankers. arXiv preprint\\narXiv:2311.01555 (2023).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 72, 'page_label': '73', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 73\\n[340] Weiwei Sun, Lingyong Yan, Zheng Chen, Shuaiqiang Wang, Haichao Zhu, Pengjie Ren, Zhumin Chen, Dawei Yin,\\nMaarten Rijke, and Zhaochun Ren. 2024. Learning to tokenize for generative retrieval.Advances in Neural Information\\nProcessing Systems 36 (2024).\\n[341] Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun\\nRen. 2023. Is ChatGPT good at search? investigating large language models as re-ranking agents. arXiv preprint\\narXiv:2304.09542 (2023).\\n[342] Manan Suri, Puneet Mathur, Franck Dernoncourt, Kanika Goswami, Ryan A. Rossi, and Dinesh Manocha. 2024.\\nVisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal Retrieval-Augmented Generation.\\narXiv:2412.10704 [cs.CL] https://arxiv.org/abs/2412.10704\\n[343] Dídac Surís, Sachit Menon, and Carl Vondrick. 2023. Vipergpt: Visual inference via python execution for reasoning.\\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision . 11888–11898.\\n[344] Adiba Tabassum and Shweta A Dhondse. 2015. Text detection using MSER and stroke width transform. In 2015 Fifth\\nInternational Conference on Communication Systems and Network Technologies . IEEE, 568–571.\\n[345] Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi,\\nand Jonathan Berant. 2021. Multimodalqa: Complex question answering over text, tables and images. arXiv preprint\\narXiv:2104.06039 (2021).\\n[346] Sijun Tan, Xiuyu Li, Shishir Patil, Ziyang Wu, Tianjun Zhang, Kurt Keutzer, Joseph E Gonzalez, and Raluca Ada Popa.\\n2024. Lloco: Learning long contexts offline. arXiv preprint arXiv:2404.07979 (2024).\\n[347] Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito. 2023. SlideVQA: A\\nDataset for Document Visual Question Answering on Multiple Images. arXiv:2301.04883 [cs.CL] https://arxiv.org/\\nabs/2301.04883\\n[348] Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. 2021. Visualmrc: Machine reading comprehension on document\\nimages. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 35. 13878–13888.\\n[349] Raphael Tang, Xinyu Zhang, Xueguang Ma, Jimmy Lin, and Ferhan Ture. 2023. Found in the middle: Permutation\\nself-consistency improves listwise ranking in large language models. arXiv preprint arXiv:2310.07712 (2023).\\n[350] Xu Tang, Yijing Wang, Jingjing Ma, Xiangrong Zhang, Fang Liu, and Licheng Jiao. 2023. Interacting-enhancing\\nfeature transformer for cross-modal remote-sensing image and text retrieval. IEEE Transactions on Geoscience and\\nRemote Sensing 61 (2023), 1–15.\\n[351] Zineng Tang, Ziyi Yang, Mahmoud Khademi, Yang Liu, Chenguang Zhu, and Mohit Bansal. 2024. Codi-2: In-context\\ninterleaved and interactive any-to-any generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition . 27425–27434.\\n[352] Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta,\\net al. 2022. Transformer memory as a differentiable search index. Advances in Neural Information Processing Systems\\n35 (2022), 21831–21843.\\n[353] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,\\nAndrew M Dai, Anja Hauth, Katie Millican, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv\\npreprint arXiv:2312.11805 (2023).\\n[354] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. Beir: A heterogenous\\nbenchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663 (2021).\\n[355] Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong\\nLu, Jie Zhou, et al . 2024. Mm-interleaved: Interleaved image-text generative modeling via multi-modal feature\\nsynchronizer. arXiv preprint arXiv:2401.10208 (2024).\\n[356] Kaibin Tian, Ruixiang Zhao, Zijie Xin, Bangxiang Lan, and Xirong Li. 2024. Holistic Features are almost Sufficient\\nfor Text-to-Video Retrieval. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\\n17138–17147.\\n[357] Anthony Meng Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, and Steven CH Hoi. 2022. Plug-and-play vqa:\\nZero-shot vqa by conjoining large pretrained models with zero training. arXiv preprint arXiv:2210.08773 (2022).\\n[358] Rubèn Tito, Dimosthenis Karatzas, and Ernest Valveny. 2023. Hierarchical multimodal transformers for multipage\\ndocvqa. Pattern Recognition 144 (2023), 109834.\\n[359] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang,\\nShusheng Yang, Adithya Iyer, Xichen Pan, et al . 2024. Cambrian-1: A fully open, vision-centric exploration of\\nmultimodal llms. arXiv preprint arXiv:2406.16860 (2024).\\n[360] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. 2024. Eyes wide shut? exploring\\nthe visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition. 9568–9578.\\n[361] Atousa Torabi, Niket Tandon, and Leonid Sigal. 2016. Learning language-visual embedding for movie understanding\\nwith natural-language. arXiv preprint arXiv:1609.08124 (2016).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 73, 'page_label': '74', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='74 Trovato et al.\\n[362] A Vaswani. 2017. Attention is all you need. Advances in Neural Information Processing Systems (2017).\\n[363] Thorsten Wagner and Hans-Gerd Lipinski. 2013. IJBlob: an ImageJ library for connected component analysis and\\nshape analysis. Journal of Open Research Software 1, 1 (2013).\\n[364] Ao Wang, Fengyuan Sun, Hui Chen, Zijia Lin, Jungong Han, and Guiguang Ding. 2024. [CLS] Token Tells Everything\\nNeeded for Training-free Efficient MLLMs. arXiv preprint arXiv:2412.05819 (2024).\\n[365] Andong Wang, Bo Wu, Sunli Chen, Zhenfang Chen, Haotian Guan, Wei-Ning Lee, Li Erran Li, and Chuang Gan. 2024.\\nSOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition . 13384–13394.\\n[366] Chenyu Wang, Weixin Luo, Qianyu Chen, Haonan Mai, Jindi Guo, Sixun Dong, Zhengxin Li, Lin Ma, Shenghua Gao,\\net al. 2024. Tool-lmm: A large multi-modal model for tool agent learning. arXiv e-prints (2024), arXiv–2401.\\n[367] Dongsheng Wang, Natraj Raman, Mathieu Sibue, Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh\\nNourbakhsh, and Xiaomo Liu. 2023. DocLLM: A layout-aware generative language model for multimodal document\\nunderstanding. arXiv preprint arXiv:2401.00908 (2023).\\n[368] Fei Wang, Xingyu Fu, James Y Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou,\\nKai Zhang, et al. 2024. MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding. arXiv\\npreprint arXiv:2406.09411 (2024).\\n[369] Jinyu Wang, Jingjing Fu, Rui Wang, Lei Song, and Jiang Bian. 2025. PIKE-RAG: sPecIalized KnowledgE and Rationale\\nAugmented Generation. arXiv:2501.11551 [cs.CL] https://arxiv.org/abs/2501.11551\\n[370] Jian Wang, Yonghao He, Cuicui Kang, Shiming Xiang, and Chunhong Pan. 2015. Image-text cross-modal retrieval via\\nmodality-specific feature learning. In Proceedings of the 5th ACM on International Conference on Multimedia Retrieval .\\n347–354.\\n[371] Jiamian Wang, Guohao Sun, Pichao Wang, Dongfang Liu, Sohail Dianat, Majid Rabbani, Raghuveer Rao, and Zhiqiang\\nTao. 2024. Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition . 16551–16560.\\n[372] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. 2024. Measuring multimodal\\nmathematical reasoning with math-vision dataset. arXiv preprint arXiv:2402.14804 (2024).\\n[373] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei.\\n2022. Simlm: Pre-training with representation bottleneck for dense passage retrieval. arXiv preprint arXiv:2207.02578\\n(2022).\\n[374] Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel. 2017. Fvqa: Fact-based visual question\\nanswering. IEEE transactions on pattern analysis and machine intelligence 40, 10 (2017), 2413–2427.\\n[375] Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, and Anthony Dick. 2015. Explicit knowledge-based\\nreasoning for visual question answering. arXiv preprint arXiv:1511.02570 (2015).\\n[376] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao\\nDong, et al. 2024. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035 (2024).\\n[377] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song\\nXiXuan, et al. 2024. Cogvlm: Visual expert for pretrained language models. Advances in Neural Information Processing\\nSystems 37 (2024), 121475–121499.\\n[378] Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhenhang Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu,\\nZhiguo Cao, et al. 2023. The all-seeing project: Towards panoptic visual recognition and understanding of the open\\nworld. arXiv preprint arXiv:2308.01907 (2023).\\n[379] Xinfeng Wang, Jin Cui, Yoshimi Suzuki, and Fumiyo Fukumoto. 2024. Rdrec: Rationale distillation for llm-based\\nrecommendation. arXiv preprint arXiv:2405.10587 (2024).\\n[380] Xiaodan Wang, Lei Li, Zhixu Li, Xuwu Wang, Xiangru Zhu, Chengyu Wang, Jun Huang, and Yanghua Xiao. 2023.\\nAgree: Aligning cross-modal entities for image-text retrieval upon vision-language pre-trained models. InProceedings\\nof the Sixteenth ACM International Conference on Web Search and Data Mining . 456–464.\\n[381] Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas\\nBertasius, Mohit Bansal, et al. 2024. Mementos: A comprehensive benchmark for multimodal large language model\\nreasoning over image sequences. arXiv preprint arXiv:2401.10529 (2024).\\n[382] Xinyu Wang, Bohan Zhuang, and Qi Wu. 2024. Modaverse: Efficiently transforming modalities with llms. InProceedings\\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 26606–26616.\\n[383] Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai\\nZhao, Zheng Liu, et al . 2022. A neural corpus indexer for document retrieval. Advances in Neural Information\\nProcessing Systems 35 (2022), 25600–25614.\\n[384] Yan Wang, Yuting Su, Wenhui Li, Jun Xiao, Xuanya Li, and An-An Liu. 2023. Dual-path rare content enhancement\\nnetwork for image and text matching. IEEE Transactions on Circuits and Systems for Video Technology 33, 10 (2023),\\n6144–6158.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 74, 'page_label': '75', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 75\\n[385] Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. 2023. Learning to filter context\\nfor retrieval-augmented generation. arXiv preprint arXiv:2311.08377 (2023).\\n[386] Zheng Wang, Zhenwei Gao, Mengqun Han, Yang Yang, and Heng Tao Shen. 2024. Estimating the Semantics via\\nSector Embedding for Image-Text Retrieval. IEEE Transactions on Multimedia (2024).\\n[387] Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu,\\nSadhika Malladi, et al . 2024. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. arXiv\\npreprint arXiv:2406.18521 (2024).\\n[388] Zheng Wang, Xing Xu, Jiwei Wei, Ning Xie, Yang Yang, and Heng Tao Shen. 2024. Semantics disentangling for\\ncross-modal retrieval. IEEE Transactions on Image Processing 33 (2024), 2226–2237.\\n[389] Zihan Wang, Yujia Zhou, Yiteng Tu, and Zhicheng Dou. 2023. NOVO: learnable and interpretable document identifiers\\nfor model-based IR. InProceedings of the 32nd ACM International Conference on Information and Knowledge Management .\\n2656–2665.\\n[390] Jônatas Wehrmann and Rodrigo C Barros. 2018. Bidirectional retrieval made simple. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition . 7718–7726.\\n[391] Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. 2024. Uniir:\\nTraining and benchmarking universal multimodal information retrievers. In European Conference on Computer Vision .\\nSpringer, 387–404.\\n[392] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, En Yu, Jianjian Sun, Chunrui Han, and Xiangyu\\nZhang. 2024. Small language model meets with reinforced vision vocabulary. arXiv preprint arXiv:2401.12503 (2024).\\n[393] Wei Wei, Jiabin Tang, Lianghao Xia, Yangqin Jiang, and Chao Huang. 2024. Promptmm: Multi-modal knowledge\\ndistillation for recommendation with prompt-tuning. In Proceedings of the ACM Web Conference 2024 . 3217–3228.\\n[394] Haoyang Wen, Honglei Zhuang, Hamed Zamani, Alexander Hauptmann, and Michael Bendersky. 2024. Multimodal\\nreranking for knowledge-intensive visual question answering. arXiv preprint arXiv:2407.12277 (2024).\\n[395] Weixi Weng, Jieming Zhu, Xiaojun Meng, Hao Zhang, Rui Zhang, and Chun Yuan. 2024. Learning to Compress\\nContexts for Efficient Knowledge-based Visual Question Answering. arXiv preprint arXiv:2409.07331 (2024).\\n[396] David Wingate, Mohammad Shoeybi, and Taylor Sorensen. 2022. Prompt compression and contrastive conditioning\\nfor controllability and toxicity reduction in language models. arXiv preprint arXiv:2210.03162 (2022).\\n[397] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. 2023. Visual chatgpt:\\nTalking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671 (2023).\\n[398] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong\\nYan, Guangtao Zhai, et al. 2023. Q-bench: A benchmark for general-purpose foundation models on low-level vision.\\narXiv preprint arXiv:2309.14181 (2023).\\n[399] Penghao Wu and Saining Xie. 2024. V?: Guided visual search as a core mechanism in multimodal llms. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 13084–13094.\\n[400] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. 2024. Next-gpt: Any-to-any multimodal llm. In\\nForty-first International Conference on Machine Learning .\\n[401] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. 2024. NExT-GPT: Any-to-Any Multimodal LLM.\\narXiv:2309.05519 [cs.AI] https://arxiv.org/abs/2309.05519\\n[402] Wenhao Wu, Haipeng Luo, Bo Fang, Jingdong Wang, and Wanli Ouyang. 2023. Cap4video: What can auxiliary\\ncaptions do for text-video retrieval?. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition. 10704–10713.\\n[403] Renqiu Xia, Song Mao, Xiangchao Yan, Hongbin Zhou, Bo Zhang, Haoyang Peng, Jiahao Pi, Daocheng Fu, Wenjie\\nWu, Hancheng Ye, et al. 2024. DocGenome: An Open Large-scale Scientific Document Benchmark for Training and\\nTesting Multi-modal Large Language Models. arXiv preprint arXiv:2406.11633 (2024).\\n[404] Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Min Dou, Botian Shi, Junchi\\nYan, et al. 2024. Chartx & chartvlm: A versatile benchmark and foundation model for complicated chart reasoning.\\narXiv preprint arXiv:2402.12185 (2024).\\n[405] Chen-Wei Xie, Jianmin Wu, Yun Zheng, Pan Pan, and Xian-Sheng Hua. 2022. Token embeddings alignment for\\ncross-modal retrieval. In Proceedings of the 30th ACM International Conference on Multimedia . 4555–4563.\\n[406] Yifei Xin, Dongchao Yang, and Yuexian Zou. 2023. Improving text-audio retrieval by text-aware attention pooling and\\nprior matrix revised loss. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing\\n(ICASSP). IEEE, 1–5.\\n[407] Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang,\\nFeng Wu, et al. 2024. Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy\\nreduction. arXiv preprint arXiv:2410.17247 (2024).\\n[408] Guoxin Xiong, Meng Meng, Tianzhu Zhang, Dongming Zhang, and Yongdong Zhang. 2024. Reference-Aware Adaptive\\nNetwork for Image-Text Matching. IEEE Transactions on Circuits and Systems for Video Technology (2024).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 75, 'page_label': '76', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='76 Trovato et al.\\n[409] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020.\\nApproximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808\\n(2020).\\n[410] Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. Recomp: Improving retrieval-augmented lms with compression and\\nselective augmentation. arXiv preprint arXiv:2310.04408 (2023).\\n[411] Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao,\\nand Ping Luo. 2024. Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence (2024).\\n[412] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. 2020. Layoutlm: Pre-training of text\\nand layout for document image understanding. In Proceedings of the 26th ACM SIGKDD international conference on\\nknowledge discovery & data mining . 1192–1200.\\n[413] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang\\nChe, et al. 2020. Layoutlmv2: Multi-modal pre-training for visually-rich document understanding. arXiv preprint\\narXiv:2012.14740 (2020).\\n[414] Zhengzhuo Xu, Sinan Du, Yiyan Qi, Chengjin Xu, Chun Yuan, and Jian Guo. 2023. Chartbench: A benchmark for\\ncomplex visual reasoning in charts. arXiv preprint arXiv:2312.15915 (2023).\\n[415] Ikuya Yamada, Akari Asai, and Hannaneh Hajishirzi. 2021. Efficient passage retrieval with hashing for open-domain\\nquestion answering. arXiv preprint arXiv:2106.00882 (2021).\\n[416] Le Yan, Zhen Qin, Honglei Zhuang, Rolf Jagerman, Xuanhui Wang, Michael Bendersky, and Harrie Oosterhuis. 2024.\\nConsolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing. arXiv preprint\\narXiv:2404.11791 (2024).\\n[417] Siming Yan, Min Bai, Weifeng Chen, Xiong Zhou, Qixing Huang, and Li Erran Li. 2024. Vigor: Improving visual\\ngrounding of large vision language models with fine-grained reward modeling. In European Conference on Computer\\nVision. Springer, 37–53.\\n[418] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and CUI Bin. 2024. Mastering text-to-image\\ndiffusion: Recaptioning, planning, and generating with multimodal llms. In Forty-first International Conference on\\nMachine Learning .\\n[419] Song Yang, Qiang Li, Wenhui Li, Xuanya Li, and An-An Liu. 2022. Dual-level representation enhancement on\\ncharacteristic and context for image-text retrieval. IEEE Transactions on Circuits and Systems for Video Technology 32,\\n11 (2022), 8037–8050.\\n[420] Tianchi Yang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, and Qi Zhang. 2023. Auto\\nsearch indexer for end-to-end document retrieval. arXiv preprint arXiv:2310.12455 (2023).\\n[421] Xiangpeng Yang, Linchao Zhu, Xiaohan Wang, and Yi Yang. 2024. DGL: Dynamic Global-Local Prompt Tuning for\\nText-Video Retrieval. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 38. 6540–6548.\\n[422] Yang Yang, Chubing Zhang, Yi-Chu Xu, Dianhai Yu, De-Chuan Zhan, and Jian Yang. 2021. Rethinking Label-Wise\\nCross-Modal Retrieval from A Semantic Sharing Perspective.. In IJCAI. 3300–3306.\\n[423] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael\\nZeng, and Lijuan Wang. 2023. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint\\narXiv:2303.11381 (2023).\\n[424] Zhen Yang, Yingxue Zhang, Fandong Meng, and Jie Zhou. 2023. Teal: Tokenize and embed all for multi-modal large\\nlanguage models. arXiv preprint arXiv:2311.04589 (2023).\\n[425] Linli Yao, Lei Li, Shuhuai Ren, Lean Wang, Yuanxin Liu, Xu Sun, and Lu Hou. 2024. Deco: Decoupling token\\ncompression from semantic abstraction in multimodal large language models. arXiv preprint arXiv:2405.20985 (2024).\\n[426] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng\\nTian, et al. 2023. mplug-docowl: Modularized multimodal large language model for document understanding. arXiv\\npreprint arXiv:2307.02499 (2023).\\n[427] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\\nYaya Shi, et al. 2023. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint\\narXiv:2304.14178 (2023).\\n[428] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. 2024.\\nmplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedings of the\\nieee/cvf conference on computer vision and pattern recognition . 13040–13051.\\n[429] Dongyi Yi, Guibo Zhu, Chenglin Ding, Zongshu Li, Dong Yi, and Jinqiao Wang. 2025. MME-Industry: A Cross-Industry\\nMultimodal Evaluation Benchmark. arXiv preprint arXiv:2501.16688 (2025).\\n[430] Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Xiaoshui Huang, Zhiyong Wang, Lu Sheng,\\nLei Bai, et al. 2024. Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark.\\nAdvances in Neural Information Processing Systems 36 (2024).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 76, 'page_label': '77', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 77\\n[431] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo\\nLiu, et al. 2024. Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language models\\ntowards multitask agi. arXiv preprint arXiv:2404.16006 (2024).\\n[432] Chanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Minbyul Jeong, and Jaewoo Kang. 2024. Compact: Compressing\\nretrieved documents actively for question answering. arXiv preprint arXiv:2407.09014 (2024).\\n[433] Soyoung Yoon, Eunbi Choi, Jiyeon Kim, Hyeongu Yun, Yireun Kim, and Seung-won Hwang. 2024. Listt5: Listwise\\nreranking with fusion-in-decoder improves zero-shot retrieval. arXiv preprint arXiv:2402.15838 (2024).\\n[434] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang,\\nBrian Karrer, Shelly Sheynin, et al. 2023. Scaling autoregressive multi-modal models: Pretraining and instruction\\ntuning. arXiv preprint arXiv:2309.02591 2, 3 (2023), 3.\\n[435] Qinhan Yu, Zhiyou Xiao, Binghui Li, Zhengren Wang, Chong Chen, and Wentao Zhang. 2025. MRAMG-Bench: A\\nBeyondText Benchmark for Multimodal Retrieval-Augmented Multimodal Generation.arXiv preprint arXiv:2502.04176\\n(2025).\\n[436] Shi Yu, Zhenghao Liu, Chenyan Xiong, Tao Feng, and Zhiyuan Liu. 2021. Few-shot conversational dense retrieval.\\nIn Proceedings of the 44th International ACM SIGIR Conference on research and development in information retrieval .\\n829–838.\\n[437] Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan\\nLiu, et al. 2024. Visrag: Vision-based retrieval-augmented generation on multi-modality documents. arXiv preprint\\narXiv:2410.10594 (2024).\\n[438] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng,\\nMaosong Sun, et al. 2024. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional\\nhuman feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 13807–13816.\\n[439] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\\n2023. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490 (2023).\\n[440] Xiaohan Yu, Zhihan Yang, and Chong Chen. 2025. Unveiling the Potential of Multimodal Retrieval Augmented\\nGeneration with Planning. arXiv preprint arXiv:2501.15470 (2025).\\n[441] Youngjae Yu, Hyungjin Ko, Jongwook Choi, and Gunhee Kim. 2017. End-to-end concept word detection for video\\ncaptioning, retrieval, and question answering. In Proceedings of the IEEE conference on computer vision and pattern\\nrecognition. 3165–3173.\\n[442] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. 2019. Activitynet-qa: A dataset\\nfor understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial\\nIntelligence, Vol. 33. 9127–9134.\\n[443] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. 2024. Osprey:\\nPixel understanding with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition . 28202–28211.\\n[444] Zhengqing Yuan, Zhaoxu Li, Weiran Huang, Yanfang Ye, and Lichao Sun. 2023. Tinygpt-v: Efficient multimodal large\\nlanguage model via small backbones. arXiv preprint arXiv:2312.16862 (2023).\\n[445] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming\\nRen, Yuxuan Sun, et al. 2024. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark\\nfor expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 9556–9567.\\n[446] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang,\\nHuan Sun, et al. 2024. Mmmu-pro: A more robust multi-discipline multimodal understanding benchmark. arXiv\\npreprint arXiv:2409.02813 (2024).\\n[447] Sukmin Yun, Haokun Lin, Rusiru Thushara, Mohammad Qazim Bhat, Yongxin Wang, Zutao Jiang, Mingkai Deng,\\nJinhong Wang, Tianhua Tao, Junbo Li, et al. 2024. Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation\\nFramework for Multimodal LLMs. arXiv preprint arXiv:2406.20098 (2024).\\n[448] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. From recognition to cognition: Visual commonsense\\nreasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 6720–6731.\\n[449] Hansi Zeng, Chen Luo, Bowen Jin, Sheikh Muhammad Sarwar, Tianxin Wei, and Hamed Zamani. 2024. Scalable and\\neffective generative information retrieval. In Proceedings of the ACM on Web Conference 2024 . 1441–1452.\\n[450] Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, Tao Kong, and Ruihua\\nSong. 2024. What matters in training a gpt4-style language model with multimodal inputs?. In Proceedings of the\\n2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies (Volume 1: Long Papers) . 7930–7957.\\n[451] Zhixiong Zeng and Wenji Mao. 2022. A comprehensive empirical study of vision-language pre-trained model for\\nsupervised cross-modal retrieval. arXiv preprint arXiv:2201.02772 (2022).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 77, 'page_label': '78', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='78 Trovato et al.\\n[452] ChengXiang Zhai et al. 2008. Statistical language models for information retrieval a critical review. Foundations and\\nTrends® in Information Retrieval 2, 3 (2008), 137–213.\\n[453] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2021. Jointly optimizing query\\nencoder and product quantization to improve retrieval performance. In Proceedings of the 30th ACM International\\nConference on Information & Knowledge Management . 2487–2496.\\n[454] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2021. Optimizing dense retrieval\\nmodel training with hard negatives. In Proceedings of the 44th International ACM SIGIR Conference on Research and\\nDevelopment in Information Retrieval . 1503–1512.\\n[455] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2022. Learning discrete representations\\nvia constrained clustering for effective and efficient dense retrieval. In Proceedings of the Fifteenth ACM International\\nConference on Web Search and Data Mining . 1328–1336.\\n[456] Erhan Zhang, Xingzhu Wang, Peiyuan Gong, Yankai Lin, and Jiaxin Mao. 2024. Usimagent: Large language models for\\nsimulating search users. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development\\nin Information Retrieval . 2687–2692.\\n[457] Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu,\\nShuyue Guo, et al. 2024. Cmmmu: A chinese massive multi-discipline multimodal understanding benchmark. arXiv\\npreprint arXiv:2401.11944 (2024).\\n[458] Hanqi Zhang, Chong Chen, Lang Mei, Qi Liu, and Jiaxin Mao. 2024. Mamba Retriever: Utilizing Mamba for Effective\\nand Efficient Dense Retrieval. In Proceedings of the 33rd ACM International Conference on Information and Knowledge\\nManagement. 4268–4272.\\n[459] Hang Zhang, Yeyun Gong, Yelong Shen, Jiancheng Lv, Nan Duan, and Weizhu Chen. 2021. Adversarial retriever-ranker\\nfor dense text retrieval. arXiv preprint arXiv:2110.03611 (2021).\\n[460] Hang Zhang, Xin Li, and Lidong Bing. 2023. Video-llama: An instruction-tuned audio-visual language model for\\nvideo understanding. arXiv preprint arXiv:2306.02858 (2023).\\n[461] Han Zhang, Hongwei Shen, Yiming Qiu, Yunjiang Jiang, Songlin Wang, Sulong Xu, Yun Xiao, Bo Long, and Wen-Yun\\nYang. 2021. Joint learning of deep retrieval model and product quantization based embedding index. In Proceedings of\\nthe 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1718–1722.\\n[462] Jinxu Zhang, Yongqi Yu, and Yu Zhang. 2024. CREAM: coarse-to-fine retrieval and multi-modal efficient tuning for\\ndocument VQA. In Proceedings of the 32nd ACM International Conference on Multimedia . 925–934.\\n[463] Junyuan Zhang, Qintong Zhang, Bin Wang, Linke Ouyang, Zichen Wen, Ying Li, Ka-Ho Chow, Conghui He, and\\nWentao Zhang. 2024. OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation.\\narXiv:2412.02592 [cs.CV] https://arxiv.org/abs/2412.02592\\n[464] Longhui Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, and Min Zhang. 2023. A two-stage\\nadaptation of large language models for text ranking. arXiv preprint arXiv:2311.16720 (2023).\\n[465] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang\\nZhang, Shuangrui Ding, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He,\\nXingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. 2023. InternLM-XComposer: A Vision-Language Large Model\\nfor Advanced Text-image Comprehension and Composition. arXiv:2309.15112 [cs.CV] https://arxiv.org/abs/2309.15112\\n[466] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang,\\nLinke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li,\\nWenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang.\\n2024. InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and\\nOutput. arXiv:2407.03320 [cs.CV] https://arxiv.org/abs/2407.03320\\n[467] Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, and Zhicheng Dou. 2024. Compressing lengthy context\\nwith ultragist. arXiv preprint arXiv:2405.16635 (2024).\\n[468] Qi Zhang, Zhen Lei, Zhaoxiang Zhang, and Stan Z Li. 2020. Context-aware attention network for image-text retrieval.\\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 3536–3545.\\n[469] Qianchi Zhang, Hainan Zhang, Liang Pang, Hongwei Zheng, and Zhiming Zheng. 2024. AdaComp: Extractive\\nContext Compression with Adaptive Predictor for Retrieval-Augmented Large Language Models. arXiv preprint\\narXiv:2409.01579 (2024).\\n[470] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei\\nChang, Yu Qiao, et al. 2025. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?.\\nIn European Conference on Computer Vision . Springer, 169–186.\\n[471] Shunyu Zhang, Yaobo Liang, Ming Gong, Daxin Jiang, and Nan Duan. 2022. Multi-view document representation\\nlearning for open-domain dense retrieval. arXiv preprint arXiv:2203.08372 (2022).\\n[472] Shilong Zhang, Peize Sun, Shoufa Chen, Minn Xiao, Wenqi Shao, Wenwei Zhang, Yu Liu, Kai Chen, and Ping Luo.\\n[n. d.]. GPT4roi: Instruction tuning large language model on regionof-interest, 2024. In URL https://openreview.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 78, 'page_label': '79', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 79\\nnet/forum.\\n[473] Tianyu Zhang, Suyuchen Wang, Lu Li, Ge Zhang, Perouz Taslakian, Sai Rajeswar, Jie Fu, Bang Liu, and Yoshua Bengio.\\n2024. VCR: Visual Caption Restoration. arXiv preprint arXiv:2406.06462 (2024).\\n[474] Tao Zhang, Ziqi Zhang, Zongyang Ma, Yuxin Chen, Zhongang Qi, Chunfeng Yuan, Bing Li, Junfu Pu, Yuxuan Zhao,\\nZehua Xie, et al. 2024. m𝑅2AG: Multimodal Retrieval-Reflection-Augmented Generation for Knowledge-Based VQA.\\narXiv preprint arXiv:2411.15041 (2024).\\n[475] Xinyu Zhang, Sebastian Hofstätter, Patrick Lewis, Raphael Tang, and Jimmy Lin. 2023. Rank-without-gpt: Building\\ngpt-independent listwise rerankers on open-source large language models. arXiv preprint arXiv:2312.02969 (2023).\\n[476] Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie\\nLi, and Min Zhang. 2024. GME: Improving Universal Multimodal Retrieval by Multimodal LLMs. arXiv preprint\\narXiv:2412.16855 (2024).\\n[477] Yan Zhang, Zhong Ji, Di Wang, Yanwei Pang, and Xuelong Li. 2024. USER: Unified semantic enhancement with\\nmomentum contrast for image-text retrieval. IEEE Transactions on Image Processing (2024).\\n[478] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. 2023. Llavar: Enhanced\\nvisual instruction tuning for text-rich image understanding. arXiv preprint arXiv:2306.17107 (2023).\\n[479] Yidan Zhang, Ting Zhang, Dong Chen, Yujing Wang, Qi Chen, Xing Xie, Hao Sun, Weiwei Deng, Qi Zhang, Fan Yang,\\net al. 2024. Irgen: Generative modeling for image retrieval. In European Conference on Computer Vision . Springer,\\n21–41.\\n[480] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang,\\nQingsong Wen, Zhang Zhang, et al. 2024. MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution\\nReal-World Scenarios that are Difficult for Humans? arXiv preprint arXiv:2408.13257 (2024).\\n[481] Zheng Zhang, Chengquan Zhang, Wei Shen, Cong Yao, Wenyu Liu, and Xiang Bai. 2016. Multi-Oriented Text\\nDetection with Fully Convolutional Networks. arXiv:1604.04018 [cs.CV] https://arxiv.org/abs/1604.04018\\n[482] Bingchen Zhao, Yongshuo Zong, Letian Zhang, and Timothy Hospedales. 2024. Benchmarking Multi-Image Un-\\nderstanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning. arXiv\\npreprint arXiv:2406.12742 (2024).\\n[483] Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei, Hongyu Zhou, Jianjian Sun, Yuang Peng, Runpei Dong,\\nChunrui Han, et al. 2023. Chatspot: Bootstrapping multimodal llms via precise referring instruction tuning. arXiv\\npreprint arXiv:2307.09474 (2023).\\n[484] Shiyu Zhao, Zhenting Wang, Felix Juefei-Xu, Xide Xia, Miao Liu, Xiaofang Wang, Mingfu Liang, Ning Zhang,\\nDimitris N Metaxas, and Licheng Yu. 2024. Accelerating Multimodel Large Language Models by Searching Optimal\\nVision Token Reduction. arXiv preprint arXiv:2412.00556 (2024).\\n[485] Shengwei Zhao, Linhai Xu, Yuying Liu, and Shaoyi Du. 2023. Multi-grained representation learning for cross-modal\\nretrieval. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information\\nRetrieval. 2194–2198.\\n[486] Weichao Zhao, Hao Feng, Qi Liu, Jingqun Tang, Shu Wei, Binghong Wu, Lei Liao, Yongjie Ye, Hao Liu, Wengang\\nZhou, et al. 2024. Tabpedia: Towards comprehensive visual table understanding with concept synergy. arXiv preprint\\narXiv:2406.01326 (2024).\\n[487] Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang. 2023. Bubogpt: Enabling visual\\ngrounding in multi-modal llms. arXiv preprint arXiv:2307.08581 (2023).\\n[488] Zijia Zhao, Haoyu Lu, Yuqi Huo, Yifan Du, Tongtian Yue, Longteng Guo, Bingning Wang, Weipeng Chen, and Jing\\nLiu. 2024. Needle In A Video Haystack: A Scalable Synthetic Framework for Benchmarking Video MLLMs. arXiv\\npreprint arXiv:2406.09367 (2024).\\n[489] Liangli Zhen, Peng Hu, Xu Wang, and Dezhong Peng. 2019. Deep supervised cross-modal retrieval. In Proceedings of\\nthe IEEE/CVF conference on computer vision and pattern recognition . 10394–10403.\\n[490] Kaizhi Zheng, Xuehai He, and Xin Eric Wang. 2023. Minigpt-5: Interleaved vision-and-language generation via\\ngenerative vokens. arXiv preprint arXiv:2310.02239 (2023).\\n[491] Liu Zheng and Shao Yingxia. 2022. RetroMAE: Pre-training retrieval-oriented transformers via masked auto-encoder.\\narXiv: 2205.12035 (2022).\\n[492] Xiaoyang Zheng, Zilong Wang, Sen Li, Ke Xu, Tao Zhuang, Qingwen Liu, and Xiaoyi Zeng. 2023. Make: Vision-\\nlanguage pre-training based product retrieval in taobao search. In Companion Proceedings of the ACM Web Conference\\n2023. 356–360.\\n[493] Chenyu Zhou, Mengdan Zhang, Peixian Chen, Chaoyou Fu, Yunhang Shen, Xiawu Zheng, Xing Sun, and Rongrong\\nJi. 2024. VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models. arXiv preprint\\narXiv:2406.10228 (2024).\\n[494] Dong Zhou, Fang Lei, Lin Li, Yongmei Zhou, and Aimin Yang. 2024. Cross-Modal Interaction via Reinforcement\\nFeedback for Audio-Lyrics Retrieval. IEEE/ACM Transactions on Audio, Speech, and Language Processing (2024).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 79, 'page_label': '80', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='80 Trovato et al.\\n[495] Junjie Zhou, Zheng Liu, Shitao Xiao, Bo Zhao, and Yongping Xiong. 2024. VISTA: visualized text embedding for\\nuniversal multi-modal retrieval. arXiv preprint arXiv:2406.04292 (2024).\\n[496] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and\\nZheng Liu. 2024. MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding. arXiv preprint\\narXiv:2406.04264 (2024).\\n[497] Kun Zhou, Yeyun Gong, Xiao Liu, Wayne Xin Zhao, Yelong Shen, Anlei Dong, Jingwen Lu, Rangan Majumder, Ji-Rong\\nWen, Nan Duan, et al. 2022. Simans: Simple ambiguous negatives sampling for dense text retrieval. arXiv preprint\\narXiv:2210.11773 (2022).\\n[498] Tianshuo Zhou, Sen Mei, Xinze Li, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu, Yu Gu, and Ge Yu. 2023. MARVEL:\\nunlocking the multi-modal capability of dense retrieval via visual module plugin. arXiv preprint arXiv:2310.14037\\n(2023).\\n[499] Wangchunshu Zhou, Yuchen Eleanor Jiang, Ryan Cotterell, and Mrinmaya Sachan. 2023. Efficient prompting via\\ndynamic in-context learning. arXiv preprint arXiv:2305.11170 (2023).\\n[500] Yujia Zhou, Zhicheng Dou, and Ji-Rong Wen. 2023. Enhancing generative retrieval with reinforcement learning\\nfrom relevance feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing .\\n12481–12490.\\n[501] Yujia Zhou, Jing Yao, Zhicheng Dou, Ledell Wu, Peitian Zhang, and Ji-Rong Wen. 2022. Ultron: An ultimate retriever\\non corpus with a model-based indexer. arXiv preprint arXiv:2208.09257 (2022).\\n[502] Yu-Jia Zhou, Jing Yao, Zhi-Cheng Dou, Ledell Wu, and Ji-Rong Wen. 2023. DynamicRetriever: a pre-trained model-\\nbased IR system without an explicit index. Machine Intelligence Research 20, 2 (2023), 276–288.\\n[503] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei\\nLi, et al. 2023. Languagebind: Extending video-language pretraining to n-modality by language-based semantic\\nalignment. arXiv preprint arXiv:2310.01852 (2023).\\n[504] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language\\nunderstanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023).\\n[505] Dongsheng Zhu, Xunzhu Tang, Weidong Han, Jinghui Lu, Yukun Zhao, Guoliang Xing, Junfeng Wang, and Dawei\\nYin. 2024. Vislinginstruct: Elevating zero-shot learning in multi-modal language models with autonomous instruction\\noptimization. arXiv preprint arXiv:2402.07398 (2024).\\n[506] Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie Zhao, Hengshuang Zhao, Xiaohua Wang, and Ying Shan. 2023.\\nVl-gpt: A generative pre-trained transformer for vision and language understanding and generation. arXiv preprint\\narXiv:2312.09251 (2023).\\n[507] Yichen Zhu, Minjie Zhu, Ning Liu, Zhiyuan Xu, and Yaxin Peng. 2024. Llava-phi: Efficient multi-modal assistant\\nwith small language model. In Proceedings of the 1st International Workshop on Efficient Multimedia Computing under\\nLimited. 18–22.\\n[508] Zhengyuan Zhu, Daniel Lee, Hong Zhang, Sai Sree Harsha, Loic Feujio, Akash Maharaj, and Yunyao Li. 2024. MuRAR:\\nA Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering.\\narXiv:2408.08521 [cs.IR] https://arxiv.org/abs/2408.08521\\n[509] Honglei Zhuang, Zhen Qin, Kai Hui, Junru Wu, Le Yan, Xuanhui Wang, and Michael Bendersky. 2023. Beyond yes and\\nno: Improving zero-shot llm rankers via scoring fine-grained relevance labels. arXiv preprint arXiv:2310.14122 (2023).\\n[510] Honglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui, Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, and Michael Bendersky.\\n2023. Rankt5: Fine-tuning t5 for text ranking with ranking losses. In Proceedings of the 46th International ACM SIGIR\\nConference on Research and Development in Information Retrieval . 2308–2313.\\n[511] Shengyao Zhuang, Bing Liu, Bevan Koopman, and Guido Zuccon. 2023. Open-source large language models are\\nstrong zero-shot query likelihood models for document ranking. arXiv preprint arXiv:2310.13243 (2023).\\n[512] Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuccon, and Daxin Jiang. 2022. Bridging\\nthe gap between indexing and retrieval for differentiable search index with query generation. arXiv preprint\\narXiv:2206.10128 (2022).\\n[513] Justin Zobel and Alistair Moffat. 2006. Inverted files for text search engines. ACM computing surveys (CSUR) 38, 2\\n(2006), 6–es.\\n[514] Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, and Timothy Hospedales. 2024. Safety fine-tuning at\\n(almost) no cost: A baseline for vision large language models. arXiv preprint arXiv:2402.02207 (2024).\\nReceived 20 February 2007; revised 12 March 2009; accepted 5 June 2009\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 1\\nNatural Language Processing Advancements By\\nDeep Learning: A Survey\\nAmirsina Torﬁ, Member, IEEE, Rouzbeh A. Shirvani, Yaser Keneshloo, Nader Tavaf,\\nand Edward A. Fox, Fellow, IEEE\\nAbstract—Natural Language Processing (NLP) helps empower\\nintelligent machines by enhancing a better understanding of the\\nhuman language for linguistic-based human-computer communi-\\ncation. Recent developments in computational power and the ad-\\nvent of large amounts of linguistic data have heightened the need\\nand demand for automating semantic analysis using data-driven\\napproaches. The utilization of data-driven strategies is pervasive\\nnow due to the signiﬁcant improvements demonstrated through\\nthe usage of deep learning methods in areas such as Computer\\nVision, Automatic Speech Recognition, and in particular, NLP.\\nThis survey categorizes and addresses the different aspects and\\napplications of NLP that have beneﬁted from deep learning. It\\ncovers core NLP tasks and applications, and describes how deep\\nlearning methods and models advance these areas. We further\\nanalyze and compare different approaches and state-of-the-art\\nmodels.\\nIndex Terms—Natural Language Processing, Deep Learning,\\nArtiﬁcial Intelligence\\nI. I NTRODUCTION\\nN\\nATURAL Language Processing (NLP) is a sub-discipline\\nof computer science providing a bridge between natural\\nlanguages and computers. It helps empower machines to un-\\nderstand, process, and analyze human language [1]. NLP’s sig-\\nniﬁcance as a tool aiding comprehension of human-generated\\ndata is a logical consequence of the context-dependency\\nof data. Data becomes more meaningful through a deeper\\nunderstanding of its context, which in turn facilitates text\\nanalysis and mining. NLP enables this with the communication\\nstructures and patterns of humans.\\nDevelopment of NLP methods is increasingly reliant on\\ndata-driven approaches which help with building more pow-\\nerful and robust models [2]–[4]. Recent advances in com-\\nputational power, as well as greater availability of big data,\\nenable deep learning, one of the most appealing approaches\\nin the NLP domain [2], [3], [5], especially given that deep\\nlearning has already demonstrated superior performance in\\nadjoining ﬁelds like Computer Vision [6]–[10] and Speech\\nRecognition [11]–[13]. These developments led to a paradigm\\nshift from traditional to novel data-driven approaches aimed\\nat advancing NLP. The reason behind this shift was simple:\\nnew approaches are more promising regarding results, and are\\neasier to engineer.\\nAmirsina Torﬁ, Yaser Keneshloo, and Edward A. Fox were with the\\nDepartment of Computer Science, Virginia Polytechnic Institute and State\\nUniversity, Blacksburg, V A, 24060 USA e-mail: (amirsina.torﬁ@gmail.com,\\nyaserkl@vt.edu, fox@vt.edu). Rouzbeh A. Shirvani is an independent re-\\nsearcher, e-mail: (rouzbeh.asghari@gmail.com). Nader Tavaf was with the\\nUniversity of Minnesota Twin Cities, Minneapolis, MN, 55455 USA e-mail:\\n(tavaf001@umn.edu).\\nAs a sequitur to remarkable progress achieved in adjacent\\ndisciplines utilizing deep learning methods, deep neural net-\\nworks have been applied to various NLP tasks, including part-\\nof-speech tagging [14]–[17], named entity recognition [18],\\n[18]–[21], and semantic role labeling [22]–[25]. Most of the\\nresearch efforts in deep learning associated with NLP appli-\\ncations involve either supervised learning 1 or unsupervised\\nlearning2.\\nThis survey covers the emerging role of deep learning in the\\narea of NLP, across a broad range of categories. The research\\npresented in [26] is primarily focused on architectures, with\\nlittle discussion of applications. More recent works [4], [27]\\nare speciﬁc to certain applications or certain sub-ﬁelds of\\nNLP [21]. Here we build on previous works by describing\\nthe challenges, opportunities, and evaluations of the impact of\\napplying deep learning to NLP problems.\\nThis survey has six sections, including this introduction.\\nSection 2 lays out the theoretical dimensions of NLP and\\nartiﬁcial intelligence, and looks at deep learning as an ap-\\nproach to solving real-world problems. It motivates this study\\nby addressing the question: Why use deep learning in NLP?\\nThe third section discusses fundamental concepts necessary\\nto understand NLP, covering exemplary issues in representa-\\ntion, frameworks, and machine learning. The fourth section\\nsummarizes benchmark datasets employed in the NLP domain.\\nSection 5 focuses on some of the NLP applications where deep\\nlearning has demonstrated signiﬁcant beneﬁt. Finally, Section\\n6 provides a conclusion, also addressing some open problems\\nand promising areas for improvement.\\nII. B ACKGROUND\\nNLP has long been viewed as one aspect of artiﬁcial\\nintelligence (AI), since understanding and generating natural\\nlanguage are high-level indications of intelligence. Deep learn-\\ning is an effective AI tool, so we next situate deep learning in\\nthe AI world. After that we explain motivations for applying\\ndeep learning to NLP.\\nA. Artiﬁcial Intelligence and Deep Learning\\nThere have been “islands of success” where big data are\\nprocessed via AI capabilities to produce information to achieve\\ncritical operational goals (e.g., fraud detection). Accordingly,\\n1Learning from training data to predict the type of new unseen test examples\\nby mapping them to known pre-deﬁned labels.\\n2Making sense of data without sticking to speciﬁc tasks and supervisory\\nsignals.\\narXiv:2003.01200v4  [cs.CL]  27 Feb 2021'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 2\\nscientists and consumers anticipate enhancement across a\\nvariety of applications. However, achieving this requires un-\\nderstanding of AI and its mechanisms and means (e.g., algo-\\nrithms). Ted Greenwald, explaining AI to those who are not\\nAI experts, comments: ”Generally AI is anything a computer\\ncan do that formerly was considered a job for a human” [28].\\nAn AI goal is to extend the capabilities of information\\ntechnology (IT) from those to (1) generate, communicate,\\nand store data, to also (2) process data into the knowledge\\nthat decision makers and others need [29]. One reason is\\nthat the available data volume is increasing so rapidly that\\nit is now impossible for people to process all available data.\\nThis leaves two choices: (1) much or even most existing data\\nmust be ignored or (2) AI must be developed to process the\\nvast volumes of available data into the essential pieces of\\ninformation that decision-makers and others can comprehend.\\nDeep learning is a bridge between the massive amounts of\\ndata and AI.\\n1) Deﬁnitions: Deep learning refers to applying deep neu-\\nral networks to massive amounts of data to learn a procedure\\naimed at handling a task . The task can range from simple\\nclassiﬁcation to complex reasoning. In other words, deep\\nlearning is a set of mechanisms ideally capable of deriving an\\noptimum solution to any problem given a sufﬁciently extensive\\nand relevant input dataset. Loosely speaking, deep learning\\nis detecting and analyzing important structures/features in the\\ndata aimed at formulating a solution to a given problem. Here,\\nAI and deep learning meet. One version of the goal or ambition\\nbehind AI is enabling a machine to outperform what the human\\nbrain does. Deep learning is a means to this end.\\n2) Deep Learning Architectures: Numerous deep learning\\narchitectures have been developed in different research areas,\\ne.g., in NLP applications employing recurrent neural networks\\n(RNNs) [30], convolutional neural networks (CNNs) [31], and\\nmore recently, recursive neural networks [32]. We focus our\\ndiscussion on a review of the essential models, explained in\\nrelevant seminal publications.\\nMulti Layer Perceptron: A multilayer perceptron (MLP)\\nhas at least three layers (input, hidden, and output layers). A\\nlayer is simply a collection of neurons operating to transform\\ninformation from the previous layer to the next layer. In the\\nMLP architecture, the neurons in a layer do not communicate\\nwith each other. An MLP employs nonlinear activation func-\\ntions. Every node in a layer connects to all nodes in the next\\nlayer, creating a fully connected network (Fig. 1). MLPs are\\nthe simplest type of Feed-Forward Neural Networks (FNNs).\\nFNNs represent a general category of neural networks in which\\nthe connections between the nodes do not create any cycle, i.e.,\\nin a FNN there is no cycle of information ﬂow.\\nConvolutional Neural Networks: Convolutional neural\\nnetworks (CNNs), whose architecture is inspired by the human\\nvisual cortex, are a subclass of feed-forward neural networks.\\nCNNs are named after the underlying mathematical operation,\\nconvolution, which yields a measure of the interoperability of\\nits input functions. Convolutional neural networks are usually\\nemployed in situations where data is or needs to be represented\\nwith a 2D or 3D data map. In the data map representation,\\nthe proximity of data points usually corresponds to their\\nFig. 1. The general architecture of a MLP.\\ninformation correlation.\\nIn convolutional neural networks where the input is an\\nimage, the data map indicates that image pixels are highly cor-\\nrelated to their neighboring pixels. Consequently, the convolu-\\ntional layers have 3 dimensions: width, height, and depth. That\\nassumption possibly explains why the majority of research\\nefforts dedicated to CNNs are conducted in the Computer\\nVision ﬁeld [33].\\nA CNN takes an image represented as an array of numeric\\nvalues. After performing speciﬁc mathematical operations, it\\nrepresents the image in a new output space. This operation is\\nalso called feature extraction, and helps to capture and rep-\\nresent key image content. The extracted features can be used\\nfor further analysis, for different tasks. One example is image\\nclassiﬁcation, which aims to categorize images according to\\nsome predeﬁned classes. Other examples include determining\\nwhich objects are present in an image and where they are\\nlocated. See Fig. 2.\\nIn the case of utilizing CNNs for NLP, the inputs are sen-\\ntences or documents represented as matrices. Each row of the\\nmatrix is associated with a language element such as a word\\nor a character. The majority of CNN architectures learn word\\nor sentence representations in their training phase. A variety\\nof CNN architectures were used in various classiﬁcation tasks\\nsuch as Sentiment Analysis and Topic Categorization [31],\\n[34]–[36]. CNNs were employed for Relation Extraction and\\nRelation Classiﬁcation as well [37], [38].\\nRecurrent Neural Network: If we line up a sequence of\\nFNNs and feed the output of each FNN as an input to the next\\none, a recurrent neural network (RNN) will be constructed.\\nLike FNNs, layers in an RNN can be categorized into input,\\nhidden, and output layers. In discrete time frames, sequences\\nof input vectors are fed as the input, one vector at a time,\\ne.g., after inputting each batch of vectors, conducting some\\noperations and updating the network weights, the next input\\nbatch will be fed to the network. Thus, as shown in Fig. 3,\\nat each time step we make predictions and use parameters of\\nthe current hidden layer as input to the next time step.\\nHidden layers in recurrent neural networks can carry infor-\\nmation from the past, in other words, memory. This character-\\nistic makes them speciﬁcally useful for applications that deal'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 3\\nFig. 2. A typical CNN architecture for object detection. The network provides a feature representation with attention to the speciﬁc region of an image\\n(example shown on the left) that contains the object of interest. Out of the multiple regions represented (see an ordering of the image blocks, giving image\\npixel intensity, on the right) by the network, the one with the highest score will be selected as the main candidate.\\nFig. 3. Recurrent Neural Network (RNN), summarized on the left, expanded\\non the right, for N timesteps, with X indicating input, h hidden layer, and\\nO output\\nwith a sequence of inputs such as language modeling [39], i.e.,\\nrepresenting language in a way that the machine understands.\\nThis concept will be described later in detail.\\nRNNs can carry rich information from the past. Consider\\nthe sentence: “Michael Jackson was a singer; some people\\nconsider him King of Pop.” It’s easy for a human to identify\\nhim as referring to Michael Jackson. The pronoun him happens\\nseven words after Michael Jackson; capturing this dependency\\nis one of the beneﬁts of RNNs, where the hidden layers in an\\nRNN act as memory units. Long Short Term Memory Network\\n(LSTM) [40] is one of the most widely used classes of RNNs.\\nLSTMs try to capture even long time dependencies between\\ninputs from different time steps. Modern Machine Translation\\nand Speech Recognition often rely on LSTMs.\\nFig. 4. Schematic of an Autoencoder\\nAutoencoders: Autoencoders implement unsupervised\\nmethods in deep learning. They are widely used in dimension-\\nality reduction3 or NLP applications which consist of sequence\\n3Dimensionality reduction is an unsupervised learning approach which is\\nthe process of reducing the number of variables that were used to represent\\nthe data by identifying the most crucial information.\\nto sequence modeling (see Section III-B [39]. Fig. 4 illustrates\\nthe schematic of an Autoencoder. Since autoencoders are\\nunsupervised, there is no label corresponding to each input.\\nThey aim to learn a code representation for each input. The\\nencoder is like a feed-forward neural network in which the\\ninput gets encoded into a vector (code). The decoder operates\\nsimilarly to the encoder, but in reverse, i.e., constructing\\nan output based on the encoded input. In data compression\\napplications, we want the created output to be as close as\\npossible to the original input. Autoencoders are lossy, meaning\\nthe output is an approximate reconstruction of the input.\\nFig. 5. Generative Adversarial Networks\\nGenerative Adversarial Networks: Goodfellow [41] intro-\\nduced Generative Adversarial Networks (GANs) . As shown in\\nFig. 5, a GAN is a combination of two neural networks, a\\ndiscriminator and a generator. The whole network is trained\\nin an iterative process. First, the generator network generates a\\nfake sample. Then the discriminator network tries to determine\\nwhether this sample (ex.: an input image) is real or fake, i.e.,\\nwhether it came from the real training data (data used for\\nbuilding the model) or not. The goal of the generator is to fool\\nthe discriminator in a way that the discriminator believes the\\nartiﬁcial (i.e., generated) samples synthesized by the generator\\nare real.\\nThis iterative process continues until the generator produces\\nsamples that are indistinguishable by the discriminator. In'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 4\\nother words, the probability of classifying a sample as fake\\nor real becomes like ﬂipping a fair coin for the discriminator.\\nThe goal of the generative model is to capture the distribution\\nof real data while the discriminator tries to identify the fake\\ndata. One of the interesting features of GANs (regarding being\\ngenerative) is: once the training phase is ﬁnished, there is no\\nneed for the discrimination network, so we solely can work\\nwith the generation network. In other words, having access to\\nthe trained generative model is sufﬁcient.\\nDifferent forms of GANs has been introduced, e.g., Sim\\nGAN [8], Wasserstein GAN [42], info GAN [43], and DC\\nGAN [44]. In one of the most elegant GAN implementations\\n[45], entirely artiﬁcial, yet almost perfect, celebrity faces are\\ngenerated; the pictures are not real, but fake photos produced\\nby the network. GAN’s have since received signiﬁcant atten-\\ntion in various applications and have generated astonishing\\nresult [46]. In the NLP domain, GANs often are used for text\\ngeneration [47], [48].\\nB. Motivation for Deep Learning in NLP\\nDeep learning applications are predicated on the choices\\nof (1) feature representation and (2) deep learning algo-\\nrithm alongside architecture. These are associated with data\\nrepresentation and learning structure, respectively. For data\\nrepresentation, surprisingly, there usually is a disjunction\\nbetween what information is thought to be important for\\nthe task at hand, versus what representation actually yields\\ngood results. For instance, in sentiment analysis, lexicon\\nsemantics, syntactic structure, and context are assumed by\\nsome linguists to be of primary signiﬁcance. Nevertheless,\\nprevious studies based on the bag-of-words (BoW) model\\ndemonstrated acceptable performance [49]. The bag-of-words\\nmodel [50], often viewed as the vector space model, involves\\na representation which accounts only for the words and\\ntheir frequency of occurrence. BoW ignores the order and\\ninteraction of words, and treats each word as a unique feature.\\nBoW disregards syntactic structure, yet provides decent results\\nfor what some would consider syntax-dependent applications.\\nThis observation suggests that simple representations, when\\ncoupled with large amounts of data, may work as well or better\\nthan more complex representations. These ﬁndings corroborate\\nthe argument in favor of the importance of deep learning\\nalgorithms and architectures.\\nOften the progress of NLP is bound to effective language\\nmodeling. A goal of statistical language modeling is the prob-\\nabilistic representation of word sequences in language, which\\nis a complicated task due to the curse of dimensionality. The\\nresearch presented in [51] was a breakthrough for language\\nmodeling with neural networks aimed at overcoming the curse\\nof dimensionality by (1) learning a distributed representation\\nof words and (2) providing a probability function for se-\\nquences.\\nA key challenge in NLP research, compared to other do-\\nmains such as Computer Vision, seems to be the complexity\\nof achieving an in-depth representation of language using\\nstatistical models. A primary task in NLP applications is to\\nprovide a representation of texts, such as documents. This in-\\nvolves feature learning, i.e., extracting meaningful information\\nto enable further processing and analysis of the raw data.\\nTraditional methods begin with time-consuming hand-\\ncrafting of features, through careful human analysis of a\\nspeciﬁc application, and are followed by development of\\nalgorithms to extract and utilize instances of those features.\\nOn the other hand, deep supervised feature learning methods\\nare highly data-driven and can be used in more general efforts\\naimed at providing a robust data representation.\\nDue to the vast amounts of unlabeled data, unsupervised\\nfeature learning is considered to be a crucial task in NLP. Un-\\nsupervised feature learning is, in essence, learning the features\\nfrom unlabeled data to provide a low-dimensional representa-\\ntion of a high-dimensional data space. Several approaches such\\nas K-means clustering and principal component analysis have\\nbeen proposed and successfully implemented to this end. With\\nthe advent of deep learning and abundance of unlabeled\\ndata, unsupervised feature learning becomes a crucial task for\\nrepresentation learning, a precursor in NLP applications. Cur-\\nrently, most of the NLP tasks rely on annotated data, while a\\npreponderance of unannotated data further motivates research\\nin leveraging deep data-driven unsupervised methods.\\nGiven the potential superiority of deep learning approaches\\nin NLP applications, it seems crucial to perform a com-\\nprehensive analysis of various deep learning methods and\\narchitectures with particular attention to NLP applications.\\nIII. C ORE CONCEPTS IN NLP\\nA. Feature Representation\\nDistributed representations are a series of compact, low\\ndimensional representations of data, each representing some\\ndistinct informative property. For NLP systems, due to issues\\nrelated to the atomic representation of the symbols, it is\\nimperative to learn word representations.\\nAt ﬁrst, let’s concentrate on how the features are rep-\\nresented, and then we focus on different approaches for\\nlearning word representations. The encoded input features can\\nbe characters, words [32], sentences [52], or other linguistic\\nelements. Generally, it is more desirable to provide a compact\\nrepresentation of the words than a sparse one.\\nFig. 6. Considering a given sequence, the skip-thought model generates the\\nsurrounding sequences using the trained encoder. The assumption is that the\\nsurrounding sentences are closely related, contextually.\\nHow to select the structure and level of text representa-\\ntion used to be an unresolved question. After proposing the\\nword2vec approach [53], subsequently, doc2vec was proposed\\nin [52] as an unsupervised algorithm and was called Paragraph'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 5\\nVector (PV). The goal behind PV is to learn ﬁxed-length rep-\\nresentations from variable-length text parts such as sentences\\nand documents. One of the main objectives of doc2vec is\\nto overcome the drawbacks of models such as BoW and to\\nprovide promising results for applications such as text classi-\\nﬁcation and sentiment analysis. A more recent approach is the\\nskip-thought model which applies word2vec at the sentence-\\nlevel [54]. By utilizing an encoder-decoder architecture, this\\nmodel generates the surrounding sentences using the given\\nsentence (Fig. 6). Next, let’s investigate different kinds of\\nfeature representation.\\n1) One-Hot Representation: In one-hot encoding, each\\nunique element that needs to be represented has its dimen-\\nsion which results in a very high dimensional, very sparse\\nrepresentation. Assume the words are represented with the\\none-hot encoding method. Regarding representation structure,\\nthere is no meaningful connection between different words in\\nthe feature space. For example, highly correlated words such\\nas ‘ocean’ and ‘water’ will not be closer to each other (in the\\nrepresentation space) compared to less correlated pairs such as\\n‘ocean’ and ‘ﬁre.’ Nevertheless, some research efforts present\\npromising results using one-hot encoding [2].\\n2) Continuous Bag of Words: Continuous Bag-of-Words\\nmodel (CBOW) has frequently been used in NLP applica-\\ntions. CBOW tries to predict a word given its surrounding\\ncontext, which usually consists of a few nearby words [55].\\nCBOW is neither dependent on the sequential order of words\\nnor necessarily on probabilistic characteristics. So it is not\\ngenerally used for language modeling. This model is typi-\\ncally trained to be utilized as a pre-trained model for more\\nsophisticated tasks. An alternative to CBOW is the weighted\\nCBOW (WCBOW) [56] in which different vectors get different\\nweights reﬂective of relative importance in context. The sim-\\nplest example can be document categorization where features\\nare words and weights are TF-IDF scores [57] of the associated\\nwords.\\n3) Word-Level Embedding: Word embedding is a learned\\nrepresentation for context elements in which, ideally, words\\nwith related semantics become highly correlated in the rep-\\nresentation space. One of the main incentives behind word\\nembedding representations is the high generalization power\\nas opposed to sparse, higher dimensional representations [58].\\nUnlike the traditional bag-of-words model in which different\\nwords have entirely different representations regardless of their\\nusage or collocations, learning a distributed representation\\ntakes advantage of word usage in context to provide similar\\nrepresentations for semantically correlated words. There are\\ndifferent approaches to create word embeddings. Several re-\\nsearch efforts, including [53], [55], used random initialization\\nby uniformly sampling random numbers with the objective of\\ntraining an efﬁcient representation of the model on a large\\ndataset. This setup is intuitively acceptable for initialization\\nof the embedding for common features such as part-of-speech\\ntags. However, this may not be the optimum method for rep-\\nresentation of less frequent features such as individual words.\\nFor the latter, pre-trained models, trained in a supervised or\\nunsupervised manner, are usually leveraged for increasing the\\nperformance.\\n4) Character-Level Embedding: The methods mentioned\\nearlier are mostly at higher levels of representation. Lower-\\nlevel representations such as character-level representation\\nrequire special attention as well, due to their simplicity of\\nrepresentation and the potential for correction of unusual\\ncharacter combinations such as misspellings [2]. For generat-\\ning character-level embeddings, CNNs have successfully been\\nutilized [14].\\nCharacter-level embeddings have been used in different\\nNLP applications [59]. One of the main advantages is the\\nability to use small model sizes and represent words with\\nlower-level language elements [14]. Here word embeddings\\nare models utilizing CNNs over the characters. Another mo-\\ntivation for employing character-level embeddings is the out-\\nof-vocabulary word (OOV) issue which is usually encountered\\nwhen, for the given word, there is no equivalent vector in\\nthe word embedding. The character-level approach may sig-\\nniﬁcantly alleviate this problem. Nevertheless, this approach\\nsuffers from a weak correlation between characters and se-\\nmantic and syntactic parts of the language. So, considering\\nthe aforementioned pros and cons of utilizing character-level\\nembeddings, several research efforts tried to propose and im-\\nplement higher-level approaches such as using sub-words [60]\\nto create word embeddings for OOV instances as well as\\ncreating a semantic bridge between the correlated words [61].\\nB. Seq2Seq Framework\\nMost underlying frameworks in NLP applications rely on\\nsequence-to-sequence (seq2seq) models in which not only the\\ninput but also the output is represented as a sequence. These\\nmodels are common in various applications including machine\\ntranslation4, text summarization 5, speech-to-text, and text-to-\\nspeech applications6.\\nThe most common seq2seq framework is comprised of an\\nencoder and a decoder. The encoder ingests the sequence of\\ninput data and generates a mid-level output which is subse-\\nquently consumed by the decoder to produce the series of ﬁnal\\noutputs. The encoder and decoder are usually implemented via\\na series of Recurrent Neural Networks or LSTM [40] cells.\\nThe encoder takes a sequence of length T, X =\\n{x1,x2,··· ,xT}, where xt ∈ V = {1,··· ,|V|} is the\\nrepresentation of a single input coming from the vocabulary\\nV, and then generates the output state ht. Subsequently, the\\ndecoder takes the last state from the encoder, i.e., ht, and\\nstarts generating an output of size L, Y′= {y′\\n1,y′\\n2,··· ,y′\\nL},\\nbased on its current state, st, and the ground-truth output yt.\\nIn different applications, the decoder could take advantage\\nof more information such as a context vector [62] or intra-\\nattention vectors [63] to generate better outputs.\\nOne of the most widely training approaches for seq2seq\\nmodels is called Teacher Forcing [64]. Let us deﬁne y =\\n4The input is a sequence of words from one language (e.g., English) and\\nthe output is the translation to another language (e.g., French).\\n5The input is a complete document (sequence of words) and the output is\\na summary of it (sequence of words).\\n6The input is an audio recording of a speech (sequence of audible elements)\\nand the output is the speech text (sequence of words).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 6\\n{y1,y2,··· ,yL}as the ground-truth output sequence corre-\\nspondent to a given input sequence X. The model training\\nbased on the maximum-likelihood criterion employs the fol-\\nlowing cross-entropy (CE) loss minimization:\\nLCE = −\\nL∑\\nt=1\\nlog pθ(yt|yt−1,st,X) (1)\\nwhere θ is the parameters of the model optimized during the\\ntraining.\\nOnce the model is optimized using the cross-entropy loss,\\nit can generate an entire sequence as follows. Let ˆyt denote\\nthe output generated by the model at time t. Then, the next\\noutput is generated by:\\nˆyt = arg max\\ny\\npθ(y|ˆyt−1,st) (2)\\nIn NLP applications, one can improve the output by using\\nbeam search to ﬁnd a reasonably good output sequence [3].\\nDuring beam search, rather than using argmax for selecting\\nthe best output, we choose the top K outputs at each step,\\ngenerate K different paths for the output sequence, and ﬁnally\\nchoose the one that provides better performance as the ﬁnal\\noutput. Although, there has been some recent studies [65],\\n[66] on improving the beam search by incorporating a similar\\nmechanism during training of them model, studying this is\\noutside the scope of this paper.\\nGiven a series of the ground-truth output Y and the gener-\\nated model output ˆY, the model performance is evaluated us-\\ning a task-speciﬁc measures such as ROUGE [67], BLEU [68],\\nand METEOR [69]. As an example, ROUGE L, which is an\\nevaluation metric in NLP tasks, uses the largest common sub-\\nstring between ground-truth Y and model output ˆY to evaluate\\nthe generated output.\\nC. Reinforcement Learning in NLP\\nAlthough the seq2seq models explained in Section III-B\\nachieve great successes w.r.t. traditional methods, there are\\nsome issues with how these models are trained. Generally\\nspeaking, seq2seq models like the ones used in NLP applica-\\ntions face two issues: (1) exposure bias and (2) inconsistency\\nbetween training time and test time measurements [70].\\nMost of the popular seq2seq models are minimizing cross-\\nentropy loss as their optimization objective via Teacher Forc-\\ning (Section III-B). In teacher forcing, during the training of\\nthe model, the decoder utilizes two inputs, the former decoder\\noutput state st−1 and the ground-truth input yt, to determine its\\ncurrent output state st. Moreover, it employs them to create\\nthe next token, i.e., ˆyt. However, at test time, the decoder\\nfully relies on the previously created token from the model\\ndistribution. As the ground-truth data is not available, such\\na step is necessary to predict the next action. Henceforth, in\\ntraining, the decoder input is coming from the ground truth,\\nwhile, in the test phase, it relies on the previous prediction.\\nThis exposure bias [71] induces error growth through output\\ncreation at the test phase. One approach to remedy this\\nproblem is to remove the ground-truth dependency in training\\nby solely relying on model distribution to minimize the cross-\\nentropy loss. Scheduled sampling [64] is one popular method\\nto handle this setback. During scheduled sampling, we ﬁrst\\npre-train the model using cross-entropy loss and then slowly\\nreplace the ground-truth with samples the model generates.\\nThe second obstacle with seq2seq models is that, when\\ntraining is ﬁnished using the cross-entropy loss, it is typically\\nevaluated using non-differentiable measures such as ROUGE\\nor METEOR. This will form an inconsistency between the\\ntraining objective and the test evaluation metric. Recently, it\\nhas been demonstrated that both of these problems can be tack-\\nled by utilizing techniques from reinforcement learning [70].\\nAmong most of the well-known models in reinforcement\\nlearning, policy gradient techniques [72] such as the REIN-\\nFORCE algorithm [73] and actor-critic based models such as\\nvalue-based iteration [74], and Q-learning [75], are among the\\nmost common techniques used in deep learning in NLP.\\nUsing the model predictions (versus the ground-truth) for\\nthe sequence to sequence modeling and generation, at training\\ntime, was initially introduced by Daume et al. [76]. According\\nto their approach, SEARN, the structured prediction can be\\ncharacterized as one of the reinforcement learning cases as\\nfollows: The model employs its predictions to produce a\\nsequence of actions (words sequences). Then, at each time\\nstep, a greedy search algorithm is employed to learn the\\noptimal action, and the policy will be trained to predict that\\nparticular action.\\nFig. 7. A simple Actor-Critic framework.\\nIn Actor-Critic training, the actor is usually the same neural\\nnetwork used to generate the output, while the critic is a\\nregression model that estimates how the actor performed on\\nthe input data. The actor later receives the feedback from the\\ncritic and improves its actions. Fig 7 shows this framework.\\nIt is worth noting that action in most of the NLP-related\\napplications is like selecting the next output token while the\\nstate is the decoder output state at each stage of decoding.\\nThese models have mostly been used for robotic [77] and\\nAtari games [78] due to the small action space in these\\napplications. However, when we use them in NLP applications,\\nthey face multiple challenges. The action space in most of the\\nNLP applications could be deﬁned as the number of tokens\\nin the vocabulary (usually between 50K to 150K tokens).\\nComparing this to the action space in a simple Atari game,\\nwhich on average has less than 20 actions [78], shows why\\nthese Actor-Critic models face difﬁculties when applied to\\nNLP applications. A major challenge is the massive action\\nspace in NLP applications, which not only causes difﬁculty'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 7\\nfor the right action selection, but also will make the training\\nprocess very slow. This makes the process of ﬁnding the best\\nActor-Critic model very complicated and model convergence\\nusually requires a lot of tweaks to the models.\\nIV. D ATASETS\\nMany different researchers for different tasks use bench-\\nmark datasets, such as those discussed below. Benchmarking\\nin machine learning refers to the assessment of methods\\nand algorithms, comparing those regarding their capability to\\nlearn speciﬁc patterns. Benchmarking aids validation of a new\\napproach or practice, relative to other existing methods.\\nBenchmark datasets typically take one of three forms.\\n1) The ﬁrst is real-world data, obtained from various real-\\nworld experiments.\\n2) The second is synthetic data, artiﬁcially generated to\\nmimic real-world patterns. Synthetic data is generated\\nfor use instead of real data. Such datasets are of spe-\\ncial interest in applications where the amount of data\\nrequired is much larger than that which is available, or\\nwhere privacy considerations are crucial and strict, such\\nas in the healthcare domain.\\n3) The third type are toy datasets, used for demonstration\\nand visualization purposes. Typically they are artiﬁcially\\ngenerated; often there is no need to represent real-world\\ndata patterns.\\nThe foundation of Deep Learning utilization is the avail-\\nability of data to teach the system about pattern identiﬁcation.\\nThe effectiveness of the model depends on the quality of\\nthe data. Despite the successful implementation of universal\\nlanguage modeling techniques such as BERT [79], however,\\nsuch models can be used solely for pre-training the models.\\nAfterward, the model needs to be trained on the data associated\\nwith the desired task. Henceforth, based on the everyday\\ndemands in different machine domains such as NLP, creating\\nnew datasets is crucial.\\nOn the other hand, creating new datasets is not usually an\\neasy matter. Informally speaking, the newly created dataset\\nshould be: the right data to train on, sufﬁcient for the eval-\\nuation, and accurate to work on. Answering the questions of\\n“what is the meaning of right and accurate data” is highly\\napplication-based. Basically, the data should have sufﬁcient\\ninformation, which depends on the quality and quantity of the\\ndata.\\nTo create a dataset, the ﬁrst step is always asking “what are\\nwe trying to do and what problem do we need to solve?”\\nand “what kind of data do we need and how much of it\\nis required?” The next step is to create training and testing\\nportions. The training data set is used to train a model to\\nknow how to ﬁnd the connections between the inputs and\\nthe associated outputs. The test data set is used to assess the\\nintelligence of the machine, i.e., how well the trained model\\ncan operate on the unseen test samples. Next, we must conduct\\ndata preparation to make sure the data and its format is simple\\nand understandable for human experts. After that, the issue\\nof data accessibility and ownership may arise. Distribution of\\ndata may need to have speciﬁc authorizations, especially if we\\nare dealing with sensitive or private data.\\nGiven the aforementioned roadmap, creating proper datasets\\nis complicated and of great importance. That’s why few\\ndatasets are frequently chosen by the researchers and develop-\\ners for benchmarking. A summary of widely used benchmark\\ndatasets is provided in Table I.\\nV. D EEP LEARNING FOR NLP TASKS\\nThis section describes NLP applications using deep learn-\\ning. Fig. 8 shows representative NLP tasks (and the categories\\nthey belong to). A fundamental question is: ”How can we\\nevaluate an NLP algorithm, model, or system?” In [80],\\nsome of the most common evaluation metrics have been\\ndescribed. This reference explains the fundamental principles\\nof evaluating NLP systems.\\nA. Basic Tasks\\n1) Part-Of-Speech Tagging: Part-of-Speech tagging is one\\nof the basic tasks in Natural Language Processing. It is the\\nprocess of labeling words with their part of speech categories.\\nPart of speech is leveraged for many crucial tasks such\\nas named entity recognition. One commonly used dataset\\nfor Part-of-Speech tagging is the WSJ corpus 7. This dataset\\ncontains over a million tokens and has been utilized widely as\\na benchmark dataset for the performance assessment of POS\\ntagging systems. Traditional methods are still performing very\\nwell for this task [16]. However, neural network based methods\\nhave been proposed for Part-of-Speech tagging [81].\\nFor example, the deep neural network architecture named\\nCharWNN has been developed to join word-level and\\ncharacter-level representations using convolutional neural net-\\nworks for POS tagging [14]. The emphasis in [14] is the\\nimportance of character-level feature extraction as their exper-\\nimental results show the necessity of employing hand-crafted\\nfeatures in the absence of character-level features for achieving\\nthe state-of-the-art. In [82], a wide variety of neural network\\nbased models have been proposed for sequence tagging tasks,\\ne.g., LSTM networks, bidirectional LSTM networks, LSTM\\nnetworks with a CRF 8 layer, etc. Sequence tagging itself\\nincludes part of speech tagging, chunking, and named entity\\nrecognition. Likewise, a globally normalized transition-based\\nneural network architecture has been proposed for POS-\\ntagging [83]. State-of-the-art results are summarized in Table\\nII. In [17], authors propose a bidirectional LSTM to perform\\nparts of speech tagging and show that it performs better than\\nconventional machine learning techniques on the same dataset.\\nMore recently, in [84], authors use a pretrained BERT model\\nin combination with one bidirectional LSTM layer and train\\nthe latter layer only and outperform the prior state-of-the art\\nPOS architectures.\\n2) Parsing: Parsing is assigning a structure to a recognized\\nstring. There are different types of parsing. Constituency\\nParsing refers in particular to assigning a syntactic structure\\nto a sentence. A greedy parser has been introduced in [92]\\nwhich performs a syntactic and semantic summary of content\\n7Penn Treebank Wall Street Journal (WSJ-PTB).\\n8Conditional Random Field.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 8\\nTABLE I\\nBENCHMARK DATASETS .\\nTask Dataset Link\\nMachine Translation WMT 2014 EN-DE\\nWMT 2014 EN-FR http://www-lium.univ-lemans.fr/ ∼schwenk/cslm joint paper/\\nText Summarization\\nCNN/DM\\nNewsroom\\nDUC\\nGigaword\\nhttps://cs.nyu.edu/ ∼kcho/DMQA/\\nhttps://summari.es/\\nhttps://www-nlpir.nist.gov/projects/duc/data.html\\nhttps://catalog.ldc.upenn.edu/LDC2012T21\\nReading Comprehension\\nQuestion Answering\\nQuestion Generation\\nARC\\nCliCR\\nCNN/DM\\nNewsQA\\nRACE\\nSQuAD\\nStory Cloze Test\\nNarativeQA\\nQuasar\\nSearchQA\\nhttp://data.allenai.org/arc/\\nhttp://aclweb.org/anthology/N18-1140\\nhttps://cs.nyu.edu/ ∼kcho/DMQA/\\nhttps://datasets.maluuba.com/NewsQA\\nhttp://www.qizhexie.com/data/RACE leaderboard\\nhttps://rajpurkar.github.io/SQuAD-explorer/\\nhttp://aclweb.org/anthology/W17-0906.pdf\\nhttps://github.com/deepmind/narrativeqa\\nhttps://github.com/bdhingra/quasar\\nhttps://github.com/nyu-dl/SearchQA\\nSemantic Parsing\\nAMR parsing\\nATIS (SQL Parsing)\\nWikiSQL (SQL Parsing)\\nhttps://amr.isi.edu/index.html\\nhttps://github.com/jkkummerfeld/text2sql-data/tree/master/data\\nhttps://github.com/salesforce/WikiSQL\\nSentiment Analysis\\nIMDB Reviews\\nSST\\nYelp Reviews\\nSubjectivity Dataset\\nhttp://ai.stanford.edu/ ∼amaas/data/sentiment/\\nhttps://nlp.stanford.edu/sentiment/index.html\\nhttps://www.yelp.com/dataset/challenge\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/\\nText Classiﬁcation\\nAG News\\nDBpedia\\nTREC\\n20 NewsGroup\\nhttp://www.di.unipi.it/ ∼gulli/AG corpus of news articles.html\\nhttps://wiki.dbpedia.org/Datasets\\nhttps://trec.nist.gov/data.html\\nhttp://qwone.com/ ∼jason/20Newsgroups/\\nNatural Language Inference\\nSNLI Corpus\\nMultiNLI\\nSciTail\\nhttps://nlp.stanford.edu/projects/snli/\\nhttps://www.nyu.edu/projects/bowman/multinli/\\nhttp://data.allenai.org/scitail/\\nSemantic Role Labeling Proposition Bank\\nOneNotes\\nhttp://propbank.github.io/\\nhttps://catalog.ldc.upenn.edu/LDC2013T19\\nTABLE II\\nPOS TAGGING STATE -OF-THE -ART MODELS EVALUATED ON THE\\nWSJ-PTB DATASET.\\nModel Accuracy\\nCharacter-aware neural language models [85] 97.53\\nTransfer Learning + GRU [86] 97.55\\nBi-directional LSTM + CNNs + CRF [87] 97.55\\nAdversarial Training + Bi-LSTM [88] 97.59\\nCharacter Composition + Bi-LSTM [89] 97.78\\nString Embedding + LSTM [90] 97.85\\nMeta-BiLSTM [91] 97.96\\nusing vector representations. To enhance the results achieved\\nby [92], the approach proposed in [93] focuses on learning\\nmorphological embeddings. Recently, deep neural network\\nmodels outperformed traditional algorithms. State-of-the-art\\nresults are summarized in Table III.\\nAnother type of parsing is called Dependency Parsing. De-\\npendency structure shows the structural relationships between\\nthe words in a targeted sentence. In dependency parsing,\\nphrasal elements and phrase-structure rules do not contribute\\nto the process. Rather, the syntactic structure of the sentence\\nis expressed only in terms of the words in the sentence and\\nthe associated relations between the words.\\nTABLE III\\nCONSTITUENCY PARSING STATE -OF-THE -ART MODELS EVALUATED ON\\nTHE WSJ-PTB DATASET.\\nModel Accuracy\\nRecurrent neural network grammars (RNNG) [94] 93.6\\nIn-order traversal over syntactic trees + LSTM [95] 94.2\\nModel Combination and Reranking [96] 94.6\\nSelf-Attentive Encoder [97] 95.1\\nNeural networks have shown their superiority regarding\\ngeneralizability and reducing the feature computation cost. In\\n[98], a novel neural network-based approach was proposed for\\na transition-based dependency parser. Neural network based\\nmodels that operate on task-speciﬁc transition systems have\\nalso been utilized for dependency parsing [83]. A regularized\\nparser with bi-afﬁne classiﬁers has been proposed for the pre-\\ndiction of arcs and labels [99]. Bidirectional-LSTMs have been\\nused in dependency parsers for feature representation [100].\\nA new control structure has been introduced for sequence-to-\\nsequence neural networks based on the stack LSTM and has\\nbeen used in transition-based parsing [101]. [102] presents a\\ntransition based multilingual dependency parser which uses\\na bidirectional LSTM to adapt to target languages. In [103],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 9\\nFig. 8. NLP tasks investigated in this study.\\nthe authors provide a comparison on the state of the art deep\\nlearning based parsing methods on a clinical text parsing task.\\nMore recently, in [104], a second-order TreeCRF extension\\nwas added to the biafﬁne [105] parser to demonstrate that\\nstructural learning can further improve parsing performance\\nover the state-of-the-art bi-afﬁne models.\\n3) Semantic Role Labeling: Semantic Role Labeling (SRL)\\nis the process of identiﬁcation and classiﬁcation of text argu-\\nments. It is aimed at the characterization of elements to deter-\\nmine “who” did “what” to “whom” as well as “how,” “where,”\\nand “when.” It identiﬁes the predicate-argument structure of a\\nsentence. The predicate, in essence, refers to “what,” while the\\narguments consist of the associated participants and properties\\nin the text. The goal of SRL is to extract the semantic relations\\nbetween the predicate and the related arguments.\\nMost of the previously-reported research efforts are based\\non explicit representations of semantic roles. Recently, deep\\nlearning approaches have achieved the SRL state-of-the-art\\nwithout taking the explicit syntax representation into consider-\\nation [106]. On the other hand, it is argued that the utilization\\nof syntactic information can be leveraged to improve the per-\\nformance of syntactic-agnostic9 models [107]. A linguistically-\\ninformed self-attention (LISA) model has been proposed to\\nleverage both multi-task learning and self-attention for effec-\\n9Note that being syntactic-agnostic does not imply discarding syntactic\\ninformation. It means they are not explicitly employed.\\ntive utilization of the syntactic information for SRL [108].\\nCurrent state-of-the-art methods employ joint prediction of\\npredicates and arguments [109], novel word representation ap-\\nproaches [110], and self-attention models [111]; see Table IV.\\nResearchers in [25] focus on syntax and contextualized word\\nrepresentation to present a unique multilingual SRL model\\nbased on a biafﬁne scorer, argument pruning and bidirectional\\nLSTMs, (see also [112]).\\nTABLE IV\\nSEMANTIC ROLE LABELING CURRENT STATE -OF-THE -ART MODELS\\nEVALUATED ON THE ONTO NOTES DATASET [113]. T HE ACCURACY\\nMETRIC IS F1 SCORE .\\nModel Accuracy ( F1)\\nSelf-Attention + RNN [111] 83.9\\nContextualized Word Representations [110] 84.6\\nArgumented Representations + BiLSTM [109] 85.3\\nB. Text Classiﬁcation\\nThe primary objective of text classiﬁcation is to assign\\npredeﬁned categories to text parts (which could be a word,\\nsentence, or whole document) for preliminary classiﬁcation\\npurposes and further organization and analysis. A simple ex-\\nample is the categorization of given documents as to political\\nor non-political news articles.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 10\\nThe use of CNNs for sentence classiﬁcation, in which train-\\ning the model on top of pretrained word-vectors through ﬁne-\\ntuning, has resulted in considerable improvements in learning\\ntask-speciﬁc vectors [31]. Later, a Dynamic Convolutional\\nNeural Network (DCNN) architecture – essentially a CNN\\nwith a dynamic k-max pooling method – was applied to\\ncapture the semantic modeling of sentences [114]. In addi-\\ntion to CNNs, RNNs have been used for text classiﬁcation.\\nAn LSTM-RNN architecture has been utilized in [115] for\\nsentence embedding with particular superiority in a deﬁned\\nweb search task. A Hierarchical Attention Network (HAN) has\\nbeen utilized to capture the hierarchical structure of text, with\\na word-level and sentence-level attention mechanism [116].\\nSome models used the combination of both RNNs and\\nCNNs for text classiﬁcation such as [117]. This is a recurrent\\narchitecture in addition to max-pooling with an effective word\\nrepresentation method, and demonstrates superiority compared\\nto simple window-based neural network approaches. Another\\nuniﬁed architecture is the C-LSTM proposed in [118] for\\nsentence and document modeling in classiﬁcation. Current\\nstate-of-the-art methods are summarized in Table V. A more\\nrecent review of the deep learning based methods for text clas-\\nsiﬁcation is provided in [119]. The latter focuses on different\\narchitectures used for this task, including most recent works\\nin CNN based models, as well as RNN based models, and\\ngraph neural networks. In [120], authors provide a comparison\\nbetween various deep learning methods for text classiﬁcation,\\nconcluding that GRUs and LSTMs can actually perform better\\nthan CNN-based models.\\nTABLE V\\nTHE CLASSIFICATION ACCURACY OF STATE -OF-THE -ART METHODS ,\\nEVALUATED ON THE AG NEWS CORPUS DATASET [2].\\nModel Accuracy\\nCNN [121] 91.33\\nDeep Pyramid CNN [122] 93.13\\nCNN [123] 93.43\\nUniversal Language Model Fine-tuning (ULMFiT) [124] 94.99\\nC. Information Extraction\\nInformation extraction identiﬁes structured information\\nfrom “unstructured” data such as social media posts and\\nonline news. Deep learning has been utilized for information\\nextraction regarding subtasks such as Named Entity Recogni-\\ntion, Relation Extraction , Coreference Resolution, and Event\\nExtraction.\\n1) Named Entity Recognition: Named Entity Recogni-\\ntion (NER) aims to locate and categorize named entities in\\ncontext into pre-deﬁned categories such as the names of people\\nand places. The application of deep neural networks in NER\\nhas been investigated by the employment of CNN [125] and\\nRNN architectures [126], as well as hybrid bidirectional LSTM\\nand CNN architectures [19]. NeuroNER [127], a named-entity\\nrecognition tool, operates based on artiﬁcial neural networks.\\nState-of-the-art models are reported in Table VI. [21] provides\\nan extensive discussion on recent deep learning methods for\\nnamed entity recognition. The latter concludes that the work\\npresented in [128] outperforms other recent models (with an\\nF-score of 93.5 on the CoNLL03 dataset).\\nTABLE VI\\nSTATE OF THE ART MODELS REGARDING NAME ENTITY RECOGNITION .\\nEVALUATION IS PERFORMED ON THE CONLL-2003 S HARED TASK\\nDATASET [129]. T HE EVALUATION METRIC IS F1 SCORE .\\nModel Accuracy\\nSemi-supervised Sequence Modeling [130] 92.61\\nGoogle BERT [131] 92.8\\nContextual String Embeddings [90] 93.09\\n2) Relation Extraction: Relation Extraction aims to ﬁnd\\nthe semantic relationships between entity pairs. The recursive\\nneural network (RNN) model has been proposed for semantic\\nrelationship classiﬁcation by learning compositional vector\\nrepresentations [132]. For relation classiﬁcation, CNN archi-\\ntectures have been employed as well, by extracting lexical\\nand sentence level features [37]. More recently, in [133],\\nbidirectional tree-structured LSTMs were shown to perform\\nwell for relation extraction. [134] provides a more recent\\nreview on relation extraction.\\n3) Coreference Resolution: Coreference resolution includes\\nidentiﬁcation of the mentions in a context that refer to the\\nsame entity. For instance, the mentions “car,” “Camry,” and\\n“it” could all refer to the same entity. For the ﬁrst time in [135],\\nReinforcement Learning (RL) was applied to coreference\\nresolution. Current widely used methods leverage an attention\\nmechanism [136]. More recently, in [137], authors adopt a\\nreinforcement learning policy gradient approach to coreference\\nresolution and provide state-of-the art performance on the\\nEnglish OntoNotes v5.0 benchmark task. [138] reformulates\\ncoreference resolution as a span prediction task as in question\\nanswering and provide superior performance on the CoNLL-\\n2012 benchmark task.\\n4) Event Extraction: A speciﬁc type of extracted infor-\\nmation from text is an event. Such extraction may involve\\nrecognizing trigger words related to an event and assign-\\ning labels to entity mentions that represent event triggers.\\nConvolutional neural networks have been utilized for event\\ndetection; they handle problems with feature-based approaches\\nincluding exhaustive feature engineering and error propagation\\nphenomena for feature generation [139]. In 2018, Nguyen\\nand Grishman applied graph-CNN (GCCN) where the con-\\nvolutional operations are applied to syntactically dependent\\nwords as well as consecutive words [140]; their adding entity\\ninformation reﬂected the state-of-the-art using CNN models.\\n[141] uses a novel inverse reinforcement learning approach\\nbased on generative adversarial networks (imitation learning)\\nto tackle joint entity and event extraction. More recently, in\\n[142], authors proposed a model for document-level event\\nextraction using a combined dependency-based GCN (for\\nlocal context) and a hypergraph (as an aggregator for global\\ncontext).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 11\\nD. Sentiment analysis\\nThe primary goal in sentiment analysis is the extraction\\nof subjective information from text by contextual mining.\\nSentiment analysis is considered high-level reasoning based on\\nsource data. Sentiment analysis is sometimes called opinion\\nmining, as its primary goal is to analyze human opinion,\\nsentiments, and even emotions regarding products, problems,\\nand varied subjects. Seminal works on sentiment analysis or\\nopinion mining include [143], [144]. Since 2000, much atten-\\ntion has been given to sentiment analysis, due to its relation\\nto a wide variety of applications [145], its associations with\\nnew research challenges, and the availability of abundant data.\\n[146] provides a more recent review of the sentiment analysis\\nmethods relying on deep learning and gives an insightful\\ndiscussion on the drawbacks as well as merits of deep learning\\nmethods for sentiment analysis.\\nA critical aspect of research in sentiment analysis is content\\ngranularity. Considering this criterion, sentiment analysis is\\ngenerally divided into three categories/levels: document level,\\nsentence level, and aspect level.\\n1) Document-level Sentiment Analysis: At the document\\nlevel, the task is to determine whether the whole document\\nreﬂects a positive or negative sentiment about exactly one\\nentity. This differs from opinion mining regarding multiple\\nentries. The Gated Recurrent Neural Network architecture\\nhas been utilized successfully for effectively encoding the\\nsentences’ relations in the semantic structure of the docu-\\nment [147]. Domain adaptation has been investigated as well,\\nto deploy the trained model on unseen new sources [148].\\nMore recently, in [149] authors provide an LSTM-based model\\nfor document-level sentiment analysis that captures semantic\\nrelations between sentences. In [150], authors use a CNN-\\nbidirectional LSTM model to process long texts.\\n2) Sentence-level Sentiment Analysis: At the sentence-\\nlevel, sentiment analysis determines the positivity, negativity,\\nor neutrality regarding an opinion expressed in a sentence. One\\ngeneral assumption for sentence-level sentiment classiﬁcation\\nis the existence of only one opinion from a single opinion\\nholder in an expressed sentence. Recursive autoencoders have\\nbeen employed for sentence-level sentiment label prediction\\nby learning the vector space representations for phrases [151].\\nLong Short-Term Memory (LSTM) recurrent models have\\nalso been utilized for tweet sentiment prediction [152]. The\\nSentiment Treebank and Recursive Neural Tensor Networks\\n[153] have shown promise for predicting ﬁne-grained sen-\\ntiment labels. [154] provides a cloud-based hybrid machine\\nlearning model for sentence level sentiment analysis. More\\nrecently in [155], propose A Lexicalized Domain Ontology\\nand a Regularized Neural Attention model (ALDONAr) for\\nsentence-level aspect-based sentiment analysis that uses a\\nCNN classiﬁcation module with BERT word embeddings and\\nachieves state-of-the art results.\\n3) Aspect-level Sentiment Analysis: Document-level and\\nsentence-level sentiment analysis usually focus on the senti-\\nment itself, not the target of the sentiment, e.g., a product.\\nAspect-level sentiment analysis directly targets an opinion,\\nwith the assumption of the existence of the sentiment and its\\ntarget. A document or sentence may not have a generally posi-\\ntive or negative sentiment, but may have multiple subparts with\\ndifferent targets, each with a positive or negative sentiment.\\nThis can make aspect-level analysis even more challenging\\nthan other types of sentiment categorization.\\nAspect-level sentiment analysis usually involves Aspect\\nSentiment Classiﬁcation and Aspect Extraction . The former\\ndetermines opinions on different aspects (positive, neutral,\\nor negative) while the latter identiﬁes the target aspect for\\nevaluation in context. As an example consider the following\\nsentence: “This car is old. It must be repaired and sold!” .\\n“This car” is what is subject to evaluation and must be\\nextracted ﬁrst. Here, the opinion about this aspect is negative.\\nFor aspect-level sentiment classiﬁcation, attention-based\\nLSTMs are proposed to connect the aspect and sentence\\ncontent for sentiment classiﬁcation [156]. For aspect extrac-\\ntion, deep learning has successfully been proposed in opinion\\nmining [157]. State-of-the-art methods rely on converting\\naspect-based sentiment analysis to sentence-pair classiﬁcation\\ntasks [79], post-training approaches [158] on the popular\\nlanguage model BERT [131], and employment of pre-trained\\nembeddings [159]. [160] provides a recent comparative review\\non aspect-based sentiment analysis. Also recently, [161] pro-\\nposed a dual-attention model which tries to extract the implicit\\nrelation between the aspect and opinion terms. In [162] authors\\npropose a novel Aspect-Guided Deep Transition model for\\naspect-based sentiment analysis.\\nE. Machine Translation\\nMachine Translation (MT) is one of the areas of NLP\\nthat has been profoundly affected by the advances in deep\\nlearning. The ﬁrst subsection below explains methods used in\\nthe pre-deep learning period, as explained in reference NLP\\ntextbooks such as “Speech and Language Processing” [163].\\nThe remainder of this section is dedicated to delving into\\nrecent innovations in MT which are based on neural networks,\\nstarted by [164]. [165], [166] provide reviews on various deep\\nlearning architectures used for MT.\\n1) Traditional Machine Translation: One of the ﬁrst\\ndemonstrations of machine translation happened in 1954 [167]\\nin which the authors tried to translate from Russian to English.\\nThis translation system was based on six simple rules, but\\nhad a very limited vocabulary. It was not until the 1990s that\\nsuccessful statistical implementations of machine translation\\nemerged as more bilingual corpora became available [163].\\nIn [68] the BLEU score was introduced as a new evaluation\\nmetric, allowing more rapid improvement than when the only\\napproach involved using human labor for evaluation.\\n2) Neural Machine Translation: It was after the success\\nof the neural network in image classiﬁcation tasks that re-\\nsearchers started to use neural networks in machine translation\\n(NMT). Around 2013, research groups started to achieve\\nbreakthrough results in NMT. Unlike traditional statistical\\nmachine translation, NMT is based on an end-to-end neural\\nnetwork [168]. This implies that there is no need for extensive\\npreprocessing and word alignments. Instead, the focus shifted\\ntoward network structure.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 12\\nFig. 11 shows an example of an end-to-end recurrent neural\\nnetwork for machine translation. A sequence of input tokens\\nis fed into the network. Once it reaches an end-of-sentence\\n(EOS) token, it starts generating the output sequence. The\\noutput sequence is generated in the same recurrent manner as\\nthe input sequence until it reaches an end-of-sentence token.\\nOne major advantage of this approach is that there is no need\\nto specify the length of the sequence; the network takes it\\ninto account automatically. In other words, the end-of-sentence\\ntoken determines the length of the sequence. Networks implic-\\nitly learn that longer input sentences usually lead to longer\\noutput sentences with varying length, and that ordering can\\nchange. For instance, the second example in Fig. 9 shows that\\nadjectives generally come before nouns in English but after\\nnouns in Spanish. There is no need to explicitly specify this\\nsince the network can capture such properties. Moreover, the\\namount of memory that is used by NMT is just a fraction\\nof the memory that is used in traditional statistical machine\\ntranslation [169].\\nFig. 9. Alignment in Machine Translation\\n[164] was one of the early works that incorporated recurrent\\nneural networks for machine translation. They were able to\\nachieve a perplexity (a measure where lower values indicate\\nbetter models) that was 43% less than the state-of-the-art\\nalignment based translation models. Their recurrent continuous\\ntranslation model (RCTM) is able to capture word ordering,\\nsyntax, and meaning of the source sentence explicitly. It maps\\na source sentence into a probability distribution over sentences\\nin the target language. RCTM estimates the probability P(f|e)\\nof translating a sentence e = e1 + ... + ek in the source\\nlanguage to target language sentence f = f1 +...+fm. RCTM\\nestimates P(f|e) by considering source sentence e as well as\\nthe preceding words in the target language f1:i−1:\\nP(f|e) =\\nm∏\\ni=1\\nP(fi|f1:i−1,e) (3)\\nThe representation generated by RCTM acts on n-grams in\\nthe lower layers, and acts more on the whole sentence as one\\nmoves to the upper layers. This hierarchical representation is\\nperformed by applying different layers of convolution. First a\\ncontinuous representation of each word is generated; i.e., if\\nthe sentence is e= e1...ek, the representation of the word ei\\nwill be v(ei) ∈Rq×1. This will result in sentence matrix Ee ∈\\nRq×k in which Ee\\n:,i = v(ei). This matrix representation of the\\nsentence will be fed into a series of convolution layers in order\\nto generate the ﬁnal representation e for the recurrent neural\\nnetwork. The approach is illustrated in Fig. 10. Equations for\\nthe pipeline are as follows.\\ns= S.csm(e) (4)\\nh1 = σ(I.v(f1) +s) (5)\\nhi+1 = σ(R.hi + I.v(fi+1) +s) (6)\\noi+1 = O.hi (7)\\nIn order to take into account the sentence length, the authors\\nintroduced RCTM II which estimates the length of the target\\nsentence. RCTM II was able to achieve better perplexity on\\nWMT datasets (see top portion of Table I) than other existing\\nmachine translation systems.\\nFig. 10. Recurrent Continuous Translation Models (RCTM) [164].\\nIn another line of work, [170] presented an end-to-end\\nsequence learning approach without heavy assumptions on\\nthe structure of the sequence. Their approach consists of two\\nLSTMs, one for mapping the input to a vector of ﬁxed di-\\nmension and another LSTM for decoding the output sequence\\nfrom the vector. Their model was able to handle long sentences\\nas well as sentence representations that are sensitive to word\\norder. As shown in Fig. 11, the model reads ”ABC” as an\\ninput sequence and produces ”WXYZ” as output sequence.\\nThe < EOS >token indicates the end of prediction. The\\nnetwork was trained by maximizing the log probability of the\\ntranslation ( η) given the input sequence ( ζ). In other words,\\nthe objective function is:\\n1/|D|\\n∑\\n(η,ζ)∈D\\nlogP(η|ζ) (8)\\nD is the training set and |D| is its size. One of the\\nnovelties of their approach was reversing word order of the\\nsource sentence. This helps the LSTM to learn long term\\ndependencies.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 13\\nFig. 11. Sequence to sequence learning with LSTM.\\nHaving a ﬁxed-length vector in the decoder phase is one\\nof the bottlenecks of the encoder-decoder approach. [168]\\nargues that a network will have a hard time compressing\\nall the information from the input sentence into a ﬁxed-size\\nvector. They address this by allowing the network to search\\nsegments of the source sentence that are useful for predicting\\nthe translation. Instead of representing the input sentence as a\\nﬁxed-size vector, in [168] the input sentence is encoded to a\\nsequence of vectors and a subset of them is chosen by using\\na method called attention mechanism as shown in Fig. 12.\\nIn their approach P(yi|y1,...,y i−1,X) =g(yi−1,si,ci), in\\nwhich si = f(si−1,yi−1,ci). While previously cwas the same\\nfor all time steps, here ctakes a different value, ci, at each time\\nstep. This accounts for the attention mechasim (context vector)\\naround that speciﬁc time step. ci is computed according to the\\nfollowing:\\nci = ∑Tx\\nj=1 αijhj, αij = exp(eij)∑Tx\\nk=1 exp(eik) , eij = a(si−1,hj).\\nHere ais the alignment model that is represented by a feed\\nforward neural network. Also hj = [\\n→\\nhT\\nj ,\\n←\\nhT\\nj ], which is a way to\\ninclude information both about preceding and following words\\nin hj. The model was able to outperform the simple encoder-\\ndecoder approach regardless of input sentence length.\\nImproved machine translation models continue to emerge,\\ndriven in part by the growth in people’s interest and need\\nto understand other languages Most of them are variants of\\nthe end-to-end decoder-encoder approach. For example, [171]\\ntries to deal with the problem of rare words. Their LSTM\\nnetwork consists of encoder and decoder layers using residual\\nlayers along with the attention mechanism. Their system\\nwas able to decrease training time, speed up inference, and\\nhandle translation of rare words. Comparisons between some\\nof the state-of-the-art neural machine translation models are\\nsummarized in Table VII.\\nTABLE VII\\nTHE MACHINE TRANSLATION STATE -OF-THE -ART MODELS EVALUATED\\nON THE English-German dataset of ACL 2014 Ninth Workshop on Statistical\\nMachine TRranslation. THE EVALUATION METRIC IS BLEU SCORE .\\nModel Accuracy\\nConvolutional Seq-to-Seq [172] 25.2\\nAttention Is All You Need [173] 28.4\\nWeighted Transformer [174] 28.9\\nSelf Attention [175] 29.2\\nDeepL Translation Machine 10 33.3\\nBack-translation [176] 35.0\\nMore recently, [177] provides an interesting single-model\\nimplementation of massively multilingual NMT. In [178],\\nauthors use BERT to extract contextual embeddings and com-\\nFig. 12. Attention Mechasim for Neural Machine Translation [168].\\nbine BERT with an attention-based NMT model and provide\\nstate-of-the-art results on various benchmark datasets. [179]\\nproposes mBART which is a seq-to-seq denoising autoen-\\ncoder and reports that using a pretrained, locked (i.e. no\\nmodiﬁcations) mBART improves performance in terms of\\nthe BLEU point. [180] proposes an interesting adversarial\\nframework for robustifying NMT against noisy inputs and\\nreports performance gains over the Transformer model. [181]\\nis also an insightful recent work where the authors sample\\ncontext words from the predicted sequence as well as the\\nground truth to try to reconcile the training and inference\\nprocesses. Finally, [182] is a successful recent effort to prevent\\nthe forgetting that often accompanies in translating pre-trained\\nlanguage models to other NMT task. [182] achieves that aim\\nprimarily by using a dynamically gated model and asymptotic\\ndistillation.\\nF . Question Answering\\nQuestion answering (QA) is a ﬁne-grained version of Infor-\\nmation Retrieval (IR). In IR a desired set of information has to\\nbe retrieved from a set of documents. The desired information\\ncould be a speciﬁc document, text, image, etc. On the other\\nhand, in QA speciﬁc answers are sought, typically ones that\\ncan be inferred from available documents. Other areas of NLP\\nsuch as reading comprehension and dialogue systems intersect\\nwith question answering.\\nResearch in computerized question answering has pro-\\nceeded since the 1960s. In this section, we present a general\\noverview of question answering system history, and focus on\\nthe breakthroughs in the ﬁeld. Like all other ﬁelds in NLP,\\nquestion answering was also impacted by the advancement of\\ndeep learning [183], so we provide an overview of QA in deep\\nlearning contexts. We brieﬂy visit visual question answering\\nas well.\\n1) Rule-based Question Answering: Baseball [184] is one\\nof the early works (1961) on QA where an effort was made to\\nanswer questions related to baseball games by using a game\\ndatabase. The baseball system consists of (1) question read-in,\\n(2) dictionary lookup for words in the question, (3) syntactic\\n(POS) analysis of the words in question, (4) content analysis'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 14\\nfor extracting the input question, and (5) estimating relevance\\nregarding answering the input question.\\nIBM’s [185] statistical question answering system consisted\\nof four major components:\\n1) Question/Answer Type Classiﬁcation\\n2) Query Expansion/Information Retrieval\\n3) Name Entity Making\\n4) Answer Selection\\nSome QA systems fail when semantically equivalent re-\\nlationships are phrased differently. [186] addressed this by\\nproposing fuzzy relation matching based on mutual informa-\\ntion and expectation maximization.\\n2) Question answering in the era of deep learning: Smart-\\nphones (Siri, Ok Google, Alexa, etc.) and virtual personal\\nassistants are common examples of QA systems with which\\nmany interact on a daily basis. While earlier such systems\\nemployed rule-based methods, today their core algorithm is\\nbased on deep learning. Table VIII presents some questions\\nand answers provided by Siri on an iPhone.\\nTABLE VIII\\nTYPICAL QUESTION ANSWERING PERFORMANCE BASED ON DEEP\\nLEARNING .\\nQuestion Answer\\nWho invented polio vaccine? The answer I found is Jonas Salk\\nWho wrote Harry Potter? J.K.Rowling wrote Harry Potter in\\n1997\\nWhen was Einstein born? Albert Einstein was born March\\n14, 1879\\n[188] was one of the ﬁrst machine learning based papers\\nthat reported results on QA for a reading comprehension\\ntest. The system tries to pick a sentence in the database that\\nhas an answer to a question, and a feature vector represents\\neach question-sentence pair. The main contribution of [188]\\nis proposing a feature vector representation framework which\\nis aimed to provide information for learning the model. There\\nare ﬁve classiﬁers (location, date, etc.), one for each type of\\nquestion. They were able to achieve accuracy competitive with\\nprevious approaches.\\nAs illustrated in Fig. 13, [187] uses convolutional neural\\nnetworks in order to encode Question-Answer sentence pairs\\nin the form of ﬁxed length vectors regardless of the length\\nof the input sentence. Instead of using distance measures like\\ncosine correlation, they incorporate a non-linear tensor layer to\\nmatch the relevance between question and answer. Equation 9\\ncalculates the matching degree between question q and its\\ncorresponding answer a.\\ns(q,a) =uTf(vT\\nqM[1:r]va + V\\n[vq\\nva\\n]\\n+ b) (9)\\nf is the standard element-wise non-linearity function,\\nM[1:r]∈Rns×ns×r\\nis a tensor, V ∈Rr×2ns , b ∈Rr, u ∈Rr.\\nThe model tries to capture the interaction between question\\nand answer. Inspired by ﬁndings in neuroscience, [81] incorpo-\\nrated episodic memory 11 in their Dynamic Memory Network\\n11A kind of long-term memory that includes conscious recall of previous\\nactivities together with their meaning.\\n(DMN). By processing input sequences and questions, DMN\\nforms episodic memories to answer relevant questions. As\\nillustrated in Fig. 14, their system is trained based on raw\\nInput-Question-Answer triplets.\\nDMN consists of four modules that communicate with each\\nother as shown in Fig. 15. The input module encodes raw\\ninput text into a distributed vector representation; likewise\\nthe question module encodes a question into its distributed\\nvector representation. The episodic memory module uses the\\nattention mechanism in order to focus on a speciﬁc part of\\nthe input module. Through an iterative process, this module\\nproduces a memory vector representation that considers the\\nquestion as well as previous memory. The answer module\\nuses the ﬁnal memory vector to generate an answer. The model\\nimproved upon state-of-the-art results on tasks such as the ones\\nshown in Fig. 14. DMN is one of the architectures that could\\npotentially be used for a variety of NLP applications such as\\nclassiﬁcation, question answering, and sequence modeling.\\n[189] introduced a Dynamic Coattention Network (DCN)\\nin order to address local maxima corresponding to incorrect\\nanswers; it is considered to be one of the best approaches to\\nquestion answering.\\n3) Visual Question Answering: Given an input image, Vi-\\nsual Question Answering (VQA) tries to answer a natural\\nlanguage question about the image [190]. VQN addresses mul-\\ntiple problems such as object detection, image segmentation,\\nsentiment analysis, etc. [190] introduced the task of VQA\\nby providing a dataset containing over 250K images, 760K\\nquestions, and around 10M answers. [191] proposed a neural-\\nbased approach to answer the questions regarding the input\\nimages. As illustrated in Fig. 16, Neural-Image-QA is a deep\\nnetwork consisting of CNN and LSTM. Since the questions\\ncan have multiple answers, the problem is decomposed into\\npredicting a set of answer words aq,x = {a1,a2,...,a N(q,x)}\\nfrom a ﬁnite vocabulary set ν where N(q,x) represents the\\ncount of answer words regarding a given question.\\nDo humans and computers look at the same regions to\\nanswer questions about an image? [193] tries to answer this\\nquestion by conducting large-scale studies on human attention\\nin VQA. Their ﬁndings show that VQAs do not seem to\\nbe looking at the same regions as humans. Finally, [192]\\nincorporates a spatial memory network for VQA. Fig. 17\\nshows the inference process of their model. As illustrated in\\nthe ﬁgure, the speciﬁc attention mechanism in their system can\\nhighlight areas of interest in the input image. [194] introduces\\nBLOCK, a bilinear fusion model based on superdiagonal\\ntensor decomposition for the VQA task, with state-of-the-\\nart performance and the code made public on github. To\\nimprove the generalization of existing models to test data of\\ndifferent distribution, [195] introduces a self-critical training\\nobjective to help ﬁnd visual regions of prominent visual/textual\\ncorrelation with a focus on recognizing inﬂuential objects and\\ndetecting and devaluing incorrect dominant answers.\\nG. Document Summarization\\nDocument summarization refers to a set of problems involv-\\ning generation of summary sentences given one or multiple\\ndocuments as input.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 15\\nFig. 13. Fixed length vector sentence representation for input Questions and Answers [187].\\nFig. 14. Example of Dynamic Memory Network (DMN) input-question-\\nanswer triplet\\nFig. 15. Interaction between four modules of Dynamic Memory Network [78].\\nGenerally, text summarization ﬁts into two categories:\\n1) Extractive Summarization, where the goal is to iden-\\ntify the most salient sentences in the document and\\nreturn them as the summary.\\n2) Abstractive Summarization, where the goal is to gen-\\nerate summary sentences from scratch; they may contain\\nnovel words that do not appear in the original document.\\nEach of these methods has its own advantages and disad-\\nvantages. Extractive summarization is prone to generate long\\nand sometimes overlapping summary sentences; however, the\\nresult reﬂects the author’s mode of expression. Abstractive\\nFig. 16. Neural Image Question Answering [191].\\nFig. 17. Spatial Memory Network for VQA. Bright Areas are regions the\\nmodel is attending [192].\\nmethods generate a shorter summary but they are hard to train.\\nThere is a vast amount of research on the topic of text\\nsummarization using extractive and abstractive methods. As\\none of the earliest works on using neural networks for ex-\\ntractive summarization, [196] proposed a framework that\\nused a ranking technique to extract the most salient sentences\\nin the input. This model was improved by [197] which\\nused a document-level encoder to represent sentences, and\\na classiﬁer to rank these sentences. On the other hand, in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 16\\nabstractive summarization, it was [198] which, for the ﬁrst\\ntime, used attention over a sequence-to-sequence (seq2seq)\\nmodel for the problem of headline generation. However, since\\nsimple attention models perform worse than extractive models,\\ntherefore more effective attention models such as graph-based\\nattention [199] and transformers [173] have been proposed for\\nthis task. To further improve abstractive text summarization\\nmodels, [200] proposed the ﬁrst pointer-generator model and\\napplied it to the DeepMind QA dataset [201]. As a result\\nof this work, the CNN/Daily Mail dataset emerged which is\\nnow one of the widely used datasets for the summarization\\ntask. A copy mechanism was also adopted by [202] for\\nsimilar tasks. But their analysis reveals a key problem with\\nattention-based encoder-decoder models: they often generate\\nunusual summaries consisting of repeated phrases. Recently,\\n[62] reached state-of-the-art results on the abstractive text\\nsummarization using a similar framework. They alleviated the\\nunnatural summaries by avoiding generating unknown tokens\\nand replacing these words with tokens from the input article.\\nLater, researchers moved their focus to methods that use\\nsentence-embedding to ﬁrst select the most salient sentence\\nin the document and then change them to make them more\\nabstractive [203], [204]. In these models, salient sentences\\nare extracted ﬁrst and then a paraphrasing model is used to\\nmake them abstractive. The extraction employs a sentence\\nclassiﬁer or ranker while the abstractor tries to remove the\\nextra information in a sentence and present it as a shorter\\nsummary. Fast-RL [203] is the ﬁrst framework in this family of\\nworks. In Fast-RL, the extractor is pre-trained to select salient\\nsentences and the abstractor is pre-trained using a pointer-\\ngenerator model to generate paraphrases. Finally, to merge\\nthese two non-differentiable components, they propose using\\nActor-Critic Q-learning methods in which the actor receives\\na single document and generates the output while the critic\\nevaluates the output based on comparison with the ground-\\ntruth summary.\\nThough the standard way to evaluate the performance of\\nsummarization models is with ROUGE [67] and BLEU [68],\\nthere are major problems with such measures. For instance, the\\nROUGE measure focuses on the number of shared n-grams\\nbetween two sentences. Such a method incorrectly assigns\\na low score to an abstractive summary that uses different\\nwords yet provides an excellent paraphrase that humans would\\nrate highly. Clearly, better automated evaluation methods are\\nneeded in such cases.\\nThere are additional problems with current summarization\\nmodels. Shi et al. [205] provides a comprehensive survey on\\ntext summarization.\\n[206] provides a recent survey on summarization methods.\\n[207] provides an advanced composite deep learning model,\\nbased on LSTMs and Restricted Boltzmann Machine, for\\nmulti-doc opinion summarization. A very inﬂuential recent\\nwork, [208], introduces H IBERT ( HIerachical Bidirectional\\nEncoder Representations from Transformers) as a pre-trained\\ninitialization for document summarization and report state-of-\\nthe-art performance.\\nH. Dialogue Systems\\nDialogue Systems are quickly becoming a principal in-\\nstrument in human-computer interaction, due in part to their\\npromising potential and commercial value [209]. One appli-\\ncation is automated customer service, supporting both online\\nand bricks-and-mortar businesses. Customers expect an ever-\\nincreasing level of speed, accuracy, and respect while dealing\\nwith companies and their services. Due to the high cost of\\nknowledgeable human resources, companies frequently turn\\nto intelligent conversational machines. Note that the phrases\\nconversational machines and dialogue machines are often used\\ninterchangeably.\\nDialogue systems are usually task-based or non-task-\\nbased (Fig. 18). Though there might be Automatic Speech\\nRecognition (ASR) and Language-to-Speech (L2S) compo-\\nnents in a dialogue system, the discussion of this section is\\nsolely about the linguistic components of dialogue systems;\\nconcepts associated with speech technology are ignored.\\nDespite useful statistical models employed in the backend\\nof dialogue systems (especially in language understanding\\nmodules), most deployed dialogue systems rely on expensive\\nhand-crafted and manual features for operation. Furthermore,\\nthe generalizability of these manually engineered systems to\\nother domains and functionalities is problematic. Hence, recent\\nattention has focused on deep learning for the enhancement of\\nperformance, generalizability, and robustness. Deep learning\\nfacilitates the creation of end-to-end task-oriented dialogue\\nsystems, which enriches the framework to generalize conver-\\nsations beyond annotated task-speciﬁc dialogue resources.\\n1) Task-based Systems: The structure of a task-based dia-\\nlogue system usually consists of the following elements:\\n• Natural Language Understanding (NLU) : This compo-\\nnent deals with understanding and interpreting user’s\\nspoken context by assigning a constituent structure to the\\nspoken utterance (e.g., a sentence) and captures its syn-\\ntactic representation and semantic interpretation, to allow\\nthe back-end operation/task. NLU is usually leveraged\\nregardless of the dialogue context.\\n• Dialogue Manager (DM) : The generated representation\\nby NLU would be handled by the dialogue manager,\\nwhich investigates the context and returns a reasonable\\nsemantic-related response.\\n• Natural Language Generation (NLG) : The natural lan-\\nguage generation (NLG) component produces an utter-\\nance based on the response provided by the DM compo-\\nnent.\\nThe general pipeline is as follows: NLU module (i.e.,\\nsemantic decoder) transforms the output of the speech recog-\\nnition module to some dialogue elements. Then the DM\\nprocesses these dialogue elements and provides a suitable\\nresponse which is fed to the NLG for response generation.\\nThe main pipeline in NLU is to classify the user query domain\\nand user intent, and ﬁll a set of slots to create a semantic\\nframe. It is usually customary to perform the intent prediction\\nand the slot ﬁlling simultaneously [210]. Most of the task-\\noriented dialogue systems employ slot-ﬁlling approaches to\\nclassify user intent in the speciﬁc domain of the conversation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 17\\nFig. 18. The framework of a dialogue system. A dialogue system can be task oriented or used for natural language generation based on the user input which\\nis also known as a chat bot.\\nFor this aim, having predeﬁned tasks is required; this depends\\non manually crafted states with different associated slots.\\nHenceforth, a designed dialogue system would be of limited\\nor no use for other tasks.\\nRecent task-oriented dialogue systems have been designed\\nbased on deep reinforcement learning, which provided promis-\\ning results regarding performance [211], domain adapta-\\ntion [212], and dialogue generation [213]. This was due to\\na shift towards end-to-end trainable frameworks to design\\nand deploy task-oriented dialogue systems. Instead of the\\ntraditionally utilized pipeline, an end-to-end framework in-\\ncorporates and uses a single module that deals with external\\ndatabases. Despite the tractability of end-to-end dialogue\\nsystems (i.e., easy to train and simple to engineer), due to\\ntheir need for interoperability with external databases via\\nqueries, they are not well-suited for task-oriented settings.\\nSome approaches to this challenge include converting the user\\ninput into internal representations [214], combining supervised\\nand reinforced learning [215], and extending the memory\\nnetwork approach [216] for question-answering to a dialog\\nsystem [217].\\n2) Non-task-based Systems: As opposed to task-based dia-\\nlogue systems, the goal behind designing and deploying non-\\ntask-based dialogue systems is to empower a machine with\\nthe ability to have a natural conversation with humans [218].\\nTypically, chatbots are of one of the following types: retrieval-\\nbased methods and generative methods. Retrieval-based mod-\\nels have access to information resources and can provide more\\nconcise, ﬂuent, and accurate responses. However, they are\\nlimited regarding the variety of responses they can provide\\ndue to their dependency on backend data resources. Generative\\nmodels, on the other hand, have the advantage of being able\\nto produce suitable responses when such responses are not in\\nthe corpus. However, as opposed to retrieval-based models,\\nthey are more prone to grammatical and conceptual mistakes\\narising from their generative models.\\nRetrieval-based methods select an appropriate response\\nfrom the candidate responses. Therefore, the key element is the\\nquery-response operation. In general, this problem has been\\nformulated as a search problem and uses IR techniques for task\\ncompletion [219]. Retrieval-based methods usually employ\\neither Single-turn Response Matching or Multi-turn Response\\nMatching. In the ﬁrst type, the current query (message) is\\nsolely used to select a suitable response [220]. The latter\\ntype takes the current message and previous utterances as the\\nsystem input and retrieves a response based on the instant and\\ntemporal information. The model tries to choose a response\\nwhich considers the whole context to guarantee conversation\\nconsistency. An LSTM-based model has been proposed [221]\\nfor context and response vectors creation. In [222], various\\nfeatures and multiple data inputs have been incorporated to\\nbe ingested using a deep learning framework. Current base\\nmodels regarding retrieval-based chatbots rely on multi-turn\\nresponse selection augmented by an attention mechanism and\\nsequence matching [223].\\nGenerative models don’t assume the availability of pre-\\ndeﬁned responses. New responses are produced from scratch\\nand are based on the trained model. Generative models are\\ntypically based on sequence to sequence models and map an\\ninput query to a target element as the response. In general,\\ndesigning and implementing a dialogue agent to be able to\\nconverse at the human level is very challenging. The typical\\napproach usually consists of learning and imitating human\\nconversation. For this goal, the machine is generally trained on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 18\\nlarge corpora of conversations. However, this does not directly\\nremedy the issue of encountering out-of-corpus conversation .\\nThe question is: How can an agent be taught to generate\\nproper responses to conversations that it never has seen? It\\nmust handle content that is not exactly available in the data\\ncorpus that the machine has been trained on, due to the lack\\nof content matching between the query and the corresponding\\nresponse, resulting from the wide range of plausible queries\\nthat humans can provide.\\nTo tackle the aforementioned general problem, some fun-\\ndamental questions must be answered: (1) What are the core\\ncharacteristics of a natural conversation? (2) How can these\\ncharacteristics be measured? (3) How can we incorporate this\\nknowledge in a machine, i.e., the dialogue system? Effective\\nintegration of these three elements determines the intelligence\\nof a machine. A qualitative criterion is to observe if the\\ngenerated utterances can be distinguished from natural human\\ndialogues. For quantitative evaluation, adversarial evaluation\\nwas initially used for quality assessment of sentence gener-\\nation [224] and employed for quality evaluation of dialogue\\nsystems [225]. Recent advancements in sequence to sequence\\nmodeling encouraged many research efforts regarding natural\\nlanguage generation [226]. Furthermore, deep reinforcement\\nlearning yields promising performance in natural language\\ngeneration [213].\\n3) Final note on dialogue systems: Despite remarkable\\nadvancements in AI and much attention dedicated to dia-\\nlogue systems, in reality, successful commercial tools, such\\nas Apple’s Siri and Amazon’s Alexa, still heavily rely on\\nhandcrafted features. It still is very challenging to design and\\ntrain data-driven dialogue machines given the complexity of\\nthe natural language, the difﬁculties in framework design, and\\nthe complex nature of available data sources.\\nVI. C ONCLUSION\\nIn this article, we presented a comprehensive survey of\\nthe most distinguished works in Natural Language Processing\\nusing deep learning. We provided a categorized context for\\nintroducing different NLP core concepts, aspects, and applica-\\ntions, and emphasized the most signiﬁcant conducted research\\nefforts in each associated category. Deep learning and NLP are\\ntwo of the most rapidly developing research topics nowadays.\\nDue to this rapid progress, it is hoped that soon, new effective\\nmodels will supersede the current state-of-the-art approaches.\\nThis may cause some of the references provided in the survey\\nto become dated, but those are likely to be cited by new\\npublications that describe improved methods\\nNeverthless, one of the essential characteristics of this\\nsurvey is its educational aspect, which provides a precise\\nunderstanding of the critical elements of this ﬁeld and explains\\nthe most notable research works. Hopefully, this survey will\\nguide students and researchers with essential resources, both\\nto learn what is necessary to know, and to advance further the\\nintegration of NLP with deep learning.\\nREFERENCES\\n[1] C. D. Manning, C. D. Manning, and H. Sch ¨utze, Foundations of\\nstatistical natural language processing . MIT Press, 1999.\\n[2] X. Zhang, J. Zhao, and Y . LeCun, “Character-level convolutional\\nnetworks for text classiﬁcation,” in Advances in neural information\\nprocessing systems, pp. 649–657, 2015.\\n[3] K. Cho, B. Van Merri ¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares,\\nH. Schwenk, and Y . Bengio, “Learning phrase representations us-\\ning RNN encoder-decoder for statistical machine translation,” arXiv\\npreprint arXiv:1406.1078, 2014.\\n[4] S. Wu, K. Roberts, S. Datta, J. Du, Z. Ji, Y . Si, S. Soni, Q. Wang,\\nQ. Wei, Y . Xiang, B. Zhao, and H. Xu, “Deep learning in clinical\\nnatural language processing: a methodical review,” Journal of the\\nAmerican Medical Informatics Association , vol. 27, pp. 457–470, mar\\n2020.\\n[5] R. Collobert and J. Weston, “A uniﬁed architecture for natural lan-\\nguage processing: Deep neural networks with multitask learning,” in\\nProceedings of the 25th international conference on Machine learning ,\\npp. 160–167, ACM, 2008.\\n[6] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and\\nL. Fei-Fei, “Large-scale video classiﬁcation with convolutional neural\\nnetworks,” in Proceedings of the IEEE conference on Computer Vision\\nand Pattern Recognition, pp. 1725–1732, 2014.\\n[7] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, “Learning and transferring\\nmid-level image representations using convolutional neural networks,”\\nin Proceedings of the IEEE conference on Computer Vision and Pattern\\nRecognition, pp. 1717–1724, 2014.\\n[8] A. Shrivastava, T. Pﬁster, O. Tuzel, J. Susskind, W. Wang, and R. Webb,\\n“Learning from simulated and unsupervised images through adversarial\\ntraining,” in Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pp. 2107–2116, 2017.\\n[9] A. V oulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis,\\n“Deep Learning for Computer Vision: A Brief Review,”Computational\\nIntelligence and Neuroscience , Feb 2018.\\n[10] N. O’Mahony, S. Campbell, A. Carvalho, S. Harapanahalli, G. V .\\nHernandez, L. Krpalkova, D. Riordan, and J. Walsh, “Deep learning vs.\\ntraditional computer vision,” in Advances in Computer Vision (K. Arai\\nand S. Kapoor, eds.), (Cham), pp. 128–144, Springer International\\nPublishing, 2020.\\n[11] A. Graves and N. Jaitly, “Towards end-to-end speech recognition with\\nrecurrent neural networks,” in International Conference on Machine\\nLearning, pp. 1764–1772, 2014.\\n[12] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg,\\nC. Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen, et al. , “Deep\\nspeech 2: End-to-end speech recognition in English and Mandarin,” in\\nICML, pp. 173–182, 2016.\\n[13] U. Kamath, J. Liu, and J. Whitaker, Deep learning for NLP and speech\\nrecognition, vol. 84. Springer, 2019.\\n[14] C. D. Santos and B. Zadrozny, “Learning character-level representa-\\ntions for part-of-speech tagging,” in Proceedings of the 31st Interna-\\ntional Conference on Machine Learning (ICML-14) , pp. 1818–1826,\\n2014.\\n[15] B. Plank, A. Søgaard, and Y . Goldberg, “Multilingual part-of-speech\\ntagging with bidirectional long short-term memory models and auxil-\\niary loss,” arXiv preprint arXiv:1604.05529 , 2016.\\n[16] C. D. Manning, “Part-of-speech tagging from 97% to 100%: is it\\ntime for some linguistics?,” in International Conference on Intelligent\\nText Processing and Computational Linguistics, pp. 171–189, Springer,\\n2011.\\n[17] R. D. Deshmukh and A. Kiwelekar, “Deep learning techniques for\\npart of speech tagging by natural language processing,” in 2020\\n2nd International Conference on Innovative Mechanisms for Industry\\nApplications (ICIMIA), pp. 76–81, IEEE, 2020.\\n[18] G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and\\nC. Dyer, “Neural architectures for named entity recognition,” arXiv\\npreprint arXiv:1603.01360, 2016.\\n[19] J. P. Chiu and E. Nichols, “Named entity recognition with bidirectional\\nLSTM-CNNs,” arXiv preprint arXiv:1511.08308 , 2015.\\n[20] V . Yadav and S. Bethard, “A survey on recent advances in\\nnamed entity recognition from deep learning models,” arXiv preprint\\narXiv:1910.11470, 2019.\\n[21] J. Li, A. Sun, J. Han, and C. Li, “A survey on deep learning for\\nnamed entity recognition,” IEEE Transactions on Knowledge and Data\\nEngineering, 2020.\\n[22] J. Zhou and W. Xu, “End-to-end learning of semantic role labeling\\nusing recurrent neural networks,” in Proceedings of the 53rd Annual\\nMeeting of the Association for Computational Linguistics and the\\n7th International Joint Conference on Natural Language Processing\\n(Volume 1: Long Papers), vol. 1, pp. 1127–1137, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 19\\n[23] D. Marcheggiani, A. Frolov, and I. Titov, “A simple and accurate\\nsyntax-agnostic neural model for dependency-based semantic role\\nlabeling,” arXiv preprint arXiv:1701.02593 , 2017.\\n[24] L. He, K. Lee, M. Lewis, and L. Zettlemoyer, “Deep semantic role\\nlabeling: What works and what’s next,” in Proceedings of the 55th\\nAnnual Meeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers), vol. 1, pp. 473–483, 2017.\\n[25] S. He, Z. Li, and H. Zhao, “Syntax-aware multilingual semantic role\\nlabeling,” arXiv preprint arXiv:1909.00310 , 2019.\\n[26] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent trends in\\ndeep learning based natural language processing,” IEEE Computational\\nIntelligence Magazine, vol. 13, no. 3, pp. 55–75, 2018.\\n[27] Y . Kang, Z. Cai, C.-W. Tan, Q. Huang, and H. Liu, “Natural language\\nprocessing (NLP) in management research: A literature review,” Jour-\\nnal of Management Analytics , vol. 7, pp. 139–172, apr 2020.\\n[28] T. Greenwald, “What exactly is artiﬁcial in-\\ntelligence, anyway?.” https://www.wsj.com/articles/\\nwhat-exactly-is-artiﬁcial-intelligence-anyway-1525053960, April\\n2018. Wall Street Journal Online Article.\\n[29] U. Sivarajah, M. M. Kamal, Z. Irani, and V . Weerakkody, “Critical\\nanalysis of big data challenges and analytical methods,” Journal of\\nBusiness Research, vol. 70, pp. 263–286, 2017.\\n[30] Z. C. Lipton, J. Berkowitz, and C. Elkan, “A critical review of\\nrecurrent neural networks for sequence learning,” arXiv preprint\\narXiv:1506.00019, 2015.\\n[31] Y . Kim, “Convolutional neural networks for sentence classiﬁcation,”\\narXiv preprint arXiv:1408.5882 , 2014.\\n[32] R. Socher, C. C. Lin, C. Manning, and A. Y . Ng, “Parsing natural scenes\\nand natural language with recursive neural networks,” in Proceedings\\nof the 28th international conference on machine learning (ICML-11) ,\\npp. 129–136, 2011.\\n[33] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\\nwith deep convolutional neural networks,” in Advances in neural\\ninformation processing systems , pp. 1097–1105, 2012.\\n[34] C. dos Santos and M. Gatti, “Deep convolutional neural networks for\\nsentiment analysis of short texts,” in Proceedings of COLING 2014, the\\n25th International Conference on Computational Linguistics: Technical\\nPapers, pp. 69–78, 2014.\\n[35] R. Johnson and T. Zhang, “Effective use of word order for text\\ncategorization with convolutional neural networks,” arXiv preprint\\narXiv:1412.1058, 2014.\\n[36] R. Johnson and T. Zhang, “Semi-supervised convolutional neural\\nnetworks for text categorization via region embedding,” in Advances\\nin neural information processing systems , pp. 919–927, 2015.\\n[37] D. Zeng, K. Liu, S. Lai, G. Zhou, and J. Zhao, “Relation classiﬁcation\\nvia convolutional deep neural network,” in Proceedings of COLING\\n2014, the 25th International Conference on Computational Linguistics:\\nTechnical Papers, pp. 2335–2344, 2014.\\n[38] T. H. Nguyen and R. Grishman, “Relation extraction: Perspective from\\nconvolutional neural networks,” in Proceedings of the 1st Workshop on\\nVector Space Modeling for Natural Language Processing , pp. 39–48,\\n2015.\\n[39] T. Mikolov, M. Karaﬁ ´at, L. Burget, J. ˇCernock`y, and S. Khudanpur,\\n“Recurrent neural network based language model,” in Eleventh Annual\\nConference of the International Speech Communication Association ,\\n2010.\\n[40] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\\n[41] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\\nS. Ozair, A. Courville, and Y . Bengio, “Generative adversarial nets,”\\nin Advances in neural information processing systems , pp. 2672–2680,\\n2014.\\n[42] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,” arXiv\\npreprint arXiv:1701.07875, 2017.\\n[43] X. Chen, Y . Duan, R. Houthooft, J. Schulman, I. Sutskever, and\\nP. Abbeel, “Infogan: Interpretable representation learning by informa-\\ntion maximizing generative adversarial nets,” in Advances in neural\\ninformation processing systems , pp. 2172–2180, 2016.\\n[44] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation\\nlearning with deep convolutional generative adversarial networks,”\\narXiv preprint arXiv:1511.06434 , 2015.\\n[45] T. Karras, T. Aila, S. Laine, and J. Lehtinen, “Progressive growing\\nof GANs for improved quality, stability, and variation,” arXiv preprint\\narXiv:1710.10196, 2017.\\n[46] N. Tavaf, A. Torﬁ, K. Ugurbil, and P.-F. Van de Moortele,\\n“GRAPPA-GANs for Parallel MRI Reconstruction,” arXiv preprint\\narXiv:2101.03135, Jan 2021.\\n[47] L. Yu, W. Zhang, J. Wang, and Y . Yu, “Seqgan: Sequence generative\\nadversarial nets with policy gradient,” in Thirty-First AAAI Conference\\non Artiﬁcial Intelligence , 2017.\\n[48] J. Li, W. Monroe, T. Shi, S. Jean, A. Ritter, and D. Jurafsky,\\n“Adversarial learning for neural dialogue generation,” arXiv preprint\\narXiv:1701.06547, 2017.\\n[49] B. Pang, L. Lee, and S. Vaithyanathan, “Thumbs up?: sentiment classi-\\nﬁcation using machine learning techniques,” inProceedings of the ACL-\\n02 conference on Empirical methods in natural language processing-\\nVolume 10 , pp. 79–86, Association for Computational Linguistics,\\n2002.\\n[50] Z. S. Harris, “Distributional structure,” Word, vol. 10, no. 2-3, pp. 146–\\n162, 1954.\\n[51] Y . Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural proba-\\nbilistic language model,” Journal of machine learning research, vol. 3,\\nno. Feb., pp. 1137–1155, 2003.\\n[52] Q. Le and T. Mikolov, “Distributed representations of sentences\\nand documents,” in International Conference on Machine Learning ,\\npp. 1188–1196, 2014.\\n[53] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,\\n“Distributed representations of words and phrases and their compo-\\nsitionality,” in Advances in neural information processing systems ,\\npp. 3111–3119, 2013.\\n[54] R. Kiros, Y . Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Tor-\\nralba, and S. Fidler, “Skip-thought vectors,” in Advances in neural\\ninformation processing systems , pp. 3294–3302, 2015.\\n[55] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of\\nword representations in vector space,” arXiv preprint arXiv:1301.3781,\\n2013.\\n[56] G. Lebanon et al. , Riemannian geometry and statistical machine\\nlearning. LAP LAMBERT Academic Publishing, 2015.\\n[57] J. Leskovec, A. Rajaraman, and J. D. Ullman, Mining of massive\\ndatasets. Cambridge University Press, 2014.\\n[58] Y . Goldberg, “Neural network methods for natural language process-\\ning,” Synthesis Lectures on Human Language Technologies , vol. 10,\\nno. 1, pp. 1–309, 2017.\\n[59] J. Wehrmann, W. Becker, H. E. Cagnini, and R. C. Barros, “A character-\\nbased convolutional neural network for language-agnostic Twitter sen-\\ntiment analysis,” in Neural Networks (IJCNN), 2017 International Joint\\nConference on, pp. 2384–2391, IEEE, 2017.\\n[60] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, “Enriching word\\nvectors with subword information,” arXiv preprint arXiv:1607.04606 ,\\n2016.\\n[61] J. Botha and P. Blunsom, “Compositional morphology for word rep-\\nresentations and language modelling,” in International Conference on\\nMachine Learning, pp. 1899–1907, 2014.\\n[62] A. See, P. J. Liu, and C. D. Manning, “Get to the point: Summarization\\nwith pointer-generator networks,” in ACL, vol. 1, pp. 1073–1083, 2017.\\n[63] R. Paulus, C. Xiong, and R. Socher, “A deep reinforced model for\\nabstractive summarization,” arXiv preprint arXiv:1705.04304 , 2017.\\n[64] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, “Scheduled sampling\\nfor sequence prediction with recurrent neural networks,” in Advances\\nin Neural Information Processing Systems , pp. 1171–1179, 2015.\\n[65] K. Goyal, G. Neubig, C. Dyer, and T. Berg-Kirkpatrick, “A continuous\\nrelaxation of beam search for end-to-end training of neural sequence\\nmodels,” in Thirty-Second AAAI Conference on Artiﬁcial Intelligence ,\\n2018.\\n[66] W. Kool, H. Van Hoof, and M. Welling, “Stochastic beams and where\\nto ﬁnd them: The gumbel-top-k trick for sampling sequences with-\\nout replacement,” in International Conference on Machine Learning ,\\npp. 3499–3508, 2019.\\n[67] C.-Y . Lin, “Rouge: A package for automatic evaluation of summaries,”\\nin Text summarization branches out , pp. 74–81, 2004.\\n[68] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: a method\\nfor automatic evaluation of machine translation,” in Proceedings of\\nthe 40th annual meeting on Association for Computational Linguistics ,\\npp. 311–318, Association for Computational Linguistics, 2002.\\n[69] S. Banerjee and A. Lavie, “METEOR: An automatic metric for\\nMT evaluation with improved correlation with human judgments,” in\\nProceedings of the ACL workshop on intrinsic and extrinsic evaluation\\nmeasures for machine translation and/or summarization , pp. 65–72,\\n2005.\\n[70] Y . Keneshloo, T. Shi, C. K. Reddy, and N. Ramakrishnan, “Deep rein-\\nforcement learning for sequence to sequence models,” arXiv preprint\\narXiv:1805.09461, 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 20\\n[71] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba, “Sequence\\nlevel training with recurrent neural networks,” arXiv preprint\\narXiv:1511.06732, 2015.\\n[72] W. Zaremba and I. Sutskever, “Reinforcement learning neural Turing\\nmachines-revised,” arXiv preprint arXiv:1505.00521 , 2015.\\n[73] R. J. Williams, “Simple statistical gradient-following algorithms for\\nconnectionist reinforcement learning,” in Reinforcement Learning ,\\npp. 5–32, Springer, 1992.\\n[74] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\\nMIT Press, 2018.\\n[75] C. J. Watkins and P. Dayan, “Q-learning,” Machine Learning, vol. 8,\\nno. 3-4, pp. 279–292, 1992.\\n[76] H. Daum ´e, J. Langford, and D. Marcu, “Search-based structured\\nprediction,” Machine learning, vol. 75, no. 3, pp. 297–325, 2009.\\n[77] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training of\\ndeep visuomotor policies,” The Journal of Machine Learning Research,\\nvol. 17, no. 1, pp. 1334–1373, 2016.\\n[78] V . Mnih, N. Heess, A. Graves, et al. , “Recurrent models of visual\\nattention,” in Advances in neural information processing systems ,\\npp. 2204–2212, 2014.\\n[79] C. Sun, L. Huang, and X. Qiu, “Utilizing BERT for aspect-based\\nsentiment analysis via constructing auxiliary sentence,” arXiv preprint\\narXiv:1903.09588, 2019.\\n[80] P. Resnik and J. Lin, “Evaluation of NLP systems,” The handbook\\nof computational linguistics and natural language processing , vol. 57,\\npp. 271–295, 2010.\\n[81] A. Kumar, O. Irsoy, P. Ondruska, M. Iyyer, J. Bradbury, I. Gulrajani,\\nV . Zhong, R. Paulus, and R. Socher, “Ask me anything: Dynamic\\nmemory networks for natural language processing,” in International\\nConference on Machine Learning , pp. 1378–1387, 2016.\\n[82] Z. Huang, W. Xu, and K. Yu, “Bidirectional LSTM-CRF models for\\nsequence tagging,” arXiv preprint arXiv:1508.01991 , 2015.\\n[83] D. Andor, C. Alberti, D. Weiss, A. Severyn, A. Presta, K. Ganchev,\\nS. Petrov, and M. Collins, “Globally normalized transition-based neural\\nnetworks,” arXiv preprint arXiv:1603.06042 , 2016.\\n[84] X. Xue and J. Zhang, “Part-of-speech tagging of building codes\\nempowered by deep learning and transformational rules,” Advanced\\nEngineering Informatics, vol. 47, p. 101235, 2021.\\n[85] L. Liu, J. Shang, X. Ren, F. F. Xu, H. Gui, J. Peng, and J. Han,\\n“Empower sequence labeling with task-aware neural language model,”\\nin Thirty-Second AAAI Conference on Artiﬁcial Intelligence , 2018.\\n[86] Z. Yang, R. Salakhutdinov, and W. W. Cohen, “Transfer learning for\\nsequence tagging with hierarchical recurrent networks,” arXiv preprint\\narXiv:1703.06345, 2017.\\n[87] X. Ma and E. Hovy, “End-to-end sequence labeling via bi-directional\\nLSTM-CNNs-CRF,” arXiv preprint arXiv:1603.01354 , 2016.\\n[88] M. Yasunaga, J. Kasai, and D. Radev, “Robust multilingual\\npart-of-speech tagging via adversarial training,” arXiv preprint\\narXiv:1711.04903, 2017.\\n[89] W. Ling, T. Lu ´ıs, L. Marujo, R. F. Astudillo, S. Amir, C. Dyer, A. W.\\nBlack, and I. Trancoso, “Finding function in form: Compositional\\ncharacter models for open vocabulary word representation,” arXiv\\npreprint arXiv:1508.02096, 2015.\\n[90] A. Akbik, D. Blythe, and R. V ollgraf, “Contextual string embeddings\\nfor sequence labeling,” in Proceedings of the 27th International Con-\\nference on Computational Linguistics , pp. 1638–1649, 2018.\\n[91] B. Bohnet, R. McDonald, G. Simoes, D. Andor, E. Pitler, and\\nJ. Maynez, “Morphosyntactic tagging with a Meta-BiLSTM model over\\ncontext sensitive token encodings,” arXiv preprint arXiv:1805.08237 ,\\n2018.\\n[92] J. Legrand and R. Collobert, “Joint RNN-based greedy parsing and\\nword composition,” arXiv preprint arXiv:1412.7028 , 2014.\\n[93] J. Legrand and R. Collobert, “Deep neural networks for syntactic\\nparsing of morphologically rich languages,” in Proceedings of the\\n54th Annual Meeting of the Association for Computational Linguistics\\n(Volume 2: Short Papers), pp. 573–578, 2016.\\n[94] A. Kuncoro, M. Ballesteros, L. Kong, C. Dyer, G. Neubig, and N. A.\\nSmith, “What do recurrent neural network grammars learn about\\nsyntax?,” arXiv preprint arXiv:1611.05774 , 2016.\\n[95] J. Liu and Y . Zhang, “In-order transition-based constituent parsing,”\\narXiv preprint arXiv:1707.05000 , 2017.\\n[96] D. Fried, M. Stern, and D. Klein, “Improving neural parsing by\\ndisentangling model combination and reranking effects,” arXiv preprint\\narXiv:1707.03058, 2017.\\n[97] N. Kitaev and D. Klein, “Constituency parsing with a self-attentive\\nencoder,” arXiv preprint arXiv:1805.01052 , 2018.\\n[98] D. Chen and C. Manning, “A fast and accurate dependency parser using\\nneural networks,” in Proceedings of the 2014 conference on empirical\\nmethods in natural language processing (EMNLP), pp. 740–750, 2014.\\n[99] T. Dozat and C. D. Manning, “Deep biafﬁne attention for neural\\ndependency parsing,” arXiv preprint arXiv:1611.01734 , 2016.\\n[100] E. Kiperwasser and Y . Goldberg, “Simple and accurate dependency\\nparsing using bidirectional LSTM feature representations,” arXiv\\npreprint arXiv:1603.04351, 2016.\\n[101] C. Dyer, M. Ballesteros, W. Ling, A. Matthews, and N. A. Smith,\\n“Transition-based dependency parsing with stack long short-term mem-\\nory,” arXiv preprint arXiv:1505.08075 , 2015.\\n[102] S. Jaf and C. Calder, “Deep learning for natural language parsing,”\\nIEEE Access, vol. 7, pp. 131363–131373, 2019.\\n[103] Y . Zhang, F. Tiryaki, M. Jiang, and H. Xu, “Parsing clinical text\\nusing the state-of-the-art deep learning based parsers: a systematic\\ncomparison,” BMC medical informatics and decision making , vol. 19,\\nno. 3, p. 77, 2019.\\n[104] Y . Zhang, Z. Li, and M. Zhang, “Efﬁcient second-order treecrf for\\nneural dependency parsing,” arXiv preprint arXiv:2005.00975 , 2020.\\n[105] T. Dozat and C. D. Manning, “Deep biafﬁne attention for neural\\ndependency parsing,” 2017.\\n[106] Z. Tan, M. Wang, J. Xie, Y . Chen, and X. Shi, “Deep semantic role\\nlabeling with self-attention,” arXiv preprint arXiv:1712.01586 , 2017.\\n[107] D. Marcheggiani and I. Titov, “Encoding sentences with graph\\nconvolutional networks for semantic role labeling,” arXiv preprint\\narXiv:1703.04826, 2017.\\n[108] E. Strubell, P. Verga, D. Andor, D. Weiss, and A. McCallum,\\n“Linguistically-informed self-attention for semantic role labeling,”\\narXiv preprint arXiv:1804.08199 , 2018.\\n[109] L. He, K. Lee, O. Levy, and L. Zettlemoyer, “Jointly predicting\\npredicates and arguments in neural semantic role labeling,” arXiv\\npreprint arXiv:1805.04787, 2018.\\n[110] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\\nand L. Zettlemoyer, “Deep contextualized word representations,” arXiv\\npreprint arXiv:1802.05365, 2018.\\n[111] Z. Tan, M. Wang, J. Xie, Y . Chen, and X. Shi, “Deep semantic role\\nlabeling with self-attention,” in Thirty-Second AAAI Conference on\\nArtiﬁcial Intelligence, 2018.\\n[112] Z. Li, S. He, H. Zhao, Y . Zhang, Z. Zhang, X. Zhou, and X. Zhou,\\n“Dependency or span, end-to-end uniform semantic role labeling,” in\\nProceedings of the AAAI Conference on Artiﬁcial Intelligence , vol. 33,\\npp. 6730–6737, 2019.\\n[113] S. Pradhan, A. Moschitti, N. Xue, H. T. Ng, A. Bj ¨orkelund,\\nO. Uryupina, Y . Zhang, and Z. Zhong, “Towards robust linguistic anal-\\nysis using OntoNotes,” in Proceedings of the Seventeenth Conference\\non Computational Natural Language Learning , pp. 143–152, 2013.\\n[114] N. Kalchbrenner, E. Grefenstette, and P. Blunsom, “A convo-\\nlutional neural network for modelling sentences,” arXiv preprint\\narXiv:1404.2188, 2014.\\n[115] H. Palangi, L. Deng, Y . Shen, J. Gao, X. He, J. Chen, X. Song,\\nand R. Ward, “Deep sentence embedding using long short-term\\nmemory networks: Analysis and application to information retrieval,”\\nIEEE/ACM Transactions on Audio, Speech and Language Processing\\n(TASLP), vol. 24, no. 4, pp. 694–707, 2016.\\n[116] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, “Hierar-\\nchical attention networks for document classiﬁcation,” in Proceedings\\nof the 2016 Conference of the North American Chapter of the Associ-\\nation for Computational Linguistics: Human Language Technologies ,\\npp. 1480–1489, 2016.\\n[117] S. Lai, L. Xu, K. Liu, and J. Zhao, “Recurrent convolutional neural\\nnetworks for text classiﬁcation.,” in AAAI, vol. 333, pp. 2267–2273,\\n2015.\\n[118] C. Zhou, C. Sun, Z. Liu, and F. Lau, “A C-LSTM neural network for\\ntext classiﬁcation,” arXiv preprint arXiv:1511.08630 , 2015.\\n[119] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu,\\nand J. Gao, “Deep learning based text classiﬁcation: A comprehensive\\nreview,”arXiv preprint arXiv:2004.03705 , 2020.\\n[120] M. Zulqarnain, R. Ghazali, Y . M. M. Hassim, and M. Rehan, “A\\ncomparative review on deep learning models for text classiﬁcation,”\\nIndones. J. Electr. Eng. Comput. Sci, vol. 19, no. 1, pp. 325–335, 2020.\\n[121] A. Conneau, H. Schwenk, L. Barrault, and Y . LeCun, “Very deep\\nconvolutional networks for text classiﬁcation,” in Proceedings of the\\n15th Conference of the European Chapter of the Association for\\nComputational Linguistics: Volume 1, Long Papers , vol. 1, pp. 1107–\\n1116, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 21\\n[122] R. Johnson and T. Zhang, “Deep pyramid convolutional neural net-\\nworks for text categorization,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics (Volume 1:\\nLong Papers), vol. 1, pp. 562–570, 2017.\\n[123] R. Johnson and T. Zhang, “Supervised and semi-supervised text\\ncategorization using LSTM for region embeddings,” arXiv preprint\\narXiv:1602.02373, 2016.\\n[124] J. Howard and S. Ruder, “Universal language model ﬁne-tuning for\\ntext classiﬁcation,” in Proceedings of the 56th Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers) ,\\nvol. 1, pp. 328–339, 2018.\\n[125] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and\\nP. Kuksa, “Natural language processing (almost) from scratch,”Journal\\nof Machine Learning Research, vol. 12, no. Aug., pp. 2493–2537, 2011.\\n[126] G. Mesnil, X. He, L. Deng, and Y . Bengio, “Investigation of recurrent-\\nneural-network architectures and learning methods for spoken language\\nunderstanding.,” in Interspeech, pp. 3771–3775, 2013.\\n[127] F. Dernoncourt, J. Y . Lee, and P. Szolovits, “NeuroNER: an easy-to-\\nuse program for named-entity recognition based on neural networks,”\\nConference on Empirical Methods on Natural Language Processing\\n(EMNLP), 2017.\\n[128] A. Baevski, S. Edunov, Y . Liu, L. Zettlemoyer, and M. Auli, “Cloze-\\ndriven pretraining of self-attention networks,” 2019.\\n[129] E. F. Tjong Kim Sang and F. De Meulder, “Introduction to the CoNLL-\\n2003 shared task: Language-independent named entity recognition,” in\\nProceedings of the seventh conference on Natural language learning\\nat HLT-NAACL 2003-Volume 4, pp. 142–147, Association for Compu-\\ntational Linguistics, 2003.\\n[130] K. Clark, M.-T. Luong, C. D. Manning, and Q. V . Le, “Semi-\\nsupervised sequence modeling with cross-view training,”arXiv preprint\\narXiv:1809.08370, 2018.\\n[131] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\\ntraining of deep bidirectional transformers for language understanding,”\\narXiv preprint arXiv:1810.04805 , 2018.\\n[132] R. Socher, B. Huval, C. D. Manning, and A. Y . Ng, “Semantic compo-\\nsitionality through recursive matrix-vector spaces,” in Proceedings of\\nthe 2012 joint conference on empirical methods in natural language\\nprocessing and computational natural language learning , pp. 1201–\\n1211, Association for Computational Linguistics, 2012.\\n[133] Z. Geng, G. Chen, Y . Han, G. Lu, and F. Li, “Semantic relation\\nextraction using sequential and tree-structured lstm with attention,”\\nInformation Sciences, vol. 509, pp. 183–192, 2020.\\n[134] X. Han, T. Gao, Y . Lin, H. Peng, Y . Yang, C. Xiao, Z. Liu, P. Li,\\nM. Sun, and J. Zhou, “More data, more relations, more context and\\nmore openness: A review and outlook for relation extraction,” arXiv\\npreprint arXiv:2004.03186, 2020.\\n[135] K. Clark and C. D. Manning, “Deep reinforcement learn-\\ning for mention-ranking coreference models,” arXiv preprint\\narXiv:1609.08667, 2016.\\n[136] K. Lee, L. He, and L. Zettlemoyer, “Higher-order coreference resolu-\\ntion with coarse-to-ﬁne inference,” arXiv preprint arXiv:1804.05392 ,\\n2018.\\n[137] H. Fei, X. Li, D. Li, and P. Li, “End-to-end deep reinforcement learning\\nbased coreference resolution,” in Proceedings of the 57th Annual\\nMeeting of the Association for Computational Linguistics , pp. 660–\\n665, 2019.\\n[138] W. Wu, F. Wang, A. Yuan, F. Wu, and J. Li, “Corefqa: Coreference\\nresolution as query-based span prediction,” in Proceedings of the 58th\\nAnnual Meeting of the Association for Computational Linguistics ,\\npp. 6953–6963, 2020.\\n[139] Y . Chen, L. Xu, K. Liu, D. Zeng, and J. Zhao, “Event extraction via\\ndynamic multi-pooling convolutional neural networks,” in Proceedings\\nof the 53rd Annual Meeting of the Association for Computational\\nLinguistics and the 7th International Joint Conference on Natural\\nLanguage Processing (Volume 1: Long Papers) , vol. 1, pp. 167–176,\\n2015.\\n[140] T. H. Nguyen and R. Grishman, “Graph convolutional networks with\\nargument-aware pooling for event detection,” in Thirty-Second AAAI\\nConference on Artiﬁcial Intelligence , 2018.\\n[141] T. Zhang, H. Ji, and A. Sil, “Joint entity and event extraction with\\ngenerative adversarial imitation learning,” Data Intelligence , vol. 1,\\nno. 2, pp. 99–120, 2019.\\n[142] W. Zhao, J. Zhang, J. Yang, T. He, H. Ma, and Z. Li, “A novel\\njoint biomedical event extraction framework via two-level modeling\\nof documents,” Information Sciences, vol. 550, pp. 27–40, 2021.\\n[143] T. Nasukawa and J. Yi, “Sentiment analysis: Capturing favorability\\nusing natural language processing,” in Proceedings of the 2nd Interna-\\ntional Conference on Knowledge Capture , pp. 70–77, ACM, 2003.\\n[144] K. Dave, S. Lawrence, and D. M. Pennock, “Mining the peanut gallery:\\nOpinion extraction and semantic classiﬁcation of product reviews,” in\\nProceedings of the 12th international conference on World Wide Web ,\\npp. 519–528, ACM, 2003.\\n[145] A. R. Pathak, B. Agarwal, M. Pandey, and S. Rautaray, “Application of\\ndeep learning approaches for sentiment analysis,” in Deep Learning-\\nBased Approaches for Sentiment Analysis , pp. 1–31, Springer, 2020.\\n[146] A. Yadav and D. K. Vishwakarma, “Sentiment analysis using deep\\nlearning architectures: a review,”Artiﬁcial Intelligence Review, vol. 53,\\nno. 6, pp. 4335–4385, 2020.\\n[147] D. Tang, B. Qin, and T. Liu, “Document modeling with gated recurrent\\nneural network for sentiment classiﬁcation,” in Proceedings of the\\n2015 conference on empirical methods in natural language processing ,\\npp. 1422–1432, 2015.\\n[148] X. Glorot, A. Bordes, and Y . Bengio, “Domain adaptation for large-\\nscale sentiment classiﬁcation: A deep learning approach,” in Proceed-\\nings of the 28th international conference on machine learning (ICML-\\n11), pp. 513–520, 2011.\\n[149] G. Rao, W. Huang, Z. Feng, and Q. Cong, “Lstm with sentence\\nrepresentations for document-level sentiment classiﬁcation,” Neuro-\\ncomputing, vol. 308, pp. 49–57, 2018.\\n[150] M. Rhanoui, M. Mikram, S. Yousﬁ, and S. Barzali, “A cnn-bilstm\\nmodel for document-level sentiment analysis,” Machine Learning and\\nKnowledge Extraction, vol. 1, no. 3, pp. 832–847, 2019.\\n[151] R. Socher, J. Pennington, E. H. Huang, A. Y . Ng, and C. D. Manning,\\n“Semi-supervised recursive autoencoders for predicting sentiment dis-\\ntributions,” in Proceedings of the conference on empirical methods in\\nnatural language processing , pp. 151–161, Association for Computa-\\ntional Linguistics, 2011.\\n[152] X. Wang, Y . Liu, S. Chengjie, B. Wang, and X. Wang, “Predicting\\npolarities of tweets by composing word embeddings with long short-\\nterm memory,” in Proceedings of the 53rd Annual Meeting of the\\nAssociation for Computational Linguistics and the 7th International\\nJoint Conference on Natural Language Processing (Volume 1: Long\\nPapers), vol. 1, pp. 1343–1353, 2015.\\n[153] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng,\\nand C. Potts, “Recursive deep models for semantic compositionality\\nover a sentiment treebank,” in Proceedings of the 2013 conference\\non empirical methods in natural language processing , pp. 1631–1642,\\n2013.\\n[154] R. Arulmurugan, K. Sabarmathi, and H. Anandakumar, “Classiﬁcation\\nof sentence level sentiment analysis using cloud machine learning\\ntechniques,” Cluster Computing, vol. 22, no. 1, pp. 1199–1209, 2019.\\n[155] D. Me ˇskel˙e and F. Frasincar, “Aldonar: A hybrid solution for sentence-\\nlevel aspect-based sentiment analysis using a lexicalized domain ontol-\\nogy and a regularized neural attention model,” Information Processing\\n& Management, vol. 57, no. 3, p. 102211, 2020.\\n[156] Y . Wang, M. Huang, L. Zhao,et al., “Attention-based LSTM for aspect-\\nlevel sentiment classiﬁcation,” in Proceedings of the 2016 Conference\\non Empirical Methods in Natural Language Processing , pp. 606–615,\\n2016.\\n[157] Y . Ma, H. Peng, T. Khan, E. Cambria, and A. Hussain, “Sentic lstm: a\\nhybrid network for targeted aspect-based sentiment analysis,”Cognitive\\nComputation, vol. 10, no. 4, pp. 639–650, 2018.\\n[158] H. Xu, B. Liu, L. Shu, and P. S. Yu, “BERT post-training for review\\nreading comprehension and aspect-based sentiment analysis,” arXiv\\npreprint arXiv:1904.02232, 2019.\\n[159] H. Xu, B. Liu, L. Shu, and P. S. Yu, “Double embeddings and\\nCNN-based sequence labeling for aspect extraction,” arXiv preprint\\narXiv:1805.04601, 2018.\\n[160] H. H. Do, P. Prasad, A. Maag, and A. Alsadoon, “Deep learning for\\naspect-based sentiment analysis: a comparative review,”Expert Systems\\nwith Applications, vol. 118, pp. 272–299, 2019.\\n[161] S. Rida-E-Fatima, A. Javed, A. Banjar, A. Irtaza, H. Dawood, H. Da-\\nwood, and A. Alamri, “A multi-layer dual attention deep learning model\\nwith reﬁned word embeddings for aspect-based sentiment analysis,”\\nIEEE Access, vol. 7, pp. 114795–114807, 2019.\\n[162] Y . Liang, F. Meng, J. Zhang, J. Xu, Y . Chen, and J. Zhou, “A\\nnovel aspect-guided deep transition model for aspect based sentiment\\nanalysis,” arXiv preprint arXiv:1909.00324 , 2019.\\n[163] D. Jurafsky and J. H. Martin, Speech and Language Processing .\\nPrentice Hall, 2008.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 22\\n[164] N. Kalchbrenner and P. Blunsom, “Recurrent continuous translation\\nmodels,” in Proceedings of the 2013 Conference on Empirical Methods\\nin Natural Language Processing , pp. 1700–1709, 2013.\\n[165] S. P. Singh, A. Kumar, H. Darbari, L. Singh, A. Rastogi, and S. Jain,\\n“Machine translation using deep learning: An overview,” in 2017\\ninternational conference on computer, communications and electronics\\n(comptelix), pp. 162–167, IEEE, 2017.\\n[166] S. Yang, Y . Wang, and X. Chu, “A survey of deep learning techniques\\nfor neural machine translation,” arXiv preprint arXiv:2002.07526 ,\\n2020.\\n[167] L. E. Dostert, “The Georgetown-IBM experiment,” 1955). Machine\\ntranslation of languages. John Wiley & Sons, New York , pp. 124–135,\\n1955.\\n[168] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine translation by\\njointly learning to align and translate,” arXiv preprint arXiv:1409.0473,\\n2014.\\n[169] K. Cho, B. Van Merri ¨enboer, D. Bahdanau, and Y . Bengio, “On the\\nproperties of neural machine translation: Encoder-decoder approaches,”\\narXiv preprint arXiv:1409.1259 , 2014.\\n[170] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to sequence learning\\nwith neural networks,” in Advances in neural information processing\\nsystems, pp. 3104–3112, 2014.\\n[171] Y . Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi, W. Macherey,\\nM. Krikun, Y . Cao, Q. Gao, K. Macherey, et al. , “Google’s neural\\nmachine translation system: Bridging the gap between human and\\nmachine translation,” arXiv preprint arXiv:1609.08144 , 2016.\\n[172] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y . N. Dauphin,\\n“Convolutional sequence to sequence learning,” arXiv preprint\\narXiv:1705.03122, 2017.\\n[173] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in\\nAdvances in Neural Information Processing Systems , pp. 5998–6008,\\n2017.\\n[174] K. Ahmed, N. S. Keskar, and R. Socher, “Weighted transformer\\nnetwork for machine translation,” arXiv preprint arXiv:1711.02132 ,\\n2017.\\n[175] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative\\nposition representations,” arXiv preprint arXiv:1803.02155 , 2018.\\n[176] S. Edunov, M. Ott, M. Auli, and D. Grangier, “Understanding back-\\ntranslation at scale,” arXiv preprint arXiv:1808.09381 , 2018.\\n[177] R. Aharoni, M. Johnson, and O. Firat, “Massively multilingual neural\\nmachine translation,” 2019.\\n[178] J. Zhu, Y . Xia, L. Wu, D. He, T. Qin, W. Zhou, H. Li, and T.-Y . Liu,\\n“Incorporating bert into neural machine translation,” 2020.\\n[179] Y . Liu, J. Gu, N. Goyal, X. Li, S. Edunov, M. Ghazvininejad,\\nM. Lewis, and L. Zettlemoyer, “Multilingual denoising pre-training\\nfor neural machine translation,” Transactions of the Association for\\nComputational Linguistics, vol. 8, pp. 726–742, 2020.\\n[180] Y . Cheng, L. Jiang, and W. Macherey, “Robust neural machine transla-\\ntion with doubly adversarial inputs,” arXiv preprint arXiv:1906.02443,\\n2019.\\n[181] W. Zhang, Y . Feng, F. Meng, D. You, and Q. Liu, “Bridging the gap\\nbetween training and inference for neural machine translation,” 2019.\\n[182] J. Yang, M. Wang, H. Zhou, C. Zhao, W. Zhang, Y . Yu, and L. Li,\\n“Towards making the most of bert in neural machine translation,” in\\nProceedings of the AAAI Conference on Artiﬁcial Intelligence , vol. 34,\\npp. 9378–9385, 2020.\\n[183] A. Bordes, S. Chopra, and J. Weston, “Question answering with\\nsubgraph embeddings,” arXiv preprint arXiv:1406.3676 , 2014.\\n[184] B. F. Green Jr, A. K. Wolf, C. Chomsky, and K. Laughery, “Baseball:\\nan automatic question-answerer,” in Papers presented at the May 9-11,\\n1961, Western Joint IRE-AIEE-ACM Computer Conference , pp. 219–\\n224, ACM, 1961.\\n[185] A. Ittycheriah, M. Franz, W.-J. Zhu, A. Ratnaparkhi, and R. J. Mam-\\nmone, “IBM’s statistical question answering system.,” in TREC, 2000.\\n[186] H. Cui, R. Sun, K. Li, M.-Y . Kan, and T.-S. Chua, “Question answering\\npassage retrieval using dependency relations,” in Proceedings of the\\n28th annual international ACM SIGIR conference on Research and\\ndevelopment in information retrieval , pp. 400–407, ACM, 2005.\\n[187] X. Qiu and X. Huang, “Convolutional neural tensor network architec-\\nture for community-based question answering.,” in IJCAI, pp. 1305–\\n1311, 2015.\\n[188] H. T. Ng, L. H. Teo, and J. L. P. Kwan, “A machine learning\\napproach to answering questions for reading comprehension tests,”\\nin Proceedings of the 2000 Joint SIGDAT conference on Empirical\\nmethods in natural language processing and very large corpora: held\\nin conjunction with the 38th Annual Meeting of the Association for\\nComputational Linguistics-Volume 13 , pp. 124–132, Association for\\nComputational Linguistics, 2000.\\n[189] C. Xiong, V . Zhong, and R. Socher, “Dynamic coattention networks\\nfor question answering,” arXiv preprint arXiv:1611.01604 , 2016.\\n[190] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zit-\\nnick, and D. Parikh, “VQA: Visual question answering,” inProceedings\\nof the IEEE international conference on computer vision , pp. 2425–\\n2433, 2015.\\n[191] M. Malinowski, M. Rohrbach, and M. Fritz, “Ask your neurons:\\nA neural-based approach to answering questions about images,” in\\nProceedings of the IEEE international conference on computer vision ,\\npp. 1–9, 2015.\\n[192] H. Xu and K. Saenko, “Ask, attend and answer: Exploring question-\\nguided spatial attention for visual question answering,” in European\\nConference on Computer Vision , pp. 451–466, Springer, 2016.\\n[193] A. Das, H. Agrawal, L. Zitnick, D. Parikh, and D. Batra, “Human\\nattention in visual question answering: Do humans and deep networks\\nlook at the same regions?,”Computer Vision and Image Understanding,\\nvol. 163, pp. 90–100, 2017.\\n[194] H. Ben-Younes, R. Cadene, N. Thome, and M. Cord, “Block: Bilinear\\nsuperdiagonal fusion for visual question answering and visual relation-\\nship detection,” in Proceedings of the AAAI Conference on Artiﬁcial\\nIntelligence, vol. 33, pp. 8102–8109, 2019.\\n[195] J. Wu and R. J. Mooney, “Self-critical reasoning for robust visual\\nquestion answering,” 2019.\\n[196] R. Nallapati, F. Zhai, and B. Zhou, “SummaRuNNer: A recurrent\\nneural network based sequence model for extractive summarization of\\ndocuments.,” in AAAI, pp. 3075–3081, 2017.\\n[197] S. Narayan, S. B. Cohen, and M. Lapata, “Ranking sentences for ex-\\ntractive summarization with reinforcement learning,” in NAACL:HLT,\\nvol. 1, pp. 1747–1759, 2018.\\n[198] A. M. Rush, S. Chopra, and J. Weston, “A neural attention model for\\nabstractive sentence summarization,” in EMNLP, 2015.\\n[199] J. Tan, X. Wan, and J. Xiao, “Abstractive document summarization with\\na graph-based attentional neural model,” inACL, vol. 1, pp. 1171–1181,\\n2017.\\n[200] R. Nallapati, B. Zhou, C. dos Santos, C. Gulcehre, and B. Xiang,\\n“Abstractive text summarization using sequence-to-sequence RNNs\\nand beyond,” in Proceedings of The 20th SIGNLL Conference on\\nComputational Natural Language Learning , pp. 280–290, 2016.\\n[201] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay,\\nM. Suleyman, and P. Blunsom, “Teaching machines to read and\\ncomprehend,” in NIPS, pp. 1693–1701, 2015.\\n[202] J. Gu, Z. Lu, H. Li, and V . O. Li, “Incorporating copying mechanism in\\nsequence-to-sequence learning,” in ACL, vol. 1, pp. 1631–1640, 2016.\\n[203] Y .-C. Chen and M. Bansal, “Fast abstractive summarization with\\nreinforce-selected sentence rewriting,” in ACL, 2018.\\n[204] Q. Zhou, N. Yang, F. Wei, S. Huang, M. Zhou, and T. Zhao, “Neu-\\nral document summarization by jointly learning to score and select\\nsentences,” in ACL, pp. 654–663, ACL, 2018.\\n[205] T. Shi, Y . Keneshloo, N. Ramakrishnan, and C. K. Reddy, “Neural\\nabstractive text summarization with sequence-to-sequence models,”\\narXiv preprint arXiv:1812.02303 , 2018.\\n[206] C. Ma, W. E. Zhang, M. Guo, H. Wang, and Q. Z. Sheng, “Multi-\\ndocument summarization via deep learning techniques: A survey,”\\narXiv preprint arXiv:2011.04843 , 2020.\\n[207] A. Abdi, S. Hasan, S. M. Shamsuddin, N. Idris, and J. Piran, “A hybrid\\ndeep learning architecture for opinion-oriented multi-document sum-\\nmarization based on multi-feature fusion,” Knowledge-Based Systems,\\nvol. 213, p. 106658, 2021.\\n[208] X. Zhang, F. Wei, and M. Zhou, “Hibert: Document level pre-training\\nof hierarchical bidirectional transformers for document summariza-\\ntion,” arXiv preprint arXiv:1905.06566 , 2019.\\n[209] E. Merdivan, D. Singh, S. Hanke, and A. Holzinger, “Dialogue sys-\\ntems for intelligent human computer interactions,” Electronic Notes in\\nTheoretical Computer Science , vol. 343, pp. 57–71, 2019.\\n[210] D. Hakkani-T ¨ur, G. T ¨ur, A. Celikyilmaz, Y .-N. Chen, J. Gao, L. Deng,\\nand Y .-Y . Wang, “Multi-domain joint semantic frame parsing using\\nbi-directional RNN-LSTM,” in Interspeech, pp. 715–719, 2016.\\n[211] C. Toxtli, J. Cranshaw, et al. , “Understanding chatbot-mediated task\\nmanagement,” in Proceedings of the 2018 CHI Conference on Human\\nFactors in Computing Systems , p. 58, ACM, 2018.\\n[212] V . Ilievski, C. Musat, A. Hossmann, and M. Baeriswyl, “Goal-oriented\\nchatbot dialog management bootstrapping with transfer learning,”arXiv\\npreprint arXiv:1802.00500, 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 22, 'page_label': '23', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 23\\n[213] J. Li, W. Monroe, A. Ritter, D. Jurafsky, M. Galley, and J. Gao, “Deep\\nreinforcement learning for dialogue generation,” in Proceedings of the\\nConference on Empirical Methods in Natural Language Processing ,\\npp. 1192–1202, 2016.\\n[214] T.-H. Wen, D. Vandyke, N. Mrksic, M. Gasic, L. M. Rojas-Barahona,\\nP.-H. Su, S. Ultes, and S. Young, “A network-based end-to-end train-\\nable task-oriented dialogue system,” arXiv preprint arXiv:1604.04562,\\n2016.\\n[215] J. D. Williams and G. Zweig, “End-to-end LSTM-based dialog control\\noptimized with supervised and reinforcement learning,” arXiv preprint\\narXiv:1606.01269, 2016.\\n[216] S. Sukhbaatar, J. Weston, R. Fergus, et al. , “End-to-end memory\\nnetworks,” in Advances in neural information processing systems ,\\npp. 2440–2448, 2015.\\n[217] A. Bordes, Y .-L. Boureau, and J. Weston, “Learning end-to-end goal-\\noriented dialog,” arXiv preprint arXiv:1605.07683 , 2016.\\n[218] A. Ritter, C. Cherry, and W. B. Dolan, “Data-driven response generation\\nin social media,” in Proceedings of the conference on empirical\\nmethods in natural language processing , pp. 583–593, Association for\\nComputational Linguistics, 2011.\\n[219] Z. Ji, Z. Lu, and H. Li, “An information retrieval approach to short\\ntext conversation,” arXiv preprint arXiv:1408.6988 , 2014.\\n[220] B. Hu, Z. Lu, H. Li, and Q. Chen, “Convolutional neural network\\narchitectures for matching natural language sentences,” in Advances in\\nneural information processing systems , pp. 2042–2050, 2014.\\n[221] R. Lowe, N. Pow, I. Serban, and J. Pineau, “The Ubuntu dialogue\\ncorpus: A large dataset for research in unstructured multi-turn dialogue\\nsystems,” arXiv preprint arXiv:1506.08909 , 2015.\\n[222] R. Yan, Y . Song, and H. Wu, “Learning to respond with deep neural\\nnetworks for retrieval-based human-computer conversation system,”\\nin Proceedings of the 39th International ACM SIGIR conference on\\nResearch and Development in Information Retrieval , pp. 55–64, ACM,\\n2016.\\n[223] X. Zhou, L. Li, D. Dong, Y . Liu, Y . Chen, W. X. Zhao, D. Yu, and\\nH. Wu, “Multi-turn response selection for chatbots with deep attention\\nmatching network,” in Proceedings of the 56th Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers) ,\\nvol. 1, pp. 1118–1127, 2018.\\n[224] S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Jozefowicz, and\\nS. Bengio, “Generating sentences from a continuous space,” arXiv\\npreprint arXiv:1511.06349, 2015.\\n[225] A. Kannan and O. Vinyals, “Adversarial evaluation of dialogue mod-\\nels,” arXiv preprint arXiv:1701.08198 , 2017.\\n[226] O. Vinyals and Q. Le, “A neural conversational model,” arXiv preprint\\narXiv:1506.05869, 2015.')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5c8eee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "### Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6c6d587a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 167 documents into 815 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: GOVERNMENT OF INDIA\n",
      "BUDGET 2023-2024\n",
      "SPEECH\n",
      "OF\n",
      "NIRMALA SITHARAMAN\n",
      "MINISTER OF FINANCE\n",
      "February 1,  2023...\n",
      "Metadata: {'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 0, 'page_label': '1', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 0, 'page_label': '1', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='GOVERNMENT OF INDIA\\nBUDGET 2023-2024\\nSPEECH\\nOF\\nNIRMALA SITHARAMAN\\nMINISTER OF FINANCE\\nFebruary 1,  2023'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 2, 'page_label': '3', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='CONTENTS \\nPART-A \\n Page No. \\n\\uf0b7 Introduction 1 \\n\\uf0b7 Achievements since 2014: Leaving no one behind 2 \\n\\uf0b7 Vision for Amrit Kaal – an empowered and inclusive economy 3 \\n\\uf0b7 Priorities of this Budget 5 \\ni. Inclusive Development  \\nii. Reaching the Last Mile \\niii. Infrastructure and Investment \\niv. Unleashing the Potential \\nv. Green Growth \\nvi. Youth Power  \\nvii. Financial Sector \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\uf0b7 Fiscal Management \\n24 \\nPART B \\n  \\nIndirect Taxes 27 \\n\\uf0b7 Green Mobility  \\n\\uf0b7 Electronics   \\n\\uf0b7 Electrical   \\n\\uf0b7 Chemicals and Petrochemicals   \\n\\uf0b7 Marine products  \\n\\uf0b7 Lab Grown Diamonds  \\n\\uf0b7 Precious Metals  \\n\\uf0b7 Metals  \\n\\uf0b7 Compounded Rubber  \\n\\uf0b7 Cigarettes  \\n  \\nDirect Taxes 30 \\n\\uf0b7 MSMEs and Professionals   \\n\\uf0b7 Cooperation  \\n\\uf0b7 Start-Ups  \\n\\uf0b7 Appeals  \\n\\uf0b7 Better targeting of tax concessions  \\n\\uf0b7 Rationalisation  \\n\\uf0b7 Others  \\n\\uf0b7 Personal Income Tax  \\n  \\nAnnexures 35 \\n\\uf0b7 Annexure to Part B of the Budget Speech 2023-24 \\ni. Amendments relating to Direct Taxes \\nii. Amendments relating to Indirect Taxes'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 4, 'page_label': '5', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='Budget 2023-2024 \\n \\nSpeech of \\nNirmala Sitharaman \\nMinister of Finance \\nFebruary 1, 2023 \\nHon’ble Speaker,  \\n I present the Budget for 2023-24. This is the first Budget in Amrit \\nKaal. \\nIntroduction \\n1. This Budget hopes to build on the foundation laid in the previous \\nBudget, and the blueprint drawn for India@100. We envision a prosperous \\nand inclusive India, in which the fruits of development reach all regions and \\ncitizens, especially our youth, women, farmers, OBCs, Scheduled Castes and \\nScheduled Tribes.  \\n2. In the 75 th year of our Independence, the world has recognised the \\nIndian economy as a ‘bright star’. Our current year’s economic growth is \\nestimated to be at 7 per cent. It is notable that this is the highest among all \\nthe major economies. This is in spite of the massive slowdown globally \\ncaused by Covid-19 and a war. The Indian economy is therefore on the right \\ntrack, and despite a time of challenges, heading towards a bright future.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 4, 'page_label': '5', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='caused by Covid-19 and a war. The Indian economy is therefore on the right \\ntrack, and despite a time of challenges, heading towards a bright future.  \\n3. Today as Indians stands with their head held high, and the world \\nappreciates India’s achievements and successes, we are sure that elders \\nwho had fought for India’s independence, will with joy, bless us our \\nendeavors going forward. \\nResilience amidst multiple crises \\n4. Our focus on wide-ranging reforms and sound policies, implemented \\nthrough Sabka Prayas  resulting in Jan Bhagidari  and targeted support to \\nthose in need, helped us perform well in trying times. India’s rising global'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 5, 'page_label': '6', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='2 \\n \\n \\n \\nprofile is because of several accomplishments: unique world class digital \\npublic infrastructure, e.g., Aadhaar, Co-Win and UPI; Covid vaccination drive \\nin unparalleled scale and speed; proactive role in frontier areas such as \\nachieving the climate related goals, mission LiFE, and National Hydrogen \\nMission.  \\n5. During the Covid-19 pandemic, we ensured that no one goes to bed \\nhungry, with a scheme to supply free food grains to over 80 crore persons \\nfor 28 months. Continuing our commitment to ensure food and nutritional \\nsecurity, we are implementing, from 1 st January 2023, a scheme to supply \\nfree food grain to all Antyodaya and priority households for the next one \\nyear, under PM Garib Kalyan Anna Yojana (PMGKAY). The entire \\nexpenditure of about ` 2 lakh crore will be borne by the Central \\nGovernment. \\nG20 Presidency: Steering the global agenda through challenges \\n6. In these times of global challenges, the G20 Presidency gives us a'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 5, 'page_label': '6', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='Government. \\nG20 Presidency: Steering the global agenda through challenges \\n6. In these times of global challenges, the G20 Presidency gives us a \\nunique opportunity to strengthen India’s role in the world economic order. \\nWith the theme of ‘ Vasudhaiva Kutumbakam’ , we are steering an \\nambitious, people-centric agenda to address global challenges, and to \\nfacilitate sustainable economic development.  \\nAchievements since 2014: Leaving no one behind \\n7. The government’s efforts since 2014 have ensured for all citizens a \\nbetter quality of living and a life of dignity. The per capita income has more \\nthan doubled to ` 1.97 lakh.   \\n8. In these nine years, the Indian economy has increased in size from \\nbeing 10th to 5 th largest in the world. We have significantly improved our \\nposition as a well-governed and innovative country with a conducive \\nenvironment for business as reflected in several global indices. We have \\nmade significant progress in many Sustainable Development Goals.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 6, 'page_label': '7', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='3 \\n \\n \\n \\n9. The economy has become a lot more formalised as reflected in the \\nEPFO membership more than doubling to 27 crore, and 7,400 crore digital \\npayments of ` 126 lakh crore through UPI in 2022.    \\n10. The efficient implementation of many schemes, with \\nuniversalisation of targeted benefits, has resulted in inclusive development. \\nSome of the schemes are: \\ni. 11.7 crore household toilets under Swachh Bharat Mission,  \\nii. 9.6 crore LPG connections under Ujjawala,  \\niii. 220 crore Covid vaccination of 102 crore persons,    \\niv. 47.8 crore PM Jan Dhan bank accounts, \\nv. Insurance cover for 44.6 crore persons under PM Suraksha \\nBima and PM Jeevan Jyoti Yojana, and \\nvi. Cash transfer of ` 2.2 lakh crore to over 11.4 crore farmers \\nunder PM Kisan Samman Nidhi. \\nVision for Amrit Kaal – an empowered and inclusive economy \\n11. Our vision for the Amrit Kaal  includes technology-driven and \\nknowledge-based economy with strong public finances, and a robust'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 6, 'page_label': '7', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='Vision for Amrit Kaal – an empowered and inclusive economy \\n11. Our vision for the Amrit Kaal  includes technology-driven and \\nknowledge-based economy with strong public finances, and a robust \\nfinancial sector. To achieve this, Jan Bhagidari through Sabka Saath Sabka \\nPrayas is essential.   \\n12. The economic agenda for achieving this vision focuses on three \\nthings: first, facilitating ample opportunities for citizens, especially the \\nyouth, to fulfil their aspirations; second, providing strong impetus to growth \\nand job creation; and third, strengthening macro-economic stability.    \\n13. To service these focus areas in our journey to India@100, we believe \\nthat the following four opportunities can be transformative during Amrit \\nKaal.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 7, 'page_label': '8', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='4 \\n \\n \\n \\n1) Economic Empowerment of Women : Deendayal Antyodaya Yojana \\nNational Rural Livelihood Mission has achieved remarkable success \\nby mobilizing rural women into 81 lakh Self Help Groups. We will \\nenable these groups to reach the next stage of economic \\nempowerment through formation of large producer enterprises or \\ncollectives with each having several thousand members and \\nmanaged professionally. They will be helped with supply of raw \\nmaterials and for better design, quality, branding and marketing of \\ntheir products. Through supporting policies, they will be enabled to \\nscale up their operations to serve the large consumer markets, as \\nhas been the case with several start-ups growing into ‘Unicorns’. \\n2) PM VIshwakarma KAushal Samman (PM VIKAS) : For centuries, \\ntraditional artisans and craftspeople, who work with their hands \\nusing tools, have brought renown for India. They are generally \\nreferred to as Vishwakarma. The art and handicraft created by them'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 7, 'page_label': '8', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='traditional artisans and craftspeople, who work with their hands \\nusing tools, have brought renown for India. They are generally \\nreferred to as Vishwakarma. The art and handicraft created by them \\nrepresents the true spirit of Atmanirbhar Bharat. For the first time, a \\npackage of assistance for them has been conceptualized. The new \\nscheme will enable them to improve the quality, scale and reach of \\ntheir products, integrating them with the MSME value chain. The \\ncomponents of the scheme will include not only financial support \\nbut also access to advanced skill training, knowledge of modern \\ndigital techniques and efficient green technologies, brand \\npromotion, linkage with local and global markets, digital payments, \\nand social security. This will greatly benefit the Scheduled Castes, \\nScheduled Tribes, OBCs, women and people belonging to the weaker \\nsections.  \\n3) Tourism: The country offers immense attraction for domestic as well'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 7, 'page_label': '8', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='Scheduled Tribes, OBCs, women and people belonging to the weaker \\nsections.  \\n3) Tourism: The country offers immense attraction for domestic as well \\nas foreign tourists. There is a large potential to be tapped in tourism. \\nThe sector holds huge opportunities for jobs and entrepreneurship \\nfor youth in particular.  Promotion of tourism will be taken up on \\nmission mode, with active participation of states, convergence of \\ngovernment programmes and public-private partnerships.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 8, 'page_label': '9', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='5 \\n \\n \\n \\n4) Green Growth: We are implementing many programmes for green \\nfuel, green energy, green farming, green mobility, green buildings, \\nand green equipment, and policies for efficient use of energy across \\nvarious economic sectors. These green growth efforts help in \\nreducing carbon intensity of the economy and provides for large-\\nscale green job opportunities.  \\nPriorities of this Budget \\n14. The Budget adopts the following seven priorities. They complement \\neach other and act as the ‘Saptarishi’ guiding us through the Amrit Kaal. \\n1) Inclusive Development  \\n2) Reaching the Last Mile \\n3) Infrastructure and Investment \\n4) Unleashing the Potential \\n5) Green Growth \\n6) Youth Power  \\n7) Financial Sector \\nPriority 1: Inclusive Development  \\n15. The Government’s philosophy of Sabka Saath Sabka Vikas  has \\nfacilitated inclusive development covering in specific, farmers, women, \\nyouth, OBCs, Scheduled Castes, Scheduled Tribes, divyangjan and'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 8, 'page_label': '9', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='facilitated inclusive development covering in specific, farmers, women, \\nyouth, OBCs, Scheduled Castes, Scheduled Tribes, divyangjan and \\neconomically weaker sections, and overall priority for the underprivileged \\n(vanchiton ko variyata). There has also been a sustained focus on Jammu & \\nKashmir, Ladakh and the North-East. This Budget builds on those efforts.  \\nAgriculture and Cooperation   \\nDigital Public Infrastructure for Agriculture \\n16. Digital public infrastructure for agriculture will be built as an open \\nsource, open standard and inter operable public good. This will enable'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 9, 'page_label': '10', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='6 \\n \\n \\n \\ninclusive, farmer-centric solutions through relevant information services for \\ncrop planning and health, improved access to farm inputs, credit, and \\ninsurance, help for crop estimation, market intelligence, and support for \\ngrowth of agri-tech industry and start-ups.  \\nAgriculture Accelerator Fund \\n17. An Agriculture Accelerator Fund will be set-up to encourage agri-\\nstartups by young entrepreneurs in rural areas. The Fund will aim at \\nbringing innovative and affordable solutions for challenges faced by \\nfarmers. It will also bring in modern technologies to transform agricultural \\npractices, increase productivity and profitability. \\nEnhancing productivity of cotton crop  \\n18. To enhance the productivity of extra-long staple cotton, we will \\nadopt a cluster-based and value chain approach through Public Private \\nPartnerships (PPP). This will mean collaboration between farmers, state and \\nindustry for input supplies, extension services, and market linkages.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 9, 'page_label': '10', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content=\"Partnerships (PPP). This will mean collaboration between farmers, state and \\nindustry for input supplies, extension services, and market linkages. \\nAtmanirbhar Horticulture Clean Plant Program  \\n19. We will launch an Atmanirbhar Clean Plant Program to boost \\navailability of disease-free, quality planting material for high value \\nhorticultural crops at an outlay of ` 2,200 crore. \\nGlobal Hub for Millets: ‘Shree Anna’ \\n20. “India is at the forefront of popularizing Millets, whose consumption \\nfurthers nutrition, food security and welfare of farmers,” said Hon’ble Prime \\nMinister. \\n21.  We are the largest producer and second largest exporter of ‘Shree \\nAnna’ in the world. We grow several types of ' Shree Anna'  such as  jowar, \\nragi, bajra, kuttu, ramdana, kangni, kutki, kodo, cheena, and sama. These \\nhave a number of health benefits, and have been an integral part of our \\nfood for centuries. I acknowledge with pride the huge service done by small\"),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 10, 'page_label': '11', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content=\"7 \\n \\n \\n \\nfarmers in contributing to the health of fellow citizens by growing these \\n‘Shree Anna’.  \\n22. Now to make India a global hub for ' Shree Anna', the Indian Institute \\nof Millet Research, Hyderabad will be supported as the Centre of Excellence \\nfor sharing best practices, research and technologies at the international \\nlevel.    \\nAgriculture Credit  \\n23. The agriculture credit target will be increased  \\nto ` 20 lakh crore with focus on animal husbandry, dairy and fisheries.  \\nFisheries \\n24. We will launch a new sub-scheme of PM Matsya Sampada Yojana \\nwith targeted investment of ` 6,000 crore to further enable activities of \\nfishermen, fish vendors, and micro & small enterprises, improve value chain \\nefficiencies, and expand the market. \\nCooperation \\n25. For farmers, especially small and marginal farmers, and other \\nmarginalised sections, the government is promoting cooperative-based \\neconomic development model. A new Ministry of Cooperation was formed\"),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 10, 'page_label': '11', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='marginalised sections, the government is promoting cooperative-based \\neconomic development model. A new Ministry of Cooperation was formed \\nwith a mandate to realise the vision of ‘Sahakar Se Samriddhi’. To realise \\nthis vision, the government has already initiated computerisation of 63,000 \\nPrimary Agricultural Credit Societies (PACS) with an investment of ` 2,516 \\ncrore. In consultation with all stakeholders and states, model bye-laws for \\nPACS were formulated enabling them to become multipurpose PACS. A \\nnational cooperative database is being prepared for country-wide mapping \\nof cooperative societies.  \\n26. With this backdrop, we will implement a plan to set up massive \\ndecentralised storage capacity. This will help farmers store their produce \\nand realize remunerative prices through sale at appropriate times. The \\ngovernment will also facilitate setting up of a large number of multipurpose'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 11, 'page_label': '12', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='8 \\n \\n \\n \\ncooperative societies, primary fishery societies and dairy cooperative \\nsocieties in uncovered panchayats and villages in the next 5 years.  \\nHealth, Education and Skilling \\nNursing Colleges   \\n27\\n. One hundred and fifty-seven new nursing colleges will be \\nestablished in co-location with the existing 157 medical colleges established \\nsince 2014. \\nSickle Cell Anaemia Elimination Mission \\n28. A Mission to eliminate Sickle Cell Anaemia by 2047 will be launched. \\nIt will entail awareness creation, universal screening of 7 crore people in the \\nage group of 0-40 years in affected tribal areas, and counselling through \\ncollaborative efforts of central ministries and state governments.  \\nMedical Research  \\n29. Facilities in select ICMR Labs will be made available for research by \\npublic and private medical college faculty and private sector R&D teams for \\nencouraging collaborative research and innovation. \\nPharma Innovation  \\n30. A new programme to promote research and innovation in'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 11, 'page_label': '12', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='encouraging collaborative research and innovation. \\nPharma Innovation  \\n30. A new programme to promote research and innovation in \\npharmaceuticals will be taken up through centers of excellence. We shall \\nalso encourage industry to invest in research and development in specific \\npriority areas.  \\nMultidisciplinary courses for medical devices \\n31. Dedicated multidisciplinary courses for medical devices will be \\nsupported in existing institutions to ensure availability of skilled manpower \\nfor futuristic medical technologies, high-end manufacturing and research.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 12, 'page_label': '13', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='9 \\n \\n \\n \\nTeachers’ Training \\n32. Teachers’ training will be re-envisioned through innovative \\npedagogy, curriculum transaction, continuous professional development, \\ndipstick surveys, and ICT implementation. The District Institutes of \\nEducation and Training will be developed as vibrant institutes of excellence \\nfor this purpose.   \\nNational Digital Library for Children and Adolescents  \\n33. A National Digital Library for children and adolescents  will be set-up \\nfor facilitating availability of quality books across geographies, languages, \\ngenres and levels, and device agnostic accessibility. States will be \\nencouraged to set up physical libraries for them at panchayat and ward \\nlevels and provide infrastructure for accessing the National Digital Library \\nresources. \\n34. Additionally, to build a culture of reading, and to make up for \\npandemic-time learning loss, the National Book Trust, Children’s Book Trust \\nand other sources will be encouraged to provide and replenish non-'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 12, 'page_label': '13', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='pandemic-time learning loss, the National Book Trust, Children’s Book Trust \\nand other sources will be encouraged to provide and replenish non-\\ncurricular titles in regional languages and English to these physical libraries. \\nCollaboration with NGOs that work in literacy will also be a part of this \\ninitiative. To inculcate financial literacy, financial sector regulators and \\norganizations will be encouraged to provide age-appropriate reading \\nmaterial to these libraries.  \\nPriority 2: Reaching the Last Mile \\n35. Prime Minister Vajpayee’s government had formed the Ministry of \\nTribal Affairs and the Department of Development of North-Eastern Region. \\nTo provide a sharper focus to the objective of ‘reaching the last mile’, our \\ngovernment has formed the ministries of AYUSH, Fisheries, Animal \\nHusbandry and Dairying, Skill Development, Jal Shakti and Cooperation.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 13, 'page_label': '14', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='10 \\n \\n \\n \\nAspirational Districts and Blocks Programme \\n36. Building on the success of the Aspirational Districts Programme, the \\nGovernment has recently launched the Aspirational Blocks Programme \\ncovering 500 blocks for saturation of essential government services across \\nmultiple domains such as health, nutrition, education, agriculture, water \\nresources, financial inclusion, skill development, and basic infrastructure. \\nPradhan Mantri PVTG Development Mission \\n37. To improve socio-economic conditions of the particularly vulnerable \\ntribal groups (PVTGs), Pradhan Mantri PVTG Development Mission will be \\nlaunched. This will saturate PVTG families and habitations with basic \\nfacilities such as safe housing, clean drinking water and sanitation, \\nimproved access to education, health and nutrition, road and telecom \\nconnectivity, and sustainable livelihood opportunities. An amount  \\nof ` 15,000 crore will be made available to implement the Mission in the'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 13, 'page_label': '14', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='connectivity, and sustainable livelihood opportunities. An amount  \\nof ` 15,000 crore will be made available to implement the Mission in the \\nnext three years under the Development Action Plan for the Scheduled \\nTribes.  \\nEklavya Model Residential Schools \\n38. In the next three years, centre will recruit 38,800 teachers and \\nsupport staff for the 740 Eklavya Model Residential Schools, serving 3.5 lakh \\ntribal students. \\nWater for Drought Prone Region \\n39. In the drought prone central region of Karnataka, central assistance \\nof ` 5,300 crore will be given to Upper Bhadra Project to provide \\nsustainable micro irrigation and filling up of surface tanks for drinking \\nwater.  \\nPM Awas Yojana \\n40. The outlay for PM Awas Yojana is being enhanced \\n by 66 per cent to over ` 79,000 crore.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 14, 'page_label': '15', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='11 \\n \\n \\n \\nBharat Shared Repository of Inscriptions (Bharat SHRI) \\n41. ‘Bharat Shared Repository of Inscriptions’ will be set up in a digital \\nepigraphy museum, with digitization of one lakh ancient inscriptions in the \\nfirst stage.   \\nSupport for poor prisoners \\n42. For poor persons who are in prisons and unable to afford the \\npenalty or the bail amount, required financial support will be provided.   \\n \\nPriority 3: Infrastructure & Investment \\n43. Investments in Infrastructure and productive capacity have a large \\nmultiplier impact on growth and employment. After the subdued period of \\nthe pandemic, private investments are growing again. The Budget takes the \\nlead once again to ramp up the virtuous cycle of investment and job \\ncreation.    \\n \\nCapital Investment as driver of growth and jobs \\n44. Capital investment outlay is being increased steeply for the third \\nyear in a row by 33 per cent to ` 10 lakh crore, which would be 3.3 per cent'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 14, 'page_label': '15', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='Capital Investment as driver of growth and jobs \\n44. Capital investment outlay is being increased steeply for the third \\nyear in a row by 33 per cent to ` 10 lakh crore, which would be 3.3 per cent \\nof GDP. This will be almost three times the outlay in 2019-20.   \\n45. This substantial increase in recent years is central to the \\ngovernment’s efforts to enhance growth potential and job creation, crowd-\\nin private investments, and provide a cushion against global headwinds. \\nEffective Capital Expenditure  \\n46. The direct capital investment by the Centre is complemented by the \\nprovision made for creation of capital assets through Grants-in-Aid to \\nStates. The ‘Effective Capital Expenditure’ of the Centre is budgeted at  \\n` 13.7 lakh crore, which will be 4.5 per cent of GDP.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 15, 'page_label': '16', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='12 \\n \\n \\n \\nSupport to State Governments for Capital Investment \\n47. I have decided to continue the 50-year interest free loan to state \\ngovernments for one more year to spur investment in infrastructure and to \\nincentivize them for complementary policy actions, with a significantly \\nenhanced outlay of ` 1.3 lakh crore.   \\nEnhancing opportunities for private investment in Infrastructure \\n48. The newly established Infrastructure Finance Secretariat will assist \\nall stakeholders for more private investment in infrastructure, including \\nrailways, roads, urban infrastructure and power, which are predominantly \\ndependent on public resources.  \\nHarmonized Master List of Infrastructure \\n49. The Harmonized Master List of Infrastructure will be reviewed by an \\nexpert committee for recommending the classification and financing \\nframework suitable for Amrit Kaal. \\nRailways \\n50. A capital outlay of ` 2.40 lakh crore has been provided for the'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 15, 'page_label': '16', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='expert committee for recommending the classification and financing \\nframework suitable for Amrit Kaal. \\nRailways \\n50. A capital outlay of ` 2.40 lakh crore has been provided for the \\nRailways. This highest ever outlay is about 9 times the outlay made in 2013-\\n14.  \\nLogistics \\n51. One hundred critical transport infrastructure projects, for last and \\nfirst mile connectivity for ports, coal, steel, fertilizer, and food grains sectors \\nhave been identified. They will be taken up on priority with investment of \\n` 75,000 crore, including ` 15,000 crore from private sources. \\nRegional Connectivity \\n52. Fifty additional airports, heliports, water aerodromes and advance \\nlanding grounds will be revived for improving regional air connectivity.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 16, 'page_label': '17', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='13 \\n \\n \\n \\nSustainable Cities of Tomorrow \\n53. States and cities will be encouraged to undertake urban planning \\nreforms and actions to transform our cities into ‘sustainable cities of \\ntomorrow’. This means efficient use of land resources, adequate resources \\nfor urban infrastructure, transit-oriented development, enhanced \\navailability and affordability of urban land, and opportunities for all.  \\nMaking Cities ready for Municipal Bonds \\n54. Through property tax governance reforms and ring-fencing user \\ncharges on urban infrastructure, cities will be incentivized to improve their \\ncredit worthiness for municipal bonds.   \\nUrban Infrastructure Development Fund  \\n55. Like the RIDF, an Urban Infrastructure Development Fund (UIDF) will \\nbe established through use of priority sector lending shortfall. This will be \\nmanaged by the National Housing Bank, and will be used by public agencies \\nto create urban infrastructure in Tier 2 and Tier 3 cities. States will be'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 16, 'page_label': '17', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='managed by the National Housing Bank, and will be used by public agencies \\nto create urban infrastructure in Tier 2 and Tier 3 cities. States will be \\nencouraged to leverage resources from the grants of the 15 th Finance \\nCommission, as well as existing schemes, to adopt appropriate user charges \\nwhile accessing the UIDF. We expect to make  \\navailable ` 10,000 crore per annum for this purpose. \\nUrban Sanitation \\n56. All cities and towns will be enabled for 100 per cent mechanical \\ndesludging of septic tanks and sewers to transition from manhole to \\nmachine-hole mode. Enhanced focus will be provided for scientific \\nmanagement of dry and wet waste. \\nPriority 4: Unleashing the Potential \\n57. “Good Governance is the key to a nation’s progress. Our government \\nis committed to providing a transparent and accountable administration \\nwhich works for the betterment and welfare of the common citizen,”  said \\nHon’ble Prime Minister.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 17, 'page_label': '18', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='14 \\n \\n \\n \\nMission Karmayogi \\n58. Under Mission Karmayogi, Centre, States and Union Territories are \\nmaking and implementing capacity-building plans for civil servants. The \\ngovernment has also launched an integrated online training platform, iGOT \\nKarmayogi, to provide continuous learning opportunities for lakhs of \\ngovernment employees to upgrade their skills and facilitate people-centric \\napproach.   \\n59. For enhancing ease of doing business, more than  \\n39,000 compliances have been reduced and more than  \\n3,400 legal provisions have been decriminalized. For furthering the trust-\\nbased governance, we have introduced the Jan Vishwas Bill to amend 42 \\nCentral Acts. This Budget proposes a series of measures to unleash the \\npotential of our economy.  \\nCentres of Excellence for Artificial Intelligence \\n60. For realizing the vision of “Make AI in India and Make AI work for \\nIndia”, three centres of excellence for Artificial Intelligence will be set-up in'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 17, 'page_label': '18', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='60. For realizing the vision of “Make AI in India and Make AI work for \\nIndia”, three centres of excellence for Artificial Intelligence will be set-up in \\ntop educational institutions. Leading industry players will partner in \\nconducting interdisciplinary research, develop cutting-edge applications and \\nscalable problem solutions in the areas of agriculture, health, and \\nsustainable cities. This will galvanize an effective AI ecosystem and nurture \\nquality human resources in the field. \\nNational Data Governance Policy \\n61. To unleash innovation and research by start-ups and academia, a \\nNational Data Governance Policy will be brought out. This will enable access \\nto anonymized data. \\nSimplification of Know Your Customer (KYC) process  \\n62. The KYC process will be simplified adopting a ‘risk-based’ instead of \\n‘one size fits all’ approach. The financial sector regulators will also be'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 18, 'page_label': '19', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='15 \\n \\n \\n \\nencouraged to have a KYC system fully amenable to meet the needs of \\nDigital India. \\nOne stop solution for identity and address updating \\n63. A one stop solution for reconciliation and updating of identity and \\naddress of individuals maintained by various government agencies, \\nregulators and regulated entities will be established using DigiLocker service \\nand Aadhaar as foundational identity.    \\nCommon Business Identifier   \\n64. For the business establishments required to have a Permanent \\nAccount Number (PAN), the PAN will be used as the common identifier for \\nall digital systems of specified government agencies. This will bring ease of \\ndoing business; and it will be facilitated through a legal mandate. \\nUnified Filing Process \\n65. For obviating the need for separate submission of same information \\nto different government agencies, a system of ‘Unified Filing Process’ will be \\nset-up. Such filing of information or return in simplified forms on a common'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 18, 'page_label': '19', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='to different government agencies, a system of ‘Unified Filing Process’ will be \\nset-up. Such filing of information or return in simplified forms on a common \\nportal, will be shared with other agencies as per filer’s choice.  \\nVivad se Vishwas I – Relief for MSMEs  \\n66. In cases of failure by MSMEs to execute contracts during the Covid \\nperiod, 95 per cent of the forfeited amount relating to bid or performance \\nsecurity, will be returned to them by government and government \\nundertakings.  This will provide relief to MSMEs.  \\nVivad se Vishwas II – Settling Contractual Disputes  \\n67. To settle contractual disputes of government and government \\nundertakings, wherein arbitral award is under challenge in a court, a \\nvoluntary settlement scheme with standardized terms will be introduced. \\nThis will be done by offering graded settlement terms depending on \\npendency level of the dispute.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 19, 'page_label': '20', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='16 \\n \\n \\n \\nState Support Mission  \\n68. The State Support Mission of NITI Aayog will be continued for three \\nyears for our collective efforts towards national priorities. \\nResult Based Financing  \\n69. To better allocate scarce resources for competing development \\nneeds, the financing of select schemes will be changed, on a pilot basis, \\nfrom ‘input-based’ to ‘result-based’. \\nE-Courts  \\n70. For efficient administration of justice, Phase-3 of the \\n E-Courts project will be launched with an outlay  \\nof ` 7,000 crore.  \\nFintech Services  \\n71. Fintech services in India have been facilitated by our digital public \\ninfrastructure including Aadhaar, PM Jan Dhan Yojana, Video KYC, India \\nStack and UPI. To enable more Fintech innovative services, the scope of \\ndocuments available in DigiLocker for individuals will be expanded.  \\nEntity DigiLocker  \\n72. An Entity DigiLocker will be set up for use by MSMEs, large business \\nand charitable trusts. This will be towards storing and sharing documents'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 19, 'page_label': '20', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='Entity DigiLocker  \\n72. An Entity DigiLocker will be set up for use by MSMEs, large business \\nand charitable trusts. This will be towards storing and sharing documents \\nonline securely, whenever needed, with various authorities, regulators, \\nbanks and other business entities.  \\n5G Services \\n73. One hundred labs for developing applications using  \\n5G services will be set up in engineering institutions to realise a new range \\nof opportunities, business models, and employment potential. The  labs will \\ncover, among others, applications such as smart classrooms, precision \\nfarming, intelligent transport systems, and health care applications.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 20, 'page_label': '21', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='17 \\n \\n \\n \\nLab Grown Diamonds \\n74. Lab Grown Diamonds (LGD) is a technology-and innovation-driven \\nemerging sector with high employment potential. These environment-\\nfriendly diamonds which have optically and chemically the same properties \\nas natural diamonds. To encourage indigenous production of LGD seeds and \\nmachines and to reduce import dependency, a research and development \\ngrant will be provided to one of the IITs for five years.   \\n75. To reduce the cost of production, a proposal to review the custom \\nduty rate on LGD seeds will be indicated in Part B of the speech.   \\nPriority 5: Green Growth \\n76. Hon’ble Prime Minister has given a vision for “LiFE”, or Lifestyle for \\nEnvironment, to spur a movement of environmentally conscious lifestyle. \\nIndia is moving forward firmly for the ‘panchamrit’ and net-zero carbon \\nemission by 2070 to usher in green industrial and economic transition. This \\nBudget builds on our focus on green growth.    \\nGreen Hydrogen Mission'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 20, 'page_label': '21', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='emission by 2070 to usher in green industrial and economic transition. This \\nBudget builds on our focus on green growth.    \\nGreen Hydrogen Mission \\n77. The recently launched National Green Hydrogen Mission, with an \\noutlay of ` 19,700 crores, will facilitate transition of the economy to low \\ncarbon intensity, reduce dependence on fossil fuel imports, and make the \\ncountry assume technology and market leadership in this sunrise sector. \\nOur target is to reach an annual production of 5 MMT by 2030.  \\nEnergy Transition \\n78. This Budget provides ` 35,000 crore for priority capital investments \\ntowards energy transition and net zero objectives, and energy security by \\nMinistry of Petroleum & Natural Gas.  \\nEnergy Storage Projects \\n79. To steer the economy on the sustainable development path, Battery \\nEnergy Storage Systems with capacity of 4,000 MWH will be supported with'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 21, 'page_label': '22', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='18 \\n \\n \\n \\nViability Gap Funding. A detailed framework for Pumped Storage Projects \\nwill also be formulated.  \\nRenewable Energy Evacuation \\n80. The Inter-state transmission system for evacuation and grid \\nintegration of 13 GW renewable energy from Ladakh will be constructed \\nwith investment of ` 20,700 crore including central support of ` 8,300 crore. \\nGreen Credit Programme \\n81. For encouraging behavioural change, a Green Credit Programme will \\nbe notified under the Environment (Protection) Act. This will incentivize \\nenvironmentally sustainable and responsive actions by companies, \\nindividuals and local bodies, and help mobilize additional resources for such \\nactivities.  \\nPM-PRANAM \\n82. “PM Programme for Restoration, Awareness, Nourishment and \\nAmelioration of Mother Earth” will be launched to incentivize States and \\nUnion Territories to promote alternative fertilizers and balanced use of \\nchemical fertilizers. \\nGOBARdhan scheme'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 21, 'page_label': '22', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='Amelioration of Mother Earth” will be launched to incentivize States and \\nUnion Territories to promote alternative fertilizers and balanced use of \\nchemical fertilizers. \\nGOBARdhan scheme \\n83. 500 new ‘waste to wealth’ plants under GOBARdhan (Galvanizing \\nOrganic Bio-Agro Resources Dhan) scheme will be established for promoting \\ncircular economy. These will include 200 compressed biogas (CBG) plants, \\nincluding 75 plants in urban areas, and 300 community or cluster-based \\nplants at total investment of ` 10,000 crore. I will refer to this in Part B. In \\ndue course, a 5 per cent CBG mandate will be introduced for all \\norganizations marketing natural and bio gas. For collection of bio-mass and \\ndistribution of bio-manure, appropriate fiscal support will be provided.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 22, 'page_label': '23', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='19 \\n \\n \\n \\nBhartiya Prakritik Kheti Bio-Input Resource Centres   \\n84. Over the next 3 years, we will facilitate 1 crore farmers to adopt \\nnatural farming. For this, 10,000 Bio-Input Resource Centres will be set-up, \\ncreating a national-level distributed micro-fertilizer and pesticide \\nmanufacturing network.  \\nMISHTI \\n85. Building on India’s success in afforestation, ‘Mangrove Initiative for \\nShoreline Habitats & Tangible Incomes’, MISHTI, will be taken up for \\nmangrove plantation along the coastline and on salt pan lands, wherever \\nfeasible, through convergence between MGNREGS, CAMPA Fund and other \\nsources. \\nAmrit Dharohar \\n86. Wetlands are vital ecosystems which sustain biological diversity. In \\nhis latest Mann Ki Baat, the Prime Minister said, “Now the total number of \\nRamsar sites in our country has increased to 75. Whereas, before 2014, \\nthere were only 26…”  Local communities have always been at the forefront \\nof conservation efforts. The government will promote their unique'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 22, 'page_label': '23', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='there were only 26…”  Local communities have always been at the forefront \\nof conservation efforts. The government will promote their unique \\nconservation values through Amrit Dharohar , a scheme that will be \\nimplemented over the next three years to encourage optimal use of \\nwetlands, and enhance bio-diversity, carbon stock,  \\neco-tourism opportunities and income generation for local communities.  \\nCoastal Shipping \\n87. Coastal shipping will be promoted as the energy efficient and lower \\ncost mode of transport, both for passengers and freight, through PPP mode \\nwith viability gap funding.   \\nVehicle Replacement \\n88. Replacing old polluting vehicles is an important part of greening our \\neconomy. In furtherance of the vehicle scrapping policy mentioned in \\nBudget 2021-22, I have allocated adequate funds to scrap old vehicles of'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 23, 'page_label': '24', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='20 \\n \\n \\n \\nthe Central Government. States will also be supported in replacing old \\nvehicles and ambulances.  \\nPriority 6: Youth Power  \\n89. To empower our youth and help the ‘ Amrit Peedhi ’ realize their \\ndreams, we have formulated the National Education Policy, focused on \\nskilling, adopted economic policies that facilitate job creation at scale, and \\nhave supported business opportunities.   \\nPradhan Mantri Kaushal Vikas Yojana 4.0 \\n90. Pradhan Mantri Kaushal Vikas Yojana 4.0 will be launched to skill \\nlakhs of youth within the next three years.  On-job training, industry \\npartnership, and alignment of courses with needs of industry will be \\nemphasized. The scheme will also cover new age courses for Industry 4.0 \\nlike coding, AI, robotics, mechatronics, IOT, 3D printing, drones, and soft \\nskills. To skill youth for international opportunities, 30 Skill India \\nInternational Centres will be set up across different States.  \\n \\nSkill India Digital Platform'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 23, 'page_label': '24', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='skills. To skill youth for international opportunities, 30 Skill India \\nInternational Centres will be set up across different States.  \\n \\nSkill India Digital Platform \\n91. The digital ecosystem for skilling will be further expanded with the \\nlaunch of a unified Skill India Digital platform for: \\n\\uf0b7 enabling demand-based formal skilling,  \\n\\uf0b7 linking with employers including MSMEs, and \\n\\uf0b7 facilitating access to entrepreneurship schemes.  \\nNational Apprenticeship Promotion Scheme \\n92. To provide stipend support to 47 lakh youth in three years, Direct \\nBenefit Transfer under a pan-India National Apprenticeship Promotion \\nScheme will be rolled out.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 24, 'page_label': '25', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='21 \\n \\n \\n \\nTourism \\n93. With an integrated and innovative approach, at  \\nleast 50 destinations will be selected through challenge mode. In addition to \\naspects such as physical connectivity, virtual connectivity, tourist guides, \\nhigh standards for food streets and tourists’ security, all the relevant \\naspects would be made available on an App to enhance tourist experience. \\nEvery destination would be developed as a complete package. The focus of \\ndevelopment of tourism would be on domestic as well as foreign tourists.  \\n94. Sector specific skilling and entrepreneurship development will be \\ndovetailed to achieve the objectives of the ‘Dekho Apna Desh’ initiative. \\nThis was launched as an appeal by the Prime Minister to the middle class to \\nprefer domestic tourism over international tourism. For integrated \\ndevelopment of theme-based tourist circuits, the ‘Swadesh Darshan \\nScheme’ was also launched. Under the Vibrant Villages Programme, tourism'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 24, 'page_label': '25', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='development of theme-based tourist circuits, the ‘Swadesh Darshan \\nScheme’ was also launched. Under the Vibrant Villages Programme, tourism \\ninfrastructure and amenities will also be facilitated in border villages.  \\nUnity Mall \\n95. States will be encouraged to set up a Unity Mall in their state capital \\nor most prominent tourism centre or the financial capital for promotion and \\nsale of their own ODOPs (one district, one product), GI products and other \\nhandicraft products, and for providing space for such products of all other \\nStates.   \\nPriority 7: Financial Sector \\n96. Our reforms in the financial sector and innovative use of technology \\nhave led to financial inclusion at scale, better and faster service delivery, \\nease of access to credit and participation in financial markets. This Budget \\nproposes to further these measures.    \\nCredit Guarantee for MSMEs \\n97. Last year, I proposed revamping of the credit guarantee scheme for'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 24, 'page_label': '25', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='proposes to further these measures.    \\nCredit Guarantee for MSMEs \\n97. Last year, I proposed revamping of the credit guarantee scheme for \\nMSMEs. I am happy to announce that the revamped scheme will take effect'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 25, 'page_label': '26', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='22 \\n \\n \\n \\nfrom 1st April 2023 through infusion of ` 9,000 crore in the corpus. This will \\nenable additional collateral-free guaranteed credit of ` 2 lakh crore. \\nFurther, the cost of the credit will be reduced by about 1 per cent.     \\nNational Financial Information Registry \\n98. A national financial information registry will be set up to serve as the \\ncentral repository of financial and ancillary information. This will facilitate \\nefficient flow of credit, promote financial inclusion, and foster financial \\nstability. A new legislative framework will govern this credit public \\ninfrastructure, and it will be designed in consultation with the RBI. \\nFinancial Sector Regulations  \\n99. To meet the needs of Amrit Kaal  and to facilitate optimum \\nregulation in the financial sector, public consultation, as necessary and \\nfeasible, will be brought to the process of regulation-making and issuing \\nsubsidiary directions. \\n100. To simplify, ease and reduce cost of compliance, financial sector'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 25, 'page_label': '26', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='feasible, will be brought to the process of regulation-making and issuing \\nsubsidiary directions. \\n100. To simplify, ease and reduce cost of compliance, financial sector \\nregulators will be requested to carry out a comprehensive review of existing \\nregulations. For this, they will consider suggestions from public and \\nregulated entities. Time limits to decide the applications under various \\nregulations will also be laid down. \\nGIFT IFSC  \\n101. To enhance business activities in GIFT IFSC, the following measures \\nwill be taken: \\n\\uf0b7 Delegating powers under the SEZ Act to IFSCA to avoid dual \\nregulation, \\n\\uf0b7 Setting up a single window IT system for registration and \\napproval from IFSCA, SEZ authorities, GSTN, RBI, SEBI and \\nIRDAI,'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 26, 'page_label': '27', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='23 \\n \\n \\n \\n\\uf0b7 Permitting acquisition financing by IFSC Banking Units of \\nforeign banks,  \\n\\uf0b7 Establishing a subsidiary of EXIM Bank for trade  \\nre-financing, \\n\\uf0b7 Amending IFSCA Act for statutory provisions for arbitration, \\nancillary services, and avoiding dual regulation under SEZ Act, \\nand \\n\\uf0b7 Recognizing offshore derivative instruments as valid contracts.  \\n \\nData Embassy \\n102. For countries looking for digital continuity solutions, we will \\nfacilitate setting up of their Data Embassies in GIFT IFSC.  \\nImproving Governance and Investor Protection in Banking Sector \\n103. To improve bank governance and enhance investors’ protection, \\ncertain amendments to the Banking Regulation Act, the Banking Companies \\nAct and the Reserve Bank of India Act are proposed. \\n \\nCa\\npacity Building in Securities Market \\n104. To build capacity of functionaries and professionals in the securities \\nmarket, SEBI will be empowered to develop, regulate, maintain and enforce'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 26, 'page_label': '27', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='Ca\\npacity Building in Securities Market \\n104. To build capacity of functionaries and professionals in the securities \\nmarket, SEBI will be empowered to develop, regulate, maintain and enforce \\nnorms and standards for education in the National Institute of Securities \\nMarkets and to recognize award of degrees, diplomas and certificates.  \\nCentral Data Processing Centre  \\n105. A Central Processing Centre will be setup for faster response to \\ncompanies through centralized handling of various forms filed with field \\noffices under the Companies Act.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 27, 'page_label': '28', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='24 \\n \\n \\n \\nReclaiming of shares and dividends  \\n106. For investors to reclaim unclaimed shares and unpaid dividends \\nfrom the Investor Education and Protection Fund Authority with ease, an \\nintegrated IT portal will be established. \\nDigital Payments  \\n107. Digital payments continue to find wide acceptance. In 2022, they \\nshow increase of 76 per cent in transactions  \\nand 91 per cent in value. Fiscal support for this digital public infrastructure \\nwill continue in 2023-24.  \\nAzadi Ka Amrit Mahotsav Mahila Samman Bachat Patra  \\n108. For commemorating Azadi Ka Amrit Mahotsav, a one-time new small \\nsavings scheme, Mahila Samman Savings Certificate, will be made available \\nfor a two-year period up to March 2025. This will offer deposit facility upto \\n` 2 lakh in the name of women or girls for a tenor of 2 years at fixed \\ninterest rate of 7.5 per cent with partial withdrawal option.  \\nSenior Citizens \\n109. The maximum deposit limit for Senior Citizen Savings Scheme will be'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 27, 'page_label': '28', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='interest rate of 7.5 per cent with partial withdrawal option.  \\nSenior Citizens \\n109. The maximum deposit limit for Senior Citizen Savings Scheme will be \\nenhanced from ` 15 lakh to ` 30 lakh. \\n110.  The maximum deposit limit for Monthly Income Account Scheme \\nwill be enhanced from ` 4.5 lakh to ` 9 lakh for single account and from ` 9 \\nlakh to ` 15 lakh for joint account. \\nFiscal Management \\nFifty-year interest free loan to States \\n111. The entire fifty-year loan to states has to be spent on capital \\nexpenditure within 2023-24. Most of this will be at the discretion of states, \\nbut a part will be conditional on states increasing their actual capital'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 28, 'page_label': '29', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='25 \\n \\n \\n \\nexpenditure. Parts of the outlay will also be linked to, or allocated for, the \\nfollowing purposes: \\n\\uf0b7 Scrapping old government vehicles, \\n\\uf0b7 Urban planning reforms and actions, \\n\\uf0b7 Financing reforms in urban local bodies to make them \\ncreditworthy for municipal bonds, \\n\\uf0b7 Housing for police personnel above or as part of police stations,  \\n\\uf0b7 Constructing Unity Malls, \\n\\uf0b7 Children and adolescents’ libraries and digital infrastructure, \\nand \\n\\uf0b7 State share of capital expenditure of central schemes. \\nFiscal Deficit of States \\n112. States will be allowed a fiscal deficit of 3.5 per cent of GSDP of which \\n0.5 per cent will be tied to power sector reforms.  \\nRevised Estimates 2022-23 \\n113. The Revised Estimate of the total receipts other than borrowings is  \\n` 24.3 lakh crore, of which the net tax receipts  \\nare ` 20.9 lakh crore. The Revised Estimate of the total expenditure is  \\n` 41.9 lakh crore, of which the capital expenditure is about ` 7.3 lakh crore.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 28, 'page_label': '29', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='are ` 20.9 lakh crore. The Revised Estimate of the total expenditure is  \\n` 41.9 lakh crore, of which the capital expenditure is about ` 7.3 lakh crore.      \\n114. The Revised Estimate of the fiscal deficit is 6.4 per cent of GDP, \\nadhering to the Budget Estimate.        \\nBudget Estimates 2023-24 \\n115. Coming to 2023-24, the total receipts other than borrowings and the \\ntotal expenditure are estimated at ` 27.2 lakh crore and ` 45 lakh crore \\nrespectively. The net tax receipts are estimated at ` 23.3 lakh crore.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 29, 'page_label': '30', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='26 \\n \\n \\n \\n116. The fiscal deficit is estimated to be 5.9 per cent of GDP. In my \\nBudget Speech for 2021-22, I had announced that we plan to continue the \\npath of fiscal consolidation, reaching a fiscal deficit below 4.5 per cent by \\n2025-26 with a fairly steady decline over the period. We have adhered to \\nthis path, and I reiterate my intention to bring the fiscal deficit below 4.5 \\nper cent of GDP by 2025-26.  \\n117.  To finance the fiscal deficit in 2023-24, the net market borrowings \\nfrom dated securities are estimated at ` 11.8 lakh crore. The balance \\nfinancing is expected to come from small savings and other sources. The \\ngross market borrowings are estimated at ` 15.4 lakh crore. \\nI will, now, move to Part B.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 30, 'page_label': '31', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='27 \\n \\n \\n \\nPART B \\nIndirect Taxes \\n118. My indirect tax proposals aim to promote exports, boost domestic \\nmanufacturing, enhance domestic value addition, encourage green energy \\nand mobility.  \\n119. A simplified tax structure with fewer tax rates helps in reducing \\ncompliance burden and improving tax administration. I propose to reduce \\nthe number of basic customs duty rates on goods, other than textiles and \\nagriculture, from 21 to 13. As a result, there are minor changes in the basic \\ncustom duties, cesses and surcharges on some items including toys, \\nbicycles, automobiles and naphtha. \\n \\nGreen Mobility \\n120. To avoid cascading of taxes on blended compressed natural gas, I \\npropose to exempt excise duty on GST-paid compressed bio gas contained \\nin it. To further provide impetus to green mobility, customs duty exemption \\nis being extended to import of capital goods and machinery required for \\nmanufacture of lithium-ion cells for batteries used in electric vehicles. \\nElectronics'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 30, 'page_label': '31', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='is being extended to import of capital goods and machinery required for \\nmanufacture of lithium-ion cells for batteries used in electric vehicles. \\nElectronics  \\n121. As a result of various initiatives of the Government, including the \\nPhased Manufacturing programme, mobile phone production in India has \\nincreased from 5.8 crore units valued at about ` 18,900 crore in 2014-15 to \\n31 crore units valued at over ` 2,75,000 crore in the last financial year. To \\nfurther deepen domestic value addition in manufacture of mobile phones, I \\npropose to provide relief in customs duty on import of certain parts and \\ninputs like camera lens and continue the concessional duty on lithium-ion \\ncells for batteries for another year.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 31, 'page_label': '32', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='28 \\n \\n \\n \\n122. Similarly, to promote value addition in manufacture of televisions, I \\npropose to reduce the basic customs duty on parts of open cells of TV \\npanels to 2.5 per cent.  \\nElectrical  \\n123. To rectify inversion of duty structure and encourage manufacturing \\nof electric kitchen chimneys, the basic customs duty on electric kitchen \\nchimney is being increased from 7.5 per cent to 15 per cent and that on \\nheat coils for these is proposed to be reduced from 20 per cent to 15 per \\ncent. \\nChemicals and Petrochemicals  \\n124. Denatured ethyl alcohol is used in chemical industry. \\n I propose to exempt basic customs duty on it. This will also support the \\nEthanol Blending Programme and facilitate our endeavour for energy \\ntransition. Basic customs duty is also being reduced on acid grade fluorspar \\nfrom 5 per cent to 2.5 per cent to make the domestic fluorochemicals \\nindustry competitive. Further, the basic customs duty on crude glycerin for'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 31, 'page_label': '32', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='from 5 per cent to 2.5 per cent to make the domestic fluorochemicals \\nindustry competitive. Further, the basic customs duty on crude glycerin for \\nuse in manufacture of epicholorhydrin is proposed to be reduced from 7.5 \\nper cent to 2.5 per cent. \\nMarine products \\n125. In the last financial year, marine products recorded the highest \\nexport growth benefitting farmers in the coastal states of the country. To \\nfurther enhance the export competitiveness of marine products, \\nparticularly shrimps, duty is being reduced on key inputs for domestic \\nmanufacture of shrimp feed. \\nLab Grown Diamonds \\n126. India is a global leader in cutting and polishing of natural diamonds, \\ncontributing about three-fourths of the global turnover by value. With the \\ndepletion in deposits of natural diamonds, the industry is moving towards \\nLab Grown Diamonds (LGDs) and it holds huge promise. To seize this'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 32, 'page_label': '33', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='29 \\n \\n \\n \\nopportunity, I propose to reduce basic customs duty on seeds used in their \\nmanufacture.  \\n \\nPrecious Metals \\n127. Customs Duties on dore and bars of gold and platinum were \\nincreased earlier this fiscal. I now propose to increase the duties on articles \\nmade therefrom to enhance the duty differential. I also propose to increase \\nthe import duty on silver dore, bars and articles to align them with that on \\ngold and platinum. \\nMetals \\n128. To facilitate availability of raw materials for the steel sector, \\nexemption from Basic Customs Duty on raw materials for manufacture of \\nCRGO Steel, ferrous scrap and nickel cathode is being continued. \\n129. Similarly, the concessional BCD of 2.5 per cent on copper scrap is \\nalso being continued to ensure the availability of raw materials for \\nsecondary copper producers who are mainly in the MSME sector. \\nCompounded Rubber \\n130. The basic customs duty rate on compounded rubber is being'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 32, 'page_label': '33', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='secondary copper producers who are mainly in the MSME sector. \\nCompounded Rubber \\n130. The basic customs duty rate on compounded rubber is being \\nincreased from 10 per cent to ‘25 per cent or ` 30/kg whichever is lower’, at \\npar with that on natural rubber other than latex, to curb circumvention of \\nduty.  \\nCigarettes \\n131. National Calamity Contingent Duty (NCCD) on specified cigarettes \\nwas last revised three years ago. This is proposed to be revised upwards by \\nabout 16 per cent.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 33, 'page_label': '34', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='30 \\n \\n \\n \\n \\nDirect Taxes \\n132. I now come to my direct tax proposals. These proposals aim to \\nmaintain continuity and stability of taxation, further simplify and rationalise \\nvarious provisions to reduce the compliance burden, promote the \\nentrepreneurial spirit and provide tax relief to citizens. \\n133. It has been the constant endeavour of the Income Tax Department \\nto improve Tax Payers Services by making compliance easy and smooth. Our \\ntax payers’ portal received a maximum of 72 lakh returns in a day; \\nprocessed more than 6.5 crore returns this year; average processing period \\nreduced from 93 days in financial year 13-14 to 16 days now;  \\nand 45 per cent of the returns were processed within 24 hours. We intend \\nto further improve this, roll out a next-generation Common IT Return Form \\nfor tax payer convenience, and also plan to strengthen the grievance \\nredressal mechanism.  \\nMSMEs and Professionals  \\n134. MSMEs are growth engines of our economy.  Micro enterprises with'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 33, 'page_label': '34', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='for tax payer convenience, and also plan to strengthen the grievance \\nredressal mechanism.  \\nMSMEs and Professionals  \\n134. MSMEs are growth engines of our economy.  Micro enterprises with \\nturnover up to ` 2 crore and certain professionals with turnover of up to  \\n` 50 lakh can avail the benefit of presumptive taxation. I propose to provide \\nenhanced limits of ` 3 crore and ` 75 lakh respectively, to the tax payers \\nwhose cash receipts are no more than 5 per cent. Moreover, to support \\nMSMEs in timely receipt of payments, I propose to allow deduction for \\nexpenditure incurred on payments made to them only when payment is \\nactually made.  \\nCooperation \\n135. Cooperation is a value to be cherished. In realizing our Prime \\nMinister’s goal of “Sahkar se Samriddhi ”, and his resolve to “connect the \\nspirit of cooperation with the spirit of Amrit Kaal”, in addition to the \\nmeasures proposed in Part A, I have a slew of proposals for the co-operative \\nsector.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 34, 'page_label': '35', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='31 \\n \\n \\n \\n136. First, new co-operatives that commence manufacturing activities till \\n31.3.2024 shall get the benefit of a lower tax rate of 15 per cent, as is \\npresently available to new manufacturing companies. \\n137. Secondly, I propose to provide an opportunity to sugar co-operatives \\nto claim payments made to sugarcane farmers for the period prior to \\nassessment year 2016-17 as expenditure. This is expected to provide them \\nwith a relief of almost ` 10,000 crore.  \\n138. Thirdly, I am providing a higher limit of ` 2 lakh per member for cash \\ndeposits to and loans in cash by Primary Agricultural Co-operative Societies \\n(PACS) and Primary Co-operative Agriculture and Rural Development Banks \\n(PCARDBs).  \\n139. Similarly, a higher limit of ` 3 crore for TDS on cash withdrawal is \\nbeing provided to co-operative societies. \\nStart-Ups \\n140. Entrepreneurship is vital for a country’s economic development. We \\nhave taken a number of measures for start-ups and they have borne results.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 34, 'page_label': '35', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='Start-Ups \\n140. Entrepreneurship is vital for a country’s economic development. We \\nhave taken a number of measures for start-ups and they have borne results. \\nIndia is now the third largest ecosystem for start-ups globally, and ranks \\nsecond in innovation quality among middle-income countries. I propose to \\nextend the date of incorporation for income tax benefits to start-ups from \\n31.03.23 to 31.3.24. I further propose to provide the benefit of carry \\nforward of losses on change of shareholding of start-ups from seven years \\nof incorporation to ten years. \\nAppeals \\n141. To reduce the pendency of appeals at Commissioner level, I propose \\nto deploy about 100 Joint Commissioners for disposal of small appeals. We \\nshall also be more selective in taking up cases for scrutiny of returns already \\nreceived this year.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 35, 'page_label': '36', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='32 \\n \\n \\n \\nBetter targeting of tax concessions \\n142. For better targeting of tax concessions and exemptions, \\n I propose to cap deduction from capital gains on investment in residential \\nhouse under sections 54 and 54F to ` 10 crore. Another proposal with \\nsimilar intent is to limit income tax exemption from proceeds of insurance \\npolicies with very high value. \\nRationalisation \\n143. There are a number of proposals relating to rationalisation and \\nsimplification. Income of authorities, boards and commissions set up by \\nstatutes of the Union or State for the purpose of housing, development of \\ncities, towns and villages, and regulating, or regulating and developing an \\nactivity or matter, is proposed to be exempted from income tax. Other \\nmajor measures in this direction are: \\n\\uf0b7 Removing the minimum threshold of ` 10,000/- for TDS and \\nclarifying taxability relating to online gaming; \\n\\uf0b7 Not treating conversion of gold into electronic gold receipt and vice \\nversa as capital gain;'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 35, 'page_label': '36', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='clarifying taxability relating to online gaming; \\n\\uf0b7 Not treating conversion of gold into electronic gold receipt and vice \\nversa as capital gain;  \\n\\uf0b7 Reducing the TDS rate from 30 per cent to 20 per cent on taxable \\nportion of EPF withdrawal in non-PAN cases; and \\n\\uf0b7 Taxation on income from Market Linked Debentures. \\nOthers \\n144. Other major proposals in the Finance Bill relate to the following: \\n\\uf0b7 Extension of period of tax benefits to funds relocating to IFSC, GIFT \\nCity till 31.03.2025; \\n\\uf0b7 Decriminalisation under section 276A of the Income Tax Act; \\n\\uf0b7 Allowing carry forward of losses on strategic disinvestment including \\nthat of IDBI Bank; and \\n\\uf0b7 Providing EEE status to Agniveer Fund.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 36, 'page_label': '37', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='33 \\n \\n \\n \\nPersonal Income Tax \\n145. Now, I come to what everyone is waiting for -- personal income tax. I \\nhave five major announcements to make in this regard. These primarily \\nbenefit our hard-working middle class. \\n146. The first one concerns rebate. Currently, those with income up to  \\n` 5 lakh do not pay any income tax in both old and new tax regimes. I \\npropose to increase the rebate limit to ` 7 lakh in the new tax regime. Thus, \\npersons in the new tax regime, with income up to ` 7 lakh will not have to \\npay any tax.  \\n147. The second proposal relates to middle-class individuals. \\n I had introduced, in the year 2020, the new personal income tax regime \\nwith six income slabs starting from ` 2.5 lakh. I propose to change the tax \\nstructure in this regime by reducing the number of slabs to five and \\nincreasing the tax exemption limit to ` 3 lakh. The new tax rates are: \\n` 0-3 lakh Nil \\n` 3-6 lakh 5 per cent \\n` 6-9 lakh 10 per cent \\n` 9-12 lakh 15 per cent \\n` 12-15 lakh 20 per cent'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 36, 'page_label': '37', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='increasing the tax exemption limit to ` 3 lakh. The new tax rates are: \\n` 0-3 lakh Nil \\n` 3-6 lakh 5 per cent \\n` 6-9 lakh 10 per cent \\n` 9-12 lakh 15 per cent \\n` 12-15 lakh 20 per cent \\nAbove ` 15 lakh 30 per cent \\n \\n148. This will provide major relief to all tax payers in the new regime. An \\nindividual with an annual income of ` 9 lakh will be required to pay only  \\n` 45,000/-. This is only 5 per cent of his or her income. It is a reduction of 25 \\nper cent on what he or she is required to pay now, ie, ` 60,000/-. Similarly, \\nan individual with an income of ` 15 lakh would be required to pay only  \\n` 1.5 lakh or 10 per cent of his or her income, a reduction of 20 per cent \\nfrom the existing liability of ` 1,87,500/.  \\n149. My third proposal is for the salaried class and the pensioners \\nincluding family pensioners, for whom I propose to extend the benefit of'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 37, 'page_label': '38', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='34 \\n \\n \\n \\nstandard deduction to the new tax regime. Each salaried person with an \\nincome of ` 15.5 lakh or more will thus stand to benefit by ` 52,500. \\n150. My fourth announcement in personal income tax is regarding the \\nhighest tax rate which in our country is 42.74 per cent. This is among the \\nhighest in the world. I propose to reduce the highest surcharge rate from 37 \\nper cent to 25 per cent in the new tax regime. This would result in reduction \\nof the maximum tax rate to 39 per cent. \\n151. Lastly, the limit of ` 3 lakh for tax exemption on leave encashment \\non retirement of non-government salaried employees was last fixed in the \\nyear 2002, when the highest basic pay in the government was ` 30,000/- \\npm. In line with the increase in government salaries, I am proposing to \\nincrease this limit to ` 25 lakh. \\n152. We are also making the new income tax regime as the default tax \\nregime. However, citizens will continue to have the option to avail the \\nbenefit of the old tax regime.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 37, 'page_label': '38', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='152. We are also making the new income tax regime as the default tax \\nregime. However, citizens will continue to have the option to avail the \\nbenefit of the old tax regime. \\n153. Apart from these, I am also making some other changes as given in \\nthe annexure. \\n154. As a result of these proposals, revenue of about ` 38,000 crore –  \\n` 37,000 crore in direct taxes and ` 1,000 crore in indirect taxes – will be \\nforgone while revenue of about ` 3,000 crore will be additionally mobilized. \\nThus, the total revenue forgone is about ` 35,000 crore annually. \\n155. Mr. Speaker Sir, with these words, I commend the Budget to this \\naugust House. \\n*****'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 38, 'page_label': '39', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='35 \\n \\n \\n \\nAnnexure to Part B of the Budget Speech 2023-24 \\nAmendments relating to Direct Taxes \\nA. PROVIDING TAX RELIEF UNDER NEW PERSONAL TAX REGIME \\nA.1     The new tax regime for Individual and HUF , introduced by the \\nFinance Act 2020, is now proposed to be the default regime.  \\nA.2      This regime would also become the default regime for AOP (other \\nthan co-operative), BOI and AJP.  \\nA.3      Any individual, HUF, AOP (other than co-operative), BOI or AJP not \\nwilling to be taxed under this new regime can opt to be taxed \\nunder the old regime. For those person having income under the \\nhead “profit and gains of business or profession” and having opted \\nfor old regime can revoke that option only once and after that \\nthey will continue to be taxed under the new regime. For those \\nnot having income under the head “profit and gains of business or \\nprofession”, option for old regime may be exercised in each year.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 38, 'page_label': '39', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='they will continue to be taxed under the new regime. For those \\nnot having income under the head “profit and gains of business or \\nprofession”, option for old regime may be exercised in each year. \\nA.4      Substantial relief is proposed under the new regime with new slabs \\nand tax rates as under: \\nTotal Income (`) Rate (per cent) \\nUpto 3,00,000 Nil \\nFrom 3,00,001 to 6,00,000 5 \\nFrom 6,00,001 to 9,00,000 10 \\nFrom 9,00,001 to 12,00,000 15 \\nFrom 12,00,001 to 15,00,000 20 \\nAbove 15,00,000 30 \\n \\nA.5      Resident individual with total income up to ` 5,00,000 do not pay \\nany tax due to rebate under both old and new regime. It is \\nproposed to increase the rebate for the resident individual under \\nthe new regime so that they do not pay tax if their total income is \\nup to ` 7,00,000. \\nA.6     Standard deduction of ` 50,000 to salaried individual, and'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 39, 'page_label': '40', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='36 \\n \\n \\n \\ndeduction from family pension up to ` 15,000, is currently allowed \\nonly under the old regime. It is proposed to allow these two \\ndeductions under the new regime also. \\n A.7      Surcharge on income-tax under both old regime and new regime is \\n10 per cent if income is above ` 5 0 lakh and up to ` 1 crore, 15 per \\ncent if income is above  `1 crore and up to ` 2 crore, 25 per cent if \\nincome is above ` 2 crore and up to ` 5 crore, and 37 per cent if \\nincome is above ` 5 crore. It is proposed that the for those \\nindividuals, HUF, AOP (other than co-operative), BOI and AJP \\nunder the new regime, surcharge would be same except that the \\nsurcharge rate of 37 per cent will not apply. Highest surcharge \\nshall be 25 per cent for income above \\n \\n` 2 crore. This would reduce the maximum rate from about 42.7 \\nper cent to about 39 per cent. No change in surcharge is proposed \\nfor those who opt to be under the old regime.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 39, 'page_label': '40', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='` 2 crore. This would reduce the maximum rate from about 42.7 \\nper cent to about 39 per cent. No change in surcharge is proposed \\nfor those who opt to be under the old regime. \\nA.8      Encashment of earned leave up to 10 months of average salary, at \\nthe time of retirement in case of an employee (other than an \\nemployee of the Central Government or State Government), is \\nexempt under sub-clause (ii) of clause (10AA) of section 10 of the \\nIncome-tax Act (“the Act”) to the extent notified. The maximum \\namount which can be exempted is ` 3 lakh at present. It is \\nproposed to issue notification to extend this limit to ` 25 lakh.  \\nB. SOCIO-ECONOMIC WELFARE MEASURES \\nB.1 Promoting timely payments to Micro and Small Enterprises \\nIn order to promote timely payments to micro and small \\nenterprises, it is proposed to include payments made to such \\nenterprises within the ambit of section 43B of the Act. Thus, \\ndeduction for such payments would be allowed only when actually'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 39, 'page_label': '40', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='enterprises, it is proposed to include payments made to such \\nenterprises within the ambit of section 43B of the Act. Thus, \\ndeduction for such payments would be allowed only when actually \\npaid. It will be allowed on accrual basis only if the payment is \\nwithin the time mandated under the Micro, Small and Medium \\nEnterprises Development Act. \\nB.2 Agnipath Scheme, 2022 \\nThe payment received from the Agniveer Corpus Fund by the \\nAgniveers enrolled in Agnipath Scheme, 2022 is proposed to be \\nexempt from taxes. Deduction in the computation of total income \\nis proposed to be allowed to the Agniveer on the contribution'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 40, 'page_label': '41', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='37 \\n \\n \\n \\nmade by him or the Central Government to his Seva Nidhi \\naccount. \\nB.3 Relief to sugar co-operatives from past demand \\nIt is proposed that for sugar co-operatives, for years prior to A.Y. \\n2016-17, if any deduction claimed for expenditure made on \\npurchase of sugar has been disallowed, an application may be \\nmade to the Assessing Officer, who shall recompute the income of \\nthe relevant previous year after allowing such deduction up to the \\nprice fixed or approved by the Government for such previous year.\\n \\nB.4 Increasing threshold limit for Co-operatives to withdraw cash \\nwithout TDS \\nIt is proposed to enable co-operatives to withdraw cash up to ` 3 \\ncrore in a year without being subjected to TDS on such \\nwithdrawal.  \\nB.5 Penalty for cash loan/transactions against primary co-operatives \\nIt is proposed to  amend section 269SS of the Act to provide that \\nwhere a deposit is accepted by a primary agricultural credit \\nsociety or a primary co-operative agricultural and rural'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 40, 'page_label': '41', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='It is proposed to  amend section 269SS of the Act to provide that \\nwhere a deposit is accepted by a primary agricultural credit \\nsociety or a primary co-operative agricultural and rural \\ndevelopment bank from its member or a loan is taken from a \\nprimary agricultural credit society or a primary co-operative \\nagricultural and rural development bank by its member in cash, no \\npenal consequence would arise, if the amount of such loan or \\ndeposit in cash is less than  ` 2 lakh. Further, section 269T of the \\nAct is proposed to be amended to provide that where a deposit is \\nrepaid by a primary agricultural credit society or a primary co-\\noperative agricultural and rural development bank to its member \\nor such loan is repaid to a primary agricultural credit society or a \\nprimary co-operative agricultural and rural development bank by \\nits member in cash, no penal consequence shall arise, if the \\namount of such loan or deposit in cash is less than ` 2 lakh.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 40, 'page_label': '41', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='primary co-operative agricultural and rural development bank by \\nits member in cash, no penal consequence shall arise, if the \\namount of such loan or deposit in cash is less than ` 2 lakh. \\nB.6 Relief to start-ups in carrying forward and setting off of losses \\nThe condition of continuity of at least 51 per cent shareholding for \\nsetting off of carried forward losses is relaxed for an eligible start \\nup if all the shareholders of the company continue to hold those \\nshares. At present this relaxation applies for losses incurred during \\nthe period of 7 years from incorporation of such start-up. It is'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 41, 'page_label': '42', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='38 \\n \\n \\n \\nproposed to increase this period to 10 years. \\nB.7 Extension of date of incorporation for eligible start up for \\nexemption  \\nCertain start-ups are eligible for some tax benefit if they are \\nincorporated before 1st April, 2023. The period of incorporation of \\nsuch eligible start-ups is proposed to be extended by one year to \\nbefore 1st April, 2024.  \\nB.8 Gold to Electronic Gold Receipt \\nThe conversion of physical gold to Electronic Gold Receipt and vice \\nversa is proposed not to be treated as a transfer and not to attract \\nany capital gains. This would promote investments in electronic \\nequivalent of gold. \\nB.9 Incentives to IFSC \\nRelocation of funds to IFSC has certain tax exemptions, if the \\nrelocation is before 31.03.2023. This date is proposed to be \\nextended to 31.03.2025. Further, any distributed income from the \\noffshore derivative instruments entered into with an offshore \\nbanking unit is also proposed to be exempted subject to certain \\nconditions.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 41, 'page_label': '42', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='offshore derivative instruments entered into with an offshore \\nbanking unit is also proposed to be exempted subject to certain \\nconditions. \\nB.10 Exemption to development authorities etc. \\nIt is proposed to provide exemption to any income arising to a \\nbody or authority or board or trust or commission, (not being a \\ncompany) which  has been established or constituted by or under \\na Central or State Act with the purposes of satisfying the need for \\nhousing or for planning, development or improvement of cities, \\ntowns and villages or for regulating any activity or matter, \\nirrespective of whether it is carrying out commercial activity. \\nB.11 Facilitating certain strategic disinvestments \\nTo facilitate certain strategic disinvestments, it is proposed to \\nallow carry forward of accumulated losses and unabsorbed \\ndepreciation allowance in the case of amalgamation of one or \\nmore banking company with any other banking institution or a'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 41, 'page_label': '42', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='allow carry forward of accumulated losses and unabsorbed \\ndepreciation allowance in the case of amalgamation of one or \\nmore banking company with any other banking institution or a \\ncompany subsequent to a strategic disinvestment, if such \\namalgamation takes place within 5 years of strategic \\ndisinvestment. It is also proposed to modify the definition of \\n‘strategic disinvestment’.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 42, 'page_label': '43', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='39 \\n \\n \\n \\nB.12 15 per cent concessional tax to promote new manufacturing co -\\noperative society \\nIn order to promote the growth of manufacturing in co-operative \\nsector, a new co-operative society formed on or after 01.04.2023, \\nwhich commences manufacturing or production by 31.03.2024 \\nand do not avail of any specified incentive or deduction, is \\nproposed to be allowed an option to pay tax at a concessional rate \\nof 15 per cent similar to what is available to new manufacturing \\ncompanies. \\nC. EASE OF COMPLIANCE \\nC.1 Ease in claiming deduction on amortization of preliminary \\nexpenditure \\nAt present for claiming amortization of certain preliminary \\nexpenses, the activity is to be carried out either by the assessee or \\nby a concern approved by the Board. In order to ease the process \\nof claiming amortization of these expenses it is proposed to \\nremove the condition of activity in connection with these \\nexpenses to be carried out by a concern approved by the Board.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 42, 'page_label': '43', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='of claiming amortization of these expenses it is proposed to \\nremove the condition of activity in connection with these \\nexpenses to be carried out by a concern approved by the Board. \\nFormat for reporting of such expenses by the assessee shall be \\nprescribed. \\nC.2 Increasing threshold limits for presumptive taxation schemes \\nIn order to ease compliance and to promote non-cash \\ntransactions, it is proposed to increase the threshold limits for \\npresumptive scheme of taxation for eligible businesses from ` 2 \\ncrore to ` 3 crore and for specified professions from ` 50 lakh to \\n \\n` 75 lakh. The increased limit will apply only in case the amount or \\naggregate of the amounts received during the year, in cash, does \\nnot exceed five per cent of the total gross receipts/turnover. \\nC.3 Extending the scope for deduction of tax at source at lower or nil \\nrate \\nIt is proposed to allow a taxpayer to obtain certificate of \\ndeduction of tax at source to lower or nil rate on sums on which'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 42, 'page_label': '43', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='rate \\nIt is proposed to allow a taxpayer to obtain certificate of \\ndeduction of tax at source to lower or nil rate on sums on which \\ntax is required to be deducted under section 194LBA of the Act by \\nBusiness Trusts.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 43, 'page_label': '44', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='40 \\n \\n \\n \\nD. WIDENING & DEEPENING OF TAX BASE AND ANTI AVOIDANCE \\nD.1 It is proposed to extend the deemed income accrual provision \\nrelating to sums of money exceeding fifty thousand rupees, \\nreceived from residents without consideration to a not ordinarily \\nresident with effect from 1st April, 2023. \\nD.2 It is proposed to omit the provision to allow tax exemption to \\nnews agencies set up in India solely for collection and distribution \\nof news from the financial year 2023-24.  \\nD.3 It is proposed to tax distributed income by business trusts in the \\nhands of a unit holder (other than dividend, interest or rent which \\nis already taxable) on which tax is currently avoided both in the \\nhands of unit holder as well as in the hands of business trust.   \\nD.4 It is proposed to withdraw the exemption from TDS currently \\navailable on interest payment on listed debentures. \\nD.5 With respect to presumptive schemes for non-residents, it is'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 43, 'page_label': '44', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='D.4 It is proposed to withdraw the exemption from TDS currently \\navailable on interest payment on listed debentures. \\nD.5 With respect to presumptive schemes for non-residents, it is \\nproposed to disallow carried forward and set off of loss computed \\nas per books of account with presumptive income. \\nD.6 For online games, it is proposed to provide for TDS and taxability \\non net winnings at the time of withdrawal or at the end of the \\nfinancial year. Moreover, TDS would be without the threshold of \\n \\n` 10,000. For lottery, crossword puzzles games, etc threshold limit \\n` 10,000 for TDS shall continue but shall apply to aggregate \\nwinnings during a financial year. \\nD.7     The rate of TCS for foreign remittances for education and for \\nmedical treatment is proposed to continue to be 5 per cent for \\nremittances in excess of ` 7 lakh. Similarly, the rate of TCS on \\nforeign remittances for the purpose of education through loan'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 43, 'page_label': '44', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='medical treatment is proposed to continue to be 5 per cent for \\nremittances in excess of ` 7 lakh. Similarly, the rate of TCS on \\nforeign remittances for the purpose of education through loan \\nfrom financial institutions is proposed to continue to be 0.5 per \\ncent in excess of `7 lakh. However, for foreign remittances for \\nother purposes under LRS and purchase of overseas tour program, \\nit is proposed to increase the rates of TCS from 5 per cent to 20 \\nper cent. \\nD.8 Tax on capital gains can be avoided by investing proceeds of such \\ngains in residential property. This is proposed to be capped at ` 10 \\ncrore.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 44, 'page_label': '45', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='41 \\n \\n \\n \\nD.9 The income from market linked debentures is proposed to be \\ntaxed as short-term capital gains at the applicable rates. \\nD.10 It is proposed to provide for some provisions to minimise risk to \\nrevenue due to undervaluation of inventory. \\nD.11 It is proposed to provide that where aggregate of premium for life \\ninsurance policies (other than ULIP) issued on or after 1 st April, \\n2023 is above ` 5 lakh, income from only those policies with \\naggregate premium up to ` 5 lakh shall be exempt. This will not \\naffect the tax exemption provided to the amount received on the \\ndeath of person insured. It will also not affect insurance policies \\nissued till 31st March, 2023. \\nD.12 It is proposed to amend provisions for computing capital gains in \\ncase of joint development of property to include the amount \\nreceived through cheque etc. as consideration.  \\nD.13 While interest paid on borrowed capital for acquiring or improving'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 44, 'page_label': '45', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='case of joint development of property to include the amount \\nreceived through cheque etc. as consideration.  \\nD.13 While interest paid on borrowed capital for acquiring or improving \\na property can, subject to certain conditions, be claimed as \\ndeduction from income, it can also be included in the cost of \\nacquisition or improvement on transfer, thereby reducing capital \\ngains. It is proposed to provide that the cost of acquisition or \\nimprovement shall not include the amount of interest claimed \\nearlier as deduction. \\nD.14 There are certain assets like intangible assets or rights for which \\nno consideration has been paid for acquisition and the transfer of \\nwhich may result in generation of income. Their cost of acquisition \\nis proposed to be defined to be NIL. \\nE. IMPROVING COMPLIANCE AND TAX ADMINISTRATION \\nE.1 With respect to rectification of orders by the Interim Board of \\nSettlement, it is proposed to provide that where the time-limit for'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 44, 'page_label': '45', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='E. IMPROVING COMPLIANCE AND TAX ADMINISTRATION \\nE.1 With respect to rectification of orders by the Interim Board of \\nSettlement, it is proposed to provide that where the time-limit for \\namending an order by it or for making an application to it expires \\non or after 01.02.2021 but before 01.02.2022, such time-limit shall \\nstand extended to 30.09.2023. \\nE.2 To expedite the disposal of certain appeals pending with \\nCommissioner (Appeals), it is proposed to introduce a new \\nauthority in the rank of Joint Commissioner/ Additional \\nCommissioner [JCIT(Appeals)], for appeals against certain orders'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 45, 'page_label': '46', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='42 \\n \\n \\n \\npassed by or with the approval of an authority below the rank of \\nJoint Commissioner. Certain related and consequential \\namendments are also proposed in this regard.  \\nE.3 It is proposed to reduce the minimum time period required to be \\nprovided by the transfer pricing officer to assessee for production \\nof documents and information from 30 days to 10 days. \\nE.4 It is proposed to provide for appeal against penalty orders passed \\nby Commissioner (Appeals) under certain sections of the Act \\nbefore the Appellate Tribunal. It is also proposed to provide that \\nan order under section 263 of the Act passed by the Principal \\nChief Commissioner or Chief Commissioner and any rectification \\norder for the same shall also be appealable before the Appellate \\nTribunal. Further, it is proposed to enable filing of memorandum \\nof cross-objections in all classes of cases against which appeal can \\nbe made to the Appellate Tribunal.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 45, 'page_label': '46', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='Tribunal. Further, it is proposed to enable filing of memorandum \\nof cross-objections in all classes of cases against which appeal can \\nbe made to the Appellate Tribunal. \\nE.5 It is proposed to amend section 132 of the Act, dealing with \\nsearch and seizure, to allow the authorised officer to take \\nassistance of specific domain experts like digital forensic \\nprofessionals, valuers and services of other professionals like \\nlocksmiths, carpenters etc. during the course of search and also to \\naid in accurate estimation of undisclosed income held in the form \\nof property by the assessee.  \\nE.6 Section 170A of the Act, inserted vide Finance Act, 2022 is \\nproposed to be substituted to clarify that a modified return shall \\nbe furnished by an entity to whom the order of the business \\nreorganisation applies, and to introduce provisions for assessment \\nor reassessment in cases where such modified return is furnished. \\nE.7 It is proposed that an order of assessment may be passed within a'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 45, 'page_label': '46', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='or reassessment in cases where such modified return is furnished. \\nE.7 It is proposed that an order of assessment may be passed within a \\nperiod of 12 months from the end of the relevant assessment year \\nor the financial year in which updated return is filed, as the case \\nmay be. It is also proposed that in cases where search under \\nsection 132 of the Act or requisition under section 132A of the Act \\nhas been made, the period of limitation of pending assessments \\nshall be extended by twelve months.  \\nE.8 It is proposed to make amendments to empower the Central \\nGovernment to make modifications in the already notified'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 46, 'page_label': '47', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='43 \\n \\n \\n \\nschemes regarding e -Verification, Dispute Resolution, Advance \\nRulings, Appeal and Penalty, at any time to enable better \\nimplementation of such schemes. \\nE.9 It is proposed to limit the time for furnishing of a return for \\nreassessment. Further, it is also proposed to  provide that in cases \\nwhere search related information is available after 15th March of \\nany financial year, an additional period of fifteen days shall be \\nallowed for issuance of notice, for assessment/reassessments etc, \\nunder section 148 of the Act. It is also proposed to clarify that the \\nspecified authority for granting approval shall be Principal Chief \\nCommissioner or Principal Director General or Chief Commissioner \\nor Director General. \\nE.10 It is proposed to provide a penalty of ` 5,000 if there is any \\ninaccuracy in the statement of financial transactions submitted by \\na prescribed reporting financial institution due to false or \\ninaccurate information submitted by the account holder.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 46, 'page_label': '47', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='inaccuracy in the statement of financial transactions submitted by \\na prescribed reporting financial institution due to false or \\ninaccurate information submitted by the account holder. \\nE.11 It is proposed to amend section 271C and section 276B of the Act \\nto provide for penalty and prosecution where default in TDS \\nrelates to transaction in kind. \\nE.12.   It is proposed to amend the time period for filing of appeal against \\nthe order of the Adjudicating authority under Benami Act within a \\nperiod of 45 days from the date when such order is received by \\nthe Initiating Officer or the aggrieved person. The definition of \\n‘High Court’ is also proposed to be modified to allow \\ndetermination of jurisdiction for filing appeal in the case of non-\\nresidents. \\nF. RATIONALISATION \\nF.1 The restriction on interest deductibility on interest payment to \\noverseas associated enterprise does not apply to those in the \\nbusiness of banking and insurance. It is proposed to extend this'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 46, 'page_label': '47', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='F.1 The restriction on interest deductibility on interest payment to \\noverseas associated enterprise does not apply to those in the \\nbusiness of banking and insurance. It is proposed to extend this \\nbenefit to non-banking financial companies, as may be notified. \\nF.2 TDS on payment of certain income to a non-resident is currently at \\nthe rate of 20 per cent, but the tax rate in treaties may be lower. It \\nis proposed to allow the benefit of tax treaty at the time of TDS on \\nsuch income under section 196A of the Act.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 47, 'page_label': '48', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='44 \\n \\n \\n \\nF.3 At present the TDS rate on withdrawal of taxable component from \\nEmployees’ Provident Fund Scheme in non-PAN cases is 30 per \\ncent. It is proposed to reduce it to 20 per cent, as in other non-\\nPAN cases. \\nF.4 Sometimes, tax for income of an earlier year is deducted later, \\nwhile tax thereon has already been paid in the earlier year. \\nAmendment is proposed to facilitate such taxpayers to claim \\ncredit of this TDS in the earlier year.  \\nF.5 Higher TDS/TCS rate applies, if the recipient is a non-filer i.e. who \\nhas not furnished his return of income of preceding previous year \\nand has aggregate of TDS and TCS of ` 50,000 or more. It is \\nproposed to exclude a person who is not required to furnish the \\nreturn of income for such previous year and who is notified by the \\nCentral Government in the Official Gazette in this behalf. \\nF\\n.6 It is proposed to clarify that the amount of advance tax paid is \\nreduced only once for computing the interest payable u/s 234B in'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 47, 'page_label': '48', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='Central Government in the Official Gazette in this behalf. \\nF\\n.6 It is proposed to clarify that the amount of advance tax paid is \\nreduced only once for computing the interest payable u/s 234B in \\nthe case of an updated return. \\nF.7 It is proposed to extend taxability of the consideration (share \\napplication money/ share premium) for shares exceeding the face \\nvalue of such shares to all investors including non-residents. \\nF.8 It is proposed to enable prescription of a uniform methodology for \\ncomputing the value of perquisite with respect to accommodation \\nprovided by employers to their employees. \\nF.9 It is proposed to provide a time limit for an SEZ unit to bring the \\nproceeds from exports of goods or services into India. The filing of \\nincome-tax return is also proposed to be made mandatory for \\nclaiming deduction on export income. \\nF.10 Due to changes in classification of non-banking financial \\ncompanies by the Reserve Bank of India, it is proposed to make'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 47, 'page_label': '48', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='claiming deduction on export income. \\nF.10 Due to changes in classification of non-banking financial \\ncompanies by the Reserve Bank of India, it is proposed to make \\nnecessary amendments to align such classifications in the Act with \\nthe same. \\nF.11 It is proposed to clarify that for taxability under section 28 of the \\nAct as well for tax deduction at source under section 194R of the \\nAct, the benefit could also be in cash. \\nF.12 It is proposed to make amendments relating to exemption'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 48, 'page_label': '49', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='45 \\n \\n \\n \\nprovided to charitable trusts and institution to \\n\\uf0b7 provide clarity on tax treatment on replenishment of corpus \\nand on repayment of loans/borrowings; \\n\\uf0b7 treat only 85 per cent of donation made to another trust as \\napplication; \\n\\uf0b7 omit the redundant provisions related to rolling back of \\nexemption; \\n\\uf0b7 combine provisional and regular registration in some cases; \\n\\uf0b7 modify the scope of specified violation; \\n\\uf0b7 provide for payment of tax on assets if a trust does not apply \\nfor exemption after getting provisional exemption and for re-\\nexemption after expiry of exemption; \\n\\uf0b7 align of time for furnishing of certain forms; \\n\\uf0b7 clarify that the time provided for furnishing return of income \\nfor claiming exemption shall not include the time provided for \\nfurnishing updated return. \\nF.13 It is proposed to omit certain name-based funds from section 80G \\nof the Act, which provides for deduction of donation to such funds \\nfrom the income of the donor.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 48, 'page_label': '49', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='furnishing updated return. \\nF.13 It is proposed to omit certain name-based funds from section 80G \\nof the Act, which provides for deduction of donation to such funds \\nfrom the income of the donor. \\nF.14 It is proposed to provide that where refund is due to a person, \\nsuch refund shall be set off against existing demand, and if \\nproceedings for assessment or reassessment are pending in such \\ncase, the refund due will be withheld by the Assessing Officer till \\nthe date of assessment or reassessment. \\nG. OTHERS \\nG.1 It is proposed to omit section 88 and some of the clauses of \\nsection 10 of the Act which are no longer in force. \\nG.2 It is proposed to extend tax exemption to Specified Undertaking of \\nUnit Trust of India (SUUTI) till 30 th September, 2023. It is also \\nproposed to enable the Central Government to notify the date of \\nvacation of office of administrator of SUUTI. \\nG.3 It is proposed to decriminalize certain acts of omission of'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 48, 'page_label': '49', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='proposed to enable the Central Government to notify the date of \\nvacation of office of administrator of SUUTI. \\nG.3 It is proposed to decriminalize certain acts of omission of \\nliquidators under section 276A of the Act with effect from 1 st \\nApril, 2023.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 49, 'page_label': '50', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='46 \\n \\n \\n \\nAnnexure to Part B of the Budget Speech 2023-24 \\nAmendments relating to Indirect Taxes \\n \\nA. LEGISLATIVE CHANGES IN CUSTOMS LAWS \\nA.1       Amendments in the Customs Act, 1962 \\nSection 25 (4A) is being amended to  exclude certain categories of \\nconditional customs duty exemptions from the validity period of \\ntwo years, such as, notifications issued in relation to multilateral \\nor bilateral trade agreements; obligations under international \\nagreements, treaties, conventions including with respect to UN \\nagencies, diplomats, international organizations; privileges of \\nconstitutional authorities; schemes under Foreign Trade Policy; \\nCentral Government schemes having a validity of more than two \\nyears; re-imports, temporary imports, goods imported as gifts or \\npersonal baggage; any other duties of Customs under any other \\nlaw in force including  IGST levied under section 3(7) of Customs \\nTariff Act, 1975, other than duty of customs levied under section'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 49, 'page_label': '50', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='personal baggage; any other duties of Customs under any other \\nlaw in force including  IGST levied under section 3(7) of Customs \\nTariff Act, 1975, other than duty of customs levied under section \\n12 of the Customs Act 1962. \\nSection 127C is being amended to specify a time limit of nine \\nmonths from date of filing application for passing final order by \\nSettlement Commission.  \\nA.2  Amendments in the provisions relating to Anti-Dumping Duty \\n(ADD), Countervailing Duty (CVD), and Safeguard Measures \\nSections 9, 9A, 9C of the Customs Tariff Act are being amended to \\nclarify the intent and scope of these provisions. They are also \\nbeing validated retrospectively with effect from 1st January 1995. \\nA.3      Amendments in the First Schedule to the Customs Tariff Act, 1975 \\nThe First Schedule to the Customs Tariff Act, 1975 is being \\namended to increase the rates on certain tariff items with effect \\nfrom 02.02.2023 and also modify the rates on certain other tariff'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 49, 'page_label': '50', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='The First Schedule to the Customs Tariff Act, 1975 is being \\namended to increase the rates on certain tariff items with effect \\nfrom 02.02.2023 and also modify the rates on certain other tariff \\nitems as part of rate rationalisation with effect from date of \\nassent. \\nThe First Schedule to the Customs Tariff Act is being proposed to \\nbe amended in accordance with HSN 2022 amendments.  \\nNew tariff lines are also proposed to be created, which will help in \\nbetter identification of millet-based products, mozzarella cheese, \\nmedicinal plants and their parts, certain pesticides, telecom'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 50, 'page_label': '51', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='47 \\n \\n \\n \\nproducts, synthetic diamonds, cotton, fertilizer grade urea etc. \\nThis will also help in trade facilitation by better identification of \\nthe above items, getting clarity on availing concessional import \\nduty through various notifications and thus reducing dwell time.  \\nThese changes shall come into effect from 01.05.2023. \\nA.4     Amendment in the Second Schedule to the Customs Tariff Act, \\n1975 \\nThe Second Schedule (Export Tariff) is being amended to align the \\nentries under heading 1202 with that of the First Schedule (Import \\nTariff) . \\nB. LEGISLATIVE CHANGES IN GST LAWS \\nB.1 Decriminalisation \\nSection 132 and section 138 of CGST Act are being amended, inter \\nalia, to - \\n\\uf0b7 raise the minimum threshold of tax amount for launching \\nprosecution under GST from ` one crore to ` two crore, \\nexcept for the offence of issuance of invoices without supply \\nof goods or services or both; \\n\\uf0b7 reduce the compounding amount from the present range of'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 50, 'page_label': '51', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='except for the offence of issuance of invoices without supply \\nof goods or services or both; \\n\\uf0b7 reduce the compounding amount from the present range of \\n50 per cent  to 150 per cent of tax amount to the range of 25 \\nper cent to 100 per cent; \\n\\uf0b7 decriminalize certain offences specified under clause (g), (j) \\nand (k) of sub-section (1) of section 132 of CGST Act, 2017, \\nviz.- \\no obstruction or preventing any officer in discharge of his \\nduties;  \\no deliberate tempering of material evidence; \\no failure to supply the information. \\nB.2        Facilitate e-commerce for micro enterprises \\nAmendments are being made in section 10 and section 122 of the \\nCGST Act to enable unregistered suppliers and composition \\ntaxpayers to make intra-state supply of goods through E-\\nCommerce Operators (ECOs), subject to certain conditions.  \\nB.3        Amendment to Schedule III of CGST Act, 2017 \\nParas 7, 8 (a) and 8 (b) were inserted in Schedule III of CGST Act,'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 50, 'page_label': '51', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='Commerce Operators (ECOs), subject to certain conditions.  \\nB.3        Amendment to Schedule III of CGST Act, 2017 \\nParas 7, 8 (a) and 8 (b) were inserted in Schedule III of CGST Act, \\n2017 with effect from 01.02.2019 to keep certain transactions/ \\nactivities, such as supplies of goods from a place outside the \\ntaxable territory to another place outside the taxable territory, \\nhigh sea sales and supply of warehoused goods before their home'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 51, 'page_label': '52', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='48 \\n \\n \\n \\nclearance, outside the purview of GST. In order to remove the \\ndoubts and ambiguities regarding taxability of such transactions/ \\nactivities during the period 01.07.2017 to 31.01.2019, provisions \\nare being incorporated to make the said paras effective from \\n01.07.2017. However, no refund of tax paid shall be available in \\ncases where any tax has already been paid in respect of such \\ntransactions/ activities during the period 01.07.2017 to \\n31.01.2019. \\nB.4        Return filing under GST  \\nSections 37, 39, 44 and 52 of CGST Act, 2017 are being amended \\nto restrict filing of returns/ statements to a maximum period of \\nthree years from the due date of filing of the relevant return / \\nstatement.  \\nB.5        Input Tax Credit for expenditure related to CSR \\nSection 17(5) of CGST Act is being amended to provide that input \\ntax credit shall not be available in respect of goods or services or \\nboth received by a taxable person, which are used or intended to'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 51, 'page_label': '52', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='tax credit shall not be available in respect of goods or services or \\nboth received by a taxable person, which are used or intended to \\nbe used for activities relating to his obligations under corporate \\nsocial responsibility referred to in section 135 of the Companies \\nAct, 2013. \\nB.6        Sharing of information \\nA new section 158A in CGST Act is being inserted to enable sharing \\nof the information furnished by the registered person in his return \\nor application of registration or statement of outward supplies, or \\nthe details uploaded by him for generation of electronic invoice or \\nE-way bill or any other details on the common portal, with other \\nsystems in a manner to be prescribed \\nB.7        Amendments in section 2 clause (16) of IGST Act, 2017 \\nClause (16) of section 2 of IGST Act is amended to revise the \\ndefinition of “non-taxable online recipient” by removing the \\ncondition of receipt of online information and database access or'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 51, 'page_label': '52', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='Clause (16) of section 2 of IGST Act is amended to revise the \\ndefinition of “non-taxable online recipient” by removing the \\ncondition of receipt of online information and database access or \\nretrieval services for purposes other than commerce, industry or \\nany other business or profession so as to provide for taxability of \\nOIDAR service provided by any person located in non-taxable \\nterritory to an unregistered person receiving the said services and \\nlocated in the taxable territory. Further, it also seeks to clarify that \\nthe persons registered solely in terms of clause (vi) of Section 24 \\nof CGST Act shall be treated as unregistered person for the \\npurpose of the said clause.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 52, 'page_label': '53', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='49 \\n \\n \\n \\nB.8        Online information and database access or retrieval services \\nClause (17) of section 2 of IGST Act is being amended to revise the \\ndefinition of “online information and database access or retrieval \\nservices” to remove the condition of rendering of the said supply \\nbeing essentially automated and involving minimal human \\nintervention.  \\nB.9        Place of supply in certain cases \\nProviso to sub-section (8) of section 12 of the IGST Act is being \\nomitted so as to specify the place of supply, irrespective of \\ndestination of the goods, in cases where the supplier of services \\nand recipient of services are located in India.'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 53, 'page_label': '54', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='50 \\n \\n \\n \\n \\nC. CUSTOMS DUTY RATE CHANGES \\n \\nC.1. Reduction in basic customs duty to reduce input costs, deepen value \\naddition, to promote export competitiveness, correct inverted duty \\nstructure so as to boost domestic manufacturing etc [with effect \\nfrom 02.02.2023] \\nS. \\nNo. Commodity \\nFrom \\n(per cent) \\nTo \\n(per cent) \\nI. Agricultural Products \\n1. Pecan Nuts 100  30 \\n2. Fish meal for manufacture of aquatic \\nfeed 15 5 \\n3. Krill meal for manufacture of aquatic \\nfeed 15 5 \\n4. Fish lipid oil for manufacture of aquatic \\nfeed 30 15 \\n5. Algal Prime (flour) for manufacture of \\naquatic feed 30 15 \\n6. Mineral and Vitamin Premixes for \\nmanufacture of aquatic feed 15 5 \\n7 Crude glycerin for use in manufacture \\nof Epichlorohydrin \\n7.5 2.5 \\n8 Denatured ethyl alcohol for use in \\nmanufacture of industrial chemicals. \\n5 Nil \\nII. Minerals \\n1 Acid grade fluorspar (containing by \\nweight more than 97 per cent of \\ncalcium fluoride) \\n5 2.5 \\nIII. Gems and Jewellery Sector'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 53, 'page_label': '54', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='manufacture of industrial chemicals. \\n5 Nil \\nII. Minerals \\n1 Acid grade fluorspar (containing by \\nweight more than 97 per cent of \\ncalcium fluoride) \\n5 2.5 \\nIII. Gems and Jewellery Sector \\n1. Seeds for use in manufacturing of \\nrough lab-grown diamonds \\n5 Nil'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 54, 'page_label': '55', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='51 \\n \\n \\n \\nIV. Capital Goods \\n1. Specified capital goods/machinery for \\nmanufacture of lithium-ion cell for use \\nin battery of electrically operated \\nvehicle (EVs) \\nAs \\napplicable \\nNil  \\n(up to \\n31.03.2024)\\n \\nV. IT and Electronics  \\n \\n1. Specified chemicals/items for \\nmanufacture of Pre-calcined Ferrite \\nPowder \\n7.5 Nil \\n(up to \\n31.03.2024) \\n2. Palladium Tetra Amine Sulphate for \\nmanufacture of parts of connectors \\n7.5 Nil \\n(up to \\n31.03.2024) \\n3. Camera lens and its inputs/parts for \\nuse in manufacture of camera module \\nof cellular mobile phone \\n2.5 Nil \\n4. Specified parts for manufacture of \\nopen cell of TV panel \\n5 2.5 \\nVI. Electronic Appliances \\n1. Heat coil for manufacture of electric \\nkitchen chimneys \\n20 15 \\nVII. Others \\n1. Warm blood horse imported by sports \\nperson of outstanding eminence for \\ntraining purpose \\n30 Nil \\n2. Vehicles, specified automobile \\nparts/components, sub-systems and \\ntyres when imported by notified \\ntesting agencies, for the purpose of'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 54, 'page_label': '55', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='training purpose \\n30 Nil \\n2. Vehicles, specified automobile \\nparts/components, sub-systems and \\ntyres when imported by notified \\ntesting agencies, for the purpose of \\ntesting and/ or certification, subject to \\nconditions. \\nAs \\napplicable \\nNil'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 55, 'page_label': '56', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='52 \\n \\n \\n \\nC.2. Increase in Customs duty [with effect from 02.02.2023]  \\nS. No. Commodity \\n \\nRate of duties \\nFrom \\n(per cent) \\nTo \\n(per cent) \\nI. Chemicals \\n1. Styrene 2 \\n(+0.2 SWS) \\n2.5 \\n(+0.25 \\nSWS) \\n2. Vinyl chloride monomer 2 \\n(+0.2 SWS) \\n2.5 \\n(+0.25 \\nSWS) \\nII Petrochemical \\n1 Naphtha 1 \\n(+ 0.1 SWS) \\n2.5 \\n(+0.25  SWS) \\nIII. Precious Metals \\n1. Silver (including silver plated with gold \\nor platinum), unwrought or in semi-\\nmanufactured forms, or in powder \\nform \\n7.5 \\n(+ 2.5 \\nAIDC+ 0.75 \\nSWS) \\n10 \\n(+ 5 AIDC+ \\nNil SWS) \\n2. Silver dore 6.1 \\n(+ 2.5 \\nAIDC+ 0.61  \\nSWS) \\n10 \\n(+ 4.35 \\nAIDC+ Nil \\nSWS) \\nIV. Gems and Jewellery Sector  \\n1. Articles of Precious Metals such as \\ngold/silver/platinum \\n20 \\n(+Nil AIDC \\n+2 SWS)  \\n25 \\n(+Nil AIDC \\n+Nil SWS) \\n2. Imitation Jewellery 20 or ` \\n400/kg., \\nwhichever is \\nhigher \\n \\n(+Nil AIDC +2 \\nor ` 40 per \\nKg SWS) \\n \\n25 or ` \\n600/kg., \\nwhichever is \\nhigher \\n \\n(+Nil AIDC \\n+Nil SWS)'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 56, 'page_label': '57', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='53 \\n \\n \\n \\nS. No. Commodity \\n \\nRate of duties \\nFrom \\n(per cent) \\nTo \\n(per cent) \\nV. Automobiles  \\n1 Vehicle (including electric vehicles) in \\nSemi-Knocked Down (SKD) form . \\n30 \\n(+3 SWS) \\n35 \\n(+Nil SWS) \\n2 Vehicle in Completely Built Unit (CBU) \\nform, other than with CIF more than \\nUSD 40,000 or with engine capacity \\nmore than 3000 cc for petrol-run \\nvehicle and more than 2500 cc for \\ndiesel-run vehicles, or with both \\n60 \\n(+6  SWS) \\n70 \\n(+Nil SWS) \\n3 Electrically operated Vehicle in \\nCompletely Built Unit (CBU) form, \\nother than with CIF value more than \\nUSD 40,000 \\n60 \\n(+ 6 SWS) \\n70 \\n(+Nil SWS) \\nVI. Others \\n1.  Bicycles 30 \\n \\n(+ Nil AIDC \\n+3 SWS) \\n35 \\n \\n(+ Nil AIDC \\n+Nil SWS) \\n2.  Toys and parts of toys (other than \\nparts of electronic toys) \\n60 \\n \\n(+Nil AIDC+ \\n6 SWS) \\n70 \\n \\n(+Nil AIDC+ \\nNil SWS) \\n3.  Compounded Rubber 10 \\n \\n \\n25 or ` \\n30/kg., \\nwhichever is \\nlower \\n4.  Electric Kitchen Chimney 7.5 \\n \\n15 \\n \\n* AIDC -Agriculture Infrastructure Development Cess; SWS – Social Welfare'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 56, 'page_label': '57', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='Nil SWS) \\n3.  Compounded Rubber 10 \\n \\n \\n25 or ` \\n30/kg., \\nwhichever is \\nlower \\n4.  Electric Kitchen Chimney 7.5 \\n \\n15 \\n \\n* AIDC -Agriculture Infrastructure Development Cess; SWS – Social Welfare \\nSurcharge'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 57, 'page_label': '58', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='54 \\n \\n \\n \\nD. CHANGES IN CENTRAL EXCISE \\nD.1. NCCD Duty rate  on Cigarettes [with effect from 02.02.2023]  \\n \\nDescription of goods \\nRate of excise duty \\nFrom \\n(` per 1000 \\nsticks) \\nTo \\n(` per 1000 \\nsticks) \\nOther than filter cigarettes, of length not \\nexceeding 65 mm \\n200 230 \\nOther than filter cigarettes, of length exceeding \\n65 mm but not exceeding 70 mm \\n250 290 \\nFilter cigarettes of length not exceeding 65 mm 440 510 \\nFilter cigarettes of length exceeding 65 mm but \\nnot exceeding 70 mm \\n440 510 \\nFilter cigarettes of length exceeding 70 mm but \\nnot exceeding 75 mm \\n545 630 \\nOther cigarettes 735 850 \\nCigarettes of tobacco substitutes 600 690 \\n \\n \\nD.2. Other changes in Central Excise [with effect from 02.02.2023] \\nIn order to promote green fuel, central excise duty exemption is being \\nprovided to blended Compressed Natural Gas from so much of the amount \\nas is equal to the GST paid on Bio Gas/Compressed Bio Gas contained in the \\nblended CNG. \\nE. OTHERS'),\n",
       " Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2023-02-01T05:28:04+05:30', 'moddate': '2023-02-01T08:28:21+05:30', 'title': '', 'source': '..\\\\data\\\\pdf\\\\budget_speech.pdf', 'total_pages': 58, 'page': 57, 'page_label': '58', 'source_file': 'budget_speech.pdf', 'file_type': 'pdf'}, page_content='provided to blended Compressed Natural Gas from so much of the amount \\nas is equal to the GST paid on Bio Gas/Compressed Bio Gas contained in the \\nblended CNG. \\nE. OTHERS \\nThere are few other changes of minor nature. For details of the budget \\nproposals, the Explanatory Memorandum and other relevant budget \\ndocuments may be referred to. \\n*****'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='Conversational Text Extraction with Large \\nLanguage Models Using Retrieval-Augmented \\nSystems \\nSoham Roy1, Mitul Goswami1, Nisharg Nargund1, Suneeta Mohanty1 and Prasant Kumar Pattnaik1 \\nSchool of Computer Engineering, Kalinga Institute of Industrial Technology, Patia, Bhubaneswar, 751024, India1, \\nAbstract— This study introduces a system leveraging Large \\nLanguage Models (LLMs) to extract text and enhance user \\ninteraction with PDF documents via a conversational interface. \\nUtilizing Retrieval -Augmented Generation (RAG), the system \\nprovides informative respons es to user inquiries while \\nhighlighting relevant passages within the PDF. Upon user \\nupload, the system processes the PDF, employing sentence \\nembeddings to create a document -specific vector store. This \\nvector store enables efficient retrieval of pertinent s ections in \\nresponse to user queries. The LLM then engages in a \\nconversational exchange, using the retrieved information to'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='vector store enables efficient retrieval of pertinent s ections in \\nresponse to user queries. The LLM then engages in a \\nconversational exchange, using the retrieved information to \\nextract text and generate comprehensive, contextually aware \\nanswers. While our approach demonstrates competitive \\nROUGE values compared to existing state-of-the-art techniques \\nfor text extraction and summarization, we acknowledge that \\nfurther qualitative evaluation is necessary to fully assess its \\neffectiveness in real -world applications.  The proposed system \\ngives competitive ROUGE values as compared to existing state-\\nof-the-art techniques for text extraction and summarization, \\nthus offering a valuable tool for researchers, students, and \\nanyone seeking to efficiently extract knowledge and gain \\ninsights from documents through an intuitive  question-\\nanswering interface. \\nKeywords—Large Language Model (LLM), Retrieval \\nAugmented Generation, Embeddings, Text Extraction, ROUGE \\nI. INTRODUCTION'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='answering interface. \\nKeywords—Large Language Model (LLM), Retrieval \\nAugmented Generation, Embeddings, Text Extraction, ROUGE \\nI. INTRODUCTION  \\nThe ever-growing volume of digital documents, particularly \\nPDFs, presents a significant challenge: efficiently extracting \\nknowledge from their text -heavy content. Over the years, \\nvarious tools and techniques have been developed to address \\nthis issue, from basic keyword search functionalities to more \\nadvanced text mining and natural language processing (NLP) \\nalgorithms [1]. Despite these advancements, many solutions \\nstill fall short of providing contextually relevant information \\nquickly and accurately. The e volution of artificial \\nintelligence and machine learning, particularly in the form of \\nlarge language models, has revolutionized this process, \\nenabling more sophisticated and efficient extraction of \\nknowledge from vast repositories of digital documents [2]. \\n \\nLarge language models (LLMs) have undergone significant'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='enabling more sophisticated and efficient extraction of \\nknowledge from vast repositories of digital documents [2]. \\n \\nLarge language models (LLMs) have undergone significant \\nevolution, transforming the landscape of natural language \\nprocessing (NLP) and information retrieval. While the \\nintegration of Retrieval -Augmented Generation (RAG) with \\nLLMs is noteworthy, it is essential to recognize that similar \\nframeworks have been explored in existing literature. This \\npaper aims to build upon these studies by providing a tailored \\napplication for document interaction. The advent of machine \\nlearning, particularly deep learning, marked a significant leap \\nforward, with models like Word2Vec and GloVe introducing \\nword embeddings that captured semantic relationships \\nbetween words [3]. Furthermore, Transformers utilize self -\\nattention mechanisms to process and understand text in \\nparallel, rather than sequentially, enabling them to capture'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='between words [3]. Furthermore, Transformers utilize self -\\nattention mechanisms to process and understand text in \\nparallel, rather than sequentially, enabling them to capture \\nlong-range dependencies and contextual information more \\neffectively. BERT, with its bidirectional approach, improved \\nthe understanding of context within a text, while GPT, with \\nits autoregressive nature, exc elled in text generation [4][5]. \\nHowever, while the use of these models has become \\nwidespread, the integration of retrieval augmented generation \\nfor targeted PDF interaction remains under -explored. This \\nwork focuses on addressing this niche, aiming to bridge this \\ngap by combining large la nguage models with document \\nretrieval in the conversational interface, providing a more \\ntailored application in the domain of document interaction.  \\nThese advancements in LLMs have significantly enhanced \\ntext extraction and data retrieval capabilities. This capability'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content=\"tailored application in the domain of document interaction.  \\nThese advancements in LLMs have significantly enhanced \\ntext extraction and data retrieval capabilities. This capability \\nis particularly useful for handling the ever -growing volume \\nof digital documents, enabling efficient knowledge extraction \\nand insight generation [6].  \\n \\nBuilding on the advancements in LLMs, Retrieval -\\nAugmented Generation (RAG) systems enhance the \\ncapability of these models by integrating a retrieval \\nmechanism. RAG combines information retrieval and \\ngenerative processes to produce highly accurate and \\ncontextually relevant responses [7]. In a RAG framework, the \\nsystem first retrieves relevant passages from a large corpus of \\ndocuments based on the user's query [8][9]. This combination \\nallows the model to generate responses that are both informed \\nby a broad un derstanding of language and enriched with \\nprecise, relevant details from the retrieved content. This\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content=\"allows the model to generate responses that are both informed \\nby a broad un derstanding of language and enriched with \\nprecise, relevant details from the retrieved content. This \\napproach significantly improves the model's ability to handle \\ncomplex queries and extract pertinent information from large \\ndatasets, making it an invaluabl e tool for efficient and \\naccurate knowledge extraction. In this study, the authors \\nintroduce a novel approach  for text extraction that leverages \\nan LLM system to enhance user interaction with documents \\nvia a conversational interface.  \\nII. RELATED WORK \\nRecent advancements in document understanding and \\ninformation extraction have been driven by deep learning \\ntechniques. Traditional keyword matching and rule -based \\nmethods often struggle with complex documents, while deep \\nlearning models provide more robust  solutions capable of \\naccurately handling intricate structures. For instance, M. Li\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='methods often struggle with complex documents, while deep \\nlearning models provide more robust  solutions capable of \\naccurately handling intricate structures. For instance, M. Li \\net al. introduced the “BiomedRAG” model, which supervises \\nretrieval in the biomedical domain through varied chunk \\ndatabase creation, enhancing prediction accuracy [10]. \\nSimilarly, M. D. Cyril Zakka et al. developed the “Almanac”'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='framework, which retrieves medical guidelines and treatment \\nadvice, outperforming typical LLMs in factuality, \\ncompleteness, user preference, and safety [11]. Additionally, \\nK. Yang et al. introduced \"LeanDojo,\" a RAG -based LLM \\nthat streamlines theorem provi ng with comprehensive \\ntoolkits and data [12]. P. Lewis et al. explored a fine -tuning \\nrecipe for RAG models, leveraging pre -trained parametric \\nand non -parametric memory to improve language \\ndevelopment [13]. \\n \\nZ. Feng et al. proposed an iterative retrieval -generation \\ncollaborative framework that not only allows for the use of \\nboth parametric and non-parametric knowledge but also aids \\nin the discovery of the correct reasoning path via retrieval -\\ngeneration interactions, which is critical for tasks that require \\nmulti-step reasoning [14]. J. Miao et al. demonstrated the \\ndevelopment of a specialized ChatGPT model connected with \\nan RAG system that is intended to meet the KDIGO 2023'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='multi-step reasoning [14]. J. Miao et al. demonstrated the \\ndevelopment of a specialized ChatGPT model connected with \\nan RAG system that is intended to meet the KDIGO 2023 \\ncriteria for chronic kidney disease [15]. In a different domain, \\nH. Li et al. demonstrated the efficacy of leveraging attention \\nprocesses in neural networks to focus on key areas of material \\nfor better question answering in language models [16]. \\nSimilarly, Y. Zhang et al. suggested a unique M ulti-Modal \\nKnowledge-aware Hierarchical Attention Network \\n(MKHAN) to efficiently leverage a multi -modal knowledge \\ngraph (MKG) for explainable medical question answering \\n[17]. However, these approaches are often tailored to specific \\nuse cases, lacking the g eneralizability required for broader \\ndocument interaction tasks. \\nOur work builds upon these advancements by presenting a \\nRAG-inspired system for the interactive exploration of user -\\nuploaded PDF documents. We employ advanced sentence'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='Our work builds upon these advancements by presenting a \\nRAG-inspired system for the interactive exploration of user -\\nuploaded PDF documents. We employ advanced sentence \\nembeddings to ensure efficient retrieval of relevant content. \\nBy integrating this contex t into the response generation \\nprocess of the large language model (LLM), we create a more \\ntailored and contextually rich user experience. This approach \\nallows users to engage in focused conversations that explore \\nthe specific content and nuances of the up loaded PDFs, \\nthereby enhancing the effectiveness and relevance of \\ninformation retrieval and dialogue within the system. \\nIII. RETRIEVAL AUGMENTED GENERATION  \\nThe Retrieval -Augmented Generation (RAG) architecture \\nrepresents a sophisticated integration of information retrieval \\n(IR) and generative modeling techniques, designed to \\nenhance the precision and relevance of generated responses \\nin natural language proces sing tasks. The RAG process'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content=\"(IR) and generative modeling techniques, designed to \\nenhance the precision and relevance of generated responses \\nin natural language proces sing tasks. The RAG process \\ncommences with a robust retrieval component that sifts \\nthrough a vast corpus of documents to pinpoint relevant \\npassages aligned with the user's query. Traditional IR \\ntechniques like TF -IDF and BM25 evaluate the statistical \\nrelevance of terms across documents, prioritizing those that \\nclosely match the query [18]. Fig. 1 demonstrates the detailed \\nRAG architecture. Advanced methods such as neural \\nretrievers, exemplified by Dense Passage Retrieval (DPR), \\nemploy deep learning models to encode documents into dense \\nembeddings, capturing semantic relationships and enhancing \\ncontextual understanding. \\n \\n \\n \\nFig. 1. RAG Architecture \\n \\nOnce relevant passages are identified, they undergo encoding \\ninto document embeddings. These embeddings encapsulate \\nthe semantic meaning and context of the retrieved text,\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content=\"Once relevant passages are identified, they undergo encoding \\ninto document embeddings. These embeddings encapsulate \\nthe semantic meaning and context of the retrieved text, \\nemploying techniques like sentence embeddings from models \\nsuch as the Universal Sentence Encoder or BERT [19]. These \\nembeddings serve as enriched inputs to the subsequent stage \\nof the RAG architecture. Integration with a generative model, \\ntypically an LLM such as GPT, marks the next critical phase. \\nThe generative model utilizes the contex tual information \\nembedded in the document embeddings to produce responses \\nthat are not only grammatically accurate and fluent but also \\ncontextually aligned with the user's query [20]. By integrating \\ndetailed context from the embeddings, the generative mode l \\nensures that its responses are informed by both the broad \\nlinguistic knowledge it has learned during training and the \\nspecific details extracted from the retrieved passages. To\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='ensures that its responses are informed by both the broad \\nlinguistic knowledge it has learned during training and the \\nspecific details extracted from the retrieved passages. To \\nfurther refine performance, the RAG architecture often \\ninvolves fine -tuning t he generative model on task -specific \\ndatasets [21].  \\nIV. PROPOSED METHODOLOGY \\nA. Data Chunking \\nThe integration of the PyPDF2 library enables efficient text \\nextraction and management of PDF documents within the \\nmodel. Initially, a PdfReader object is created to represent the \\nentire PDF, facilitating seamless interaction with its content \\n[22]. \\n \\n \\nFig. 2. Workflow of the Proposed Model \\n \\nTo extract the raw text, the model iteratively traverses each \\npage using a loop, employing the extract_text() method. The \\nextracted texts are then consolidated into a single string \\nvariable, pdf_text, which captures the entire textual content \\nof the PDF. G iven the potentially large size of pdf_text, the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='extracted texts are then consolidated into a single string \\nvariable, pdf_text, which captures the entire textual content \\nof the PDF. G iven the potentially large size of pdf_text, the \\nmodel implements text chunking to improve computational'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='efficiency. Equation (1) outlines the mathematical process for \\ntext chunking, where n represents the number of desired \\nchunks, chunk_size indicates the size of each chunk, and  \\nchunk_overlap defines the overlap between consecutive \\nchunks. \\n𝑖𝑛 = (𝑛 − 1) × (𝑐ℎ𝑢𝑛𝑘_𝑠𝑖𝑧𝑒 − 𝑐ℎ𝑢𝑛𝑘_𝑜𝑣𝑒𝑟𝑙𝑎𝑝)           (1) \\n \\nEquation (1) calculates the starting index for each chunk \\nbased on its position n, chunk size, and overlap, ensuring that \\neach chunk overlaps with the previous ones by a specified \\nnumber of characters. To determine the ending index for each \\nchunk, Equation (2) provides a formula where jn indicates the \\nending index of chunk n. \\n𝑗𝑛 = 𝑖𝑛 + 𝑐ℎ𝑢𝑛𝑘_𝑠𝑖𝑧𝑒                                                     (2) \\n \\nThis approach allows for the systematic division of large text \\ninto smaller segments, facilitating easier processing and \\nanalysis in natural language processing tasks such as \\ninformation retrieval, text summarization, and machine'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='into smaller segments, facilitating easier processing and \\nanalysis in natural language processing tasks such as \\ninformation retrieval, text summarization, and machine \\ntranslations. Each chunk is associated with metadata to enrich \\nthe context and facilitate easier retrieval of specific text \\nsegments. Metadata is organized as a list of dictionaries, with \\neach dictionary corresponding to a chunk in the text list. \\nTypically, metadata includes a key -value pair where the key \\ndenotes the origin or source identifier of the chunk within the \\nPDF signifies the page number, and \"pl\" denotes paragraph \\nlevel). This approach allows for precise tracking and retrieval \\nof information within the PDF document, enhan cing the \\nmodel’s capability to handle and manipulate textual content \\neffectively in various applications and user interactions. \\nB. Vector Embeddings For Efficient Retrieval \\nIn preparing text for efficient retrieval, the model utilizes'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='effectively in various applications and user interactions. \\nB. Vector Embeddings For Efficient Retrieval \\nIn preparing text for efficient retrieval, the model utilizes \\nsentence embedding techniques to convert text chunks into \\nnumerical representations. This step is crucial for enabling \\nfast and accurate retrieval of semantically similar sentences \\nor passages f rom a document. Sentence embedding \\ntechniques are designed to map sentences from their original \\nhigh-dimensional textual space into a lower -dimensional \\nvector space. This transformation allows for efficient \\ncomparison and retrieval of sentences based on their semantic \\ncontent. A widely used approach for generating these \\nembeddings is employing pre -trained sentence transformers. \\nIn the model, the specific embedding used is “ sentence-\\ntransformers/all-MiniLM-L6-v2”. These models are trained \\non extensive text corpora and have learned to encapsulate the \\nsemantic essence of sentences within vector representations'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='transformers/all-MiniLM-L6-v2”. These models are trained \\non extensive text corpora and have learned to encapsulate the \\nsemantic essence of sentences within vector representations \\n[23]. The sentence embedding function is defined in Equation \\n(3) where S is a sentence composed of a sequence of words, \\nW is the embedding matrix for the vocabulary, and  f is the \\nsentence embedding function that maps a sentence S to a \\nvector 𝑠 ∈ 𝑅𝑑 . \\n \\n𝑠 = 𝑓(𝑠)                                                                            (3) \\n \\nThis function f often involves multiple steps, including word \\nembeddings, contextual embeddings using transformer \\nmodels, and aggregation methods. Each word 𝜔𝑖 in the \\nsentence S is mapped to a vector 𝑤i  using an embedding \\nmatrix W in Equation (4) where 𝑊[𝜔𝑖]'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='models, and aggregation methods. Each word 𝜔𝑖 in the \\nsentence S is mapped to a vector 𝑤i  using an embedding \\nmatrix W in Equation (4) where 𝑊[𝜔𝑖] \\n \\n𝑤𝑖 = 𝑊[𝜔𝑖]                                                                       (4)                                                                                                                                              \\n \\nTo obtain a single fixed -dimensional vector representing the \\nentire sentence, an aggregation method using mean pooling \\nis applied to the contextual embeddings. Equation (5) \\ncomputes the average of the contextual embeddings of all \\nwords in the sentence, res ulting in the final sentence \\nembedding  \\n \\n𝑆 =  \\n1\\n𝑛 ∑ ℎ𝑛\\n1                                                                          (5) \\n \\nThe vector S in Equation (5) is the sentence embedding, \\nwhich captures the meaning of the sentence in a way that \\nallows for efficient comparison and retrieval in natural \\nlanguage processing tasks. The model uses'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='which captures the meaning of the sentence in a way that \\nallows for efficient comparison and retrieval in natural \\nlanguage processing tasks. The model uses \\nHuggingFaceEmbedings class from langchain -\\ncommunity.embeddings module to work with the pre-trained \\nsentence transformer model. To load the model, the model \\nname is specified along with any extra configuration options. \\nThe embeddings object generates vector representations for \\neach text chunk using t he compute_embeddings method, \\nwhich takes a list of chunks as input and outputs \\ncorresponding embedding vectors that capture their meanings \\nin numerical form. These vectors are then combined with \\nmetadata, which includes information about the source of \\neach chunk within the PDF document. This integration results \\nin a final list of document representations optimized for \\nefficient retrieval. Consequently, the model can quickly and \\naccurately locate relevant passages in response to user'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content=\"in a final list of document representations optimized for \\nefficient retrieval. Consequently, the model can quickly and \\naccurately locate relevant passages in response to user \\nqueries, leveraging the meanings captured in the embeddings \\nalong with contextual details from the metadata. \\nC. Building The Conversational Retrieval Chain(CRC) \\n \\n This comprehensive approach involves several key \\ncomponents that synergize to deliver a seamless user \\nexperience. \\n \\n• Large Language Model  \\n \\nAt the heart of the CRC is the LLM, which generates \\nresponses to user queries. The model utilizes the Groq LLM \\n(llm_groq), integrated through the langchain_groq library. \\nThis pre -trained LLM leverages its extensive knowledge \\nbase, derived from vast amounts  of text data, to understand \\nand answer user questions accurately. The LLM's capability \\nto generate coherent and contextually appropriate responses \\nmakes it a crucial component of the CRC. \\n \\n• Retriever\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content=\"and answer user questions accurately. The LLM's capability \\nto generate coherent and contextually appropriate responses \\nmakes it a crucial component of the CRC. \\n \\n• Retriever \\n \\nThe retriever component is responsible for fetching relevant \\ninformation from the document based on the user's query. \\nThe model employs the faiss library from \\nlangchain_community.vectorstores to create a vector store \\nusing the document embeddings generated  earlier. These \\nembeddings transform text chunks into numerical \\nrepresentations that capture their semantic content. The \\nvector store allows for efficient retrieval of document \\nsections (chunks) that are semantically similar to the user's \\nquery. The as_ret riever method of the vector store object is\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='used to create a retriever object that integrates into the CRC, \\nenabling precise and relevant information retrieval. \\n \\n• Memory \\n \\nMemory management is essential for preserving \\nconversational context. The model employs the \\nConversationBufferMemory class from langchain.memory to \\nstore past user queries and LLM responses. This history is \\ncrucial for the LLM to reference prior interactio ns when \\ngenerating current responses. The memory is set up with \\nkeys: memory_key=\"chat_history\" for conversation history \\nand output_key=\"answer\" for the LLM\\'s responses. This \\nconfiguration facilitates more coherent and contextually \\naware interactions. \\n \\n• Chain Configuration \\n \\n To integrate these components, the \\nConversationalRetrievalChain.from_llm method is employed \\nwith specific parameters. The LLM parameter is configured \\nto utilize thr Groq LLM object(llm_groq). The chain_type is \\ndesignated as “stuff”, indicating a focus on retrieving factual'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='with specific parameters. The LLM parameter is configured \\nto utilize thr Groq LLM object(llm_groq). The chain_type is \\ndesignated as “stuff”, indicating a focus on retrieving factual \\ncontent from the document. The retriever parameter is linked \\nto the retriever object generated from the vector store,  \\nensuring efficient retrieval of relevant document section.  The \\nmemory parameter is set to conversation buffer memory \\nobject. Further, return_source_documents is True instructing \\nchains to return chunks along with responses. This ensures \\nthe accurate answers enriched with relevant context from the \\ndocuments.  \\nD. User Interaction And Model Response \\nThe model enables a natural and interactive conversation \\nbetween users and their uploaded PDF documents. The \\nprocess starts when users input their questions through a text \\nfield integrated into the Streamlit interface, ensuring that \\ninitiating queries is s traightforward and accessible. Users'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content=\"process starts when users input their questions through a text \\nfield integrated into the Streamlit interface, ensuring that \\ninitiating queries is s traightforward and accessible. Users \\ntype their questions into the provided input field \\n(st.text_input), and upon submission, the system promptly \\ncaptures the query and processes it using the retrieval chain. \\nThe chain.invoke method efficiently directs the  query to \\nsubsequent stages of the workflow. \\nAt the core of the model's functionality is the Conversational \\nRetrieval Chain. To generate contextually rich responses, the \\nchain first accesses the conversation history via the \\nConversationBufferMemory [25], which retains past user \\nqueries and responses,  ensuring that the current interaction \\nbenefits from previous exchanges. \\nSubsequently, the system utilizes a retriever that operates on \\na pre -constructed document vector store, comprising \\nembeddings of text chunks extracted from the PDF. The\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content=\"Subsequently, the system utilizes a retriever that operates on \\na pre -constructed document vector store, comprising \\nembeddings of text chunks extracted from the PDF. The \\nretriever searches for document sections that are semantically \\nsimilar to the user's query, using cosine similarity to evaluate \\nhow closely related two vectors are within the embedding \\nspace. Equation (6) illustrates the concept of cosine \\nsimilarity. \\n \\ncos(𝑢, 𝑣) =\\n𝑢.𝑣\\n(‖𝑢‖|‖𝑣‖)\\n                                                      (6) \\n \\nCosine similarity scores range from -1 (completely \\ndissimilar) to 1 (identical). The model retrieves the document \\nsections with the highest cosine similarity scores, indicating \\ntheir relevance to the user's query. These retrieved sections, \\nalong with the c onversation context, are then fed into the \\nGroq LLM. By leveraging its pre -trained knowledge and the \\nspecific context from the retrieved text, the Groq LLM \\ngenerates comprehensive and accurate responses to user\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content=\"Groq LLM. By leveraging its pre -trained knowledge and the \\nspecific context from the retrieved text, the Groq LLM \\ngenerates comprehensive and accurate responses to user \\nquestions. \\n \\nWhen relevant document sections are retrieved, the model \\nenhances responses by referencing these sources. It assigns \\nunique identifiers to each retrieved section and may include \\nthese references in the response text. This method not only \\nensures transparen cy but also allows users to verify the \\ninformation's origin. To improve usability, Streamlit \\nexpanders (st.expander) are used, enabling users to easily \\nview the content of the retrieved document sections. By \\nclicking on the corresponding source names, user s can \\nexpand and read the specific excerpts that informed the \\nLLM's response. This interactive feature allows users to \\nexplore the document content more deeply, enhancing their \\nunderstanding and engagement with the material. \\nV. EXPERIMENTATIONS AND RESULTS\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content=\"LLM's response. This interactive feature allows users to \\nexplore the document content more deeply, enhancing their \\nunderstanding and engagement with the material. \\nV. EXPERIMENTATIONS AND RESULTS \\nTo assess the model's capability to navigate and summarize \\ncomplex academic materials, we employed ROUGE (Recall-\\nOriented Understudy for Gisting Evaluation) scores, a widely \\naccepted metric for evaluating the quality of automatically \\ngenerated summaries ag ainst human -written references. \\nHowever, relying solely on ROUGE metrics may not \\nadequately reflect the system's interactive and conversational \\naspects. Therefore, future studies will include qualitative \\nevaluations to examine user interaction quality and the \\nmodel's effectiveness from an end-user perspective, ensuring \\na comprehensive assessment of its performance. \\nThe evaluation utilized a carefully curated dataset comprising \\ntop-cited research articles, known for their dense information\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content=\"a comprehensive assessment of its performance. \\nThe evaluation utilized a carefully curated dataset comprising \\ntop-cited research articles, known for their dense information \\ncontent, technical language, and intricate methodologies. \\nThese articles posed significant challenges, making them \\nwell-suited for rigorously testing the model's summarization \\ncapabilities. The article abstracts served as input reference \\nsummaries for calculating the ROUGE scores for each \\ndocument. This analysis provided valuable insights into the \\nmodel's proficiency in accurately capturing and summarizing \\ncritical findings from highly technical and detailed research \\nliterature. ROUGE measures the overlap of n-grams between \\nthe generated text and the reference text, mathematically \\nrepresented in Equation (6), where the maximum number of \\nn-grams co -occurring in both the candidate and reference \\nsummaries is considered. \\n \\n𝑅𝑂𝑈𝐺𝐸 =  \\n∑ ∑ 𝐶𝑜𝑢𝑛𝑡_𝑚𝑎𝑡𝑐ℎ(𝑔𝑟𝑎𝑚𝑛)𝑔𝑟𝑎𝑚𝑛∈𝑆𝑆∈{𝑆𝑢𝑚𝑚𝑎𝑟𝑖𝑒𝑠}\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content=\"represented in Equation (6), where the maximum number of \\nn-grams co -occurring in both the candidate and reference \\nsummaries is considered. \\n \\n𝑅𝑂𝑈𝐺𝐸 =  \\n∑ ∑ 𝐶𝑜𝑢𝑛𝑡_𝑚𝑎𝑡𝑐ℎ(𝑔𝑟𝑎𝑚𝑛)𝑔𝑟𝑎𝑚𝑛∈𝑆𝑆∈{𝑆𝑢𝑚𝑚𝑎𝑟𝑖𝑒𝑠}\\n∑ ∑ 𝐶𝑜𝑢𝑛𝑡(𝑔𝑟𝑎𝑚𝑛)𝑔𝑟𝑎𝑚𝑛∈𝑆𝑆∈{𝑆𝑢𝑚𝑚𝑎𝑟𝑖𝑒𝑠}\\n       (6) \\n \\nThe study specifically used ROUGE -1 (unigrams) and \\nROUGE-2 (bigrams) in the evaluation. ROUGE -L measures \\nthe longest common subsequence (LCS) between the \\ncandidate and reference summaries. It's calculated using \\nEquations (7), and (8) followed by Equation (9), where X is \\nthe reference summary of length m, Y is the candidate\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='summary of length n, and β is typically set to favor recall ( β \\n>1). \\n \\n𝑅𝑂𝑈𝐺𝐸 − 𝐿𝑅𝑒𝑐𝑎𝑙𝑙 =  \\n𝐿𝐶𝑆(𝑋,𝑌)\\n𝑚                                            (7) \\n𝑅𝑂𝑈𝐺𝐸 − 𝐿𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 =  \\n𝐿𝐶𝑆(𝑋,𝑌)\\n𝑛                                        (8) \\n𝑅𝑂𝑈𝐺𝐸 − 𝐿𝐹 =  \\n(1+𝛽2)∙𝑅∙𝑃\\n𝑅+𝛽2∙𝑃                                                (9) \\n \\nTo evaluate the model performance, the authors tested it with \\na custom dataset of various research papers sourced from top \\nresearch databases and analyzed the ROUGE scores of the \\ngenerated answers. The relatively moderate ROUGE scores \\ncan be attributed to the model’s focus on condensing \\nextensive content into concise responses. This indicates the \\nmodel’s tendency to prioritize brevity and specific ity over \\nword-for-word overlap.  The average representative scores \\nobtained from evaluating upon the dataset, are given in Table. \\nI. \\nTABLE I.  PERFORMANCE METRICS OF THE MODEL \\nPerformance Metric Score Values (Average) \\nROUGE – 1 0.4604 \\nROUGE – 2 0.3576'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content=\"obtained from evaluating upon the dataset, are given in Table. \\nI. \\nTABLE I.  PERFORMANCE METRICS OF THE MODEL \\nPerformance Metric Score Values (Average) \\nROUGE – 1 0.4604 \\nROUGE – 2 0.3576 \\nROUGE - L 0.4283 \\n \\nThe scores indicate that approximately 46% of individual \\nwords (ROUGE -1) and around 35% of bigram phrases \\n(ROUGE-2) in the generated responses matched those found \\nin the original documents. The ROUGE -L score, which lies \\nbetween ROUGE -1 and ROUGE -2, demonst rates some \\npreservation of word order while accommodating gaps and \\nrephrasing. The relatively low ROUGE scores highlight the \\nsystem's capability to distill information into concise answers \\ninstead of replicating large text segments. Moreover, the \\ncomplexity and dense information structure of the input \\nresearch articles create s a high bar for any model aiming to \\nbalance conciseness with informativeness.  Good summaries \\noften rephrase ideas, leading to lower word-for-word matches\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='research articles create s a high bar for any model aiming to \\nbalance conciseness with informativeness.  Good summaries \\noften rephrase ideas, leading to lower word-for-word matches \\nbut potentially better conveyance of key concepts. \\nFurthermore, the system focuses on providing specific \\nanswers to questions, naturally leading to lower overlap with \\nthe full tex t of the documents. Moreover, the significant \\nlength difference between focused answers and entire articles \\nfurther contributes to the lower ROUGE scores. Table. II \\ncompares the model performance with other SOTA \\napproaches. \\nTABLE II.  COMPARISON OF PROPOSED MODEL PERFORMANCE \\nMETRICS \\nModel ROUGE - 1 ROUGE - 2 ROUGE - L \\nRAG-PDF \\n (Our Model) \\n0.4604 0.3576 0.4283 \\nML + RL \\nROUGE + Novel, \\nWith LM  [26] \\n \\n0.4019 \\n \\n0.1738 \\n \\n0.3752 \\nCOSUM [27] 0.4908 0.2379 0.2834 \\nLatent Semantic \\nAnalysis [28] \\n0.4621 0.2618 0.3479 \\nEdgeSumm [29] 0.5379 0.2858 0.4979 \\nGenerative \\nAdversarial \\nNetwork  [30] \\n \\n0.3992 \\n \\n0.1765'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='0.3752 \\nCOSUM [27] 0.4908 0.2379 0.2834 \\nLatent Semantic \\nAnalysis [28] \\n0.4621 0.2618 0.3479 \\nEdgeSumm [29] 0.5379 0.2858 0.4979 \\nGenerative \\nAdversarial \\nNetwork  [30] \\n \\n0.3992 \\n \\n0.1765 \\n \\n0.3671 \\nTFRSP [31] 0.2483 0.2874 0.2043 \\nTable II presents a comparative analysis of various models \\nbased on ROUGE -1, ROUGE -2, and ROUGE -L scores, \\nwhich evaluate summary quality against reference \\nsummaries. The RAG -PDF model demonstrates strong \\nperformance, achieving a ROUGE -1 score of 0.4604, \\nROUGE-2 score of 0.3576, and ROUGE -L score of 0.4283, \\nindicating its effectiveness in capturing both individual words \\nand longer sequences for coherent summaries. \\nWhile EdgeSumm excels in ROUGE -1 and ROUGE -L, its \\nlower ROUGE -2 score reveals limitations in bigram \\ncoherence. Our model balances coherence, particularly in \\ncomplex technical text. In contrast, the ML + RL ROUGE + \\nNovel model shows poorer performance, espe cially in \\nROUGE-2 (0.1738) and ROUGE -L (0.3752), suggesting'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='complex technical text. In contrast, the ML + RL ROUGE + \\nNovel model shows poorer performance, espe cially in \\nROUGE-2 (0.1738) and ROUGE -L (0.3752), suggesting \\nchallenges in capturing bi -gram sequences. COSUM \\nperforms well in ROUGE -1 (0.4908) but lacks coherence in \\nlonger sequences with lower ROUGE -2 (0.2379) and \\nROUGE-L (0.2834). \\nLatent Semantic Analysis is comparable to our model in \\nROUGE-1 (0.4621) but falls short in ROUGE-2 (0.2618) and \\nROUGE-L (0.3479). The Generative Adversarial Network \\nmodel exhibits low scores across metrics, particularly in \\nROUGE-2 (0.1765). Lastly, the TFR SP model scores the \\nlowest in ROUGE -1 (0.2483) and ROUGE -L (0.2043), \\nindicating significant challenges in summary generation. \\n \\nWhile ROUGE metrics provide useful insights, they may not \\nfully capture user experience or interaction quality. \\nTherefore, future work will focus on incorporating user -\\ncentered evaluations, including qualitative feedback and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='fully capture user experience or interaction quality. \\nTherefore, future work will focus on incorporating user -\\ncentered evaluations, including qualitative feedback and \\ninteraction analysis, to align the system’s performance with \\nreal-world needs. \\nVI. CONCLUSION AND FUTURE WORK \\nThe model introduces a unique approach for interacting with \\nPDF documents via a conversational interface, harnessing the \\npower of LLMs and RAG. This system enables users to \\nextract valuable insights from complex and text -heavy \\nmaterials effectively. One of its standout features is its focus \\non the specific content of uploaded PDFs, rather than relying \\non extensive external knowledge bases. By employing \\nsentence embeddings, the model converts text chunks into \\nnumerical vectors and utilizes cosine similarity for efficient \\nretrieval, aligning responses closely with user intent. \\nPerformance evaluations reveal competitive ROUGE \\nscores—0.4604 for ROUGE -1, 0.3576 for ROUGE -2, and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content=\"retrieval, aligning responses closely with user intent. \\nPerformance evaluations reveal competitive ROUGE \\nscores—0.4604 for ROUGE -1, 0.3576 for ROUGE -2, and \\n0.4283 for ROUGE-L—demonstrating the model's capability \\nto capture essential content a nd structure while \\noutperforming many existing models in summarization and \\nquestion answering. \\nTo enhance the practical application of this system, future \\nwork will aim to generalize its approach for a wider variety \\nof document types. This will include refining the retrieval \\nmechanism to accommodate diverse structures, such as legal, \\nfinancial, and multimodal documents, thereby increasing the \\nsystem's versatility in real -world scenarios. We also plan to \\nincorporate reinforcement learning techniques to improve \\nuser interactions, allowing the model to adapt dynamically \\nbased on feedback. Exploring the incorporation of knowledge \\ngraphs and ontologies may also improve semantic\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='user interactions, allowing the model to adapt dynamically \\nbased on feedback. Exploring the incorporation of knowledge \\ngraphs and ontologies may also improve semantic \\nunderstanding and contextualization. Furthermore, refining \\nthe model with user interaction data and reinforcement'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='learning can facilitate more personalized responses, ensuring \\nthat the system continuously evolves to meet user needs \\neffectively. \\nREFERENCES \\n[1] Khurana, D., Koli, A., Khatter, K., & Singh, S. (2023). Natural \\nlanguage processing: State of the art, current trends, and challenges. \\nMultimedia Tools and Applications, 82(4), 3713–3744. \\n[2] Bahl, L. R., Brown, P. F., de Souza, P. V., & Mercer, R. L. (1989). A \\ntree-based statistical language model for natural language speech \\nrecognition. IEEE Transactions on Acoustics, Speech, and Signal \\nProcessing, 37(7), 1001–1008. \\n[3] Curto, G., Jojoa Acosta, M. F., & Comim, F. (2024). Are AI systems \\nbiased against the poor? A machine learning analysis using Word2Vec \\nand GloVe embeddings. AI & Society, 39(3), 617–632. \\n[4] Zheng, X., Zhang, C., & Woodland, P. C. (2021). Adapting GPT, GPT-\\n2, and BERT language models for speech recognition. In 2021 IEEE \\nAutomatic Speech Recognition and Understanding Workshop (ASRU) \\n(pp. 162–168).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='2, and BERT language models for speech recognition. In 2021 IEEE \\nAutomatic Speech Recognition and Understanding Workshop (ASRU) \\n(pp. 162–168). \\n[5] Qu, Y., Liu, P., Song, W., Liu, L., & Cheng, M. (2020). A text \\ngeneration and prediction system: Pre -training on new corpora using \\nBERT and GPT -2. In 2020 IEEE 10th International Conference on \\nElectronics Information and Emergency Communication (ICEIEC) \\n(pp. 323–326). \\n[6] Wang, L., Ma, C., & Feng, X. (2024). A survey on large language \\nmodel-based autonomous agents. Frontiers of Computer Science, 18, \\n186345.  \\n[7] Xu, L., Lu, L., Liu, M., & others. (2024). Nanjing Yunjin intelligent \\nquestion-answering system based on knowledge graphs and retrieval -\\naugmented generation technology. Heritage Science, 12, 118. \\n[8] Louis, A., van Dijck, G., & Spanakis, G. (2024). Interpretable Long -\\nForm Legal Question Answering with Retrieval -Augmented Large \\nLanguage Models. Proceedings of the AAAI Conference on Artificial'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='Form Legal Question Answering with Retrieval -Augmented Large \\nLanguage Models. Proceedings of the AAAI Conference on Artificial \\nIntelligence, 38(20), 22266-22275. \\n[9] Yang, X., Chen, A., PourNejatian, N., & others. (2022). A large \\nlanguage model for electronic health records. npj Digital Medicine, 5, \\n194.  \\n[10] Li, M., Kilicoglu, H., Xu, H., & Zhang, R. (2024). BiomedRAG: A \\nretrieval-augmented large language model for biomedicine. ArXiv: \\nComputation and Language. \\n[11] Hiesinger, W., Zakka, C., Chaurasia, A., Shad, R., Dalal, A., Kim, J., \\nMoor, M., Alexander, K., Ashley, E., Boyd, J., Boyd, K., Hirsch, K., \\nLanglotz, C., & Nelson, J. (2023). Almanac: Retrieval -augmented \\nlanguage models for clinical medicine. Research Square.  \\n[12] Yang, K., Swope, A., Gu, A., Chalamala, R., Song, P., Yu, S., Godil, \\nS., Prenger, R. J., & Anandkumar, A. (2023). LeanDojo: Theorem \\nproving with retrieval -augmented language models. In Advances in'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='S., Prenger, R. J., & Anandkumar, A. (2023). LeanDojo: Theorem \\nproving with retrieval -augmented language models. In Advances in \\nNeural Information Processing Systems 36 (NeurIPS 2023). \\n[13] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., \\nKüttler, H., Lewis, M., Yih, W., Rocktäschel, T., Riedel, S., & Kiela, \\nD. (2020). Retrieval -augmented generation for knowledge -intensive \\nNLP tasks. In Advances in Neural Information Processing Systems 33 \\n(NeurIPS 2020). \\n[14] Feng, Z., Feng, X., Zhao, D., Yang, M., & Qin, B. (2024). Retrieval -\\ngeneration synergy augmented large language models. In ICASSP \\n2024 – IEEE International Conference on Acoustics, Speech and Signal \\nProcessing (pp. 11661–11665).  \\n[15] Miao, J., Thongprayoon, C., Suppadungsuk, S., Garcia Valencia, O., & \\nCheungpasitporn, W. (2024). Integrating retrieval -augmented \\ngeneration with large language models in nephrology: Advancing \\npractical applications. Medicina.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='Cheungpasitporn, W. (2024). Integrating retrieval -augmented \\ngeneration with large language models in nephrology: Advancing \\npractical applications. Medicina.  \\n[16] Hao, T., Li, X., He, Y., & others. (2022). Recent progress in leveraging \\ndeep learning methods for question answering. Neural Computing & \\nApplications, 34, 2765–2783.  \\n[17] Zhang, Y., Qian, S., Fang, Q., & Xu, C. (2019). Multi -modal \\nknowledge-aware hierarchical attention network for explainable \\nmedical question answering. In Proceedings of the 27th ACM \\nInternational Conference on Multimedia (pp. 1178–1187). \\n[18] Sawarkar, K., Mangal, A., & Solanki, S. R. (2024). Blended RAG: \\nImproving RAG (Retriever -Augmented Generation) accuracy with \\nsemantic search and hybrid query-based retrievers. ArXiv: Information \\nRetrieval.  \\n[19] Arif, N., Latif, S., & Latif, R. (2021). Question classification using \\nUniversal Sentence Encoder and deep contextualized transformer. In'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='Retrieval.  \\n[19] Arif, N., Latif, S., & Latif, R. (2021). Question classification using \\nUniversal Sentence Encoder and deep contextualized transformer. In \\n2021 14th International Conference on Developments in eSystems \\nEngineering (DeSE) (pp. 206–211).  \\n[20] Goswami, M., Panda, N., Mohanty, S., & Pattnaik, P. K. (2023). \\nMachine learning techniques and routing protocols in 5G and 6G \\nmobile network communication system – An overview. In 2023 7th \\nInternational Conference on Trends in Electronics and Informatics \\n(ICOEI) (pp. 1094–1101).  \\n[21] Li, H., Su, Y., Cai, D., Wang, Y., & Liu, L. (2022). A survey on \\nretrieval-augmented text generation. ArXiv: Computation and \\nLanguage. \\n[22] Bui, D. D. A., Del Fiol, G., & Jonnalagadda, S. (2016). PDF text \\nclassification to leverage information extraction from publication \\nreports. Journal of Biomedical Informatics, 61, 141–148.  \\n[23] Heimerl, F., Kralj, C., Möller, T., & Gleicher, M. (2022). embComp:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='reports. Journal of Biomedical Informatics, 61, 141–148.  \\n[23] Heimerl, F., Kralj, C., Möller, T., & Gleicher, M. (2022). embComp: \\nVisual interactive comparison of vector embeddings. IEEE \\nTransactions on Visualization and Computer Graphics, 28(8), 2953 –\\n2969.  \\n[24] Goswami, M., Mohanty, S., & Pattnaik, P. K. (2024). Optimization of \\nmachine learning models through quantization and data bit reduction \\nin healthcare datasets. Franklin Open, 8. \\n[25] Singh, A., Ehtesham, S., Mahmud, R., & Kim, J. -H. (2024). \\nRevolutionizing mental health care through LangChain: A journey with \\na large language model. In 2024 IEEE 14th Annual Computing and \\nCommunication Workshop and Conference (CCWC) (pp. 73–78).  \\n[26] Kryściński, W., Paulus, R., Xiong, C., & Socher, R. (2018). Improving \\nabstraction in text summarization. ArXiv: Computation and Language.  \\n[27] Alguliyev, R. M., Aliguliyev, R. M., Isazade, N. R., Abdi, A., & Idris, \\nN. B. (2018). COSUM: Text summarization based on clustering and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-01-17T00:36:33+05:30', 'title': 'Paper Title (use style: paper title)', 'author': 'IEEE', 'moddate': '2025-01-17T00:36:33+05:30', 'source': '..\\\\data\\\\pdf\\\\Conversational Text Extraction with LLM.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6', 'source_file': 'Conversational Text Extraction with LLM.pdf', 'file_type': 'pdf'}, page_content='[27] Alguliyev, R. M., Aliguliyev, R. M., Isazade, N. R., Abdi, A., & Idris, \\nN. B. (2018). COSUM: Text summarization based on clustering and \\noptimization. Expert Systems, 36.  \\n[28] Ozsoy, M. G., Alpaslan, F. N., & Cicekli, I. (2011). Text \\nsummarization using latent semantic analysis. Journal of Information \\nScience, 37(4), 405–417.  \\n[29] El-Kassas, W. S., Salama, C. R., Rafea, A. A., & Mohamed, H. K. \\n(2020). EdgeSumm: Graph -based framework for automatic text \\nsummarization. Information Processing & Management, 57(6).  \\n[30] Liu, L., Lu, Y., Yang, M., Qu, Q., Zhu, J., & Li, H. (2018). Generative \\nadversarial network for abstractive text summarization. Proceedings of \\nthe AAAI Conference on Artificial Intelligence, 32(1). \\n[31] M. S. M., R. M. P., A. R. E., & E. S. G. S. R. (2020). Text \\nsummarization using text frequency ranking sentence prediction. In \\n2020 4th International Conference on Computer, Communication and \\nSignal Processing (ICCCSP) (pp. 1–6).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 0, 'page_label': '1', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation\\nLANG MEI, Huawei Cloud BU, China\\nSIYU MO, Huawei Cloud BU, China\\nZHIHAN YANG, Huawei Cloud BU, China\\nCHONG CHEN∗, Huawei Cloud BU, China\\nMultimodal Retrieval-Augmented Generation (MRAG) represents a significant advancement in enhancing the\\ncapabilities of large language models (LLMs) by integrating multimodal data, such as text, images, and videos,\\ninto the retrieval and generation processes. Traditional Retrieval-Augmented Generation (RAG) systems, which\\nprimarily rely on textual data, have shown promise in reducing hallucinations and improving response accuracy\\nby dynamically incorporating external knowledge. However, these systems are limited by their reliance on\\ntext-only modalities, which restricts their ability to leverage the rich, contextual information available in\\nmultimodal data. MRAG addresses this limitation by extending the RAG framework to include multimodal'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 0, 'page_label': '1', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='multimodal data. MRAG addresses this limitation by extending the RAG framework to include multimodal\\nretrieval and generation, thereby enabling more comprehensive and contextually relevant responses. In MRAG,\\nthe retrieval step involves locating and integrating relevant knowledge from diverse modalities, while the\\ngeneration step utilizes multimodal large language models (MLLMs) to produce answers that incorporate\\ninformation from multiple data types. This approach not only enhances the quality of question-answering\\nsystems but also significantly reduces the incidence of hallucinations by grounding responses in factual,\\nmultimodal knowledge. Recent research has demonstrated that MRAG outperforms traditional text-modal\\nRAG, particularly in scenarios where visual and textual information are both critical for understanding and\\nresponding to queries. This survey systematically reviews the current state of MRAG research, focusing'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 0, 'page_label': '1', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='responding to queries. This survey systematically reviews the current state of MRAG research, focusing\\non four key aspects: essential components and technologies, datasets, evaluation methods and metrics, and\\nexisting limitations. By analyzing these dimensions, we aim to provide a comprehensive understanding of\\nhow MRAG can be effectively constructed and improved. Additionally, we highlight current challenges and\\npropose future research directions, encouraging further exploration into this promising paradigm. Our work\\nunderscores the potential of MRAG to revolutionize multimodal information retrieval and generation, offering\\na forward-looking perspective on its development and applications.\\nCCS Concepts: • Information systems →Multimedia and multimodal retrieval ; Language models; •\\nComputing methodologies →Natural language processing .\\nAdditional Key Words and Phrases: Multimodal Retrieval-Augmented Generation, Multimodal Large Language'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 0, 'page_label': '1', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Computing methodologies →Natural language processing .\\nAdditional Key Words and Phrases: Multimodal Retrieval-Augmented Generation, Multimodal Large Language\\nModel, Multimodal Document Parsing and Indexing, Multimodal Search Planning\\nACM Reference Format:\\nLang Mei, Siyu Mo, Zhihan Yang, and Chong Chen. 2018. A Survey on Multimodal Retrieval-Augmented\\nGeneration. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email\\n(Conference acronym ’XX). ACM, New York, NY, USA, 80 pages. https://doi.org/XXXXXXX.XXXXXXX\\n∗Chong Chen is the corresponding author.\\nAuthors’ Contact Information: Lang Mei, Huawei Cloud BU, Beijing, China, meilang1@huawei.com; Siyu Mo, Huawei Cloud\\nBU, Beijing, China, mosiyu@huawei.com; Zhihan Yang, Huawei Cloud BU, Beijing, China, yangzhihan4@huawei.com;\\nChong Chen, Huawei Cloud BU, Beijing, China, chenchong55@huawei.com.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 0, 'page_label': '1', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='BU, Beijing, China, mosiyu@huawei.com; Zhihan Yang, Huawei Cloud BU, Beijing, China, yangzhihan4@huawei.com;\\nChong Chen, Huawei Cloud BU, Beijing, China, chenchong55@huawei.com.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\\nConference acronym ’XX, Woodstock, NY\\n© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 978-1-4503-XXXX-X/2018/06\\nhttps://doi.org/XXXXXXX.XXXXXXX'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 0, 'page_label': '1', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Conference acronym ’XX, Woodstock, NY\\n© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 978-1-4503-XXXX-X/2018/06\\nhttps://doi.org/XXXXXXX.XXXXXXX\\n, Vol. 1, No. 1, Article . Publication date: April 2018.\\narXiv:2504.08748v1  [cs.IR]  26 Mar 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 1, 'page_label': '2', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='2 Trovato et al.\\n1 Introduction\\nLarge language models (LLMs), especially the Transformer-based variants, have achieved extraor-\\ndinary success in many language-related tasks. Through pre-training on extensive, high-quality\\ninstruction datasets, LLMs can learn a wide range of language patterns, structures, and factual\\nknowledge. These pre-trained LLMs can generate human-like text with high degrees of fluency and\\ncoherence, and attain strong performance on question-answering tasks, which demonstrates their\\nability to understand and respond to a wide range of queries. However, despite their impressive\\ncapabilities, LLMs still face significant limitations. One of the primary challenges lies in their\\nperformance within specific domains or knowledge-intensive tasks. While these models are often\\ntrained on diverse and extensive datasets, such datasets may not cover the depth of knowledge\\nrequired for highly specialized fields or real-time information updates. This can be particularly'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 1, 'page_label': '2', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='trained on diverse and extensive datasets, such datasets may not cover the depth of knowledge\\nrequired for highly specialized fields or real-time information updates. This can be particularly\\nproblematic in areas like medicine, law, finance, and other technical fields where precision and\\nup-to-date knowledge are to be prioritized. When handling queries that extend beyond the scope\\nof their training knowledge or require the most current information, LLMs may generate responses\\nthat are speculative or based on patterns they have learned, rather than on verified facts. This\\ncan result in misleading, incorrect, or even entirely fabricated answers, a phenomenon known as\\n\"hallucination\". Minimizing the incidence of hallucinations is important for enhancing the reliability\\nof LLMs in providing accurate and context-relevant information across different domains.\\nRecently, Retrieval-Augmented Generation (RAG) has emerged as an effective solution to mitigate'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 1, 'page_label': '2', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='of LLMs in providing accurate and context-relevant information across different domains.\\nRecently, Retrieval-Augmented Generation (RAG) has emerged as an effective solution to mitigate\\nhallucinations, by enhancing the generation capabilities of large language models (LLMs) through\\nthe retrieval of relevant external knowledge. Existing RAG systems typically operate through\\na two-step process: retrieval and generation. In the retrieval step, the goal is to quickly locate\\nrelevant knowledge that is semantically similar to the query from a large-scale document collection.\\nSince the relevant knowledge is often scattered across various parts of documents, each document\\nis pre-processed into multiple chunks. Additional chunks may be created through manual or\\nautomated methods. This process, known as document chunkerization, ensures that fine-grained\\nknowledge can be retrieved more efficiently. In the generation step, the retrieved document chunks'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 1, 'page_label': '2', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='automated methods. This process, known as document chunkerization, ensures that fine-grained\\nknowledge can be retrieved more efficiently. In the generation step, the retrieved document chunks\\nare combined with the query to form an augmented input. This augmented input provides the LLM\\nwith context that includes external knowledge. Furthermore, RAG allows LLMs to dynamically\\nintegrate the latest information during the inference stage. This capability ensures that the model’s\\nresponses are not only based on static, pre-trained knowledge but are continuously updated\\nwith current and relevant data. By retrieving and referencing external knowledge, RAG grounds\\nthe generated responses in factual information, thereby significantly reducing the occurrence of\\nhallucinations. However, previous research on RAG systems has primarily focused on knowledge\\nbases built from plain text and LLMs pre-trained on plain text, ignoring other rich sources of'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 1, 'page_label': '2', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='hallucinations. However, previous research on RAG systems has primarily focused on knowledge\\nbases built from plain text and LLMs pre-trained on plain text, ignoring other rich sources of\\nknowledge available for query responses in the real world, such as videos and images, referred to\\nas \"multimodal data\".\\nMultimodal data refers to data that comes from multiple sources or formats. This can include text,\\nimages, audio, video, and other types of data. In real-world scenarios, humans naturally interact\\nwith multimodal data, such as browsing web pages that combine text, images, and videos in mixed\\nlayouts. By analyzing images or videos alongside text, the user can better understand the context\\nof the content, and thus improve the satisfaction with the quality of the answers. For example, if\\na passenger inquires about how to store luggage while flying, it will be clearer that the system\\nprovides relevant graphic guides or instructional videos. However, transferring the capabilities'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 1, 'page_label': '2', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='a passenger inquires about how to store luggage while flying, it will be clearer that the system\\nprovides relevant graphic guides or instructional videos. However, transferring the capabilities\\nof LLMs to the domain of multimodal text and images remains an active area of research, as\\nplain-text LLMs are typically trained only on textual corpora and lack perceptual abilities for visual\\nsignals. How to effectively incorporate multimodal data is important to enhance the capability of\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 2, 'page_label': '3', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 3\\nLLMs. In recent years, the development of multimodal generative models has showcased additional\\napplication possibilities. Apart from textual generative models, multimodal generative models\\nhave been increasingly applied in fields such as human-computer interaction, robot control, image\\nsearch, and speech generation. Similarly, based on multimodal generative models and multimodal\\ndata, how to effectively process Multimodal Retrieval-Augmented Generation (MRAG) is an issue\\nthat needs to be explored.\\nRecently, some research have demonstrated that MRAG with multimodal data outperforms\\ntraditional text-modal RAG. By enhancing the generation capabilities of multimodal large language\\nmodels (MLLMs) through the retrieval of external multimodal knowledge, MRAG system can\\nfurther enhance question answering capabilities and quality, thereby further reducing hallucination'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 2, 'page_label': '3', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='models (MLLMs) through the retrieval of external multimodal knowledge, MRAG system can\\nfurther enhance question answering capabilities and quality, thereby further reducing hallucination\\nissues. The main differences between text-modal RAG and MRAG lie in retrieval and generation. In\\nthe retrieval step, the former only needs to consider retrieving relevant textual knowledge from\\na large document collection, while the latter needs to consider how to retrieve and integrate the\\nrelevant knowledge under different modalities, as well as the relationships between knowledge in\\ndifferent modalities. In the generation step, the former only needs to consider the input text query\\nand relevant textual knowledge, and output a text answer based on the LLM. The latter, however,\\nneeds to consider how to utilize the input query from different modalities and multimodal retrieval\\nknowledge, and output an answer that includes information from different modalities based on the\\nMLLM.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 2, 'page_label': '3', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='knowledge, and output an answer that includes information from different modalities based on the\\nMLLM.\\nConsidering the immense potential of MRAG in this field, this survey aims to systematically\\nreview and analyze the current state and main challenges of MRAG. We discuss existing research\\nfrom several key perspectives: 1) What important components and technologies are involved in\\nMRAG? 2) What types of datasets can be used for the evaluation of MRAG? 3) What methods and\\nmetrics are used to evaluate MRAG? 4) What limitations exist in the different aspects of MRAG? We\\nexplore the main challenges faced by MRAG, and hope to provide clearer guidelines for their future\\ndevelopment. In summary, the main contributions of this paper are as follows:\\n•Comprehensive and Timely Survey: We conducted an extensive survey on the emerging\\nparadigm of multimodal Retrieval-Augmented Generation, systematically reviewing the current\\nstate of research and development in this field.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 2, 'page_label': '3', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='paradigm of multimodal Retrieval-Augmented Generation, systematically reviewing the current\\nstate of research and development in this field.\\n•Systematic Analysis from Four Key Perspectives: Our survey is organized around four key\\naspects: essential components and technologies, datasets, evaluation methods and metrics, and\\nlimitations. This structured approach allows for a detailed understanding of how MRAG can be\\nefficiently constructed, its reliability issues, and how it can be further improved.\\n•Current Challenges and Future Research Directions: We discuss the existing challenges of\\nMRAG, highlight potential research opportunities and directions, and provide a forward-looking\\nperspective on the future development of this paradigm, encouraging researchers to delve deeper\\ninto this exciting field.\\nWe have provided an overall introduction to this survey paper. The section 2 presents a compre-\\nhensive overview of multimodal retrieval-augmented generation, covering multiple developmental'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 2, 'page_label': '3', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='We have provided an overall introduction to this survey paper. The section 2 presents a compre-\\nhensive overview of multimodal retrieval-augmented generation, covering multiple developmental\\nstages. The section 3 delves into the technical details of multimodal retrieval-augmented generation,\\nfocusing on key components such as multimodal retrieval, multimodal generation, etc. In section 4,\\nwe discuss how to comprehensively evaluate multimodal retrieval-augmented generation systems\\nusing datasets, including specialized assessments for different competency areas. The section 5\\nintroduces relevant metrics for evaluating multimodal retrieval-augmented generation systems.\\nIn section 6, we outline the current technical challenges associated with multimodal retrieval-\\naugmented generation. In section 7, based on previous investigation of MRAG, we summarize\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 3, 'page_label': '4', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='4 Trovato et al.\\nfuture work in this field, and provide some suggestions. Finally, we give the conclusion of the paper\\nin section 8.\\n2 Overview of MRAG\\nMultimodal Retrieval-Augmented Generation (MRAG) represents a significant evolution of the\\ntraditional Retrieval-Augmented Generation (RAG) framework, building upon its foundational\\nstructure while extending its capabilities to process diverse data modalities. While RAG is limited\\nto processing plain text, MRAG integrates multimodal data, including images, audio, video, and\\ntext, enabling it to address more complex and diverse real-world applications where information\\nspans multiple modalities.\\nIn the early stages of MRAG development, researchers converted multimodal data into unified\\ntextual representations. This approach allowed for a seamless transition from RAG to MRAG\\nby leveraging existing text-based retrieval and generation mechanisms. Although this strategy'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 3, 'page_label': '4', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='textual representations. This approach allowed for a seamless transition from RAG to MRAG\\nby leveraging existing text-based retrieval and generation mechanisms. Although this strategy\\nsimplified multimodal data integration and improved the end-to-end user experience, it introduced\\nsignificant limitations. For instance, the conversion process often resulted in the loss of modality-\\nspecific information, such as visual details in images or tonal nuances in audio, restricting the\\nsystem’s ability to fully exploit the potential of multimodal inputs. Subsequent research has focused\\non addressing these limitations by developing more advanced methods to optimize MRAG systems.\\nThese advancements have substantially enhanced MRAG’s performance and versatility, achieving\\nstate-of-the-art results across various multimodal tasks. This paper categorizes the evolution of\\nMRAG into three distinct stages:\\n2.1 MRAG1.0\\nThe initial stage of the MRAG framework, commonly termed \"pseudo-MRAG\", emerged as a'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 3, 'page_label': '4', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='MRAG into three distinct stages:\\n2.1 MRAG1.0\\nThe initial stage of the MRAG framework, commonly termed \"pseudo-MRAG\", emerged as a\\nstraightforward extension of the highly successful RAG paradigm. This stage was rapidly adopted\\ndue to its adherence to RAG’s core principles, with modifications to support multimodal data. As\\nillustrated in Figure 1, the MRAG1.0 architecture consists of three key components: Document\\nParsing and Indexing, Retrieval, and Generation.\\n•Document Parsing and Indexing: This component is responsible for processing multimodal\\ndocuments in formats such as Word, Excel, PDF, and HTML. It extracts textual content using\\nOptical Character Recognition (OCR) or format-specific parsing techniques. A document layout\\ndetection model is then utilized to segment the document into structured elements, including\\ntitles, paragraphs, images, videos, tables, and footers. For textual content, a chunking strategy is'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 3, 'page_label': '4', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='detection model is then utilized to segment the document into structured elements, including\\ntitles, paragraphs, images, videos, tables, and footers. For textual content, a chunking strategy is\\napplied to segment or group semantically coherent passages. For multimodal data, specialized\\nmodels are used to generate captions describing images, videos, and other non-textual elements.\\nThese chunks and captions are encoded into vector representations using an embedding model\\nand stored in a vector database. The choice of embedding model is crucial, as it significantly\\nimpacts the performance and effectiveness of downstream retrieval tasks.\\n•Retrieval: This component processes user queries by encoding them into vector representations\\nusing the same embedding model applied during indexing. The query vectors are then utilized\\nto retrieve the top- 𝑘 most relevant chunks and captions from the vector database, typically'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 3, 'page_label': '4', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='using the same embedding model applied during indexing. The query vectors are then utilized\\nto retrieve the top- 𝑘 most relevant chunks and captions from the vector database, typically\\nemploying cosine similarity as the relevance metric. Duplicate or overlapping information from\\nchunks and captions is merged to create a consolidated set of external knowledge, which is\\nsubsequently integrated into the prompt for the generation phase. This ensures the system\\nretrieves contextually relevant information to deliver accurate and informed responses.\\n•Generation: In the Generation phase, the MRAG system synthesizes the user’s query and\\nretrieved documents into a coherent prompt. A large language model (LLM) generates a response\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 4, 'page_label': '5', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 5\\nDocuments Parsing\\n(Extractive-Based, Plain Text)\\nTable \\nCaptions\\nGeneration Retrieval\\nIndexing\\nDocuments\\n……\\nText\\nText Parsing  \\nModel\\nImage Caption \\nModel\\nMultimodal Data\\nImage\\nTable\\n……\\nTable Parsing \\nModel\\nText Chunks\\nImage \\nCaptions\\nText Embedding \\nModel\\nText \\nVector DB\\nRelevant Text \\nChunks\\nRelevant Image \\nCaptions\\nPrompt\\nLLMs\\nQuery + History\\n（Text Only）\\nOCR-Based \\nModel\\nText Embedding \\nModel\\nAnswer\\n（Text Only）\\nFig. 1. The architecture of MRAG1.0, often termed \"pseudo-MRAG\", closely resembles traditional RAG,\\nconsisting of three modules: Document Parsing and Indexing, Retrieval, and Generation. While the overall\\nprocess remains largely unchanged, the key distinction lies in the Document Parsing stage. In this stage,\\nspecialized models are employed to convert diverse modal data into modality-specific captions. These captions\\nare then stored alongside textual data for utilization in subsequent stages.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 4, 'page_label': '5', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='specialized models are employed to convert diverse modal data into modality-specific captions. These captions\\nare then stored alongside textual data for utilization in subsequent stages.\\nby integrating its parametric knowledge with the retrieved external information. This approach\\nenhances response accuracy and timeliness, particularly in domain-specific contexts, while\\nreducing the risk of hallucinations common in LLM outputs. In multi-turn dialogues, the system\\nincorporates conversational history into the prompt, enabling contextually aware and seamless\\ninteractions.\\nDespite its initial success, MRAG1.0 exhibited several notable limitations that constrained its\\neffectiveness:\\n•Cumbersome Document Parsing: Converting multimodal data into textual captions introduced\\nsubstantial complexity to the system. This necessitated distinct models for processing different\\ndata modalities, increasing both computational overhead and system intricacy. Additionally,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 4, 'page_label': '5', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='substantial complexity to the system. This necessitated distinct models for processing different\\ndata modalities, increasing both computational overhead and system intricacy. Additionally,\\nthe conversion process frequently often to multimodal information loss. For instance, image\\ncaptions typically provided only coarse-grained descriptions, failing to capture fine-grained\\ndetails essential for accurate retrieval and generation.\\n•Bottleneck of Retrieval: While text vector retrieval technology is well-established, MRAG1.0\\nencountered challenges in achieving high recall accuracy. Similar to traditional RAG, the chunking\\nstrategy for text segmentation often fragmented keywords, making some content irretrievable.\\nAdditionally, transforming multimodal data into text, while enabling non-textual data retrieval,\\nintroduced additional information loss. These issues collectively created a bottleneck, limiting\\nthe system’s ability to retrieve comprehensive and accurate information.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 4, 'page_label': '5', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='introduced additional information loss. These issues collectively created a bottleneck, limiting\\nthe system’s ability to retrieve comprehensive and accurate information.\\n•Challenges in Generation: Unlike traditional RAG, MRAG1.0 required processing not only\\ntext chunks but also image captions and other multimodal data. Effectively organizing these\\ndiverse elements into coherent prompts while minimizing redundancy and preserving relevant\\ninformation posed a significant challenge. Additionally, the \"Garbage In, Garbage Out\" (GIGO)\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 5, 'page_label': '6', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='6 Trovato et al.\\nIndexingDocuments Parsing\\n(Extractive-Based, Multimodal)\\nGeneration Retrieval\\nDocuments\\n……\\nText\\nText Parsing  \\nModel\\nMultimodal Data\\nImage\\nTable\\n……\\nText Chunks\\nText Embedding \\nModel\\nText Vector \\nDB\\nText Only \\nPrompt\\nLMMs\\nQuery + History\\n(Text with Multimodal Data)\\nOCR-Based \\nModel\\nText Embedding \\nModel\\nAnswer\\n(Text Only)\\nMultimodal  \\nEmbedding Model\\nMLLMs\\n Multimoda \\nData Captions\\nMultimodal \\nEmbedding Model\\nRelevant \\nPlain Text  Data\\n……\\nText Chunks\\nImage \\nCaptions\\nRelevant \\nMultimodal Data\\n……\\nTable\\nImage\\nMultimodal \\nVector DB\\nText Only？\\nMultimodal \\nPrompt\\nY\\nN\\nMLLMs\\nFig. 2. The architecture of MRAG2.0 retains multimodal data through document parsing and indexing, while\\nintroducing multimodal retrieval and MLLMs for answer generation, truly entering the multimodal era.\\nprinciple highlighted the sensitivity of LLMs to input quality. Information loss during parsing\\nand retrieval increased the risk of incorporating irrelevant data, compromising the robustness'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 5, 'page_label': '6', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='principle highlighted the sensitivity of LLMs to input quality. Information loss during parsing\\nand retrieval increased the risk of incorporating irrelevant data, compromising the robustness\\nand reliability of the generated responses.\\nThe limitations of MRAG1.0 created a performance ceiling, highlighting the need for more advanced\\ntechnological solutions. The system’s reliance on text-based representations for multimodal data,\\nalong with inherent challenges in retrieval and generation, revealed critical gaps in multimodal\\nunderstanding, retrieval efficiency, and generation robustness. Subsequent iterations of MRAG\\nmust address these issues by adopting more sophisticated models, enhancing information retention\\nduring parsing, and improving the integration of multimodal data into retrieval and generation\\nprocesses.\\n2.2 MRAG2.0\\nWith the rapid evolution of multimodal technologies, MRAG has transitioned into a \"true mul-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 5, 'page_label': '6', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='processes.\\n2.2 MRAG2.0\\nWith the rapid evolution of multimodal technologies, MRAG has transitioned into a \"true mul-\\ntimodal\" era, termed MRAG2.0. Unlike its predecessor MRAG1.0, MRAG2.0 not only supports\\nuser queries with multimodal inputs but also preserves the original multimodal data within the\\nknowledge base. By leveraging the capabilities of MLLMs, the generation module can now process\\nmultimodal data directly, minimizing information loss during data conversion. As illustrated in\\nFigure 2, the MRAG2.0 architecture incorporates several key optimizations:\\n•MLLMs Captions: The representational capabilities of MLLMs have significantly advanced, es-\\npecially in captioning tasks. MRAG2.0 leverages a single, unified MLLM—or multiple MLLMs—to\\nextract captions from multimodal documents. This approach replaces the conventional paradigm\\nof using separate models for different modalities, simplifying the document parsing module and\\nreducing its complexity.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 5, 'page_label': '6', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='of using separate models for different modalities, simplifying the document parsing module and\\nreducing its complexity.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 6, 'page_label': '7', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 7\\n•Multimodal Retrieval: MRAG2.0 enhances its retrieval module to support multimodal user\\ninputs by preserving original multimodal data and enabling cross-modal retrieval. This allows text-\\nbased queries to directly retrieve relevant multimodal data, combining caption-based recall with\\ncross-modal search capabilities. The dual retrieval approach enriches data sources for downstream\\ntasks while minimizing data loss, improving accuracy and robustness for downstream tasks.\\n•Multimodal Generation: To fully leverage original multimodal data, the generation module\\nin MRAG2.0 has been enhanced by integrating MLLMs, enabling the synthesis of user queries\\nand retrieval results into a coherent prompt. When retrieval results are accurate and the input\\ncomprises original multimodal data, the generation module mitigates information loss typically\\nassociated with modality conversion. This enhancement has significantly improved the accuracy'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 6, 'page_label': '7', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='comprises original multimodal data, the generation module mitigates information loss typically\\nassociated with modality conversion. This enhancement has significantly improved the accuracy\\nof question-answering (QA) tasks, especially in scenarios involving interrelated multimodal data.\\nDespite these advancements, MRAG2.0 encounters several emerging challenges: 1) Integrating\\nmultimodal data inputs may reduce the accuracy of traditional textual query descriptions. Further-\\nmore, current multimodal retrieval capabilities remain inferior to text-based retrieval, potentially\\nlimiting the overall accuracy of the retrieval module. 2) The diversity of data formats presents new\\nchallenges for the generation module. Efficiently organizing these diverse data forms and clearly\\ndefining inputs for generation are critical areas requiring further exploration and prioritization.\\n2.3 MRAG3.0\\nAs illustrated in Figure 3, the MRAG3.0 system represents a significant evolution from its predeces-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 6, 'page_label': '7', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='2.3 MRAG3.0\\nAs illustrated in Figure 3, the MRAG3.0 system represents a significant evolution from its predeces-\\nsors, introducing structural and functional innovations that enhance its capabilities across multiple\\ndimensions. This new paradigm shift is characterized by three key advancements: 1) Enhanced\\nDocument Parsing: A novel approach retains document page screenshots during parsing, minimiz-\\ning information loss in database storage. 2) True End-to-End Multimodality: While earlier versions\\nemphasized multimodal capabilities in knowledge base construction and system input, MRAG3.0\\nintroduces multimodal output capabilities, completing the end-to-end multimodal framework. 3)\\nScenario Expansion: Moving beyond traditional focus on understanding capabilities—primarily ap-\\nplied in VQA (Visual Question Answering) scenarios reliant on knowledge bases, the new paradigm\\nintegrates understanding and generation capabilities through module adjustments and additions.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 6, 'page_label': '7', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='plied in VQA (Visual Question Answering) scenarios reliant on knowledge bases, the new paradigm\\nintegrates understanding and generation capabilities through module adjustments and additions.\\nThis unification significantly broadens the system’s applicability. In the following sections, we will\\ndetail the scenarios supported by MRAG3.0 and the specific module modifications enabling these\\nadvanced capabilities.\\n2.3.1 Scenario for MRAG.\\n•Retrieval-Augmented Scenario: This scenario addresses cases where LLMs or MLLMs alone\\ncannot adequately answer user queries. MRAG3.0 retrieves relevant content from external\\nknowledge bases to provide accurate answers, leveraging its enhanced retrieval capabilities.\\n•VQA Scenario: This scenario serves as a critical test for evaluating the fundamental capabilities\\nof MLLMs, which generate responses directly from user inputs containing text and multimodal\\nqueries without retrieval. The new MRAG paradigm introduces a search planning module,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 6, 'page_label': '7', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='of MLLMs, which generate responses directly from user inputs containing text and multimodal\\nqueries without retrieval. The new MRAG paradigm introduces a search planning module,\\nenabling dynamic routing and retrieval to minimize unnecessary searches and the inclusion of\\nirrelevant information.\\n•Multimodal Generation Scenario: This primarily pertains to multimodal generation tasks,\\nsuch as text-to-image or text-to-video generation. While the original MRAG framework primarily\\naddressed understanding tasks, the new MRAG paradigm extends its capabilities by modifying\\nmultiple generation modules, unifying the solutions for both understanding and generation\\ntasks within a single framework. Following integration, the generation scenarios are further\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 7, 'page_label': '8', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='8 Trovato et al.\\nIndexing\\nDocuments Parsing\\nRepresentation-based Module\\nGeneration\\nRetrieval\\nMultimodal\\nSearch Planning\\nDocuments\\n……\\nExtractive-Based Module\\n（Same with MRAG2.0）\\nDocuments \\nScreenShots \\nEmbedding Model\\nText Embedding \\nModel\\nMultiModal \\nEmbedding Model\\nText \\nVector DB\\nMultimodal \\nVector DB\\nRelevant \\nPlain Text  Data\\n……\\nText Chunks\\nImage Captions\\nRelevant \\nMultimodal Data\\n……\\nTable\\nImage\\nRelevant \\nDocuments \\nScreenshots\\nText Embedding \\nModel\\nMultiModal \\nEmbedding Model\\nDocuments \\nScreenShoots \\nEmbedding Model\\nRetrieval \\nClassification\\nQuery Reformulation\\nNeed Search？\\nQuery + History\\n（Text with Multimodal data）\\nNew Query\\nText Only?\\nText Only?\\nText Only Prompt\\n(No search)\\nMultimodal Prompt\\n(No search)\\nText Only Prompt\\n(With search)\\nMultimodal Prompt\\n(With search)\\nLMMs\\nMLLMs\\nDocuments \\nScreenShots \\nVector DB\\nY\\nN\\nY\\nN\\nMultimodal Answer\\nDocuments \\nScreenshots\\nText Only?\\nAugmented \\nMultimodal Output\\nPosition identification \\nCandidate set Retrieval\\nMatching and Insertion\\nY\\nN\\nN\\nY'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 7, 'page_label': '8', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='ScreenShots \\nVector DB\\nY\\nN\\nY\\nN\\nMultimodal Answer\\nDocuments \\nScreenshots\\nText Only?\\nAugmented \\nMultimodal Output\\nPosition identification \\nCandidate set Retrieval\\nMatching and Insertion\\nY\\nN\\nN\\nY\\nFig. 3. MRAG3.0 architecture integrates document screenshots during the document parsing and indexing\\nstages to minimize information loss. At the input stage, it incorporates a Multimodal Search Planning module,\\nunifying Visual Question Answering (VQA) and Retrieval-Augmented Generation (RAG) tasks while refining\\nuser query precision. At the output stage, the Multimodal Retrieval-Augmented Composition module enhances\\nanswer generation by transforming plain text into multimodal formats, thereby enriching information delivery.\\nenhanced by Retrieval-Augmentation (RA), which significantly improves the overall performance\\nof generation tasks (see Figure 4).\\n•Fusion Multimodal Output Scenario: This scenario is distinct from those previously men-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 7, 'page_label': '8', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='of generation tasks (see Figure 4).\\n•Fusion Multimodal Output Scenario: This scenario is distinct from those previously men-\\ntioned but represents a significant aspect of the new paradigm, warranting separate discussion. In\\ntraditional settings, the final output is typically a plain text response. However, the new paradigm\\nenhances the generation module to produce outputs that integrate multiple modalities within a\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 8, 'page_label': '9', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 9\\nPlease create a pixel art illustration of Huawei \\nStream Back Slope Village with whimsical fairy tale \\nelements?\\nAccording to the user‘s description, the generated \\npictures are as follows.\\nTraditional Multimodal Generation\\nIt’s not Huawei Stream Back Slope Village\\nPlease create a pixel art illustration of Huawei \\nStream Back Slope Village with whimsical fairy tale \\nelements?\\nI have found these pictures related to Huawei \\nStream Back Slope Village.\\nMultimodal Generation with MRAG\\nGood!\\nThe final generated pictures are as follows.\\nFig. 4. The user aims to generate images depicting \"Huawei Stream Back Slope Village. \" Due to the location’s\\nobscurity and the model’s limited knowledge, it may produce inaccurate representations, such as images of\\nhouses by a stream. By integrating retrieval-augmented capabilities, the model can access relevant information\\nbeforehand, enabling the generation of precise and contextually accurate images.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 8, 'page_label': '9', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='houses by a stream. By integrating retrieval-augmented capabilities, the model can access relevant information\\nbeforehand, enabling the generation of precise and contextually accurate images.\\nsingle response (e.g., combining text, images, or videos). This can be further categorized into\\nthree sub-scenarios (see Figure 5).\\n– Multimodal Data is Answer: The query can be answered directly through multimodal data\\nwithout any text, as the adage \"a picture is worth a thousand words\" suggests.\\n– Multimodal Data Enhances Accuracy: The integration of multimodal data enhances the\\naccuracy of responses, particularly in instructional contexts such as \"How to register for\\na Gmail account. \". By generating answers that interweave text and image, users can more\\neffectively comprehend and follow the required operations.\\n– Multimodal Data Enhances Richness: While multimodal data is not essential, its inclusion\\ncan significantly enhance user experience. For instance, when responding to a query such'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 8, 'page_label': '9', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='– Multimodal Data Enhances Richness: While multimodal data is not essential, its inclusion\\ncan significantly enhance user experience. For instance, when responding to a query such\\nas \"Please introduce the Eiffel Tower. \", supplementing the textual explanation with relevant\\nimages or a brief video can offer users a more engaging and visually enriched experience.\\n2.3.2 Modified Modules.\\n•Documents Parsing and Indexing: To minimize information loss and enhance the accuracy\\nof document retrieval, the document parsing and indexing module has been upgraded with\\ninnovative technologies. This new approach preserves document screenshots during parsing,\\naddressing the information loss issues inherent in previous methods. By utilizing fine-tuned\\nMLLMs, the system vectorizes and indexes these document screenshots, enabling efficient\\nretrieval of relevant document screenshots based on user queries. This optimization not only'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 8, 'page_label': '9', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='MLLMs, the system vectorizes and indexes these document screenshots, enabling efficient\\nretrieval of relevant document screenshots based on user queries. This optimization not only\\nimproves the reliability of the knowledge base but also paves the way for advanced multimodal\\nretrieval capabilities.\\n•Generation: Previously, the generation module relied exclusively on large models with under-\\nstanding capabilities. The new paradigm integrates large models with generation capabilities,\\nunifying reasoning and generation scenarios at the system architecture level. Additionally, by\\nincorporating a multimodal output enhancement submodule, it facilitates a shift from text-based\\nanswers to mixed multimodal answers. The implementation methods can be categorized into\\ntwo types:\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 9, 'page_label': '10', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content=\"10 Trovato et al.\\nYou can learn the route of Columbus's voyage through \\nthe video below.\\n1: Place the eggs and a small amount of salt into a container, \\nand mix well.\\n2: Add warm water and continue to stir, then strain it.\\n3: Steam for 10 minutes, drizzle with sesame oil. It's done!\\nWatch the video below for more detailed instructions.\\nPlease introduce the Egyptian pyramids.\\nThe Egyptian pyramids are among the most iconic and \\nfascinating structures in history.\\nBuilt primarily as tombs for the pharaohs of ancient Egypt, \\nthey served as grand final resting places to ensure their \\nsafe passage into the afterlife. The pyramids are mainly \\nlocated in Giza, near Cairo. The Great Pyramid of Khufu \\nis the largest and most well-known, standing at an \\nastonishing height.\\nConstructed with precision, these pyramids were built \\nusing massive limestone blocks. Thousands of workers, \\nincluding skilled artisans, were involved in their \\nconstruction, which took many years. The architecture is\"),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 9, 'page_label': '10', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content=\"using massive limestone blocks. Thousands of workers, \\nincluding skilled artisans, were involved in their \\nconstruction, which took many years. The architecture is \\nremarkable, with the sides of the pyramids aligned almost \\nperfectly with the four cardinal directions.\\nWhat was the route of Columbus's voyage?\\nScenario IIScenario I\\nMultimodal Data is Answer Multimodal Data Enhances Accuracy\\nScenario III\\nMultimodal Data Enhances Richness\\nWhat do the Alaskan Malamute and the \\nSiberian Husky look like, respectively?\\nThe left is the Alaskan Malamute, and the right is the \\nSiberian Husky .\\nHow to make steamed egg custard?\\nFig. 5. Multimodal output in QA scenarios can be categorized into three distinct types. In sub-scenario I, the\\nuser’s query can be fully addressed using only images or videos, without requiring supplementary textual\\ninformation. Sub-scenario II involves a step-by-step explanation that combines text and images to ensure\"),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 9, 'page_label': '10', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='information. Sub-scenario II involves a step-by-step explanation that combines text and images to ensure\\nclarity and precision; omitting the images may lead to user confusion at specific steps. In sub-scenario III,\\nsupplementary images enrich the information conveyed in the answer, but their removal does not compromise\\nthe answer’s accuracy.\\n– Native MLLM-Based Output: In this task, the generation of multimodal data is entirely model-\\ndriven, eliminating the need for external data sources to supplement the model responses.\\nThe most straightforward approach involves using a unified MLLM to produce the desired\\nmultimodal output in a single step, ensuring seamless integration of diverse data types, such\\nas text, images, or audio, within a cohesive framework.\\n– Augmented Multimodal Output: This method utilizes pre-existing multimodal data to\\nenhance textual responses. After generating the text, the system executes three sequential'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 9, 'page_label': '10', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='– Augmented Multimodal Output: This method utilizes pre-existing multimodal data to\\nenhance textual responses. After generating the text, the system executes three sequential\\nsubtasks to create the final multimodal output: 1) Position Identification: The system deter-\\nmines optimal insertion points within the text where multimodal elements (e.g., images, videos,\\ngraphs) can be integrated to complement or clarify the content. This step ensures that the mul-\\ntimodal data aligns contextually with the text. 2) Candidate Set Retrieval: Relevant multimodal\\ndata is retrieved from external sources, such as the web or a knowledge base, by querying and\\nfiltering potential candidates that best match the text’s context and intent. 3) Matching and\\nInsertion: The system selects the most appropriate multimodal element from the retrieved\\ncandidate set based on relevance, quality, and coherence. The chosen data is then seamlessly'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 9, 'page_label': '10', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Insertion: The system selects the most appropriate multimodal element from the retrieved\\ncandidate set based on relevance, quality, and coherence. The chosen data is then seamlessly\\nintegrated into the identified positions, producing a cohesive and enriched multimodal answer.\\n2.3.3 New Modules.\\n•Multimodal Search Planning: This module tackles key decision-making challenges in MRAG\\nsystems by focusing on two core tasks: retrieval classification and query reformulation. Given a\\nmultimodal query Q= (𝑞,𝑣), where 𝑞is the textual component and 𝑣 is the visual component,\\nthe module is designed to optimize information acquisition. Specifically, retrieval classification\\ninvolves determining the relevance and category of the multimodal query to guide the search\\ntoward the most appropriate data sources. Query reformulation, on the other hand, refines the\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 10, 'page_label': '11', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 11\\nquery by integrating textual and visual cues to improve retrieval accuracy and comprehensive-\\nness. By combining these tasks, the module strengthens the system’s ability to handle complex\\nmultimodal inputs, ensuring more effective and contextually relevant information retrieval.\\n– Retrieval Classification: This task determines the optimal retrieval strategy 𝑎∗from the\\naction space A= {𝑎𝑛𝑜𝑛𝑒,𝑎𝑡𝑒𝑥𝑡,𝑎𝑖𝑚𝑎𝑔𝑒}based on the current query and optionally the retrieved\\nhistorical documents. The decision process is formulated as:\\n𝑎∗= 𝑎𝑟𝑔𝑚𝑎𝑥\\n𝑎∈A\\nF𝑅𝐶 (𝑎 |Q,D) (1)\\nwhere the retrieval control module F𝑅𝐶 evaluates the utility of retrieval actions by considering\\nquery characteristics, the MLLM’s inherent capabilities, and, when available, the retrieved\\ndocuments Dfrom previous iterations. For example, in multi-hop scenarios, after retrieving\\nvisual information in the initial round, the module may leverage accumulated knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 10, 'page_label': '11', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='documents Dfrom previous iterations. For example, in multi-hop scenarios, after retrieving\\nvisual information in the initial round, the module may leverage accumulated knowledge\\nto determine subsequent actions, such as text-based retrieval or direct generation. Existing\\nMRAG frameworks typically follow a rigid pipeline with predetermined retrieval actions, which\\nposes significant limitations. Recent studies [125] have shown that compulsive image-to-image\\nretrieval can be counterproductive, as retrieved images may introduce misleading information,\\ndegrading MLLM performance. This highlights the necessity of dynamic retrieval strategy\\nselection.\\n– Query Reformulation: In scenarios where external information is required ( 𝑎∗ ≠ 𝑎𝑛𝑜𝑛𝑒)\\nfor queries, the task of query reformulation involves generating an enhanced query Q∗by\\nintegrating visual information and, when applicable, retrieved documents from previous\\niterations. This process can be formulated as:\\nQ∗= F𝑄𝑅 (Q,D) (2)'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 10, 'page_label': '11', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='integrating visual information and, when applicable, retrieved documents from previous\\niterations. This process can be formulated as:\\nQ∗= F𝑄𝑅 (Q,D) (2)\\nwhere F𝑄𝑅 denotes the query enhancement function, which utilizes visual cues and, if available,\\nhistorical retrieval results to refine the query’s precision. This task is particularly critical in real-\\nworld human interactions, where queries often rely heavily on visual context and frequently\\nemploy anaphoric references. The inherent challenges of visual incompleteness and textual\\nambiguity pose significant obstacles to retrieving relevant information through straightforward\\nsearch mechanisms. For complex queries that necessitate multi-hop reasoning, the enhanced\\nquery Q∗may be further decomposed into a series of atomic sub-queries {𝑞∗\\n1,...,𝑞 ∗\\n𝑛}. Each\\nsub-query is meticulously formulated by considering both textual and visual contexts, as well\\nas the accumulated knowledge from previous iterations, when relevant. This decomposition'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 10, 'page_label': '11', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='𝑛}. Each\\nsub-query is meticulously formulated by considering both textual and visual contexts, as well\\nas the accumulated knowledge from previous iterations, when relevant. This decomposition\\nallows for a more granular and precise retrieval process, addressing the nuanced dependencies\\nand ambiguities present in real-world queries.\\nThis dual-task approach optimizes information acquisition by minimizing unnecessary retrievals\\nwhile maximizing the relevance of retrieved content. The structured planning framework signifi-\\ncantly enhances the MRAG system’s ability to gather comprehensive and accurate information,\\nensuring computational efficiency.\\n3 Components & Technologies of MRAG\\nIn this section, we will sequentially introduce the details of the five key technical components\\nof MRAG: Multimodal Document Parsing and Indexing (section3.1), Multimodal Search Planning\\n(section 3.2), Multimodal Retrieval (section 3.3), Multimodal Generation (section 3.4)'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 10, 'page_label': '11', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='of MRAG: Multimodal Document Parsing and Indexing (section3.1), Multimodal Search Planning\\n(section 3.2), Multimodal Retrieval (section 3.3), Multimodal Generation (section 3.4)\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 11, 'page_label': '12', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='12 Trovato et al.\\n3.1 Multimodal Document Parsing and Indexing\\nMRAG systems significantly enhance the reliability and quality of generated answers, by integrating\\ntarget multimodal knowledge from external multimodal knowledge bases. Target multimodal knowl-\\nedge can be derived from various granularity in knowledge bases, including localized segments\\nwithin a single document, cross-segment references within a document, or even cross-document\\nknowledge collections. Thus, how to effectively parse, index, and organize the multimodal docu-\\nments in external knowledge bases, can largely affect the model’s utilization of target multimodal\\nknowledge, thereby determining end-to-end performance. In this section, we first classify docu-\\nments in multimodal knowledge bases according to their structure, then we provide a detailed\\nintroduction to the parsing methods and the evolution of these methods for different types of'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 11, 'page_label': '12', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='ments in multimodal knowledge bases according to their structure, then we provide a detailed\\nintroduction to the parsing methods and the evolution of these methods for different types of\\nmultimodal documents. Specifically, multimodal documents can be categorized into the following\\nthree types:\\n•Unstructured Multimodal Data: refers to various multimodal information that does not have\\na specific format or schema, such as text, images, videos, and audio. Among the unstructured\\ndata, documents with images are widely studied in MRAG. For example, SlideVQA [347] is a\\ntypical dataset for visual question-answering task, where all documents are input as images.\\n•Semi-structured Multimodal Data: mainly refers to multimodal information that lacks the\\nrigid schema of traditional relational databases but retains some organizational features, such\\nas PDFs, HTML, XML, and JSON. In such documents, rule-based methods can directly extract'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 11, 'page_label': '12', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='rigid schema of traditional relational databases but retains some organizational features, such\\nas PDFs, HTML, XML, and JSON. In such documents, rule-based methods can directly extract\\nstructural characteristics. For instance, in HTML, the title can be identified using the <title>\\ntag. A common challenge in processing these documents is that their inherent structure, easily\\ninterpretable by humans, is often lost during parsing, resulting in information loss.\\n•Structured Multimodal Data: refers to multimodal information arranged in a predefined format,\\ntypically following a fixed schema, such as relational databases and knowledge graphs. The\\nprimary challenge in handling such data is formulating an accurate structured query language\\ncorresponding to natural language.\\nIn MRAG scenarios, the primary focus is on processing and leveraging unstructured and semi-\\nstructured documents. Document parsing methods in MRAG are broadly categorized into two'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 11, 'page_label': '12', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='In MRAG scenarios, the primary focus is on processing and leveraging unstructured and semi-\\nstructured documents. Document parsing methods in MRAG are broadly categorized into two\\napproaches: extraction-based and representation-based. Each approach has distinct advantages\\nand limitations, with the choice depending on task-specific requirements such as scalability or\\ncomputational efficiency. Extraction-based methods involve a two-step process: first, multimodal\\ninformation is extracted from documents, and second, the extracted data is parsed and structured\\nfor storage and downstream use. In contrast, representation-based methods do not require explicit\\nextraction of multimodal information. Instead, these methods focus on storing document content\\nholistically, often employing representation techniques for document segments. This approach\\nenables a more comprehensive processing of document content.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 11, 'page_label': '12', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='holistically, often employing representation techniques for document segments. This approach\\nenables a more comprehensive processing of document content.\\n3.1.1 Extraction-based. Early document parsing solutions were entirely extractive. They evolved\\ngradually from plain text extraction to multimodal data extraction, depending on the type of content\\nbeing extracted. This subsection will present the process in this sequence.\\n•Plain Text Extraction. In this phase, only textual information from all modal data in the docu-\\nment was extracted. For example, for tables and images, only their textual content was captured.\\nSemi-structured documents, such as PDFs, XML, and HTML, can be parsed directly according to\\ntheir structural rules. Numerous open-source tools support such capabilities, including pymupdf\\n[3] and pdfminer [2] for PDF parsing, and jsoup [1] for HTML extraction. While this approach\\nenables simple and efficient document parsing, it has limitations: it cannot extract multimodal'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 11, 'page_label': '12', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[3] and pdfminer [2] for PDF parsing, and jsoup [1] for HTML extraction. While this approach\\nenables simple and efficient document parsing, it has limitations: it cannot extract multimodal\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 12, 'page_label': '13', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 13\\ninformation (e.g., text within images) and struggles with complex document formats. Additionally,\\nthe parsed results often suffer from significant loss of document structure information.\\nTo enhance document parsing accuracy and address the limitations of rule-based methods\\nin handling complex real-world documents, such as in Visual Document Understanding (VDU)\\ntasks, OCR (Optical Character Recognition)-based approaches have become widely adopted. The\\ntraditional OCR-based document parsing pipeline consists of three main stages: text detection,\\ntext recognition, and text parsing. Text Detection involves locating and extracting text regions\\nfrom documents. Early methods primarily relied on Connected Component Analysis (CCA) [26,\\n363] and Edge Detection algorithms [261]. For more complex layouts, techniques such as Contour\\nAnalysis and Stroke Width Transform (SWT) [73, 344] were employed to handle multi-oriented'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 12, 'page_label': '13', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='363] and Edge Detection algorithms [261]. For more complex layouts, techniques such as Contour\\nAnalysis and Stroke Width Transform (SWT) [73, 344] were employed to handle multi-oriented\\ntext. With advancements in machine learning, hybrid models combining regression-based object\\ndetection frameworks (e.g., Faster R-CNN) with semantic segmentation networks were developed\\nto address arbitrary-shaped text instances [15, 206, 481]. This stage outputs precise bounding\\nboxes or polygon coordinates around text elements, serving as the basis for subsequent processing.\\nText Recognition converts visual text representations into machine-readable text, playing a critical\\nrole in digitizing unstructured data. Its evolution can be divided into three phases: The classical\\nphase relied on handcrafted features [ 290] and statistical models [ 22], but faced challenges\\nwith fragmented processing and limited robustness. The deep learning phase introduced CNNs'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 12, 'page_label': '13', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='phase relied on handcrafted features [ 290] and statistical models [ 22], but faced challenges\\nwith fragmented processing and limited robustness. The deep learning phase introduced CNNs\\n(Convolutional Neural Networks) for feature extraction and CTC (Connectionist Temporal\\nClassification)/RNNs (Recurrent Neural Networks) for sequence modeling, with breakthroughs\\nlike CRNN enabling unified pipelines and improved accuracy on irregular text. The modern phase\\nleverages transformer architectures [191], achieving global context awareness and robustness\\nto arbitrary-shaped text. Text Parsing reconstructs semantic relationships through three key\\nsteps: Layout Analysis segments documents into logical components using rule-based heuristics\\nand graph models based on spatial and typographic cues. Syntactic Parsing extracts structured\\ndata from unstructured text using regular expressions and finite-state machines. Post-processing'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 12, 'page_label': '13', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='and graph models based on spatial and typographic cues. Syntactic Parsing extracts structured\\ndata from unstructured text using regular expressions and finite-state machines. Post-processing\\ncorrects recognition errors through contextual algorithms like language model interpolation (e.g.,\\nn-gram models and dictionary lookups). This comprehensive process ensures accurate semantic\\nreconstruction from complex documents.\\nHowever, the OCR-dependent approach has critical problems: It is not conducive to paralleliza-\\ntion and occupies a large amount of computing resources, besides, errors in the pipeline will\\npropagate downward through the system, affecting the overall performance. In recent years, with\\nthe development of Transformer architectures, the aforementioned issues have been effectively\\naddressed. It enhances global context modeling through the self-attention mechanism, signifi-\\ncantly improves processing efficiency by leveraging parallel computing, and directly maps images'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 12, 'page_label': '13', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='addressed. It enhances global context modeling through the self-attention mechanism, signifi-\\ncantly improves processing efficiency by leveraging parallel computing, and directly maps images\\nto structured text in an end-to-end training mode. This effectively eliminates the cumulative\\nerror issues associated with the multi-stage cascading of traditional OCR systems. LayoutLM\\n[412] uses the BERT architecture as the backbone and adds two new input embeddings: a 2-D\\nposition embedding and an image embedding to jointly model interactions between text and\\nlayout information across scanned document images. LayoutLMv2 [413] and LayoutLMv3 [133]\\nfurther propose a new single multimodal framework to model the interaction among text, layout,\\nand image. DocFormer [12] based on the multimodal transformer architecture proposes a novel\\nmultimodal attention layer to fuse text, vision, and spatial features in a document, thereby\\nachieving end-to-end document parsing.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 12, 'page_label': '13', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='multimodal attention layer to fuse text, vision, and spatial features in a document, thereby\\nachieving end-to-end document parsing.\\n•Multimodal Extraction. In this phase, the original format of multimodal data is preserved\\nduring extraction, allowing downstream tasks to autonomously determine subsequent operations.\\nFor semi-structured documents, extraction can be performed similarly using rule-based methods.\\nRelevant multimodal data is identified through specific tags, such as extracting images from\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 13, 'page_label': '14', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='14 Trovato et al.\\nHTML files using the \"<img>\" tag. However, this approach faces similar challenges to plain text\\nextraction.\\nThe pipeline for multimodal document parsing based on OCR consists of three steps: page\\nsegmentation, text recognition, and text parsing. Page segmentation, similar to text detection in\\nplain text extraction, locates and extracts target regions while annotating them with semantic\\nlabels (e.g., title, table, footnote). This subtask of semantic segmentation commonly employs CNN-\\nbased methods, categorized into region-based, FCN-based, and weakly supervised approaches\\n[111]. Text recognition, similar to plain text extraction, focuses on parsing text data such as\\ntitles and page text. Text parsing involves layout analysis and other operations, processing\\nmultimodal data according to downstream task requirements. In the era of LLMs, multimodal\\ndata is often converted into text for utilization, as seen in models like TableNet [286] for tables and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 13, 'page_label': '14', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='multimodal data according to downstream task requirements. In the era of LLMs, multimodal\\ndata is often converted into text for utilization, as seen in models like TableNet [286] for tables and\\nUniChart [265] for charts. This necessitates distinct models for extracting captions from different\\nmodalities. With the advancement of MLLMs, there is a trend toward unifying these models\\ninto a single MLLM framework, leveraging their robust representation capabilities [173, 227].\\nFurther developments in MLLMs enable the direct retention and input of original multimodal\\ndata during generation [312, 437, 474].\\n3.1.2 Representation-based. Although extractive-based methods have been widely adopted, they\\nsuffer from several inherent limitations: (1) The parsing process is time-consuming, involves\\nmultiple steps, and requires different models for different document types; (2) Critical information,\\nsuch as document structure, may be lost during extraction; and (3) Parsing errors can propagate'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 13, 'page_label': '14', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='multiple steps, and requires different models for different document types; (2) Critical information,\\nsuch as document structure, may be lost during extraction; and (3) Parsing errors can propagate\\nto downstream tasks. Recent advancements in MLLMs [6, 20, 218] have enabled a novel approach\\nthat directly uses document screenshots as primary data for metadata indexing, addressing these\\nissues [78, 170, 253, 342, 463]. To capture both global and local information, DSE [253] processes the\\ndocument screenshot along with its sub-images through a unified encoding framework. Additionally,\\na late interaction mechanism, inspired by ColBERT [163], has been introduced to improve recall\\nefficiency [78]. However, page-level document splitting may hinder the model’s ability to capture\\nfull context and inter-part relationships. To address this problem, a holistic document representation\\nmethod has been proposed [170], which segments large documents into passages within the token'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 13, 'page_label': '14', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='full context and inter-part relationships. To address this problem, a holistic document representation\\nmethod has been proposed [170], which segments large documents into passages within the token\\nlimit of MLLMs. Empirical studies reveal a performance gap between multimodal and text-only\\nretrieval, highlighting differences in effectiveness when using raw multimodal data versus text or\\ncombined modalities [312, 463]. Consequently, a new paradigm has emerged that leverages OCR\\nfor text indexing, document screenshots for multimodal indexing, and executes textual and visual\\nRAG in parallel. The results from both streams are then fused through modality integration to\\nproduce the final answer [342].\\n3.2 Multimodal Search Planning\\nMultimodal search planning refers to the strategies employed by MRAG systems, to effectively\\nretrieve and integrate information from multiple modalities to address complex queries. The'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 13, 'page_label': '14', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Multimodal search planning refers to the strategies employed by MRAG systems, to effectively\\nretrieve and integrate information from multiple modalities to address complex queries. The\\nplanning can be broadly categorized into two main approaches: fixed planning and adaptive\\nplanning.\\n3.2.1 Fixed Planning. Early MRAG systems typically adopt fixed planning strategies for handling\\nmultimodal queries, characterized by predetermined processing pipelines that lack flexibility in\\nadapting to diverse query requirements. These approaches can be broadly categorized based on\\ntheir retrieval modality choices:\\n•Planning for Single-modal Retrieval. Early fixed planning strategies usually focus on a single\\nmodality for retrieval, despite the multimodal nature of input queries. These approaches can be\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 14, 'page_label': '15', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 15\\nbroadly classified into text-centric and image-centric paradigms, reflecting initial efforts to adapt\\ntraditional IR query processing techniques [171, 184] to the multimodal domain.\\n– Text-centric planning approaches prioritize textual retrieval by transforming multimodal\\nqueries into text-only formats. For instance, Plug-and-Play [ 357] employs vision-language\\nmodels to convert the visual component of a query into textual descriptions, followed by text-\\nbased retrieval planning. This strategy simplifies the multimodal problem into a traditional\\ntext-based RAG pipeline, leveraging established multi-stage query processing techniques from\\nconventional IR systems. However, this approach often introduces a semantic gap between\\nthe user’s original intent and the generated textual descriptions. The conversion of visual\\nqueries to text may fail to precisely capture the user’s specific information needs, leading to'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 14, 'page_label': '15', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='the user’s original intent and the generated textual descriptions. The conversion of visual\\nqueries to text may fail to precisely capture the user’s specific information needs, leading to\\nthe retrieval of irrelevant or noisy documents that diverge from the query’s focus.\\n– Image-centric planning strategies rely solely on image-based retrieval regardless of the query\\ncharacteristics. Systems such as Wiki-LLaVA [24] demonstrate this paradigm by consistently\\ntriggering image retrieval from knowledge bases for multimodal queries. While this approach\\nensures visual information preservation, it presents practical limitations. Recent empirical\\nstudies [125] highlight that compulsive image retrieval can be counterproductive, particularly\\nwhen textual information suffices or when retrieved images introduce misleading visual\\ncontexts, impairing MLLM performance.\\nThe inflexibility of single-modality planning strategies highlights their inherent limitations: they'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 14, 'page_label': '15', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='contexts, impairing MLLM performance.\\nThe inflexibility of single-modality planning strategies highlights their inherent limitations: they\\ncannot adapt to the diverse information needs of real-world scenarios. For example, while a\\ntext-centric approach may be suitable for queries referencing visual content but focused on\\nfactual information, an image-centric strategy is more effective for queries requiring detailed\\nvisual comparisons.\\n•Planning for Multimodal Retrieval. Recent studies have begun investigating the use of\\nmultimodal information retrieval to enhance the performance of MRAG systems. Unlike single-\\nmodality approaches, these methods integrate both textual and visual knowledge sources, albeit\\nthrough fixed processing pipelines. For instance, MMSearch [147] employs a rigid multimodal\\nplanning pipeline, mandating Google Lens image searches for all image-containing queries.\\nThis is followed by a \"Requery\" phase, where LLMs reformulate the search query using the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 14, 'page_label': '15', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='planning pipeline, mandating Google Lens image searches for all image-containing queries.\\nThis is followed by a \"Requery\" phase, where LLMs reformulate the search query using the\\noriginal query, image, and Google Lens results. While this structured approach ensures systematic\\ninformation retrieval, its inflexible design often leads to unnecessary image searches, increasing\\ncomputational overhead when visual information is irrelevant to the query.\\nFixed pipeline approaches, whether single-modality or multimodality, exhibit several critical limi-\\ntations. First, their rigid retrieval strategies struggle to adapt to the diverse nature of real-world\\nqueries, where the optimal retrieval modality depends on specific information needs. Second,\\nmandatory retrieval operations often introduce redundant or irrelevant information, particularly\\nwhen certain knowledge types are unnecessary for addressing the query. Third, these approaches'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 14, 'page_label': '15', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='mandatory retrieval operations often introduce redundant or irrelevant information, particularly\\nwhen certain knowledge types are unnecessary for addressing the query. Third, these approaches\\nincur significant computational overhead, especially in multimodal pipelines handling large-scale\\nknowledge bases. As highlighted by mR2AG [474], a more fundamental issue is that not all queries\\nrequire external knowledge retrieval. Current MRAG systems frequently perform retrieval indiscrim-\\ninately, resulting in unnecessary computational costs and potential noise in response generation.\\nThese limitations emphasize the need to transition from predetermined pipelines to adaptive plan-\\nning mechanisms that dynamically adjust retrieval strategies based on query characteristics and\\nintermediate results.\\n3.2.2 Adaptive Planning. Recent studies have highlighted two key limitations in fixed pipeline\\napproaches [197]: 1) Non-adaptive Retrieval Queries: inflexible retrieval strategies that fail to adjust'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 14, 'page_label': '15', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='3.2.2 Adaptive Planning. Recent studies have highlighted two key limitations in fixed pipeline\\napproaches [197]: 1) Non-adaptive Retrieval Queries: inflexible retrieval strategies that fail to adjust\\nto evolving contexts or intermediate results; and 2) Overloaded Retrieval Queries: concatenating\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 15, 'page_label': '16', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='16 Trovato et al.\\nvisual content descriptions with input questions into a single query, leading to ambiguous retrievals\\nand irrelevant knowledge. To address these issues, OmniSearch [197] introduces a self-adaptive\\nplanning agent for multimodal retrieval, mimicking human problem-solving behavior. Instead of\\nrelying on a fixed pipeline, the system dynamically breaks down complex multimodal questions\\ninto sub-question chains with retrieval actions. At each step, the agent adapts its next action based\\non the problem-solving state and retrieved content, enabling deeper understanding of retrieved\\ninformation and adaptive refinement of retrieval strategies. CogPlanner [440] iteratively refines\\nqueries and selects retrieval strategies, enabling both parallel and sequential modeling approaches.\\n3.3 Multimodal Retrieval\\nIn this section, we present a comprehensive overview of the three critical components of multimodal'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 15, 'page_label': '16', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='3.3 Multimodal Retrieval\\nIn this section, we present a comprehensive overview of the three critical components of multimodal\\nretrieval in the MRAG system: retriever (section 3.3.1), reranker, and refiner. Each component plays\\na distinct yet interconnected role in enhancing the quality and relevance of information retrieval\\nand utilization for LLMs. We summarize the taxonomy of multimodal retrieval research in Figure 6.\\n3.3.1 RETRIEVER. The retriever is a core component that sources relevant documents from a large\\nexternal knowledge base using advanced indexing and search algorithms. It retrieves candidate\\ninformation aligned with user queries, aiming to provide a broad yet relevant set of documents to\\nsupport high-quality LLM responses. Its performance is crucial, as it directly influences the quality\\nof the downstream retrieval pipeline. As shown in Figure 7, existing retrieval methods fall into'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 15, 'page_label': '16', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='support high-quality LLM responses. Its performance is crucial, as it directly influences the quality\\nof the downstream retrieval pipeline. As shown in Figure 7, existing retrieval methods fall into\\ntwo categories based on architecture: Single/Dual-stream Structure and Generative Structure, each\\ninvolves single-modal (e.g., text, images) and cross-modal information retrieval.\\n•Single/Dual-stream Structure : The Single-stream Structure integrates multimodal fusion\\nmodules to model image-text relationships in a unified semantic space, capturing fine-grained\\ninteractions but incurring higher computational costs and slower inference, limiting scalability\\nfor large-scale multimodal retrieval tasks in real-world applications. In contrast, the Dual-stream\\nStructure uses separate vision and language streams, leveraging contrastive learning to align\\nglobal features in a shared semantic space efficiently. However, it lacks explicit multimodal'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 15, 'page_label': '16', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Structure uses separate vision and language streams, leveraging contrastive learning to align\\nglobal features in a shared semantic space efficiently. However, it lacks explicit multimodal\\ninteraction and struggles with feature alignment due to information imbalance, exacerbated by\\nthe brevity of dataset captions.\\n– Retrieval for Single-Modal. In MRAG systems, single-modal retrieval focuses on text and\\nimage retrieval. Text retrieval uses NLP techniques to extract relevant information from\\ndatasets, identifying contextually aligned documents. Image retrieval employs computer vision\\nalgorithms and feature extraction methods to encode visual data into high-dimensional vectors\\nfor similarity matching. Both modalities are essential for enhancing MRAG system performance.\\n∗Text-centric. Text retrieval, a core component of information retrieval (IR), identifies\\nrelevant textual information from large corpora or web resources in response to user queries.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 15, 'page_label': '16', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='∗Text-centric. Text retrieval, a core component of information retrieval (IR), identifies\\nrelevant textual information from large corpora or web resources in response to user queries.\\nIt is widely used in downstream applications such as question answering [161, 304], dialogue\\nsystems [334, 436, 456], web search [ 92, 269, 270], and retrieval-augmented generation\\nsystems [33, 45, 102, 330]. Recent advancements categorize text retrieval methods into two\\ntypes: sparse retrieval and dense retrieval.\\n·Sparse Text Retrieval. Early research in text retrieval focused on extracting representative\\nterms from documents, leading to the development of vector space models [320] based\\non the \"bag-of-words\" assumption, which represents documents and queries as sparse\\nterm vectors, ignoring term order. Term weighting methods like tf-idf [9, 314, 319] and\\nBM25 models [315, 316] were introduced to assign weights based on term importance'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 15, 'page_label': '16', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='term vectors, ignoring term order. Term weighting methods like tf-idf [9, 314, 319] and\\nBM25 models [315, 316] were introduced to assign weights based on term importance\\nwithin and across corpora, while inverted indexes [ 513] improved retrieval efficiency\\nby organizing corpora into term-document ID pairs. Statistical language modeling [452]\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 16, 'page_label': '17', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 17\\nMultimodal\\nRetrieval\\nRefiner\\nSoft\\nCross-\\nModal PromptMM [393], RACC [395], VTC-CLS [364], VisToG [128]\\nSingle-\\nModal\\nAskell et al . [13], PI [ 53], Context Distillation [ 331], Wingate et al . [396], Sun et al .\\n[339], Distilling step-by-step [ 122], Gist [ 279], AutoCompressor [49], ICAE [ 99],\\nPOD [190], xRAG [48], COCOM [ 308], Gist-COCO [ 193], UltraGist [ 467], LLoCO\\n[346], 500xCompressor [203], RDRec [379], SelfCP [90], QGC [25], UniICL [91]\\nHard\\nCross-\\nModal\\nLLaVolta [34], PyramidDrop [ 407], DeCo [ 425], MustDrop [ 226], G-Search [ 484],\\nG-Prune [151]\\nSingle-\\nModal\\nDynaICL [499], FILCO [ 385], Selective Context [ 196], CoT-Influx [132], RECOMP\\n[410], MEMWALKER [32], TCRA-LLM [220], LLMLingua [ 148], LongLLMLingua\\n[149], CPC [ 213], AdaComp [ 469], Prompt-SAW [11], PCRL [ 159], LLMLingua-2\\n[288], Nano-Capsulator [ 56], CompAct [432], Style-Compress [ 297], TACO-RL [323],\\nFaviComp [158]\\nReranker\\nPrompt\\nCross-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 16, 'page_label': '17', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[149], CPC [ 213], AdaComp [ 469], Prompt-SAW [11], PCRL [ 159], LLMLingua-2\\n[288], Nano-Capsulator [ 56], CompAct [432], Style-Compress [ 297], TACO-RL [323],\\nFaviComp [158]\\nReranker\\nPrompt\\nCross-\\nModal TIGeR [303], Lin et al. [210]\\nSingle-\\nModal\\nZhuang et al . [509], MCRanker [ 109], UPR [ 318], Zhuang et al . [511], Co-Prompt\\n[52], PaRaDe [ 70], DemoRank [ 228], PRP-AllPair [302], PRP-Graph [ 248], Yan et al .\\n[416], RankGPT [ 341], LRL [ 256], Tang et al . [349], TourRank [43], TDPart [ 292],\\nFIRST [309]\\nFine-tune\\nCross-\\nModal Wen et al. [394], RagVL [311]\\nSingle-\\nModal\\nNogueira and Cho [282], monoBERT [ 285], Nogueira et al . [283], Ju et al . [157],\\nDuoT5 [296], RankT5 [ 510], ListT5 [ 433], RankLLaMA [ 255], TSARankLLM [ 464],\\nQ-PEFT [294], Zhang et al. [475], PE-Rank [222]\\nRetriever\\nGenerative\\nCross-\\nModal IRGen [479], GeMKR [238], GRACE [199], ACE [76], AVG [195]\\nSingle-\\nModal\\nGENRE [61], DSI [ 352], DynamicRetriever [ 502], SEAL [ 19], DSI-QG [ 512], NCI'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 16, 'page_label': '17', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Retriever\\nGenerative\\nCross-\\nModal IRGen [479], GeMKR [238], GRACE [199], ACE [76], AVG [195]\\nSingle-\\nModal\\nGENRE [61], DSI [ 352], DynamicRetriever [ 502], SEAL [ 19], DSI-QG [ 512], NCI\\n[383], Ultron [ 501], LTRGR [ 201], GenRRL [ 500], DGR [ 202], GenRet [ 340], MINDER\\n[200], NOVO [389], LMIndexer [153], ASI [420], RIPOR [449], GLEN [175]\\nSingle/Dual-\\nStream\\nCross-\\nModal\\n•Text/Image–Image:\\nMSDS [370], VSE++ [ 75], Liu et al . [231], Wehrmann and Barros [390], Guo et al .\\n[110], DSCMR [ 489], DRCE [ 384], ESSE [ 386], SDCMR [ 388], DSVEL [ 72], CRAN\\n[299], CAAN [ 468], RANet [ 408], PVSE [ 333], PCME [ 57], RLCMR [ 422], DREN\\n[419], TGDT [ 215], HREM [ 87], TransTPS [ 18], IEFT [ 350], TEAM [ 405], CSIC\\n[234], LAPS [ 88], COTS [ 240], AGREE [380], IRRA [ 146], USER [ 477], EI-CLIP [ 250],\\nMAKE [492]\\n•Text–Video:\\nLLVE [361], Mithun et al . [276], Miech et al . [274], MMT [ 89], HiT [ 224], CLIP4Clip\\n[247], VoP [131], Cap4Video [402], DGL [421], TeachCLIP [356], T-MASS [371]'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 16, 'page_label': '17', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='MAKE [492]\\n•Text–Video:\\nLLVE [361], Mithun et al . [276], Miech et al . [274], MMT [ 89], HiT [ 224], CLIP4Clip\\n[247], VoP [131], Cap4Video [402], DGL [421], TeachCLIP [356], T-MASS [371]\\n•Text–Audio:\\nATR [239], OML [271], MGRL [485], TAP-PMR [406], CMRF [494], TTMR++ [63]\\n•Unified:\\nFLAVA [326], UniVL-DR [ 237], MARVEL [ 498], FLMR [ 211], UniIR [ 391], VISTA\\n[495], E5-V [150], VLM2VEC [ 152], GME [ 476], Ovis [ 244], ColPali [ 79], CREAM\\n[462], DSE [254]\\nSingle-\\nModal\\nTf-Idf [319], BM25 [ 316], statistical [ 452], DeepCT [ 60], HDCT [ 59], COIL [ 94],\\nuniCOIL [208], DocTTTTTquery [ 284], SPLADE [ 84], SPLADE v2 [ 83], Poly [ 137],\\nME-BERT [246], ColBERT [ 163], ColBERTer [120], MVR [ 471], MADRM [ 166],\\nrandom [127], in-batch [ 118], hard [ 161], STAR [454], ANCE [ 409], ADORE [ 454],\\nRocketQA [304], AR2 [ 459], SimANS [ 497], Lee et al . [172], Chang et al . [27], Prop\\n[251], B-PROP [ 252], Condenser [ 92], co-Condenser [ 93], Contriever [ 139], SimLM'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 16, 'page_label': '17', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='RocketQA [304], AR2 [ 459], SimANS [ 497], Lee et al . [172], Chang et al . [27], Prop\\n[251], B-PROP [ 252], Condenser [ 92], co-Condenser [ 93], Contriever [ 139], SimLM\\n[373], RetroMAE [491], Liu and Yang [214]\\nFig. 6. Taxonomy of recent advancements in multimodal retrieval research.\\nfurther advanced retrieval by estimating term probability distributions for probabilistic\\nranking. However, early sparse retrieval methods face limitations, such as assuming term\\nindependence and relying on lexical matching, which hinders their ability to capture\\ncontextual term importance or semantic relationships between terms. Consequently, these\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 17, 'page_label': '18', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='18 Trovato et al.\\nSingle/Dual-Stream Generative\\nSingle-Modal Cross-Modal\\nText Query\\n------------------\\nWhat is a new \\nenergy vehicle?\\nText Candidate\\n------------------------\\nA new energy \\nvehicle (NEV) is a \\ntype of vehicle that \\nutilizes alternative \\nenergy sources \\ninstead of ......\\nText Query\\n------------------------------------\\nWhat is a new energy vehicle?\\nVisual Candidate\\n(Image, Video, Audio, …)\\n------------------------------------\\nSingle-Modal Cross-Modal\\nText Query\\n------------------\\nWhat is a new \\nenergy vehicle?\\nText Candidate\\n---------------------------\\nA new energy vehicle \\n(NEV) is a type of \\nvehicle that utilizes \\nalternative energy \\nsources instead of \\ntraditional internal \\ncombustion engines \\nthat run on fossil fuels \\nlike gasoline or diesel.\\nLLM\\nTextID\\nIdentifier\\nTextID\\nEncoder\\nText Query\\n------------------\\nWhat is a new \\nenergy vehicle?\\nVisual Candidate\\n(Image, Video, Audio, …)\\n------------------------------------\\nMLLM\\nVisualID\\nIdentifier\\nVisualID\\nEncoder\\nText'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 17, 'page_label': '18', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Encoder\\nText Query\\n------------------\\nWhat is a new \\nenergy vehicle?\\nVisual Candidate\\n(Image, Video, Audio, …)\\n------------------------------------\\nMLLM\\nVisualID\\nIdentifier\\nVisualID\\nEncoder\\nText \\nRetriever\\nCross-Modal \\nRetriever\\nVisual Query\\n------------------------\\nVisual Candidate\\n------------------------\\nVisual\\nRetriever\\nFig. 7. The architectures of retriever in multimodal retrieval.\\nmethods struggle to understand deeper textual meanings and contextual relevance between\\nqueries and documents.\\nRecent advancements in sparse retrieval models have been driven by the integration of\\npre-trained language models (PLMs). While these approaches leverage PLMs, they remain\\nfundamentally rooted in lexical matching, enabling the reuse of traditional sparse index\\nstructures by incorporating auxiliary information such as contextualized embeddings\\n[94, 208] and extended tokens [83, 84, 284]. This research domain focuses on two main'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 17, 'page_label': '18', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='structures by incorporating auxiliary information such as contextualized embeddings\\n[94, 208] and extended tokens [83, 84, 284]. This research domain focuses on two main\\napproaches: term weighting and term expansion. Term weighting enhances relevance\\nestimation by leveraging context-specific token representations. DeepCT [60] and HDCT\\n[59] use learned token representations to estimate the context-specific importance of terms\\nwithin passages, while COIL [94] and uniCOIL [208] employ contextualized token repre-\\nsentations of exact matching terms to compute relevance via dot products and summed\\nsimilarity scores. Term expansion mitigates vocabulary mismatch by expanding queries or\\ndocuments using PLMs. For instance, DocTTTTTquery [ 284] predicts relevant queries\\nfor documents to enrich the document’s content, while SPLADE [84] and SPLADEv2 [83]\\nproject terms onto vocabulary-sized weight vectors derived from masked language model'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 17, 'page_label': '18', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='for documents to enrich the document’s content, while SPLADE [84] and SPLADEv2 [83]\\nproject terms onto vocabulary-sized weight vectors derived from masked language model\\nlogits. These vectors, aggregated via methods like summing or max pooling, effectively\\nexpand content by incorporating absent terms. Sparsity regularization ensures efficient\\nsparse representations for inverted index usage.\\nIn summary, sparse retrieval models achieve an optimal balance in cross-domain transfer,\\nretrieval efficiency, and overall effectiveness.\\n·Dense Text Retrieval. Recent advancements in deep learning [5, 50, 51, 105, 119, 167,\\n169], particularly pre-trained language models (PLMs) [23, 62, 229] based on the Trans-\\nformer architecture [ 80, 362], have increasingly adopted dense vector embeddings in\\nlow-dimensional Euclidean spaces for modeling semantic relationships between queries\\nand documents. These embeddings enable relevance measurement through Euclidean dis-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 17, 'page_label': '18', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='low-dimensional Euclidean spaces for modeling semantic relationships between queries\\nand documents. These embeddings enable relevance measurement through Euclidean dis-\\ntances or inner products. Dense retrieval methods have demonstrated strong performance\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 18, 'page_label': '19', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 19\\nacross various information retrieval tasks [161, 163, 270]. Additionally, Approximate Near-\\nest Neighbor Search (ANNS) algorithms [98, 142, 156], particularly quantization-based\\nmethods [98, 142] and their retrieval-oriented variants [415, 453, 455, 461], enable efficient\\nretrieval of top-ranked documents from large collections using precomputed ANNS indices.\\nDense retrieval techniques primarily focus on two key aspects: model architecture and\\ntraining methods.\\nFor model architecture, dense retrieval methods employ a two-tower architecture to\\nbalance retrieval efficiency and effectiveness by modeling semantic interactions between\\nqueries and documents through their representations. These methods vary in represen-\\ntation granularity, primarily falling into two categories: single-vector and multi-vector\\nrepresentations. Then, the relevance scores are computed using similarity functions (e.g.,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 18, 'page_label': '19', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='tation granularity, primarily falling into two categories: single-vector and multi-vector\\nrepresentations. Then, the relevance scores are computed using similarity functions (e.g.,\\ncosine similarity, inner product) between these embeddings. A common technique in-\\nvolves placing a special token (e.g., “[CLS]”) at the beginning of a text sequence, with\\nits learned representation capturing the overall semantics. The existing dense retrieval\\nmodels learn the query and document representations by fine-tuning PLMs like BERT [62],\\nRoBERTa [229], or Mamba Gu and Dao [106], Zhang et al. [458], or large language models\\n(LLMs) like RepLLaMA [255] on annotated datasets (e.g., MSMARCO [281], BEIR [354]).\\nHowever, single-vector bi-encoders struggle to model fine-grained semantic interactions\\nbetween queries and documents. To address this limitation, multi-vector representation en-\\nhance text representation and semantic interaction by employing multiple-representation'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 18, 'page_label': '19', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='between queries and documents. To address this limitation, multi-vector representation en-\\nhance text representation and semantic interaction by employing multiple-representation\\nbi-encoders. The Poly-encoder [137] generates multiple context codes to capture text se-\\nmantics from multiple views. ME-BERT [246] produces 𝑚representations for a candidate\\ntext using the contextualized embeddings of the first 𝑚tokens. ColBERT [163] maintains\\nper-token contextualized embeddings with a late interaction mechanism. ColBERTer [120]\\nextends ColBERT by combining single- (“[CLS]”) and multi-representation (per-token)\\nmechanisms for better performance. MVR [471] introduces multiple “[VIEW]” tokens to\\nlearn diverse representations, with a local loss to identify the best-matched view. MADRM\\n[166] learns multiple aspect embeddings for queries and texts, supervised by explicit aspect\\nannotations.\\nFor training method, to achieve optimal retrieval performance, dense retrieval mod-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 18, 'page_label': '19', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[166] learns multiple aspect embeddings for queries and texts, supervised by explicit aspect\\nannotations.\\nFor training method, to achieve optimal retrieval performance, dense retrieval mod-\\nels are typically trained using two key techniques: negative sampling and pretraining.\\nNegative sampling focuses on selecting high-quality negatives to compute the negative\\nlog-likelihood loss used for training dense retrieval models. Basic methods include ran-\\ndom sampling [127] and in-batch negatives [118, 161, 304], which increase the number\\nof negatives within memory limits but do not guarantee the inclusion of hard negatives,\\ni.e., irrelevant texts with high semantic similarity to the query. Hard negatives are critical\\nfor improving the model’s ability to distinguish relevant from irrelevant texts. Various\\napproaches have been proposed to incorporate hard negatives. BM25-retrieved documents\\nare used as static hard negatives [ 95, 161]. STAR [454] combines static hard negatives'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 18, 'page_label': '19', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='approaches have been proposed to incorporate hard negatives. BM25-retrieved documents\\nare used as static hard negatives [ 95, 161]. STAR [454] combines static hard negatives\\nwith random negatives, while ANCE [ 409] retrieves hard negatives using a warm-up\\ndense retrieval model and refreshes the document index during training. ADORE [454]\\nemploys an adaptive query encoder to retrieve top-ranked texts as hard negatives, keeping\\nthe text encoder and document index fixed. However, hard negatives may include false\\nnegatives, introducing noise that can degrade performance. RocketQA [ 304] addresses\\nthis by using a cross-encoder to filter out likely false negatives. AR2 [ 459] integrates a\\ndual-encoder retriever with a cross-encoder ranker, jointly optimized through a minimax\\nadversarial objective to produce harder negatives and improve the retriever. SimANS [497]\\nintroduces the concept of sampling ambiguous negatives, i.e., texts ranked near positives'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 18, 'page_label': '19', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='adversarial objective to produce harder negatives and improve the retriever. SimANS [497]\\nintroduces the concept of sampling ambiguous negatives, i.e., texts ranked near positives\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 19, 'page_label': '20', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='20 Trovato et al.\\nwith moderate similarity to the query. These negatives are more informative and less likely\\nto be false negatives, further enhancing model performance.\\nPretraining aims to learn universal semantic representations that generalize to down-\\nstream dense retrieval tasks. To enhance the modeling capacity of PLMs, self-supervised\\npretraining tasks, such as those proposed by Lee et al. [172] (selecting random sentences\\nas queries) and Chang et al. [27] (leveraging hyperlinks for constructing query-passage\\npairs), mimic retrieval objectives. Prop [251] and B-PROP [252] use document language\\nmodels (e.g., unigram, BERT) to sample word sets, training PLMs to predict pairwise\\npreferences. To enhance dense retrieval models, studies focus on improving the “[CLS]”\\ntoken embedding. Condenser [92] aggregates global text information for masked token\\nrecovery, while co-Condenser [93] adds a query-agnostic contrastive loss to cluster related'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 19, 'page_label': '20', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='token embedding. Condenser [92] aggregates global text information for masked token\\nrecovery, while co-Condenser [93] adds a query-agnostic contrastive loss to cluster related\\ntext segments while distancing unrelated ones. Contriever [139] generates positive pairs\\nby sampling two spans from the same text and negatives using in-batch and cross-batch\\ntexts. Following with an unbalanced architecture (strong encoder, simple decoder), SimLM\\n[373] pretrains the encoder and decoder with replaced language modeling, recovering\\noriginal tokens after replacement. It further optimizes the retriever through hard negative\\ntraining and cross-encoder distillation. RetroMAE [491] utilizes a high masking ratio for\\nthe decoder and a standard ratio for the encoder, incorporating an enhanced decoding\\nmechanism with two-stream and position-specific attention masks. Liu and Yang [214]\\nintroduces a two-stage pretraining approach, combining general-corpus pretraining with'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 19, 'page_label': '20', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='mechanism with two-stream and position-specific attention masks. Liu and Yang [214]\\nintroduces a two-stage pretraining approach, combining general-corpus pretraining with\\ndomain-specific continual pretraining, achieving strong benchmark performance.\\nHowever, single-modal retrieval is inherently limited by its inability to capture cross-modal\\nrelationships, which underscores the importance of integrating multimodal retrieval strategies\\nto bridge textual and visual semantics for more comprehensive information retrieval and\\ngeneration.\\n– Retrieval for Cross-modal. Cross-modal retrieval enables the identification of relevant data\\nin one modality (e.g., images) using a query from another (e.g., text). It enhances MRAG systems\\nby facilitating the retrieval and generation of information across diverse modalities, including\\ntext, images, audio, and video.\\n∗Text–Image Retrieval. Text–Image Retrieval aims to match images with corresponding'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 19, 'page_label': '20', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='text, images, audio, and video.\\n∗Text–Image Retrieval. Text–Image Retrieval aims to match images with corresponding\\ntextual queries by leveraging multimodal data co-occurrence, such as paired text-image\\ninstances or manual annotations, to capture semantic correlations. Existing methods can be\\ncategorized into three groups: CNN/RNN-based approaches, Transformer-based techniques,\\nand Vision-Language Pretraining (VLP) model-based methods.\\nEarly CNN/RNN-based methods [75, 110, 174, 231, 370, 390] extract features from each\\nmodality separately using MLP, CNN, and RNN, enforcing cross-modal constraints through\\npositive/negative sample construction. MSDS [370] uses CNN with a maximum likelihood-\\nbased scheme for image-text relevance. VSE++ [ 75] combines CNN and RNN with hard\\nsample mining in ranking loss. Advances include residual learning [ 231], character-level\\nconvolution [390], and disentangled representation [110] for improved feature mapping and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 19, 'page_label': '20', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='sample mining in ranking loss. Advances include residual learning [ 231], character-level\\nconvolution [390], and disentangled representation [110] for improved feature mapping and\\nretrieval. DSCMR [489] maps multimodal data into a shared space using modality-specific\\nnetworks and fully connected layers, leveraging label constraints and pairwise loss for\\ndiscriminant learning. Recent CNN/RNN-based methods improve image-text matching by\\naddressing key challenges. DRCE [384] enhances rare content representation and association\\nusing a dual-path structure, adaptive fusion, and reranking to mitigate long-tail issues. ESSE\\n[386] tackles one-to-many correspondence by projecting data as sectors with uncertainty\\napertures. SDCMR [388] employs diverse CNNs for multimodal feature extraction and a dual\\nadversarial mechanism to isolate semantic-shared features, ensuring retrieval consistency.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 20, 'page_label': '21', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 21\\nThese methods collectively advance cross-modal retrieval robustness and accuracy. Spatial\\nattention [72, 135, 299, 408, 468] is widely used in CNN/RNN-based cross-modal retrieval to\\nuncover fine-grained associations by generating weighted masks for local regions, enhancing\\nkey features while suppressing irrelevant ones. DSVEL [72] employs spatial-aware pooling\\nto align image regions with text, while CRAN [299] and CAAN [468] improve global-local\\nalignment through relation alignment and context-aware selection. RANet [ 408] refines\\nattention mechanisms with reference attention to reduce incorrect scores and adaptive\\naggregation to amplify relevant information and minimize redundancy.\\nTransformer-based methods [18, 57, 87, 215, 333, 350, 419, 422, 451] leverage multi-head\\nself-attention to encode multimodal relationships and optimize modality-specific encoders,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 20, 'page_label': '21', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Transformer-based methods [18, 57, 87, 215, 333, 350, 419, 422, 451] leverage multi-head\\nself-attention to encode multimodal relationships and optimize modality-specific encoders,\\ndemonstrating superior performance in multimodal modeling and cross-modal retrieval tasks.\\nRecent advancements in multimodal representation learning have focused on enhancing\\nTransformer architectures and feature alignment. PVSE [333] integrates self-attention and\\nresidual learning, while PCME [57] uses probabilistic embeddings to model one-to-many\\nand many-to-many correlations. RLCMR [422] tokenizes multimodal data and trains with\\na unified Transformer encoder for cross-modal semantic correlation. DREN [419] refines\\nfeature representation through character-level and context-driven augmentation. TGDT\\n[215] unifies coarse- and fine-grained learning with multimodal contrastive loss for feature\\nalignment. HREM [87] improves image-text matching by capturing multi-level intra- and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 20, 'page_label': '21', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[215] unifies coarse- and fine-grained learning with multimodal contrastive loss for feature\\nalignment. HREM [87] improves image-text matching by capturing multi-level intra- and\\ninter-modal relationships. TransTPS [ 18] extends Transformers with cross-modal multi-\\ngranularity matching and contrastive loss for better feature distinction. IEFT [350] models\\ntext-image pairs as unified entities to model their intrinsic correlation.\\nWith the rapid advancement of pretraining paradigms, Vision-Language Pretraining (VLP)\\nmodels [46, 62, 68, 107, 136, 144, 183, 194, 305], including both single- and dual-stream\\narchitectures, have leveraged large-scale visual-linguistic datasets for joint pretraining. Re-\\nsearchers have utilized the strong representational capabilities [88, 146, 234, 240, 250, 380,\\n405, 477, 492] of VLP models to significantly enhance cross-modal retrieval performance.\\nSingle-stream models like TEAM [405] align multimodal token embeddings for token-level'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 20, 'page_label': '21', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='405, 477, 492] of VLP models to significantly enhance cross-modal retrieval performance.\\nSingle-stream models like TEAM [405] align multimodal token embeddings for token-level\\nmatching, while dual-stream approaches such as COTS [240] integrate contrastive learning\\nwith token- and task-level interactions. Methods like CSIC [234] and LAPS [88] improve mul-\\ntimodal alignment by quantifying semantic significance and associating patch features with\\nwords, respectively. AGREE [380] fine-tunes and reranks cross-modal entities to harmonize\\ntheir alignment. IRRA [146] employs text-specific mask mechanism to capture fine-grained\\nintra- and inter-modal relationships. USER [477], EI-CLIP [250], and MAKE [492] leverage\\nCLIP [305] or ALIGN [144] to integrate contrastive learning and keyword enhancement\\nfor enriching representations. Overall, VLP models, through strategies such as fine-tuning,\\nreranking, and follow-up training, have become essential for improving cross-modal align-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 20, 'page_label': '21', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='for enriching representations. Overall, VLP models, through strategies such as fine-tuning,\\nreranking, and follow-up training, have become essential for improving cross-modal align-\\nment and interaction.\\n∗Text–Video Retrieval. Text-video retrieval involves matching textual descriptions with\\ncorresponding videos, requiring spatiotemporal representations to address temporal dy-\\nnamics, scene transitions, and precise text-video alignment. This task is more complex\\nthan text-image retrieval due to the need to model both visual and sequential information\\neffectively.\\nEarly CNN/RNN-based methods [64, 273, 274, 276, 361, 441] encode videos and texts into\\na shared latent space for similarity measurement. LLVE [361] employs CNNs and LSTMs\\nto extract latent features from images and texts, with LSTMs further capturing temporal\\nrelationships between video frames. Subsequent studies [274, 276] apply mean/max pooling'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 20, 'page_label': '21', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='to extract latent features from images and texts, with LSTMs further capturing temporal\\nrelationships between video frames. Subsequent studies [274, 276] apply mean/max pooling\\nto frame sequences to generate compact video-level representations, prioritizing efficiency\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 21, 'page_label': '22', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='22 Trovato et al.\\nover granularity. Later advancements incorporate additional modalities, such as audio and\\nmotion, to enhance video semantics [273]. For text encoding, simpler methods like Word2Vec,\\nLSTMs, or GRUs are commonly used [274, 276, 441], with evidence suggesting that combining\\nmultiple text encoding strategies improves retrieval performance [64].\\nTransformer-based methods [89, 224] utilize self-attention mechanisms to jointly encode\\nvideos and texts, enabling cross-modal interaction. MMT [ 89] employs mutual attention\\nbetween video and text modalities, integrating temporal information to enhance feature\\nrepresentation. Inspired by MoCo [ 116], HiT [ 224] introduces hierarchical cross-modal\\ncontrastive matching at both feature and semantic levels. Additionally, these methods [89,\\n224] encode diverse modalities in video data, such as audio and motion, further enriching\\nvideo representations.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 21, 'page_label': '22', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='224] encode diverse modalities in video data, such as audio and motion, further enriching\\nvideo representations.\\nRecently, VLP-based models [131, 247, 356, 371, 402, 421] utilize pretrained models like\\nCLIP [305] to enhance text-video tasks in text-video retrieval tasks by capturing cross-modal\\nand temporal dependencies. CLIP4Clip [247] adapt CLIP for text-video retrieval and caption-\\ning, analyzing temporal dependencies. VoP [131] introduces prompt tuning and fine-tunes\\nCLIP to model spatiotemporal video aspects, while Cap4Video [ 402] leverages zero-shot\\ncaptioning with CLIP and GPT-2 [306] for auxiliary captions. DGL [421] proposes dynamic\\nglobal-local prompt tuning, emphasizing intermodal interaction and global video informa-\\ntion through shared latent spaces and attention mechanisms. TeachCLIP [ 356] improves\\nCLIP4Clip by integrating fine-grained cross-modal knowledge from advanced models, and re-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 21, 'page_label': '22', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='tion through shared latent spaces and attention mechanisms. TeachCLIP [ 356] improves\\nCLIP4Clip by integrating fine-grained cross-modal knowledge from advanced models, and re-\\nfining text-video similarity with an frame-feature aggregation block. T-MASS [371] addresses\\ndataset limitations by enriching text embeddings with stochastic text modeling.\\n∗Text–Audio Retrieval. Text-audio retrieval involves matching textual queries with corre-\\nsponding audio content, requiring alignment of semantic text information with dynamic\\nacoustic patterns in speech, music, or environmental sounds. The challenge lies in bridging\\nthe gap between discrete text and continuous audio signals.\\nEarly CNN/RNN-based approaches [ 239, 271, 485] focus on encoding text and audio\\nseparately and aligning them in a shared space for similarity measurement. ATR [239] uses\\npretrained CNN-based audio networks with NetRVLAD pooling [143] to aggregate features'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 21, 'page_label': '22', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='separately and aligning them in a shared space for similarity measurement. ATR [239] uses\\npretrained CNN-based audio networks with NetRVLAD pooling [143] to aggregate features\\ninto a unified representation. OML [271] employs CNNs for robust audio feature extraction\\nand metric learning to enhance audio-text alignment. MGRL [ 485] leverages CNNs for\\nlocalized audio features and introduces adaptive aggregation to handle varying text–audio\\ngranularities.\\nFurthermore, Transformer-based methods [63, 406, 494] utilize multi-head attention mech-\\nanisms and fine-tuning to enhance cross-modal interactions. TAP-PMR [406] employs scaled\\ndot-product attention to enable text to focus on relevant audio frames, reducing mislead-\\ning information, while its prior matrix revised loss optimizes dual matching by addressing\\nsimilarity inconsistencies. CMRF [494] enhances audio-lyrics retrieval through directional\\ncross-modal attention and reinforcement learning to refine multimodal embeddings and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 21, 'page_label': '22', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='similarity inconsistencies. CMRF [494] enhances audio-lyrics retrieval through directional\\ncross-modal attention and reinforcement learning to refine multimodal embeddings and\\ninteractions. TTMR++ [63] integrates fine-tuned LLMs and rich metadata to generate detailed\\ntext descriptions, improving retrieval by addressing musical attributes and user preferences.\\n∗Unified-Modal Retrieval. Unified-Modal Retrieval aims to process diverse hybrid-modal\\ndata (e.g., text, images, videos) within a unified model architecture, such as transformer-based\\nPLMs, to encode all modalities into a shared feature space. This enables efficient cross-modal\\nretrieval between any pairwise combination of hybrid-modal data. With the growing demand\\nfor multimodal applications, there is an increasing need for unified multimodal retrieval\\nmodels tailored to complex scenarios. Current approaches leverage pre-trained models like\\nCLIP [305], BLIP [186], and ALIGN [144] for multimodal embedding. For instance, FLAVA'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 21, 'page_label': '22', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='models tailored to complex scenarios. Current approaches leverage pre-trained models like\\nCLIP [305], BLIP [186], and ALIGN [144] for multimodal embedding. For instance, FLAVA\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 22, 'page_label': '23', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 23\\n[326] integrates multiple modalities into a unified framework, leveraging joint pretraining\\non multimodal data with cross-modal alignment and fusion objectives. Similarly, UniVL-DR\\n[237] encodes queries and multimodal resources into a shared embedding space, employing\\na universal embedding optimization strategy with modality-balanced hard negatives and an\\nimage verbalization method to bridge the gap between images and texts. MARVEL [ 498]\\naddresses the modality gap between images and texts by incorporating visual features\\ninto the encoding process. FLMR [211] enhances image representations by using a visual\\nmodel aligned with existing text-based retrievers to supplement the image representation of\\nimage-to-text transforms. UniIR [391] introduces a unified instruction-guided multimodal\\nretriever, achieving robust generalization through instruction tuning on diverse multimodal-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 22, 'page_label': '23', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='image-to-text transforms. UniIR [391] introduces a unified instruction-guided multimodal\\nretriever, achieving robust generalization through instruction tuning on diverse multimodal-\\nIR tasks. VISTA [495] extends image understanding capability by integrating visual token\\nembeddings into a text encoder, supported by high-quality composed image-text data and\\na multi-stage training algorithm. E5-V [ 150] fine-tunes MLLMs on single-text or vision-\\ncentric relevance data, outperforming traditional image-text pair training. VLM2VEC [152]\\nproposes a contrastive training framework to convert vision-language models into embedding\\nmodels using the MMEB dataset [152]. To address modality imbalance, GME [476] trains\\nan MLLM-based dense retriever on the large-scale UMRB dataset[ 476]. Ovis [244] aligns\\nvisual and textual embeddings by integrating a learnable visual embedding table, enabling\\nprobabilistic combinations of indexed embeddings for rich visual semantics. ColPali [ 79]'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 22, 'page_label': '23', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='visual and textual embeddings by integrating a learnable visual embedding table, enabling\\nprobabilistic combinations of indexed embeddings for rich visual semantics. ColPali [ 79]\\nleverages Vision Language Models and the ViDoRe benchmark [79] to index documents from\\ntheir visual features, facilitating efficient query matching with late interaction mechanisms.\\nCREAM [462] employs a coarse-to-fine retrieval and ranking approach, combining similarity\\ncalculations with large language model-based grouping and attention pooling for MLLM-\\nbased multi-page document processing. DSE [254] fine-tunes a large vision-language model\\non 1.3 million Wikipedia web page screenshots, enabling direct encoding of document\\nscreenshots into dense representations.\\n•Generative Structure: Traditional information retrieval (IR) methods, which rely on similarity\\nmatching to return ranked lists of documents, have long been a cornerstone of information'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 22, 'page_label': '23', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='•Generative Structure: Traditional information retrieval (IR) methods, which rely on similarity\\nmatching to return ranked lists of documents, have long been a cornerstone of information\\nacquisition, dominating the field for decades. However, with the advent of pre-trained language\\nmodels, generative retrieval (GR) has emerged as a novel paradigm, garnering increasing attention\\nin recent years. GR primarily consists of two fundamental components: model training and\\ndocument identifier. Model Training aims to train generative models to effectively index and\\nretrieve documents, while enhancing the model’s capacity to memorize information from the\\ndocument corpus. This is typically achieved through sequence-to-sequence (seq2seq) training,\\nwhere the model learns to map queries to their corresponding Document Identifiers (DocIDs).\\nThe training process emphasizes optimizing the model’s understanding of semantic relationships'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 22, 'page_label': '23', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='where the model learns to map queries to their corresponding Document Identifiers (DocIDs).\\nThe training process emphasizes optimizing the model’s understanding of semantic relationships\\nbetween queries and documents, thereby improving retrieval accuracy. Document Identifiers\\n(DocIDs) serve as the target output for the generative retrieval model, and unique representations\\nof each document in the corpus. The quality of these identifiers is crucial, as they directly impact\\nthe model’s ability to memorize and retrieve document information. Effective DocIDs are often\\ngenerated using dense, low-dimensional embeddings or structured representations that capture\\nthe essential content and context of documents, enabling the model to distinguish between\\ndocuments more accurately and enhancing retrieval performance. By overcoming the limitations\\nof traditional IR in terms of content granularity and relevance matching, GR offers enhanced'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 22, 'page_label': '23', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='documents more accurately and enhancing retrieval performance. By overcoming the limitations\\nof traditional IR in terms of content granularity and relevance matching, GR offers enhanced\\nflexibility, efficiency, and creativity, better aligning with practical demands.\\n– Retrieval for Text-modal. The recent advancements in generative language models have\\ndemonstrated their ability to memorize knowledge from documents and recall knowledge to\\nrespond to user queries effectively, which focuses on the use of document identifiers (DocIDs)\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 23, 'page_label': '24', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='24 Trovato et al.\\nand their optimization for retrieval tasks. The approaches can be categorized into static DocID-\\nbased methods and learnable DocID-based methods.\\nStatic DocID-based methods rely on pre-defined, fixed document identifiers. They often\\nuse unique names, numeric formats, or structured identifiers to represent documents. GENRE\\n[61] generates entity names via constrained beam search using a prefix tree, with document\\ntitles serving as DocIDs. DSI [352] introduces numeric DocID formats, including unstructured,\\nnaively structured, and semantically structured identifiers, trained through indexing and\\nretrieval strategies. DynamicRetriever [502] uses unstructured atomic DocIDs and enhances\\nmemorization with pseudo queries. SEAL [19] representing documents with N-gram sub-string\\nidentifiers, leveraging FM-Index [82] for retrieval. DSI-QG [512] represents documents with\\ngenerated queries, re-ranked by a cross-encoder. NCI [383] generates document identifiers using'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 23, 'page_label': '24', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='identifiers, leveraging FM-Index [82] for retrieval. DSI-QG [512] represents documents with\\ngenerated queries, re-ranked by a cross-encoder. NCI [383] generates document identifiers using\\na seq2seq network with a prefix-aware decoder. It is trained on both labeled and augmented\\npseudo query-document pairs. Ultron [501] combines URLs and titles as DocIDs to uniquely\\nidentify web documents. It encodes documents into a latent semantic space using BERT [62]\\nand compresses vectors via Product Quantization (PQ) [ 98, 142], with PQ codes serving as\\nsemantic identifiers. Additional digits ensure DocID uniqueness. LTRGR [ 201] focuses on\\nlearning to rank passages directly using generative retrieval models, optimizing autoregressive\\nmodels via rank loss. GenRRL [500] integrates reinforcement learning for aligning token-level\\nDocID generation with document-level relevance estimation. DGR [202] enhances generative'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 23, 'page_label': '24', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='models via rank loss. GenRRL [500] integrates reinforcement learning for aligning token-level\\nDocID generation with document-level relevance estimation. DGR [202] enhances generative\\nretrieval through knowledge distillation, using a cross-encoder as a teacher model to provide\\nfine-grained ranking supervision. Despite these innovations, most approaches rely on static\\nDocIDs, which are not optimized for retrieval tasks, limiting their ability to capture document\\nsemantics and relationships, thereby hindering retrieval performance.\\nTo address this limitation, Learnable DocID-based methods introduce learnable document\\nrepresentations, where DocIDs are optimized during training to better capture document\\nsemantics and improve retrieval performance. GenRet [340] employs a discrete autoencoder\\nto encode documents into compact DocIDs, minimizing reconstruction error. MINDER [200]\\nenhances document representations using multi-view identifiers, including pseudo-queries,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 23, 'page_label': '24', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='to encode documents into compact DocIDs, minimizing reconstruction error. MINDER [200]\\nenhances document representations using multi-view identifiers, including pseudo-queries,\\ntitles, and sub-strings. NOVO [389] introduces learnable continuous N-gram DocIDs, refining\\nembeddings through query denoising and retrieval tasks. LMIndexer [153] generates neural\\nsequential discrete IDs via progressive training and contrastive learning, addressing semantic\\nmismatches. ASI [420] automates DocID learning, assigning similar IDs to semantically close\\ndocuments and optimizing end-to-end retrieval using an generative model. RIPOR [ 449]\\nimproves relevance scoring during sequential DocID generation using dense encoding and\\nResidual Quantization [264]. GLEN [175] employs a dynamic lexical identifier with a two-phase\\nindex learning strategy. Firstly, the keyword-based DocID are defined by extracting keywords\\nfrom documents using self-supervised signals. Secondly, dynamic DocIDs are refined by'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 23, 'page_label': '24', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='index learning strategy. Firstly, the keyword-based DocID are defined by extracting keywords\\nfrom documents using self-supervised signals. Secondly, dynamic DocIDs are refined by\\nintegrating query-document relevance, enabling efficient inference. The field of generative text\\nretrieval is evolving from static, pre-defined DocIDs to dynamic, learnable DocIDs that better\\ncapture document semantics and relationships. Learnable DocIDs, combined with advanced\\ntechniques like reinforcement learning, knowledge distillation, and contrastive learning, are\\ndriving improvements in retrieval performance.\\n– Retrieval for Cross-modal. Similarly, MLLMs are considered to memorize and retrieve\\nmultimodal content, such as images and videos, within their parameters. When presented\\nwith a user query for visual content, the MLLM is expected to \"recall\" the relevant image\\nfrom its parameters as a response. Achieving this capability presents significant challenges,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 23, 'page_label': '24', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='with a user query for visual content, the MLLM is expected to \"recall\" the relevant image\\nfrom its parameters as a response. Achieving this capability presents significant challenges,\\nparticularly in developing effective visual memory and recall mechanisms within MLLMs.\\nIRGen [479] employs a seq2seq model to predict discrete visual tokens (image identifiers) from\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 24, 'page_label': '25', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 25\\nquery images. Its key innovation is a semantic image tokenizer that encodes global features\\ninto discrete visual tokens, enabling end-to-end differentiable search for improved accuracy\\nand efficiency. GeMKR [238] integrates LLMs with visual-text features through a generative\\nmultimodal knowledge retrieval framework. It first guides multi-granularity visual learning\\nusing object-aware prefix tuning techniques to align visual features with LLMs’ text feature\\nspace, then adopts a two-step retrieval process: generating knowledge clues relevant to the\\nquery and retrieving documents based on these clues. GRACE [199] assigns unique identifier\\nstrings to represent images, training MLLMs to memorize and retrieve image identifiers from\\ntextual queries. ACE [76] combines K-Means and RQ-VAE to construct coarse and fine tokens\\nas multimodal data identifiers, aligning natural language queries with candidate identifiers.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 24, 'page_label': '25', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='textual queries. ACE [76] combines K-Means and RQ-VAE to construct coarse and fine tokens\\nas multimodal data identifiers, aligning natural language queries with candidate identifiers.\\nAVG [195] introduces autoregressive voken (i.e., visual token) generation, tokenizing images\\ninto vokens that serve as image identifiers while preserving visual and semantic alignment.\\nBy framing text-to-image retrieval as a token-to-voken generation task, AVG bridges the gap\\nbetween generative training and retrieval objectives through discriminative training, refining\\nthe learning direction during token-to-voken generation.\\n3.3.2 RERANKER. Reranker, as a critical second-stage component in multimodal retrieval, is\\ndesigned to re-rank a multimodal document list initially retrieved by a first-stage retriever. It\\nachieves this by employing advanced relevance scoring mechanisms, such as cross-attention\\nmodels, which enable more contextual interactions between queries and documents. Based on'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 24, 'page_label': '25', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='achieves this by employing advanced relevance scoring mechanisms, such as cross-attention\\nmodels, which enable more contextual interactions between queries and documents. Based on\\nthe utilization of large models, including LLMs and MLLMs, existing reranking methods can be\\ncategorized into two primary paradigms: fine-tuning-as-reranker and prompting-as-reranker.\\n•Fine-tuning-as-Reranker: The fine-tuning-as-reranker paradigm adapts PLMs to domain-\\nspecific reranking tasks through supervised fine-tuning on domain-specific datasets, addressing\\ntheir inherent lack of ranking awareness and inability to effectively measure query-document\\nrelevance.\\n– Reranking for Text-Modal : According the development of large models’ architecture,\\nreranker can be divided to three categories: encoder-only, encoder-decoder, and decoder-\\nonly.\\nEncoder-only rerankers have advanced document ranking by fine-tuning PLMs (e.g., BERT'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 24, 'page_label': '25', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='reranker can be divided to three categories: encoder-only, encoder-decoder, and decoder-\\nonly.\\nEncoder-only rerankers have advanced document ranking by fine-tuning PLMs (e.g., BERT\\n[62]) to achieve precise relevance estimation. Key examples include Nogueira and Cho [282]\\nand monoBERT [285], which format query-document pairs as query-document sequences.\\nThe relevance score is derived from the “[CLS]” token’s representation via a linear layer, with\\noptimization achieved through negative sampling and cross-entropy loss.\\nExisting research on encoder-decoder rerankers primarily formulates document ranking as a\\ngeneration task [157, 283, 296, 510], fine-tuning models like T5 to generate classification tokens\\n(e.g., “true” or “false”) for query-document pairs, with relevance scores derived from token logits\\n[283]. Extensions include multi-view learning approaches [157] that simultaneously generate\\nclassification tokens for query-document pairs and queries conditioned on documents, and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 24, 'page_label': '25', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[283]. Extensions include multi-view learning approaches [157] that simultaneously generate\\nclassification tokens for query-document pairs and queries conditioned on documents, and\\nDuoT5 [296], which compares the classification tokens of document pairs to determine relative\\nrelevance. Beyond these approaches, studies have explored alternative training losses and\\narchitectures. Contrast with previous methods that rely on text generation losses, RankT5 [510]\\ndirectly produces numerical relevance scores for each query-document pair, optimizing with\\nranking losses instead of generation losses. ListT5 [433] further advances this by processing\\nmultiple documents simultaneously, directly generating reranked lists using the Fusion-in-\\nDecoder architecture.\\nRecent studies [ 222, 255, 294, 464, 475] have explored fine-tuning decoder-only models\\nlike LLaMA for document reranking. RankLLaMA [255] formats query-document pairs into\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 25, 'page_label': '26', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='26 Trovato et al.\\nprompts and uses the last token representation for relevance scoring. TSARankLLM [ 464]\\nemploys a two-stage training approach: continuous pretraining on web-sourced relevant text\\npairs to align LLMs with ranking tasks, followed by fine-tuning with supervised data and\\ntailored loss functions. Q-PEFT [294] introduces query-dependent parameter-efficient fine-\\ntuning to generate accurate queries from documents. In contrast, listwise approaches like\\nthose in [475] and PE-Rank [222] focus on directly outputting reranked document lists. Zhang\\net al. [475] highlight the limitations of point-wise datasets with binary labels, and instead use\\nranking outputs from existing systems as gold standards to train a listwise reranker. PE-Rank\\n[222] compresses documents into single embeddings, reducing input length and improving\\nreranking efficiency.\\n– Reranking for Cross-Model : The multi-modal reranking uses the multi-modal question'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 25, 'page_label': '26', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[222] compresses documents into single embeddings, reducing input length and improving\\nreranking efficiency.\\n– Reranking for Cross-Model : The multi-modal reranking uses the multi-modal question\\nand multi-modal knowledge items to obtain the relevance score, as reranking have already\\nshown its importance in various knowledge-intensive tasks. Wen et al . [394] fine-tunes a\\npretrained MLLM to facilitate cross-item interaction between questions and knowledge items.\\nThe reranker is trained on the same dataset as the answer generator, using distant supervision\\nby checking whether answer candidates appear in the knowledge text. RagVL RETRIEVAL\\net al. [311] introduces a novel framework featuring knowledge-enhanced reranking and noise-\\ninjected training. The approach involves instruction-tuning the MLLM with a simple yet\\neffective template to enhance its ranking capability, enabling it to serve as a reranker for\\naccurately filtering the top-𝑘 retrieved images.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 25, 'page_label': '26', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='effective template to enhance its ranking capability, enabling it to serve as a reranker for\\naccurately filtering the top-𝑘 retrieved images.\\nIn summary, these approaches leverages the representational capacity of large models while\\noptimizing them for task-specific relevance signals, often achieving high reranking accuracy.\\nHowever, it requires substantial computational resources and labeled training data, resulting in\\nincreased costs.\\n•Prompting-as-Reranker: In contrast, the prompting-as-reranker paradigm leverages large\\nmodels in a zero-shot or few-shot manner by designing prompts that direct the model to gen-\\nerate relevance scores or rankings directly. This approach exploits the inherent knowledge\\nand reasoning capabilities of large models, eliminating the need for extensive fine-tuning and\\noffering greater flexibility and resource efficiency. Researchers have explored prompting LLMs\\nand MLLMs to perform ranking tasks on multimodal documents, with prompting strategies'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 25, 'page_label': '26', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='offering greater flexibility and resource efficiency. Researchers have explored prompting LLMs\\nand MLLMs to perform ranking tasks on multimodal documents, with prompting strategies\\ngenerally categorized into three types: point-wise, pair-wise, and list-wise methods.\\n– Reranking for Text-Model : LLMs are increasingly employed in text-modal reranking tasks,\\nleveraging their advanced capabilities to optimize the ranking of textual documents.\\nPoint-wise methods evaluate the relevance between a query and individual documents,\\nreranking them based on relevance scores.. Zhuang et al. [509] integrates fine-grained rele-\\nvance labels into prompts for better document distinction. MCRanker [109] addresses biases\\nin existing point-wise rerankers by generating relevance scores based on multi-perspective\\ncriteria. UPR [318] re-scores retrieved passages using a zero-shot question generation model.\\nZhuang et al. [511] show that LLMs pre-trained without supervised instruction fine-tuning'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 25, 'page_label': '26', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='criteria. UPR [318] re-scores retrieved passages using a zero-shot question generation model.\\nZhuang et al. [511] show that LLMs pre-trained without supervised instruction fine-tuning\\n(e.g., LLaMA) also exhibit strong zero-shot ranking capabilities. Despite their effectiveness,\\nthese methods often rely on suboptimal handcrafted prompts. To improve prompts for rank-\\ning tasks, Co-Prompt [52] introduces a discrete prompt optimization method for improving\\nprompt generation in reranking tasks. PaRaDe [70] proposes a difficulty-based approach to\\nselect the most challenging in-context demonstrations for prompts, though experiments reveal\\nthat this method does not significantly outperform random selection. To improve demonstra-\\ntion selection, DemoRank [228] advances demonstration selection with a dependency-aware\\ndemonstration reranker, optimizing top-ranked examples through efficient training sample\\nconstruction and a novel list-pairwise loss.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 25, 'page_label': '26', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='demonstration reranker, optimizing top-ranked examples through efficient training sample\\nconstruction and a novel list-pairwise loss.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 26, 'page_label': '27', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 27\\nPair-wise methods involve presenting LLMs with a query and a document pair, instructing\\nthem to identify the more relevant document. PRP-AllPair [302] generates all possible pairs,\\nassigns discrete relevance judgments, and aggregates these into a final relevance score per\\ndocument. PRP-Graph [248] improves this by using judgment generation probabilities and a\\ngraph-based aggregation for scoring relevance. Additionally, a post-processing technique [416]\\nrefines LLM-generated labels by aligning them with pairwise preferences while minimizing\\ndeviations from original values.\\nListwise methods directly rank document lists by incorporating queries and documents into\\nprompts, instructing LLMs to output reranked document identifiers. RankGPT [341] introduces\\ninstructional permutation generation and a sliding window strategy to address context length'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 26, 'page_label': '27', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='prompts, instructing LLMs to output reranked document identifiers. RankGPT [341] introduces\\ninstructional permutation generation and a sliding window strategy to address context length\\nlimits, while LRL [256] reorders document identifiers for candidate documents. However, these\\nmethods face challenges: (1) performance is highly sensitive to document order, revealing\\npositional bias, and (2) the sliding window strategy limits the number of documents ranked\\nper iteration. Recent advancements have attempted to address these issues: Tang et al. [349]\\npropose permutation self-consistency to mitigate bias. TourRank [43] introduces a tournament\\nmechanism, parallelizing reranking to minimize the impact of initial document order. TDPart\\n[292] employs a top-down partitioning algorithm, which processes documents to depth using\\na pivot element. FIRST [ 309] leverages the output logits of the first generated identifier to\\ndirectly obtain a ranked ordering of candidates.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 26, 'page_label': '27', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='a pivot element. FIRST [ 309] leverages the output logits of the first generated identifier to\\ndirectly obtain a ranked ordering of candidates.\\n– Reranking for Cross-Model : Prompt-Based Multimodal Reranker uses prompts to guide a\\nMLLM in reranking items. TIGeR [303] proposes a framework leveraging multimodal LLMs\\nfor zero-shot reranking via a generative retrieval approach. However, their method is limited\\nto text-only query retrieval tasks. In contrast, Lin et al. [210] extends this scope by utilizing\\nmultimodal LLMs to address diverse multimodal reranking tasks, supporting queries and\\ndocuments in text, image, or interleaved text-image formats.\\nIn summary, these approaches leverages the pre-existing knowledge and reasoning capabilities\\nof LLMs, reducing the need for extensive task-specific fine-tuning. Consequently, it provides\\ngreater flexibility and resource efficiency, particularly in scenarios with limited labeled data or'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 26, 'page_label': '27', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='of LLMs, reducing the need for extensive task-specific fine-tuning. Consequently, it provides\\ngreater flexibility and resource efficiency, particularly in scenarios with limited labeled data or\\ncomputational resources. However, its effectiveness depends heavily on the quality and design of\\nthe prompts, as well as the model’s ability to generalize its pre-trained knowledge to the specific\\ndemands of the target task.\\n3.3.3 REFINER. Theoretically, LLMs improves with more comprehensive task-relevant knowledge\\nin the retrieved and reranked context. However, unlimited input length poses practical deployment\\nchallenges: (1) Limited Context Window: LLMs have a fixed input length determined during pre-\\ntraining, and any text exceeding this limit is truncated, leading to loss of contextual semantics.\\n(2) Catastrophic Forgetting: Insufficient cache space can cause LLMs to forget previously learned\\nknowledge when processing long sequences. (3) Slow Inference Speed. Consequently, refined'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 26, 'page_label': '27', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='(2) Catastrophic Forgetting: Insufficient cache space can cause LLMs to forget previously learned\\nknowledge when processing long sequences. (3) Slow Inference Speed. Consequently, refined\\nprompts are crucial for optimizing LLM performance.\\nThe refiner is an optional yet highly impactful component that optimizes retrieved and reranked\\ninformation before its utilization by the LLM. It performs advanced processing tasks, such as\\nsummarization, distillation, or contextualization, to condense and refine content into a more\\ndigestible and actionable format. By extracting key insights, eliminating redundancies, and aligning\\ninformation with the query’s context, the refiner enhances the utility of the retrieved data, enabling\\nthe LLM to generate more coherent, accurate, and contextually relevant responses.\\nPrompt refinement can be achieved through two primary approaches: hard prompt methods and\\nsoft prompt methods. Hard prompt methods involve filtering out unnecessary or low-information'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 26, 'page_label': '27', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Prompt refinement can be achieved through two primary approaches: hard prompt methods and\\nsoft prompt methods. Hard prompt methods involve filtering out unnecessary or low-information\\ncontent, still using natural language tokens and resulting in less fluent but generalizable prompts\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 27, 'page_label': '28', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='28 Trovato et al.\\nthat can be used across LLMs with different embedding configurations. Soft prompt methods, in\\ncontrast, encode prompt information into continuous representations, producing latent vectors\\n(special tokens) that are not human-readable but optimized for model performance.\\n•Hard Prompt Refiner : Hard prompts consist of natural language tokens from the LLM/MLLM’s\\nvocabulary, representing specific words or sub-words, and can be generated by humans or\\nmodels.\\n– Refining for Text-Model : Recent advancements in prompt compression and context distil-\\nlation aim to optimize the efficiency of LLMs. DynaICL [ 499] employs a meta controller to\\ndynamically allocate in-context demonstrations based on input complexity and computational\\nconstraints. FILCO [385] distills retrieved documents using lexical and information-theoretic\\nmethods—String Inclusion, Lexical Overlap, and CXMI—training both context filtering and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 27, 'page_label': '28', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='constraints. FILCO [385] distills retrieved documents using lexical and information-theoretic\\nmethods—String Inclusion, Lexical Overlap, and CXMI—training both context filtering and\\ngeneration models for RAG tasks. CPC [213] preserves semantic integrity by using a context-\\naware encoder to remove irrelevant sentences, while AdaComp [ 469] dynamically selects\\noptimal documents via a compression-rate predictor. LLMLingua [148] introduces a coarse-\\nto-fine approach, compressing prompt components (instructions, questions, demonstrations)\\nusing a small language model (SLM) to measure token informativeness via perplexity (PPL).\\nLongLLMLingua [149] extends this to long documents, employing a linear scheduler, reorder-\\ning mechanism, and contrastive perplexity to retain question-relevant tokens while ensuring\\nkey information integrity. CoT-Influx [132] compresses GPT-4-generated Chain-of-Thought\\n(CoT) prompts using a shot-pruner and token-pruner, both implemented as MLPs trained'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 27, 'page_label': '28', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='key information integrity. CoT-Influx [132] compresses GPT-4-generated Chain-of-Thought\\n(CoT) prompts using a shot-pruner and token-pruner, both implemented as MLPs trained\\nvia reinforcement learning. These methods collectively improve performance while reducing\\nuseless CoT examples and redundant tokens. Selective Context [196] evaluates lexical unit in-\\nformativeness using a causal language model and a percentile-based filtering method to remove\\nredundancy. It calculates token self-information by predicting next-token probabilities, aggre-\\ngating these at phrase and sentence levels. Prompt-SAW [11] preserves syntactic and semantic\\nstructures by extracting key tokens via relation-aware graphs, integrating them into com-\\npressed prompts. PCRL [159] treats prompt compression as a binary classification task, using a\\nfrozen pre-trained policy language model with trainable MLP layers. The compression policy'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 27, 'page_label': '28', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='pressed prompts. PCRL [159] treats prompt compression as a binary classification task, using a\\nfrozen pre-trained policy language model with trainable MLP layers. The compression policy\\nlabels tokens as include or exclude, optimizing a reward function that balances faithfulness and\\nprompt length reduction. LLMLingua-2 [288] employs a bidirectional encoder-only model with\\na linear classification layer for compression, determining token retention or removal. RECOMP\\n[410] employs extractive and abstractive compressors to generate query-focused summaries,\\nleveraging contrastive learning and knowledge distillation. Nano-Capsulator [56] optimizes\\ncompression using reward feedback from response differences and enforces strict length con-\\nstraints. MEMWALKER [32] uses interactive prompting to build and navigate a memory tree\\nfor context summarization. CompAct [432] sequentially compresses document segments for'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 27, 'page_label': '28', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='straints. MEMWALKER [32] uses interactive prompting to build and navigate a memory tree\\nfor context summarization. CompAct [432] sequentially compresses document segments for\\nlong-context question-answering, achieving high compression rates. Style-Compress [297]\\niteratively refines prompts using diverse styles and task-specific examples, evaluated by larger\\nLLMs. TCRA-LLM [220] combines summarization and semantic compression to reduce token\\nsize. TACO-RL [323] employs reinforcement learning for task-aware prompt compression,\\nensuring low latency. FaviComp [ 158] enhances evidence familiarity by combining token\\nprobabilities from compression and target models, reducing perplexity.\\n– Refining for Cross-Model : Recent advancements in visual token compression for MLLMs\\nfocus on enhancing efficiency without significant performance loss. LLaVolta [34] introduces\\na method to reduce the number of visual tokens, enhancing training and inference efficiency'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 27, 'page_label': '28', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='focus on enhancing efficiency without significant performance loss. LLaVolta [34] introduces\\na method to reduce the number of visual tokens, enhancing training and inference efficiency\\nwithout compromising performance. To minimize information loss during compression while\\nmaintaining training efficiency, it employs a lightweight, staged training scheme. This scheme\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 28, 'page_label': '29', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 29\\nprogressively compresses visual tokens from heavy to light compression during training, en-\\nsuring no information loss during testing. PyramidDrop [407] is a visual redundancy reduction\\nstrategy for MLLMs, designed to improve efficiency in both inference and training with negli-\\ngible performance loss. It partitions the MLLM into several stages and drops a predefined ratio\\nof image tokens at the end of each stage. DeCo [425] proposes the principle of \"Decouple Com-\\npression from Abstraction, \" which involves compressing visual tokens at the patch level using\\nprojectors while allowing the LLM to handle visual semantic abstraction entirely. MustDrop\\n[226] measures the importance of each token throughout its lifecycle, including the vision\\nencoding, prefilling, and decoding stages. During vision encoding, it merges spatially adjacent\\ntokens with high similarity and establishes a key token set to retain vision-critical tokens.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 28, 'page_label': '29', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='encoding, prefilling, and decoding stages. During vision encoding, it merges spatially adjacent\\ntokens with high similarity and establishes a key token set to retain vision-critical tokens.\\nIn the prefilling stage, it further compresses vision tokens guided by text semantics using a\\ndual-attention filtering strategy. In the decoding stage, an output-aware cache policy reduces\\nthe size of the KV cache. By employing tailored strategies across these stages, MustDrop\\nachieves an optimal balance between performance and efficiency. G-Search [484] proposes a\\ngreedy search algorithm to determine the minimum number of vision tokens to retain at each\\nlayer, from shallow to deep. Based on this strategy, a parametric sigmoid function (P-Sigmoid)\\nis designed to guide token reduction at each layer of the MLLM, with parameters optimized\\nusing Bayesian Optimization. G-Prune [151] introduces a graph-based method for training-free'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 28, 'page_label': '29', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='is designed to guide token reduction at each layer of the MLLM, with parameters optimized\\nusing Bayesian Optimization. G-Prune [151] introduces a graph-based method for training-free\\nvisual token pruning. It treats visual tokens as nodes and constructs connections based on\\nsemantic similarities. Information flow is propagated through weighted links, and the most\\nimportant tokens are retained for MLLMs after iterations.\\nAlthough interpretable and transparent, the inherent ambiguity of hard prompts often hinders\\nthe precise expression of intent, limiting their effectiveness in diverse or complex scenarios.\\nCrafting accurate and impactful hard prompts demands significant human effort and may require\\nmodel-based refinement or optimization. Moreover, even minor variations in hard prompts can\\nlead to inconsistent LLM performance for identical tasks.\\n•Soft Prompt Refiner : Soft prompts are trainable, continuous vectors that match the dimension-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 28, 'page_label': '29', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='lead to inconsistent LLM performance for identical tasks.\\n•Soft Prompt Refiner : Soft prompts are trainable, continuous vectors that match the dimension-\\nality of token embeddings in LLM’s vocabulary. Unlike hard prompts, which rely on discrete\\ntokens from a predefined vocabulary, soft prompts are optimized through training to capture\\nnuanced meanings that discrete tokens cannot express. When fine-tuned on diverse datasets,\\nsoft prompts enhance the LLM’s performance across various tasks.\\n– Refining for Text-Model : Language models convert text prompts into vectors for denser\\nrepresentation, enabling compression of discrete text into continuous vectors within the model.\\nThese vectors can serve as internal parameters (internalization) or additional soft prompts\\n(encoding). Such compression extends the context window and enhances inference speed,\\nparticularly with repeated prompt usage.\\nEarly work focused on system prompt internalization. Askell et al. [13] used Knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 28, 'page_label': '29', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='particularly with repeated prompt usage.\\nEarly work focused on system prompt internalization. Askell et al. [13] used Knowledge\\nDistillation to align models with human values, while Choi et al. [53] introduced Pseudo-Input\\nGeneration, generating pseudo-inputs from prompts and distilling knowledge between teacher\\nand student models to avoid redundant inference computations. Later research compressed user\\nprompt contexts. Snell et al. [331] distilled abstract instructions, reasoning, and examples into\\nprompts with distinct distribution differences, enabling task execution without explicit prompts.\\nSun et al. [339] internalized ranking techniques for zero-shot relevance tasks, while Distilling\\nStep-by-Step [122] improved reasoning tasks by distilling rationales as additional supervision.\\nIn retrieval-augmented generation, xRAG [48] integrated compressed document embeddings\\nvia a plug-and-play projector, using self-distillation for robustness. For context compression,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 28, 'page_label': '29', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='In retrieval-augmented generation, xRAG [48] integrated compressed document embeddings\\nvia a plug-and-play projector, using self-distillation for robustness. For context compression,\\nCOCOM [308] reduced long contexts to few embeddings, balancing trade-offs between decoding\\ntime and answer quality. LLoCO [346] learned offline compressed representations for efficient\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 29, 'page_label': '30', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='30 Trovato et al.\\nQA retrieval. QGC [25] retained key information under high compression using query-guided\\ndynamic strategies. UniICL [91] unified demonstration selection, compression, and generation\\nwithin a single frozen LLM, projecting demonstrations and inputs into virtual tokens for\\nsemantic-based processing.\\nRecent advancements in prompt compression for LLMs focus on encoding hard prompts\\ninto reusable soft prompts to enhance efficiency and generalization across tasks. Early work by\\nWingate et al. [396] distilled complex hard prompts into concise soft prompts by minimizing\\noutput distribution differences, reducing inference costs. A series of works aim to enhance\\ngeneralization across diverse prompts. Gist [279] used meta-learning to encode multi-task in-\\nstructions into gist tokens, while Gist-COCO [193] employed an encoder-decoder architecture\\nto compresses original prompts into shorter gist prompts, via the Minimum Description Length'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 29, 'page_label': '30', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='structions into gist tokens, while Gist-COCO [193] employed an encoder-decoder architecture\\nto compresses original prompts into shorter gist prompts, via the Minimum Description Length\\nprinciple. UltraGist [467] optimized cross-attention for compressing ultra-long contexts into\\nnear-lossless UltraGist tokens. AutoCompressor [49] iteratively compressed contexts segments\\ninto summary vectors using a Recurrent Memory Transformer, reducing computational load.\\nOther approaches, like ICAE [99] and 500xCompressor [203], fine-tuned LoRA-adapted LLMs\\nfor context encoding and prompt compression. For LLM-based recommendations, POD [190]\\ndistilled discrete prompt templates into continuous prompt vectors with an whole-word em-\\nbedding to integrate the item ID, while RDRec [379] synthesizes training data and internalizes\\nrationales into a smaller model. SelfCP [90] balances training cost, inference efficiency, and\\ngeneration quality by compressing over-limit prompts asynchronously using frozen LLMs'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 29, 'page_label': '30', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='rationales into a smaller model. SelfCP [90] balances training cost, inference efficiency, and\\ngeneration quality by compressing over-limit prompts asynchronously using frozen LLMs\\nas the compressor and generator and trainable linear layers to project hidden states into\\nLLM-acceptable memory tokens.\\n– Refining for Cross-Model : PromptMM [393] tackles overfitting and side information inac-\\ncuracies in multi-modal recommenders by using Multi-modal Knowledge Distillation with\\nprompt-tuning. It compresses models by distilling user-item relationships and multi-modal\\ncontent from complex teacher models to lightweight student models, eliminating extra pa-\\nrameters. Soft prompt-tuning bridges the semantic gap between multi-modal context and\\ncollaborative signals, enhancing robustness. Additionally, a disentangled multi-modal list-wise\\ndistillation with modality-aware re-weighting addresses multimedia data inaccuracies. RACC'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 29, 'page_label': '30', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='collaborative signals, enhancing robustness. Additionally, a disentangled multi-modal list-wise\\ndistillation with modality-aware re-weighting addresses multimedia data inaccuracies. RACC\\n[395] compresses and aggregates retrieved knowledge for image-question pairs, generating a\\ncompact Key-Value (KV) cache modulation to adapt downstream frozen MLLMs for efficient\\ninference. VTC-CLS [364] uses the prior knowledge of the association between the [CLS] token\\nand visual tokens in the visual encoder to evaluate visual token importance, enabling Visual\\nToken Compression and shortening visual context. VisToG [128] introduces a grouping mecha-\\nnism using pretrained vision encoders to group similar image segments without segmentation\\nmasks. Semantic tokens represent image segments after linear projection and before input\\ninto the vision encoder. Isolated attention identifies and eliminates redundant visual tokens,\\nreducing computational demands.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 29, 'page_label': '30', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='into the vision encoder. Isolated attention identifies and eliminates redundant visual tokens,\\nreducing computational demands.\\nHowever, as dataset size increases, so do the computational resource requirements. Additionally,\\nsoft prompts are less interpretable than hard prompts, as their continuous vectors are not directly\\nreadable or explainable by humans.\\n3.4 Multimodal Generation\\nMultimodal generation based on Multimodal Large Language Models (MLLMs) represents a sig-\\nnificant advancement, enabling the generation of content across multiple modalities such as text,\\nimages, audio, and video. These models leverage the strengths of large language models (LLMs) and\\nextend them to handle and integrate diverse data types, creating rich, coherent, and contextually\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 30, 'page_label': '31', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 31\\nMultimodal\\nGeneration\\nModality\\nAugmentation\\nInternLM-XComposer [465], InternLM-XComposer2 [ 67], InternLM-XComposer-2.5 [ 466], MuRAR [ 508],\\n𝑀2𝑅𝐴𝐺 [259]\\nMLLM\\n•Image ⊕Text →Text\\nBLIP-2 [185], ChatSpot [ 483], OpenFlamingo [ 14], ASM [ 378], Qwen-VL [16], Kosmos-2.5 [ 249], InternLM-\\nXComposer [465], JAM [ 8], Kosmos-1 [ 130], PaLM-E [ 69], ViperGPT [ 343], PandaGPT [ 335], PaLI-X [ 40], LLaVA-\\nMed [182], LLaVAR [478], mPLUG-DocOwl [ 426], P-Former [ 145], MiniGPT-v2 [35], LLaVA [219], MiniGPT-4\\n[504], mPLUG-Owl [ 427], Otter [ 181], MultiModal-GPT [ 103], CogVLM [ 377], mPLUG-Owl2 [ 428], Monkey\\n[205], DocPedia [ 81], ShareGPT4V [ 37], mPLUG-PaperOwl [ 123], RLHF-V [438], Silkie [ 189], Lyrics [ 241], VILA\\n[209], CogAgent [ 121], Volcano [176], DRESS [ 44], LION [ 31], Osprey [ 443], LLaVA-MoLE [39], VLGuard [ 514],\\nMobileVLM V2 [55], ViGoR [ 417], V* [ 399], MobileVLM [54], TinyGPT-V [444], DocLLM [ 367], LLaVA-Phi [507],'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 30, 'page_label': '31', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='MobileVLM V2 [55], ViGoR [ 417], V* [ 399], MobileVLM [54], TinyGPT-V [444], DocLLM [ 367], LLaVA-Phi [507],\\nKAM-CoT [277], InternLM-XComposer2 [ 67], InternLM-XComposer-2.5 [ 466], MoE-LLaVA [207], VisLingInstruct\\n[505]\\n•Image ⊕Text →Image ⊕Text\\nVisual ChatGPT [ 397], DetGPT [ 295], FROMAGe [165], Shikra [ 36], GPT4RoI [ 472], SEED [ 100], LISA [ 168],\\nGILL [164], Kosmos-2 [ 293], DreamLLM [ 65], MiniGPT-5 [490], Kosmos-G [ 287], VisCPM [ 124], CM3Leon [ 434],\\nLaVIT [155], GLaMM [ 307], RPG [ 418], Vary-toy [392], CogCoM [ 298], SPHINX-X [ 216], LLaVA-Plus [223],\\nPixelLM [310], VL-GPT [506], CLOVA [96], Emu-2 [337], MM-Interleaved [355], DiffusionGPT [301]\\n•Video ⊕Text →Text\\nVideo-ChatGPT [260], VideoChat [187], Dolphins [257]\\n•Video ⊕Text →Video ⊕Text\\nVideo-LaVIT [154]\\n•Unified →Text\\nFlamingo [10], X-LLM [ 30], LanguageBind [ 503], InstructBLIP [ 219], MM-REACT [423], X-InstructBLIP [ 289],'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 30, 'page_label': '31', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='•Video ⊕Text →Video ⊕Text\\nVideo-LaVIT [154]\\n•Unified →Text\\nFlamingo [10], X-LLM [ 30], LanguageBind [ 503], InstructBLIP [ 219], MM-REACT [423], X-InstructBLIP [ 289],\\nEmbodiedGPT [280], Video-LLaMA [460], Lynx [450], LLaMA-VID [198], InternVL [47], AnyMAL [278]\\n•Unified →Image ⊕Text\\nBuboGPT [487], Emu [338], GroundingGPT [204]\\n•Unified →Unified\\nTEAL [424], GPT-4 [7], Gemini [ 353], HuggingGPT [ 325], CoDi-2 [ 351], AudioGPT [129], ModaVerse [382],\\nMLLM-Tool [366], ControlLLM [236], NExT-GPT [400]\\nFig. 8. Taxonomy of recent advancements in multimodal generation research.\\nrelevant outputs. We classify MLLMs from generative perspectives of inputs and outputs, and\\nsummarize the related researches in Figure 8.\\n3.4.1 MODALITY INPUT. With the rapid advancement of large language models in the domain\\nof textual knowledge comprehension and question-answering, researchers try to explore how\\nto enable these models to understand and process inputs from a broader range of modalities,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 30, 'page_label': '31', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='of textual knowledge comprehension and question-answering, researchers try to explore how\\nto enable these models to understand and process inputs from a broader range of modalities,\\nthereby facilitating more extensive multimodal question-answering tasks. Initial efforts focused on\\nincorporating image modality into the input of large models. For instance, Blip-2 [185] proposes\\na generic and efficient pre-training strategy that bootstraps vision-language pre-training from\\noff-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges\\nthe modality gap with a lightweight Querying Transformer, which is pre-trained in two stages.\\nThe first stage bootstraps vision-language representation learning from a frozen image encoder.\\nThe second stage bootstraps vision-to-language generative learning from a frozen language model.\\nInternlm-xcomposer2 [66] proposes a vision-language model excelling in free-form text-image com-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 30, 'page_label': '31', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='The second stage bootstraps vision-to-language generative learning from a frozen language model.\\nInternlm-xcomposer2 [66] proposes a vision-language model excelling in free-form text-image com-\\nposition and comprehension. This model goes beyond conventional vision-language understanding,\\nadeptly crafting interleaved text-image content from diverse inputs like outlines, detailed textual\\nspecifications, and reference images, enabling highly customizable content creation. DiffusionGPT\\n[301] leverages Large Language Models (LLM) to offer a unified generation system capable of\\nseamlessly accommodating various types of prompts and integrating domain-expert models. Diffu-\\nsionGPT constructs domain-specific Trees for various generative models based on prior knowledge.\\nWhen provided with an input, the LLM parses the prompt and employs the Trees-of-Thought to\\nguide the selection of an appropriate model.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 30, 'page_label': '31', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='When provided with an input, the LLM parses the prompt and employs the Trees-of-Thought to\\nguide the selection of an appropriate model.\\nAs the variety of modal data continues to expand, more complex modalities, such as video, have\\nbeen integrated into the inputs of large models. For instance, Video-ChatGPT [ 260] proposes a\\nmultimodal model that merges a video-adapted visual encoder with an LLM. The resulting model is\\ncapable of understanding and generating detailed conversations about videos. Video-LaVIT [154]\\naddress spatiotemporal dynamics limitations in video-language pretraining with an efficient video\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 31, 'page_label': '32', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='32 Trovato et al.\\ndecomposition that represents each video as keyframes and temporal motions. These are then\\nadapted to an LLM using well-designed tokenizers that discretize visual and temporal information as\\na few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference,\\nthe generated tokens from the LLM are carefully recovered to the original continuous pixel space\\nto create various video content. The proposed framework is both capable of comprehending and\\ngenerating image and video content.\\nRecently, the input for multimodal large models has evolved from specialized modal data to a\\nunified input that can handle arbitrary modal data. For instance, InstructBLIP [289] conduct a vision-\\nlanguage instruction tuning based on the pretrained BLIP-2 models. Additionally, we introduce an\\ninstruction-aware Query Transformer, which extracts informative features tailored to the given'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 31, 'page_label': '32', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='language instruction tuning based on the pretrained BLIP-2 models. Additionally, we introduce an\\ninstruction-aware Query Transformer, which extracts informative features tailored to the given\\ninstruction. InternVL [47] design a large-scale vision-language foundation model (InternVL) which\\nscales up the vision foundation model to 6 billion parameters and progressively aligns it with\\nthe LLM using web-scale image-text data from various sources. GPT-4 [7] proposes a large-scale,\\nmultimodal model which can accept image and text inputs and produce text outputs. While less\\ncapable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on\\nvarious professional and academic benchmarks. HuggingGPT [325] proposes an LLM-powered agent\\nthat leverages LLMs (eg, ChatGPT) to connect various AI models in machine learning communities\\n(eg, Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 31, 'page_label': '32', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='that leverages LLMs (eg, ChatGPT) to connect various AI models in machine learning communities\\n(eg, Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when\\nreceiving a user request, select models according to their function descriptions available in Hugging\\nFace, execute each subtask with the selected AI model, and summarize the response according to\\nthe execution results. By leveraging the strong language capability of ChatGPT and abundant AI\\nmodels in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning\\ndifferent modalities and domains. NExT-GPT [400] present an end-to-end general-purpose any-to-\\nany MM-LLM system. NExT-GPT connect an LLM with multimodal adaptors and different diffusion\\ndecoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations\\nof text, image, video, and audio. By leveraging the existing well-trained high-performing encoders'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 31, 'page_label': '32', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations\\nof text, image, video, and audio. By leveraging the existing well-trained high-performing encoders\\nand decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection\\nlayers, which not only benefits low-cost training but also facilitates convenient expansion to more\\npotential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and\\nmanually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with\\ncomplex cross-modal semantic understanding and content generation.\\n3.4.2 MODALITY OUTPUT. With the explosive growth in the capabilities of MLLMs, the ability\\nto answer questions based on multimodal inputs and generate multimodal outputs has also seen a\\nqualitative improvement. There is also increasing attention from researchers on VQA scenarios that'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 31, 'page_label': '32', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='to answer questions based on multimodal inputs and generate multimodal outputs has also seen a\\nqualitative improvement. There is also increasing attention from researchers on VQA scenarios that\\nshift from generating text results to generating multimodal results that include text.In this section,\\nwe are discussing multimodal outputs that are not scenarios like text-to-image or text-to-video,\\nwhich only generate a single modality, but rather scenarios where the answers includes text and at\\nleast one other modality of data, such as text-image output, or image-video output.In the basic VQA\\ntask, MIMOQA[328] was the first to propose the concept of multimodal output, which achieved\\nthe capability of multimodal output by transforming questions into an image-text matching task.\\nIt constructed a dual-tower model called MExBERT. The text stream, based on BERT, takes in\\nthe query and related documents to output the final text answer. The visual stream, based on'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 31, 'page_label': '32', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='It constructed a dual-tower model called MExBERT. The text stream, based on BERT, takes in\\nthe query and related documents to output the final text answer. The visual stream, based on\\nVGG-19, receives images related to the query and documents, outputting a relevance score between\\nthe image and text. The final insertion of the image is determined by this relevance score. Its\\ngroundbreaking introduction to multimodal output research has, however, certain limitations: 1) It\\nis necessary to screen out images related to the question. The model only needs to select and output\\nimages from the small number of screened ones. The task is relatively simple. 2) Multimodality is\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 32, 'page_label': '33', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 33\\nstill limited to the image modality. 3) To simplify the issue, it is still limited to scenarios where the\\ninput images must include at least one relevant image. Based on the aforementioned limitations,\\nthe latest research has made corresponding improvements[67, 259, 401, 465, 466, 508].\\nA common workflow paradigm for implementing multimodal output is to first conduct position\\nidentification after generating a text answer to determine where to insert multimodal data. Subse-\\nquently, based on the surrounding context of the corresponding positions, candidate multimodal\\ndata is retrieved. Finally, a relevance matching model is utilized to determine the final data to\\nbe inserted. InternLM-XComposer [ 465] achieves multimodal output of text and images. After\\ngenerating each paragraph of text, it calls a model to determine whether to insert an image. If'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 32, 'page_label': '33', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='be inserted. InternLM-XComposer [ 465] achieves multimodal output of text and images. After\\ngenerating each paragraph of text, it calls a model to determine whether to insert an image. If\\nit is determined that an image needs to be inserted, it will generate a caption of the image to be\\ninserted and search the web for candidate images, eventually allowing the model to select the most\\nrelevant image from candidate set for insertion. InternLM-XComposer2 and 2.5 [ 67, 466] allow\\nusers to directly input a set of candidate images on the basis of the above. MuRAR [508] has also\\nimplemented multi - modal output in RAG scenarios based on this paradigm, but it has innovated\\nthe methods of position identification and candidate set recall in RAG scenarios. It uses source\\nattribution to confirm the correspondence between the generated snippet and the retrieved snippet\\nfrom the large model input, thereby determining the insertion point, and the candidate set directly'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 32, 'page_label': '33', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='attribution to confirm the correspondence between the generated snippet and the retrieved snippet\\nfrom the large model input, thereby determining the insertion point, and the candidate set directly\\nuses the multimodal data associated with the retrieved snippet, simplifying the recall operation. In\\naddition, it has expanded the multimodal data from images to include tables and videos. 𝑀2𝑅𝐴𝐺\\n[259] employs an alternative paradigm to achieve multimodal output in the RAG scenario. It uses\\nthe user’s query to simultaneously recall associated text elements and images. Then, based on the\\nassociations of the images and text elements in the original document, they are refined. Subse-\\nquently, MLLMs are employed to vectorize the images or convert them into descriptions, which are\\ninput into the generative model in the form of placeholders. The output generates answer text and\\na simple description placeholder for the associated image. Finally, through a chain-of-thought(COT)'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 32, 'page_label': '33', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='input into the generative model in the form of placeholders. The output generates answer text and\\na simple description placeholder for the associated image. Finally, through a chain-of-thought(COT)\\nprocess, the placeholders are converted into actual images. NExT-GPT [401] employs an entirely\\ndifferent and novel paradigm. It directly trains a unified multimodal large model, unifying the\\nreasoning and generation process, and directly generates multimodal data including text, images,\\nvideos, etc., through the model [401].\\n4 Dataset for MRAG\\nTo evaluate the general capabilities of MRAG systems in real-world multimodal understanding and\\nknowledge-based question-answering tasks, we curated a collection of existing datasets designed to\\ncomprehensively evaluate the MRAG pipeline. These datasets are categorized into two classes: (1)\\nRetrieval & Generation-Joint Components, which evaluate the synergy of retrieval and generation'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 32, 'page_label': '33', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='comprehensively evaluate the MRAG pipeline. These datasets are categorized into two classes: (1)\\nRetrieval & Generation-Joint Components, which evaluate the synergy of retrieval and generation\\nby requiring systems to retrieve external knowledge and generate accurate responses; and (2)\\nGeneration, focusing solely on the model’s ability to produce contextually accurate outputs without\\nexternal retrieval. This categorization enables a detailed evaluation of MRAG systems’ strengths\\nand limitations in diverse scenarios.\\n4.1 Dataset for Retrieval & Generation\\nDatasets for Retrieval & Generation in MRAG are designed to evaluate end-to-end systems capable\\nof retrieving relevant knowledge from multimodal sources (e.g., text, images, videos) and generating\\naccurate responses. These datasets evaluate the synergistic integration of retrieval and generation,\\nfocusing on the system’s ability to dynamically utilize external knowledge to improve response'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 32, 'page_label': '33', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='accurate responses. These datasets evaluate the synergistic integration of retrieval and generation,\\nfocusing on the system’s ability to dynamically utilize external knowledge to improve response\\nquality and relevance. In this section, we introduce key benchmarks designed for diverse evaluation\\nof Retrieval & Generation tasks. Figure 9 provides an overview of existing benchmarks, while Table\\n1 summarizes the statistics of selected representative datasets.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 33, 'page_label': '34', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='34 Trovato et al.\\nDataset for\\nRetrieval & Generation\\nVisual Advertisement Ads [138]\\nVideo Understanding\\n& Reasoning KnowIT VQA [97], SOK-Bench [365]\\nVisual Commonsense\\n& Reasoning VCR [448], VisualCOMET [291]\\nComprehensive\\nKB-VQA [375], FVQA [374], KVQA [322], OK-VQA [263], S3VQA [140], ManyModalQA\\n[114], MultiModalQA [ 345], MIMOQA [ 329], A-OKVQA [321], WebQA [28], ViQuAE [ 177],\\nInfoSeek [42], Encyclopedic-VQA [272], MMSearch [ 147], MRAG-bench [ 126], MRAMG-\\nBench [435]\\nFig. 9. Categories of MRAG dataset for retrieval & generation.\\nTable 1. Summary of dataset for retrieval & generation.\\nDataset Time Statistics\\nComprehensive\\nKB-VQA [375] 2015 2,402 questions, 700 images, 1 knowledge bases.\\nFVQA [374] 2017 5,826 questions, 2,190 images, 3 knowledge bases.\\nKVQA [322] 2019 183,007 question-answer pairs about 18,880 unique entities contained within 24,602 images.\\nOK-VQA [263] 2019 14,055 questions, 10 scenarios.\\nS3VQA [140] 2021 6,765 question-image pairs.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 33, 'page_label': '34', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='OK-VQA [263] 2019 14,055 questions, 10 scenarios.\\nS3VQA [140] 2021 6,765 question-image pairs.\\nManyModalQA [114] 2020 10,190 questions with 2,873 image, 3,789 text, and 3,528 table.\\nMultiModalQA [345] 2021 29,918 questions that requires knowledge from text, tables, and images (35.7% require cross-modality\\nreasoning).\\nMIMOQA [329] 2021 56,693 QA pairs, with 401,182 images.\\nA-OKVQA [321] 2022 24,903 multiple-choice questions.\\nWebQA [28] 2022 24,929 image-based and 24,343 text-based questions.\\nViQuAE [177] 2022 3.7K questions paired with images. A Knowledge base composed of 1.5M Wikipedia articles paired with\\nimages.\\nInfoSeek [42] 2023 8.9K human-written and 1.3M semi-automated questions, 9 image classification and retrieval datasets.\\nEncyclopedic-VQA\\n[272]\\n2023 1M Image-Question-Answer triplets derived from 221k textual QA pairs from 16.7k different categories.\\nEach QA pair is combined with (up to) 5 images. 514k unique images. 15k textual single-hop questions,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 33, 'page_label': '34', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Each QA pair is combined with (up to) 5 images. 514k unique images. 15k textual single-hop questions,\\n25k multi-answer questions, and 22k two-hop questions.\\nMMSearch [147] 2024 2,901 unique images, 300 manually collected queries spanning 14 subfields.\\nMRAG-bench [126] 2024 1,353 multiple-choice questions, 16,130 images, 9 scenarios.\\nMRAMG-Bench\\n[435]\\n2025 4,800 QA pairs across three distinct domains, containing 4,346 documents and 14,190 images, with tasks\\ncategorized into three difficulty levels.\\nVisual Commonsense Reasoning\\nVCR [448] 2019 290k multiple choice QA problems derived from 110k movie scenes.\\nVisualCOMET [291] 2020 1,465,704 commonsense inferences over 59,356 images, and 139,377 distinct events.\\nVideo Understanding & Commonsense Reasoning\\nKnowIT VQA [97] 2020 24,282 human-generated QA pairs about a popular sitcom.\\nSOK-Bench [365] 2024 44K QA pairs covers over 12 types of questions, sourcing from about 10K situations. Each question is'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 33, 'page_label': '34', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='KnowIT VQA [97] 2020 24,282 human-generated QA pairs about a popular sitcom.\\nSOK-Bench [365] 2024 44K QA pairs covers over 12 types of questions, sourcing from about 10K situations. Each question is\\naccompanied by two types of answers: a direct answer and a set of four multiple-choice options.\\nVisual Advertisement\\nAds [138] 2017 202,090 questions from 64,832 image ads and 3,477 video ads.\\nEarly knowledge-based datasets include KB-VQA [375] and FVQA [374], which rely on closed\\nknowledge. FVQA, for instance, uses a fixed knowledge graph, making questions straightforward\\nonce the knowledge is known, with minimal reasoning required. KVQA [322] focuses on images in\\nWikipedia articles, primarily testing named entity recognition and Wikipedia knowledge retrieval\\nrather than commonsense reasoning. OK-VQA [263] and A-OKVQA [321] evaluate multimodal\\nreasoning using external knowledge, with A-OKVQA introducing \"rationale\" annotations to better'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 33, 'page_label': '34', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='rather than commonsense reasoning. OK-VQA [263] and A-OKVQA [321] evaluate multimodal\\nreasoning using external knowledge, with A-OKVQA introducing \"rationale\" annotations to better\\nevaluate knowledge acquisition and reasoning. S3VQA [140] extends OK-VQA by requiring object\\ndetection and web queries, but like OK-VQA, it often reduces to single retrieval tasks rather than\\ncomplex reasoning. MultiModalQA [345] pioneers complex questions requiring reasoning across\\nsnippets, tables, and images, focusing on cross-modal knowledge extraction. However, its template-\\nbased questions simplify the task to filling in blanks with modality-specific answering mechanisms.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 34, 'page_label': '35', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 35\\nManyModalQA [114] also uses snippets, images, and tables but emphasizes answer modality\\nchoice over knowledge aggregation. MIMOQA [329] introduces “Multimodal Input Multimodal\\nOutput”, requiring both text and image selections to enhance understanding. WebQA [ 28] is a\\nmanually crafted, multi-hop multimodal QA dataset that retrieves visual content but provides\\nonly textual answers, relying solely on MLLMs for reasoning, making it unsuitable for models\\ndependent on linguistic context. ViQuAE [ 177] focuses on answering questions about named\\nentities grounded in a visual context using a Knowledge Base. InfoSeek [42] and Encyclopedic-VQA\\n[272] target knowledge-based questions beyond common sense knowledge, with Encyclopedic-\\nVQA using model-generated annotations. MMSearch [147] evaluates MLLMs as multimodal search\\nengines, focusing on image-to-image retrieval. Compared with previous works, MRAG-bench'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 34, 'page_label': '35', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='VQA using model-generated annotations. MMSearch [147] evaluates MLLMs as multimodal search\\nengines, focusing on image-to-image retrieval. Compared with previous works, MRAG-bench\\n[126] evaluates MLLMs in utilizing vision-centric retrieval-augmented knowledge, identifying\\nscenarios where visual knowledge outperforms textual knowledge. MRAMG-Bench [435] evaluates\\nanswers combining text and images, leveraging multimodal data within a corpus. Additionally,\\nVCR [448] and VisualCOMET [291], derived from movie scenes, evaluate Visual Commonsense\\nReasoning. KnowIT VQA [97] and SOK-Bench [365] focus on video understanding and reasoning\\ntask, combining visual, textual, and temporal reasoning with knowledge-based questions. Ads [138]\\nproposes an automatic advertisement understanding task, featuring rich annotations on topics,\\nsentiments, and persuasive reasoning.\\n4.2 Dataset for Generation\\nThe Generation category evaluates a model’s intrinsic capacity to generate contextually accurate'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 34, 'page_label': '35', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='sentiments, and persuasive reasoning.\\n4.2 Dataset for Generation\\nThe Generation category evaluates a model’s intrinsic capacity to generate contextually accurate\\noutputs based solely on its pre-trained knowledge and internal reasoning, without external retrieval.\\nThis evaluation isolates the generation component, providing insights into the model’s foundational\\nlanguage understanding capabilities. It enables a detailed analysis of MRAG systems’ strengths\\nand limitations across diverse scenarios. In this section, we provide an overview of representative\\nbenchmarks developed for various evaluation of Generation tasks. The existing benchmarks are\\nsystematically organized in Figure 10, and the statistics of selected representative benchmarks are\\nsummarized in Table 2.\\n4.2.1 Comprehensive. To rigorously evaluate the capabilities of MLLMs, a diverse range of evalua-\\ntion benchmarks has been developed. These benchmarks are designed to test various dimensions'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 34, 'page_label': '35', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='4.2.1 Comprehensive. To rigorously evaluate the capabilities of MLLMs, a diverse range of evalua-\\ntion benchmarks has been developed. These benchmarks are designed to test various dimensions\\nof model performance. By utilizing these benchmarks, researchers can systematically quantify\\nthe strengths and limitations of MLLMs, ensuring their alignment with real-world applications\\nand user expectations. This evaluation framework not only supports the iterative improvement of\\nMLLMs but also provides a standardized basis for comparing models in terms of perceptual and\\nreasoning abilities.\\nVQA v2 [ 104], an early benchmark with 453K annotated QA pairs, focuses on open-ended\\nquestions with concise answers. VizWiz [ 112], introduced around the same time, includes 8K\\nQA pairs from visually impaired individuals’ daily lives, addressing real-world needs of disabled\\nusers. NLVR2 [336] explores multi-image vision capabilities by evaluating captions against image'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 34, 'page_label': '35', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='QA pairs from visually impaired individuals’ daily lives, addressing real-world needs of disabled\\nusers. NLVR2 [336] explores multi-image vision capabilities by evaluating captions against image\\npairs. However, these benchmarks often fail to assess modern MLLMs’ emergent capabilities, such\\nas advanced reasoning. Recent efforts like LVLM-eHub [ 411], MDVP [ 212], and LAMM [ 430]\\ncompile extensive datasets for comprehensive evaluation, revealing that while MLLMs excel in\\ncommonsense tasks, they lag in image classification, OCR, VQA, large-scale counting, fine-grained\\nattribute differentiation, and precise object localization. Fine-tuning can mitigate some of these\\nlimitations.\\nResearchers are developing specialized benchmarks to address the limitations of traditional\\nevaluations for MLLMs. Notable examples include MME [ 29], which covers 14 perception and\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 35, 'page_label': '36', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='36 Trovato et al.\\nDataset for\\nGeneration\\nMultidisciplinary ScienceQA [243], MMMU [445], CMMU [117], CMMMU [457], MMMU-Pro [446]\\nConversational QA SparklesDialogue [134], SciGraphQA [192], ConvBench [225], MMDU [235]\\nIndustry MME-Industry [429]\\nVideo Understanding\\nTGIF-QA [141], ActivityNet-QA [442], EgoSchema [ 262], Video-MME [ 85], MVBench [ 188],\\nMMBench-Video [77], MLVU [ 496], LVBench [ 376], Event-Bench [ 71], VNBench [ 488], TempCompass\\n[232], MovieChat [332]\\nMathematics MathVista [242], We-Math [300], Math-Vision [372], Olympiadbench [115], MathVerse [470]\\nStructural Document\\nFigureQA [160], DocVQA [268], VisualMRC [ 348], ChartQA [ 266], InfographicVQA [267], ChartBench\\n[414], SciGraphQA [ 192], MMC-Benchmark [ 217], MP-DocVQA [358], ChartX [ 404], DocGenome\\n[403], CharXiv [ 387], MMLongBench-Doc [ 258], ComTQA [486], Web2Code [447], VisualWebBench\\n[221], SciFIBench [313]\\nOptical Character\\nRecognition'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 35, 'page_label': '36', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[403], CharXiv [ 387], MMLongBench-Doc [ 258], ComTQA [486], Web2Code [447], VisualWebBench\\n[221], SciFIBench [313]\\nOptical Character\\nRecognition\\nTextVQA [327], OCR-VQA [275], WebSRC [41], OCRBench [ 233], VCR [473], SEED-Bench-2-Plus\\n[178]\\nComprehensive\\nVQA v2 [104], NLVR2 [ 336], VizWiz [ 112], MME [ 29], Visit-Bench [ 21], Touchstone [17], MM-Vet\\n[439], InfiMM-Eval [ 113], Q-Bench [ 398], Seed-Bench [ 180], SEED-Bench-2 [ 179], SEED-Bench-2-Plus\\n[178], LVLM-eHub [ 411], LAMM [ 430], MMT-Bench [431], RealWorldQA [4], WV-Bench [ 245], MME-\\nRealWorld [480], MMStar [ 38], CV-Bench [359], MDVP [ 212], FOCI [ 101], MMVP [ 360], V*-Bench\\n[399], MME-RealWorld [480], Visual COT [ 324], Mementos [ 381], MIRB [ 482], ReMI [ 162], MuirBench\\n[368], VEGA [493], MMBench [230], BLINK [86]\\nFig. 10. Categories of MRAG dataset for generation.\\ncognition tasks; MMBench [ 230], featuring 20 ability dimensions, including object localization'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 35, 'page_label': '36', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[368], VEGA [493], MMBench [230], BLINK [86]\\nFig. 10. Categories of MRAG dataset for generation.\\ncognition tasks; MMBench [ 230], featuring 20 ability dimensions, including object localization\\nand social reasoning; and SEED-Bench [180], which focuses on multiple-choice questions. SEED-\\nBench-2 [179] expanded the scope to 24K QA pairs, including the evaluation of both text and image\\ngeneration. MMT-Bench [431] further scaled up to 31K QA pairs across diverse scenarios. Common\\nfindings reveal that model performance improves with scale, but challenges persist in fine-grained\\nperception tasks (e.g., spatial localization), chart and visual mathematics comprehension, and\\ninterleaved image-text understanding. Open-source MLLMs have shown rapid progress, often\\nmatching or surpassing closed-source models.\\nReal-world usage scenarios are critical for evaluating model performance in practical applications.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 35, 'page_label': '36', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='matching or surpassing closed-source models.\\nReal-world usage scenarios are critical for evaluating model performance in practical applications.\\nBenchmarks like RealWorldQA [4] evaluates spatial understanding capabilities sourced from real-\\nlife scenarios, while BLINK [86] highlights tasks such as visual correspondence and multi-view\\nreasoning that challenge current MLLMs despite being intuitive for humans. WV-Bench [245] and\\nVisit-Bench [21] emphasize human preferences and instruction-following capabilities, whereas\\nV*-Bench [399] evaluates high-resolution image processing and correct visual details through\\nattribute recognition and spatial reasoning tasks. MME-RealWorld [ 480] enhances quality and\\ndifficulty with extensive annotated QA pairs and high-resolution images. These benchmarks reveal\\nthat fine-grained perception tasks remain challenging for models, while artistic style recognition'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 35, 'page_label': '36', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='difficulty with extensive annotated QA pairs and high-resolution images. These benchmarks reveal\\nthat fine-grained perception tasks remain challenging for models, while artistic style recognition\\nand relative depth perception are relatively stronger. Although closed-source models like GPT-4o\\noutperform others, human performance still surpasses general models significantly.\\nMany studies simplify evaluation into binary or multi-choice problems for easier quantification,\\nbut this approach overlooks the importance of the reasoning process, which is critical for under-\\nstanding model capabilities. To address this, some works use open-ended generation and LLM-based\\nevaluators, though these face challenges with inaccurate LLM scoring. For instance, MM-Vet [439]\\nemploys diverse question formats to assess integrated vision-language capabilities, while Touch-\\nstone [17] emphasizes real-world dialogue evaluation, arguing that multiple-choice questions are'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 35, 'page_label': '36', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='employs diverse question formats to assess integrated vision-language capabilities, while Touch-\\nstone [17] emphasizes real-world dialogue evaluation, arguing that multiple-choice questions are\\ninsufficient for evaluating multimodal dialogue capabilities. InfiMM-Eval [113] evaluates models on\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 36, 'page_label': '37', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 37\\ndeductive, abductive, and analogical reasoning across tasks, including intermediate reasoning steps,\\naligning with practical scenarios like mathematical problem-solving. These benchmarks highlight\\nthe strengths and limitations of MLLMs in complex tasks. Closed-source models excel in reasoning\\nbut struggle with complex localization, structural relationships, charts, and visual mathematics.\\nHigh-resolution data improves recognition of small objects, dense text, and fine-grained details.\\nWhile Chain-of-Thought (CoT) strategies significantly boost reasoning in closed-source models,\\ntheir impact on open-source models remains limited.\\nThe development of multimodal benchmarks emphasizes continuous refinement to accurately as-\\nsess model capabilities. MMStar [38] addresses data leakage by curating 1.5K visually-dependent QA\\npairs, while CV-Bench [359] tackles the scarcity of vision-centric benchmarks with 2.6K manually-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 36, 'page_label': '37', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='sess model capabilities. MMStar [38] addresses data leakage by curating 1.5K visually-dependent QA\\npairs, while CV-Bench [359] tackles the scarcity of vision-centric benchmarks with 2.6K manually-\\ninspected samples for 2D/3D understanding. FOCI [101] evaluates MLLMs using domain-specific\\nsubsets and supplementary classification datasets, revealing challenges in fine-grained perception.\\nMMVP [360] identifies 9 distinct patterns in CLIP-based models, showing MLLMs’ struggles with\\nvisual details, with only Gemini and GPT-4V performing above random guessing. Q-Bench [398]\\nevaluates low-level attribute perception, highlighting GPT-4V’s near-human performance. Visual-\\nCOT [324] introduces visual chain-of-thought prompts to enhance MLLMs’ focus on specific image\\nregions. To further upgrading vision capabilities on multiple image understanding, Mementos [381]\\nevaluates sequential image understanding, while MIRB [482] focuses on multi-image reasoning'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 36, 'page_label': '37', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='regions. To further upgrading vision capabilities on multiple image understanding, Mementos [381]\\nevaluates sequential image understanding, while MIRB [482] focuses on multi-image reasoning\\nacross perception, visual knowledge, and multi-hop reasoning tasks. ReMI [162] designs 13 tasks\\nwith diverse image relationships and input formats, and MuirBench [368] includes 12 multi-image\\nunderstanding tasks with unanswerable variants for robust assessment. VEGA [ 493] is specifi-\\ncally designed to evaluate interleaved image-text comprehension. The task requires models to\\nidentify relevant images and text while filtering out irrelevant information to arrive at the correct\\nanswer. Evaluation results reveal that even advanced proprietary MLLMs, such as GPT-4V and\\nGemini 1.5 Pro, achieve only modest performance, highlighting significant room for improvement\\nin interleaved information processing capabilities.\\n4.2.2 Optical Character Recognition (OCR). Multimodal benchmarks are increasingly focusing'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 36, 'page_label': '37', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='in interleaved information processing capabilities.\\n4.2.2 Optical Character Recognition (OCR). Multimodal benchmarks are increasingly focusing\\non the evaluation of Optical Character Recognition (OCR) tasks, driving progress in document\\nunderstanding. Early benchmarks like TextVQA [327] and OCR-VQA [275] evaluated standard\\ntext recognition, while WebSRC [ 41] introduces advanced structural reasoning tasks like web\\npage layout interpretation. SEED-Bench-2-Plus [178] and OCRBench [233] expanded evaluation to\\ndiverse data types, including charts, maps, and web pages, showing models achieving near state-of-\\nthe-art performance in recognizing various OCR text. VCR [473] addresses OCR task with partially\\nobscured text embedded in images, requiring content reconstruction. Despite advancements, many\\nMLLMs struggle with fine-grained OCR tasks. While models like GPT-4V perform well, they lag\\nbehind specialized OCR models. Performance varies significantly by data type, with knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 36, 'page_label': '37', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='MLLMs struggle with fine-grained OCR tasks. While models like GPT-4V perform well, they lag\\nbehind specialized OCR models. Performance varies significantly by data type, with knowledge\\ngraphs and maps posing greater challenges than simpler formats like charts, suggesting potential\\nimprovements through data-specific optimization or dedicated OCR integration.\\n4.2.3 Structural Document. Structural documents, including charts, HTML web content, and vari-\\nous document formats, play a critical role in practical applications due to their ability to efficiently\\nconvey complex information. These data types are characterized by their highly structured nature\\nand information density, distinguishing them from natural images. Unlike images, which rely on\\nvisual patterns and textures, structural documents require models to comprehend intricate layouts,\\nspatial relationships, and semantic connections between embedded elements such as text, tables,\\nand graphical components.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 36, 'page_label': '37', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='spatial relationships, and semantic connections between embedded elements such as text, tables,\\nand graphical components.\\nTo advance models capable of understanding and reasoning with such data, several benchmarks\\nhave been proposed for different types of structural documents. Early dataset FigureQA [ 160]\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 37, 'page_label': '38', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='38 Trovato et al.\\nintroduces a visual reasoning corpus with synthetic images and scientific-style figures, focusing on\\nrelationships between plot elements. ChartQA [266] emphasizes VQA with charts, ranging from\\ntasks that require both data extraction and math reasoning. ChartX [404] collects a comprehensive\\ndataset with 22 topics, 18 chart types, and 7 tasks, incorporating multiple modalities. VisualMRC\\n[348] targets visual machine reading comprehension, emphasizing natural language understanding\\nand generation. ChartBench [ 414] evaluates chart comprehension and data reliability through\\ncomplex reasoning. MMC-Benchmark [217] provides a human-annotated benchmark to assess\\nMLLMs on visual chart understanding tasks like chart information extraction, reasoning, and\\nclassification. Web2Code [447] introduces a webpage-to-code dataset for instruction tuning and\\nan evaluation framework to assess MLLMs’ webpage understanding and HTML code translation'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 37, 'page_label': '38', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='classification. Web2Code [447] introduces a webpage-to-code dataset for instruction tuning and\\nan evaluation framework to assess MLLMs’ webpage understanding and HTML code translation\\ncapabilities. VisualWebBench [221] evaluates MLLMs on various web tasks at website, element,\\nand action levels. Many charts lack data point annotations, necessitating MLLMs to infer values\\nusing chart elements. ComTQA [ 486] introduces a table VQA benchmark for perception and\\ncomprehension tasks, while DocVQA [268] focuses on document image QA with an emphasis on\\ninformation extraction tasks. InfographicVQA [267] targets understanding infographics images,\\nwhich are designed to present information concisely. Infographics exhibit diverse layouts and\\nstructures, requiring basic reasoning and arithmetic skills. As MLLMs advance, benchmarks now\\nfocus on complex chart and document understanding. For instance, DocGenome [403] analyzes'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 37, 'page_label': '38', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='structures, requiring basic reasoning and arithmetic skills. As MLLMs advance, benchmarks now\\nfocus on complex chart and document understanding. For instance, DocGenome [403] analyzes\\nscientific papers, covering tasks like information extraction, layout detection, VQA, and code\\ngeneration. CharXiv [387] targets challenging charts from scientific papers, while MP-DocVQA\\n[358] extends DocVQA to multi-page scenario, where questions are constructed based on multi-\\npage documents instead of single page. MMLongBench-Doc [ 258] focuses on long document\\nunderstanding, averaging 47.5 pages. SciGraphQA [ 192] is a synthetic dataset with 295K QA\\ndialogues about academic graphs, generated using Palm-2 from CS/ML ArXiv papers. SciFIBench\\n[313] benchmarks scientific figure interpretation, using adversarial filtering for negative examples\\nand human verification for quality assurance.\\nDespite advancements, a performance gap persists between proprietary and open-source models'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 37, 'page_label': '38', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='and human verification for quality assurance.\\nDespite advancements, a performance gap persists between proprietary and open-source models\\non conventional benchmarks. Current MLLMs continue to face challenges in reasoning tasks and\\nlong-context document comprehension, particularly in interpreting extended multimodal contexts,\\nwhich remains a critical limitation.\\n4.2.4 Mathematics. Visual math problem-solving is key to evaluating MLLMs, leading to the\\ndevelopment of specialized benchmarks. MathVista [242] pioneered this effort by aggregating 28\\nexisting datasets and introducing 3 new ones, featuring diverse tasks like logical, algebraic, and\\nscientific reasoning with various visual inputs. Subsequent benchmarks, such as Math-Vision [372]\\nand OlympiadBench [115], introduced more complex tasks and fine-grained evaluation methods.\\nWe-Math [300] decomposes problems into sub-problems to assess fundamental understanding,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 37, 'page_label': '38', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='and OlympiadBench [115], introduced more complex tasks and fine-grained evaluation methods.\\nWe-Math [300] decomposes problems into sub-problems to assess fundamental understanding,\\nwhile MathVerse [470] further evaluates MLLMs’ comprehension of math diagrams by transforming\\nproblems into six versions with varying proportions of visual and textual content.\\nDespite promising results from MLLMs, significant challenges remain. existing MLLMs often\\nstruggle with interpreting complex diagrams, rely heavily on textual cues, and address composite\\nproblems through memorization rather than underlying reasoning. These limitations highlight the\\nneed for further development in MLLM capabilities.\\n4.2.5 Video Understanding. Traditional video-QA benchmarks like TGIF-QA [141] and ActivityNet-\\nQA [442] are domain-specific, focusing on tasks related to human activities. With advancements in\\nMLLMs, new benchmarks have emerged to address more complex video understanding challenges.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 37, 'page_label': '38', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='QA [442] are domain-specific, focusing on tasks related to human activities. With advancements in\\nMLLMs, new benchmarks have emerged to address more complex video understanding challenges.\\nVideo-MME [85] explores diverse video domains with multimodal inputs and manual annotations,\\nwhile MVBench [188] reannotates existing datasets using ChatGPT. MMBench-Video [77] features\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 38, 'page_label': '39', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 39\\nfree-form questions for short to medium-length videos. Benchmarks like MLVU [496], LVBench\\n[376], Event-Bench [71], and VNBench [488] emphasize long-video understanding, testing models\\non extended multimodal contexts. VNBench [488] introduces a synthetic framework for evaluating\\ntasks like retrieval and ordering, by inserting irrelevant images or text into videos. Specialized\\nbenchmarks like EgoSchema [ 262] focus on egocentric videos. TempCompass [ 232] evaluates\\nfine-grained temporal perception, and MovieChat [ 332] targets long videos but often reduces\\ntasks to short-video problems. Current MLLMs, especially open-source ones, face challenges with\\nlong-context processing and temporal perception, underscoring the need for improved capabilities\\nin these areas.\\n4.2.6 Industry. The absence of a comprehensive benchmark for evaluating MLLMs across diverse'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 38, 'page_label': '39', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='in these areas.\\n4.2.6 Industry. The absence of a comprehensive benchmark for evaluating MLLMs across diverse\\nindustry verticals has limited understanding of their applicability in specialized real-world scenarios.\\nTo address this gap, MME-Industry [429] was developed specifically for industrial applications,\\ncovering over 21 industrial sectors such as power generation, electronics manufacturing, textile\\nproduction, steel, and chemical processing. Domain experts from each sector meticulously annotated\\nand validated test cases, ensuring the benchmark’s reliability, accuracy, and practical relevance.\\nMME-Industry thus serves as a robust tool for assessing MLLMs in industrial contexts.\\n4.2.7 Conversational QA. Current MLLMs are primarily designed for multi-round chatbot inter-\\nactions, yet most benchmarks focus on single-round QA tasks. To better align with real-world\\nconversational scenarios, multi-round QA benchmarks have been developed to simulate human-AI'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 38, 'page_label': '39', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='actions, yet most benchmarks focus on single-round QA tasks. To better align with real-world\\nconversational scenarios, multi-round QA benchmarks have been developed to simulate human-AI\\ninteractions with extended contextual histories. SparklesDialogue [134] evaluates conversational\\nproficiency across multiple images and dialogue turns, featuring flexible text-image interleaving\\nwith two rounds and four images per instance. SciGraphQA [192] constructs multi-turn QA con-\\nversations based on scientific graphs from Arxiv papers, emphasizing complex scientific discourse.\\nConvBench [225] assesses perception, reasoning, and creation capabilities across individual rounds\\nand overall conversations, revealing that MLLMs’ reasoning and creation failures often stem from\\ninadequate fine-grained perception. MMDU [235] engages models in multi-turn, multi-image con-\\nversations, with up to 20 images and 27 turns, highlighting that the performance gap between'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 38, 'page_label': '39', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='inadequate fine-grained perception. MMDU [235] engages models in multi-turn, multi-image con-\\nversations, with up to 20 images and 27 turns, highlighting that the performance gap between\\nopen-source and closed-source models is largely due to limited conversational instruction tuning\\ndata. These benchmarks collectively enhance the evaluation of MLLMs in complex, real-world\\ninteraction scenarios.\\n4.2.8 Multidisciplinary. The mastery of multidisciplinary knowledge is a key indicator of a model’s\\nexpertise, and several benchmarks have been developed to evaluate this capability. ScienceQA\\n[243] comprises scientific questions annotated with lectures and explanations, designed to facilitate\\nchain-of-thought evaluation. It spans grade-level knowledge across diverse domains. MMMU [445]\\npresents a more challenging college-level benchmark across diverse subjects, including engineering,\\nart and design, business, science, humanities, social science, and medicine. Its question format'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 38, 'page_label': '39', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='presents a more challenging college-level benchmark across diverse subjects, including engineering,\\nart and design, business, science, humanities, social science, and medicine. Its question format\\nextends beyond single image-text pairs to include interleaved text and images. Similarly, CMMU\\n[117] and CMMMU [457] provide Chinese domain-specific benchmarks for grade-level and college-\\nlevel knowledge, respectively. MMMU-Pro [446] enhances MMMU with a more robust version for\\nadvanced evaluation.\\nTable 2. Summary of dataset for generation.\\nDataset Time Statistics\\nComprehensive\\nVQA v2 [104] 2017 contains more than 443K training, 214K validation and 453K test image-question pairs.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 39, 'page_label': '40', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='40 Trovato et al.\\nNLVR2 [336] 2018 contains 107,292 examples of English sentences paired with web photographs, including 29,680 unique\\nsentences and 127,502 images. The task is to determine whether a natural language caption is true about\\na pair of photographs.\\nVizWiz [112] 2018 contains 20,000 training, 3,173 validation, and 8,000 test sets of visual questions originating from blind\\npeople.\\nMME [29] 2023 measures both perception and cognition abilities on a total of 14 subtasks\\nVisit-Bench [21] 2023 comprising 592 instances and 1,159 public images. The instances are either from 45 newly assembled\\ninstruction families or reformatted from 25 existing datasets. 10 instruction families cater to multi-image\\nquery scenarios.\\nTouchstone [17] 2023 908 questions covering 27 subtasks. The highest proportion of questions pertains to recognition, ac-\\ncounting for about 44.1%, followed by comprehension questions at 29.6%. The proportions of the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 39, 'page_label': '40', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='counting for about 44.1%, followed by comprehension questions at 29.6%. The proportions of the\\nother categories are 15.3% for basic descriptive ability, 7.4% for visual storytelling ability, and 3.6% for\\nmulti-image analysis ability.\\nMM-Vet [439] 2023 defines 6 core vision-language capabilities and examines the 16 integrations of interest derived from\\ntheir combinations. It contains 200 images, and 218 questions (samples), all paired with their respective\\nground truths.\\nInfiMM-Eval [113] 2023 It consists of 279 manually curated reasoning questions, associated with a total of 342 images. The\\ndataset is categorized into three reasoning paradigms: deductive, abductive, and analogical reasoning.\\n49 questions pertain to abductive reasoning, 181 require deductive reasoning, and 49 involve analogical\\nreasoning. Furthermore, the dataset is divided into two folds based on reasoning complexity, with 108\\nclassified as “High” reasoning complexity and 171 as “Moderate” reasoning complexity.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 39, 'page_label': '40', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='reasoning. Furthermore, the dataset is divided into two folds based on reasoning complexity, with 108\\nclassified as “High” reasoning complexity and 171 as “Moderate” reasoning complexity.\\nQ-Bench [398] 2023 consists of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its\\nlow-level attributes.\\nSeed-Bench [180] 2023 consists of 19K multiple-choice questions with accurate human annotations, which spans 12 evaluation\\ndimensions including the comprehension of both the image and video modality.\\nSEED-Bench-2 [179] 2024 comprises 24K multiple-choice questions with accurate human annotations, which span 27 dimensions,\\nincluding the evaluation of both text and image generation.\\nLVLM-eHub [411] 2024 contains 42 datasets in our LVLM-eHub. The sizes of specific datasets are 109.8K, 29.5K, 177.2K, 67.3K, and\\n8.9K for visual perception, knowledge acquisition, reasoning, commonsense, and object hallucination,\\nrespectively.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 39, 'page_label': '40', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='8.9K for visual perception, knowledge acquisition, reasoning, commonsense, and object hallucination,\\nrespectively.\\nLAMM [430] 2024 evaluate 9 common image tasks, using a total of 11 datasets with over 62,439 samples, and 3 common\\npoint cloud tasks, by utilizing 3 datasets with over 12,788 data samples.\\nMMT-Bench [431] 2024 comprises 31,325 meticulously curated multi-choice visual questions from various multimodal scenarios,\\ncovering 32 core meta-tasks and 162 subtasks in multimodal understanding.\\nRealWorldQA [4] 2024 consists of 765 images, with a question and easily verifiable answer for each image.\\nWV-Bench [245] 2024 constructed by selecting 500 high-quality samples from 8,000 user submissions in WV-ARENA.\\nMME-RealWorld\\n[480]\\n2024 constructed by collecting more than 300K images from public datasets and the Internet, filtering 13,366\\nhigh-quality images for annotation and contributing to 29,429 question-answer pairs that cover 43\\nsubtasks across 5 real-world scenarios.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 39, 'page_label': '40', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='high-quality images for annotation and contributing to 29,429 question-answer pairs that cover 43\\nsubtasks across 5 real-world scenarios.\\nMMStar [38] 2024 contains 1,500 challenging samples, each rigorously validated by humans. It identify 6 core capabili-\\nties (i.e., coarse perception, fine-grained perception, instance reasoning, logical reasoning, science &\\ntechnology, mathematics) along with 18 specific dimensions.\\nCV-Bench [359] 2024 provides 2,638 manually-inspected examples, and formulate natural language questions that evaluates\\n2D understanding via spatial relationships & object counting, and 3D understanding via depth order &\\nrelative distance.\\nMDVP [212] 2024 contains 1.6M unique image-visual prompt-text instruction-following samples, including natural images,\\ndocument images, OCR images, mobile screenshots, web screenshots, and multi-panel images.\\nFOCI [101] 2024 constructed from 5 popular classification datasets for different domains: 1) aircraft contains images of'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 39, 'page_label': '40', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='FOCI [101] 2024 constructed from 5 popular classification datasets for different domains: 1) aircraft contains images of\\n100 different aircraft types; 2) flowers contains images of 102 different flower species; 3) food covers 101\\ndishes; 4) pets contains images of 37 cat and dog breeds. 5) cars covers 196 car models. Additionally,\\nFOCI create 4 domain subsets for animals (1322 classes), plants (957 classes), food (563 classes), and\\nman-made objects (2631 classes).\\nMMVP [360] 2024 summarizes 9 prevalent patterns of the CLIP-blind pairs, such as “orientation”, “counting”, and “view-\\npoint”. Utilizing the collected CLIP-blind pairs, MMVP design 150 pairs with 300 questions.\\nV*-Bench [399] 2024 It is built based on 191 high-resolution images with an average image resolution of 2246 ×1582. V*-\\nBench contains two sub-tasks: attribute recognition and spatial relationship reasoning. The attribute\\nrecognition task has 115 samples. The spatial relationship reasoning task has 76 samples.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 39, 'page_label': '40', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Bench contains two sub-tasks: attribute recognition and spatial relationship reasoning. The attribute\\nrecognition task has 115 samples. The spatial relationship reasoning task has 76 samples.\\nMME-RealWorld\\n[480]\\n2024 contains 29,429 question-answer pairs that cover 43 subtasks across 5 real-world scenarios, where each\\none has at least 100 questions.\\nVisual COT [324] 2024 438k visual chain-of-thought question-answer pairs spans across five distinct domains, each consisting\\nof a question, an answer, and an intermediate bounding box as CoT contexts. About 98k question-answer\\npairs include extra detailed reasoning steps.\\nMementos [381] 2024 It consists of 4,761 image sequences with varying episode lengths, encompassing diverse scenarios from\\ndailylife, robotics tasks, and comic-style storyboards. Each sequence is paired with a human-annotated\\ndescription of the primary objects and their behaviors within the sequence.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 39, 'page_label': '40', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='dailylife, robotics tasks, and comic-style storyboards. Each sequence is paired with a human-annotated\\ndescription of the primary objects and their behaviors within the sequence.\\nMIRB [482] 2024 comprises 925 samples with average image number of 3.78, constructed from four distinct categories of\\nmulti-image understanding: perception, visual world knowledge, reasoning, and multi-hop reasoning.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 40, 'page_label': '41', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 41\\nReMI [162] 2024 It consists of 13 tasks that span a range of domains and properties. The tasks require reasoning over\\nup to six images, with all tasks requiring reasoning over at least two images. The images comprise a\\nvariety of heterogeneous image types.\\nMuirBench [368] 2024 It consists of 12 distinctive multi-image understanding tasks that involve 10 categories of multi-image\\nrelations, comprising 11,264 images and 2,600 multiple-choice questions, with an average of 4.3 images\\nper instance.\\nVEGA [493] 2024 contains 50k scientific literature entries, over 200k question-and-answer pairs, and a rich trove of\\n400k images. It includes the Interleaved Image-Text Comprehension subset, which is segmented into\\ntwo categories based on token length: one supports up to 4,000 tokens, while the other extends to\\n8,000 tokens. Here, images are equated to 256 tokens each. Both categories offer roughly 200k training'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 40, 'page_label': '41', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='8,000 tokens. Here, images are equated to 256 tokens each. Both categories offer roughly 200k training\\ninstances and approximately 700 test samples.\\nMMBench [230] 2025 contains 3,217 multiple-choice questions covering a diverse spectrum of 20 fine-grained skills.\\nBLINK [86] 2025 reformats 14 classic computer vision tasks, and contains 3,807 multiple-choice questions across 7.3K\\nimages, paired with single or multiple images and visual prompting.\\nOptical Character Recognition\\nTextVQA [327] 2019 contains 45,336 questions asked by humans on 28,408 images that require reasoning about text to answer.\\nEach question-image pair has 10 ground truth answers provided by humans.\\nOCR-VQA [275] 2019 comprises of 207,572 images of book covers and contains more than 1 million question-answer pairs\\nabout visual question answering by reading text in images.\\nWebSRC [41] 2021 It consists of 400K question-answer pairs, which are collected from 6.4K web pages. Along with the QA'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 40, 'page_label': '41', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='about visual question answering by reading text in images.\\nWebSRC [41] 2021 It consists of 400K question-answer pairs, which are collected from 6.4K web pages. Along with the QA\\npairs, corresponding HTML source code, screenshots, and metadata are also provided in the dataset.\\nEach question in WebSRC requires a certain structural understanding of a web page to answer.\\nOCRBench [233] 2024 includes 1000 question-answer pairs, which is consist of five components: text recognition, scene\\ntext-centric VQA, document-oriented VQA, key information extraction, and handwritten mathematical\\nexpression recognition.\\nVCR [473] 2024 It comprise 2.11M English and 346K Chinese entities, featuring captions in both languages across ‘easy’\\nand ‘hard’ difficulty levels.\\nSEED-Bench-2-Plus\\n[178]\\n2024 comprises 2.3K multiple-choice questions with precise human annotations, spanning three broad\\ncategories: Charts, Maps, and Webs.\\nStructural Document'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 40, 'page_label': '41', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='SEED-Bench-2-Plus\\n[178]\\n2024 comprises 2.3K multiple-choice questions with precise human annotations, spanning three broad\\ncategories: Charts, Maps, and Webs.\\nStructural Document\\nFigureQA [160] 2017 its training set contains 100,000 images with 1.3 million questions; the validation and test sets each\\ncontain 20,000 images with over 250, 000 questions. The images are synthetic, scientific-style figures\\nfrom five classes: line plots, dot-line plots, vertical and horizontal bar graphs, and pie charts.\\nDocVQA [268] 2021 contains 50,000 question-answer pairs with 12,767 document images sourced from documents in UCSF\\nIndustry Documents Library.\\nVisualMRC [348] 2021 It contains 30562 pairs of a question and an abstractive answer for 10,197 document images sourced\\nfrom multiple domains of webpages.\\nChartQA [266] 2022 consists of 20,882 charts curated from four different online sources. It covers 9,608 human-written'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 40, 'page_label': '41', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='from multiple domains of webpages.\\nChartQA [266] 2022 consists of 20,882 charts curated from four different online sources. It covers 9,608 human-written\\nquestions focusing on logical and visual reasoning questions, and generates another 23,111 questions\\nautomatically from human-written chart summaries.\\nInfographicVQA\\n[267]\\n2022 comprises 30,035 questions over 5,485 images. Questions in the dataset include questions grounded on\\ntables, figures and visualizations and questions that require combining multiple cues.\\nChartBench [414] 2023 includes over 68k charts and more than 600k high-quality instruction data, covering 9 major categories\\nand 42 subcategories of charts. 5 chart question-answering tasks to assess the models’ cognitive and\\nperceptual abilities.\\nSciGraphQA [192] 2023 generate 295K samples of open-vocabulary multi-turn question-answering dialogues about the graphs.\\nAs context, it provided the text-only Palm-2 with paper title, abstract, paragraph mentioning the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 40, 'page_label': '41', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='As context, it provided the text-only Palm-2 with paper title, abstract, paragraph mentioning the\\ngraph, and rich text contextual data from the graph itself, obtaining dialogues with an average 2.23\\nquestion-answer turns for each graph.\\nMMC-Benchmark\\n[217]\\n2023 consists of 2k QA pairs with 1,063 unique images, accompanied by 1,275 multiple-choice questions and\\n851 free-form questions. The average length of the questions is 15.6.\\nMP-DocVQA [358] 2023 contains 46K questions and 6K documents, with a total of 48K pages (images). On average, each question\\nis associated with 8.27 pages.\\nChartX [404] 2024 collected 48K multi-modal chart data covering 22 topics, 18 chart types, and 7 tasks. Each chart data\\nwithin this dataset includes 4 modalities, including image, Comma-Separated Values (CSV), python\\ncode, and text description. 7 chart tasks is classified into perception tasks and cognition tasks.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 40, 'page_label': '41', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='within this dataset includes 4 modalities, including image, Comma-Separated Values (CSV), python\\ncode, and text description. 7 chart tasks is classified into perception tasks and cognition tasks.\\nDocGenome [403] 2024 constructed by annotating 500K scientific documents from 153 disciplines in the arXiv open-access\\ncommunity. It contains structure data from all modalities including 13 layout attributes along with their\\nLATEX source codes. It provides 6 logical relationships between different entities within each scientific\\ndocument. It covers various document-oriented tasks.\\nCharXiv [387] 2024 involves 2,323 real-world charts handpicked from scientific papers spanning 8 major subjects published\\non arXiv, and produces more than 10K questions.\\nMMLongBench-Doc\\n[258]\\n2024 comprising 1,082 expert-annotated questions. It is constructed upon 135 lengthy PDF-formatted docu-\\nments with an average of 47.5 pages and 21,214 textual tokens. 494 questions are single-page questions'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 40, 'page_label': '41', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='ments with an average of 47.5 pages and 21,214 textual tokens. 494 questions are single-page questions\\n(with one evidence page). 365 questions are cross-page questions requiring evidence across multiple\\npages. 223 questions are designed to be unanswerable for detecting potential hallucinations.\\nComTQA [486] 2024 comprises a total of 9,070 QA pairs across 1,591 images. It contains challenging questions, such as\\nmultiple answers, mathematical calculations, and logical reasoning.\\nWeb2Code [447] 2024 contains a total of 1179.7k webpage based instruction-response pairs.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 41, 'page_label': '42', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='42 Trovato et al.\\nVisualWebBench\\n[221]\\n2024 consists of 7 tasks, and comprises 1.5K human-curated instances from 139 real websites, covering 87\\nsub-domains.\\nSciFIBench [313] 2024 consists of 2000 multiple-choice scientific figure interpretation questions split between two tasks across\\n8 categories. The questions are curated from arXiv paper figures and captions.\\nMathematics\\nMathVista [242] 2023 incorporates 28 existing multimodal datasets, including 9 math-targeted question answering (MathQA)\\ndatasets and 19 VQA datasets. In addition, it creates three new datasets (i.e., IQTest, FunctionQA,\\nPaperQA) which are tailored to evaluating logical reasoning on puzzle test figures, algebraic reasoning\\nover functional plots, and scientific reasoning with academic paper figures, respectively. It consists of\\n6,141 examples, with 736 of them being newly curated.\\nWe-Math [300] 2024 It collect and categorize 6.5K visual math problems, spanning 67 hierarchical knowledge concepts and 5'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 41, 'page_label': '42', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='6,141 examples, with 736 of them being newly curated.\\nWe-Math [300] 2024 It collect and categorize 6.5K visual math problems, spanning 67 hierarchical knowledge concepts and 5\\nlayers of knowledge granularity.\\nMath-Vision [372] 2024 comprises 3,040 mathematical problems within visual contexts across 12 grades, selected from 19 math\\ncompetitions. It contains 1,532 problems in an open-ended format and 1,508 in a multiple-choice format.\\nAll problems encompass 16 subjects over 5 levels of difficulty.\\nOlympiadbench\\n[115]\\n2024 an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,476 problems from Olympiad-\\nlevel mathematics and physics competitions, including the Chinese college entrance exam. Each problem\\nis detailed with expert-level annotations for step-by-step reasoning.\\nMathVerse [470] 2025 It contains 2,612 math problems from three fundamental math subjects, i.e., plane geometry (1,746), solid'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 41, 'page_label': '42', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='is detailed with expert-level annotations for step-by-step reasoning.\\nMathVerse [470] 2025 It contains 2,612 math problems from three fundamental math subjects, i.e., plane geometry (1,746), solid\\ngeometry (332), and functions (534). Each problem is then transformed by human annotators into six\\ndistinct versions, each offering varying degrees of information content in multimodality, contributing\\nto 15K test samples in total.\\nVideo Understanding\\nTGIF-QA [141] 2017 consists of 103,919 QA pairs collected from 56,720 animated GIFs. TGIF-QA includes four task types:\\nrepetition count, repeating action, state transition, frame QA.\\nActivityNet-QA\\n[442]\\n2019 It exploits 5,800 videos from the ActivityNet dataset, which contains about 20,000 untrimmed web\\nvideos representing 200 action classes. Each video is annotated with ten question-answer pairs using\\ncrowdsourcing to finally obtain 58,000 question-answer pairs. The maximum question length is 20 and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 41, 'page_label': '42', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='crowdsourcing to finally obtain 58,000 question-answer pairs. The maximum question length is 20 and\\nthe maximum answer length is 5. The average question length is 8.67 and average answer length is 1.85.\\nEgoSchema [262] 2023 consists of over 5000 human curated multiple choice question answer pairs, spanning over 250 hours\\nof real video data. For each question, it requires the correct answer to be selected between five given\\noptions based on a three-minute-long video clip.\\nVideo-MME [85] 2024 It contains a annotated set of 2,700 high-quality multiple-choice questions (3 per video) from 900 videos,\\n744 subtitles and 900 audio files across various scenarios. For diversity in video types, it spans 6 visual\\ndomains, with 30 subfields. For duration in temporal dimension, it encompasses both short-, medium-,\\nand long-term videos, ranging from 11 seconds to 1 hour.\\nMVBench [188] 2024 covers 20 video temporal understanding tasks that cannot be effectively solved with a single frame.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 41, 'page_label': '42', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='and long-term videos, ranging from 11 seconds to 1 hour.\\nMVBench [188] 2024 covers 20 video temporal understanding tasks that cannot be effectively solved with a single frame.\\nEach task produces 200 multiple-choice QA pairs by leveraging ChatGPT to automatically reannotate\\nexisting video datasets with their original annotations.\\nMMBench-Video [77] 2024 incorporates approximately 600 web videos from YouTube, spanning 16 major categories. Each video\\nranges in duration from 30 seconds to 6 minutes. The benchmark includes roughly 2,000 original\\nquestion-answer pairs, contributed by volunteers, covering a total of 26 fine-grained capabilities.\\nMLVU [496] 2024 consists of 3,102 questions across 9 categories with 2,593 questions for dev set and 509 questions for test\\nset. It is made up of videos of diversified lengths, spanning from 3 min to more than 2 hours. Besides,\\neach video is further partitioned as incremental segments, e.g., the first 3 min, the first 6 min, and the\\nentire video.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 41, 'page_label': '42', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='each video is further partitioned as incremental segments, e.g., the first 3 min, the first 6 min, and the\\nentire video.\\nLVBench [376] 2024 gathers an initial collection of 500 videos, each with a minimum duration of 30 minutes. Finally, these\\nvideos is annotated to select a subset of 103 videos.\\nEvent-Bench [71] 2024 includes 6 event-related tasks and 2,190 test instances.\\nVNBench [488] 2024 1,350 samples with 9 sub-tasks.\\nTempCompass [232] 2024 collects a total of 410 videos and 500 pieces of meta-information, with 9 content categories.\\nMovieChat [332] 2024 1K long videos and 13K manual question-answering pairs.\\nIndustry\\nMME-Industry [429] 2025 encompasses 21 distinct domain, comprising 1050 question-answer pairs with 50 questions per domain.\\nConversational QA\\nSparklesDialogue\\n[134]\\n2023 SparklesDialogueCC comprises 4.5K dialogues, each consisting of at least two images spanning two'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 41, 'page_label': '42', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Conversational QA\\nSparklesDialogue\\n[134]\\n2023 SparklesDialogueCC comprises 4.5K dialogues, each consisting of at least two images spanning two\\nconversational turns. SparklesDialogueVG includes 2K dialogues, each with at least three distinct images\\nacross two turns.\\nSciGraphQA [192] 2023 selected 290,000 Computer Science or Machine Learning ArXiv papers, and then used Palm-2 to generate\\n295K samples of open-vocabulary multi-turn question-answering dialogues about the graphs. As context,\\nit provided the text-only Palm-2 with paper title, abstract, paragraph mentioning the graph, and rich\\ntext contextual data from the graph itself, obtaining dialogues with an average 2.23 question-answer\\nturns for each graph.\\nConvBench [225] 2024 comprises 577 image-instruction pairs tailored for multi-round conversations. Each pair is structured\\naround three sequential instructions, each targeting a distinct cognitive skill—beginning with perception,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 41, 'page_label': '42', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='around three sequential instructions, each targeting a distinct cognitive skill—beginning with perception,\\nfollowed by reasoning, and culminating in creation. Encompassing 215 tasks, the benchmark is divided\\ninto 71 tasks focused on perception, 65 on reasoning, and 79 on creation.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 42, 'page_label': '43', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 43\\nMMDU [235] 2024 comprises 110 multi-image multi-turn dialogues with more than 1600 questions, each accompanied by\\ndetailed long-form answers. The questions in MMDU involve 2 to 20 images, with an average image&text\\ntoken length of 8.2k tokens, a maximum turn length of 27, and a maximum image&text length reaching\\n18K tokens.\\nMultidisciplinary\\nScienceQA [243] 2022 multiple-choice science question dataset containing 21,208 examples. It covers diverse topics across\\nthree subjects: natural science, social science, and language science.\\nMMMU [445] 2024 includes 11.5K multimodal questions from college exams, quizzes, and textbooks, covering 6 core\\ndisciplines. These questions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous\\nimage types.\\nCMMU [117] 2024 It consists of 3,603 questions in 7 subjects, covering knowledge from primary to high school. The'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 42, 'page_label': '43', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='image types.\\nCMMU [117] 2024 It consists of 3,603 questions in 7 subjects, covering knowledge from primary to high school. The\\nquestions can be categorized into 3 types: multiple-choice, multiple-response, and fill-in-the-blank.\\nCMMMU [457] 2024 A Chinese Multi-discipline multimodal Understanding, including 12k manually collected multimodal\\nquestions from college exams, quizzes, and textbooks, covering 6 core disciplines. These questions span\\n30 subjects and comprise 39 highly heterogeneous image typesbenchmark.\\nMMMU-Pro [446] 2024 3460 questions in total (1730 samples are in the standard format and the other 1730 are in the screenshot\\nor photo form)\\n5 Evaluation Metrics of MRAG\\nMultimodal RAG systems generally consist of four core components: document parsing, search\\nplanning, retrieval, and generation, which collectively influence their end-to-end performance.\\nAccurate and comprehensive evaluation of these components is essential, leveraging available'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 42, 'page_label': '43', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='planning, retrieval, and generation, which collectively influence their end-to-end performance.\\nAccurate and comprehensive evaluation of these components is essential, leveraging available\\nmultimodal benchmarks. In practice, three common evaluation strategies are typically employed:\\nhuman evaluation, rule-based evaluation, and LLM/MLLM-based evaluation. Each strategy offers\\ndistinct advantages and disadvantages in calculating evaluation metrics.\\n•Human evaluation: Human evaluation is widely regarded as the gold standard for assessing\\nMRAG systems, as their effectiveness is ultimately determined by human users. This method is\\nextensively used in research to ensure the reliability and relevance of model outputs. For instance,\\nBingo [58] employs human annotators to assess the accuracy of GPT-4V’s responses, with a focus\\non identifying and analyzing model biases. In hallucination detection, M-HalDetect [108] demon-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 42, 'page_label': '43', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Bingo [58] employs human annotators to assess the accuracy of GPT-4V’s responses, with a focus\\non identifying and analyzing model biases. In hallucination detection, M-HalDetect [108] demon-\\nstrates that human evaluation outperforms model-based methods in detecting subtle inaccuracies,\\nhighlighting its precision. Additionally, WV-Arena [245] uses a human voting system combined\\nwith Elo ratings to rank and compare multiple models, providing a robust benchmarking frame-\\nwork. However, human evaluation presents challenges, including increased time and labor costs,\\nwhich limit its scalability for large-scale assessments. The reliability of results can also be affected\\nby the limited number of evaluators, as individual biases may influence outcomes. To address\\nthese issues, some studies employ diverse evaluator pools and cross-validation techniques to\\nenhance the balance and representativeness of assessments. Nonetheless, the trade-off between'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 42, 'page_label': '43', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='these issues, some studies employ diverse evaluator pools and cross-validation techniques to\\nenhance the balance and representativeness of assessments. Nonetheless, the trade-off between\\nevaluation accuracy and resource expenditure remains a critical consideration in designing RAG\\nmodel evaluation methodologies.\\n•Rule-based evaluation: Rule-based evaluation metrics [41, 430, 473] are essential for assessing\\nthe performance of MRAG systems. These metrics rely on standardized evaluation tools, enabling\\nobjective, reproducible assessments with minimal human intervention. Compared to subjective\\nhuman evaluations, deterministic metrics offer significant advantages, including reduced time\\nconsumption, lower susceptibility to bias, and greater consistency across multiple assessments.\\nSuch consistency is particularly crucial for large-scale evaluations or when comparing different\\nsystems or model iterations.\\n•LLM/MLLM-based evaluation: For evaluation of MRAG systems, LLMs/MLLMs are employed'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 42, 'page_label': '43', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='systems or model iterations.\\n•LLM/MLLM-based evaluation: For evaluation of MRAG systems, LLMs/MLLMs are employed\\nto compare reference answers with generated outputs or to directly score responses. For example,\\nMM-Vet [439] uses GPT-4 to automate evaluation, generating scores for each sample based on\\nthe input question, ground truth, and model output. Similarly, TouchStone [17] and LLaVA-bench\\n[219] leverage GPT-4 to directly compare generated answers with reference answers, simplifying\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 43, 'page_label': '44', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='44 Trovato et al.\\nthe evaluation process. While integrating LLMs/MLLMs in evaluation reduces human effort, it\\nhas limitations. This approach is prone to systematic biases, such as sensitivity to the order of\\nresponse presentation. Additionally, evaluation outcomes are heavily influenced by the inherent\\ncapabilities and limitations of the LLMs/MLLMs themselves, leading to potential inconsistencies,\\nas different models may produce divergent results for the same task. These challenges underscore\\nthe need for careful model selection and evaluation design to mitigate biases and ensure reliable\\nassessments.\\n5.1 Metrics of Retrieval & Generation\\nThe evaluation of MRAG systems is essential for ensuring their effectiveness and reliability in\\nprocessing complex, multimodal data. Evaluation metrics can be broadly classified into rule-based\\nand LLM/MLLM-based approaches.\\n•Rule-based Metrics : Rule-based metrics evaluate the performance of MRAG systems using'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 43, 'page_label': '44', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='and LLM/MLLM-based approaches.\\n•Rule-based Metrics : Rule-based metrics evaluate the performance of MRAG systems using\\npredefined criteria and heuristics. These metrics are generally interpretable, transparent, and\\ncomputationally efficient, making them well-suited for tasks with well-defined benchmarks.\\nExamples of common rule-based metrics include:\\n– Exact Match (EM) : This metric evaluates whether the model’s output exactly matches the\\nground truth, offering a clear and unambiguous performance measure. It is especially valuable\\nin tasks requiring high accuracy and fidelity to reference data, such as question answering,\\nfact verification, and information retrieval. While exact match (EM) provides a straightforward\\nand interpretable evaluation, it may fall short in scenarios where semantically equivalent but\\nlexically divergent responses are acceptable.\\n– ROUGE-N (N-gram Recall) : The ROUGE metric is a widely used framework for evaluating'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 43, 'page_label': '44', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='lexically divergent responses are acceptable.\\n– ROUGE-N (N-gram Recall) : The ROUGE metric is a widely used framework for evaluating\\ntext summarization and generation tasks. ROUGE-N measures the overlap of N-grams (con-\\ntiguous sequences of N words) between generated text and one or more reference texts, with\\na strong emphasis on recall. This metric assesses how well the generated text captures the\\nessential content of the reference. For example, ROUGE-1 evaluates unigram overlap, ROUGE-2\\nfocuses on bigrams, and higher-order N-grams (e.g., ROUGE-3) capture more complex linguis-\\ntic structures. While ROUGE-N provides a quantitative measure of lexical similarity, it is often\\nsupplemented by other metrics to account for semantic coherence, fluency, and relevance,\\nparticularly in multimodal contexts where textual and non-textual data interact.\\n– BLEU: BLEU is a widely used metric in NLP for evaluating the quality of machine-generated'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 43, 'page_label': '44', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='particularly in multimodal contexts where textual and non-textual data interact.\\n– BLEU: BLEU is a widely used metric in NLP for evaluating the quality of machine-generated\\ntext by assessing its similarity to one or more reference texts. Initially designed for machine\\ntranslation, BLEU has been adapted to various NLP tasks, including multimodal generation. In\\nmultimodal settings, BLEU can evaluate the alignment between generated text and associated\\nmodalities (e.g., images, videos) by comparing the output to reference descriptions or captions.\\nHowever, while BLEU offers a quantitative measure of n-gram overlap, it has limitations\\nin capturing semantic depth, contextual coherence, and multimodal consistency, which are\\nessential for comprehensive evaluation in MRAG systems.\\n– Mean Reciprocal Rank (MRR) : MRR is a widely used metric for evaluating the performance\\nof systems that produce ranked lists of results, such as search engines, recommendation'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 43, 'page_label': '44', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='– Mean Reciprocal Rank (MRR) : MRR is a widely used metric for evaluating the performance\\nof systems that produce ranked lists of results, such as search engines, recommendation\\nsystems, or retrieval-augmented models. MRR measures the rank position of the first relevant\\nitem in the returned list, reflecting the system’s ability to surface correct or useful information\\nquickly. It is calculated as the average of the reciprocal ranks of the first relevant result across\\nmultiple queries or tasks. A higher MRR indicates better performance, as it demonstrates the\\nsystem’s effectiveness in prioritizing relevant results at the top of the list.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 44, 'page_label': '45', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 45\\n– CIDEr (Consensus-based Image Description Evaluation) : CIDEr is specifically designed to\\nmeasure the agreement between machine-generated captions and human-authored reference\\ncaptions. It utilizes a TF-IDF weighting mechanism to quantify the similarity between generated\\nand reference texts.\\n– SPICE (Semantic Propositional Image Caption Evaluation) : The evaluation of MRAG\\nsystems frequently utilizes the SPICE metric to assess the quality of generated captions.\\nSPICE prioritizes semantic fidelity by parsing captions into structured scene graphs, which\\ndepict objects, attributes, and relationships within the text. These generated scene graphs are\\nsubsequently compared to reference graphs derived from ground-truth captions. By emphasiz-\\ning semantic similarity over lexical overlap, SPICE offers a robust measure of how well the\\ngenerated content aligns with the intended meaning. This makes it particularly well-suited'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 44, 'page_label': '45', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='ing semantic similarity over lexical overlap, SPICE offers a robust measure of how well the\\ngenerated content aligns with the intended meaning. This makes it particularly well-suited\\nfor evaluating multimodal systems that integrate visual and textual information, ensuring a\\nnuanced and contextually accurate assessment of MRAG outputs.\\n– BERTScore : Evaluation of MRAG focuses on assessing the quality and relevance of outputs\\nin contexts integrating both textual and non-textual data (e.g., images, audio). A key metric\\nfor evaluating textual components is BERTScore, which utilizes contextual embeddings from\\nBERT to measure semantic similarity between generated and reference texts. Unlike traditional\\nmetrics such as BLEU or ROUGE, which depend on exact word matches or n-gram overlap,\\nBERTScore captures deeper semantic relationships by aligning tokens based on their contextual\\nembeddings.\\n– Perplexity: It measures the model’s ability to predict the next word in a sequence, with lower'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 44, 'page_label': '45', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='embeddings.\\n– Perplexity: It measures the model’s ability to predict the next word in a sequence, with lower\\nperplexity values indicating greater confidence and accuracy in predictions. This reflects a\\nstronger understanding of the underlying data distribution.\\nRule-based metrics offer objective and reproducible outcomes but frequently lack the adaptability\\nneeded to capture nuanced semantic or contextual understanding, especially in multimodal\\nenvironments where text, images, and other data types interact.\\n•LLM/MLLM-based Metrics : The emergence of LLMs and MLLMs has transformed evalua-\\ntion paradigms, enabling the use of their advanced reasoning and comprehension capabilities.\\nLLM/MLLM-based metrics now provide more holistic and context-aware assessments of MRAG\\nsystems, with key approaches including:\\n– Answer Precision: This metric measures the degree to which the knowledge in a model-\\ngenerated answer is supported or entailed by the ground truth. It assesses the accuracy and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 44, 'page_label': '45', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='– Answer Precision: This metric measures the degree to which the knowledge in a model-\\ngenerated answer is supported or entailed by the ground truth. It assesses the accuracy and\\nrelevance of retrieved information by evaluating the overlap between the model’s output and\\nthe factual or contextual basis provided by the ground truth. High answer precision indicates\\nthat the model effectively utilizes retrieved multimodal data to produce responses aligned\\nwith the expected factual content. This metric is crucial for evaluating the reliability and\\nfactual consistency of multimodal RAG systems, ensuring that generated outputs are both\\ncontextually appropriate and informationally accurate.\\n– Ground Truth Recall : This metric measures the degree to which the knowledge in the\\nground truth is accurately captured and reflected in the model-generated response. It assesses\\nthe model’s ability to retrieve and integrate relevant information from the provided knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 44, 'page_label': '45', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='ground truth is accurately captured and reflected in the model-generated response. It assesses\\nthe model’s ability to retrieve and integrate relevant information from the provided knowledge\\nbase or multimodal sources, ensuring the output aligns with the factual or contextual details\\nin the reference data. It is particularly crucial for evaluating retrieval-augmented systems,\\nas it directly quantifies the fidelity of the model’s output to the intended knowledge. Higher\\nscores indicate stronger alignment with the ground truth, reflecting enhanced retrieval and\\ngeneration capabilities.\\n– Retrieved Context Precision : This metric measures the alignment between the knowledge\\nin the retrieved context and the information in the ground truth response. It evaluates the\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 45, 'page_label': '46', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='46 Trovato et al.\\nproportion of relevant and accurate information in the retrieved context that is directly\\nsupported or entailed by the ground truth, assessing the retrieval system’s precision and\\ncontextual appropriateness in generating accurate responses. This metric is especially vital\\nin multimodal RAG systems, where integrating diverse data types (e.g., text, images, audio)\\nrequires robust evaluation of relevance and precision across modalities.\\n– Retrieved Context Recall : This metric measures the degree to which the retrieved context\\naligns with and encompasses the knowledge necessary to generate ground truth responses.\\nIt evaluates the proportion of relevant information from the ground truth captured within\\nthe retrieved context, serving as a key indicator of the retrieval system’s effectiveness in\\nsupporting accurate and comprehensive response generation. High values indicate that the\\nretrieval mechanism effectively identifies and incorporates essential knowledge, thereby'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 45, 'page_label': '46', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='supporting accurate and comprehensive response generation. High values indicate that the\\nretrieval mechanism effectively identifies and incorporates essential knowledge, thereby\\nenhancing the overall performance of the MRAG system.\\n– Faithfulness: This metric evaluates the extent to which generated text maintains factual\\nconsistency with the information in the retrieved documents, ensuring the output accurately\\nreflects the source material and minimizes hallucinations or deviations from the evidence.\\nIn MRAG systems, it also ensures alignment with multimodal retrieved content, including\\ntextual, visual, and auditory elements, maintaining consistency across modalities.\\n– Hallucination: This metric measures the proportion of generated outputs containing hal-\\nlucinated content, such as unsupported claims, fabricated information, or inaccuracies not\\nsubstantiated by the retrieved data. It is essential for evaluating the reliability and factual\\nconsistency of the model’s responses.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 45, 'page_label': '46', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='substantiated by the retrieved data. It is essential for evaluating the reliability and factual\\nconsistency of the model’s responses.\\nLLM/MLLM-based metrics are highly effective at capturing complex semantic relationships and\\ncontextual nuances, making them particularly suitable for multimodal RAG systems. However,\\nthey may inherit biases from the underlying models and demand substantial computational\\nresources.\\n•Metric Calculation: When evaluating multimodal retrieval-augmented generation systems,\\nimplementation methods for the same metric can vary significantly, primarily categorized into\\ncoarse-grained and fine-grained approaches. These methodologies differ in their granularity\\nand the depth of analysis applied to assess the quality of model-generated responses against\\nreference answers.\\n– Coarse-Grained Evaluation: Coarse-grained evaluation utilizes LLMs or MLLMs to compare\\nmodel-generated responses with reference answers. This method involves inputting both'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 45, 'page_label': '46', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='reference answers.\\n– Coarse-Grained Evaluation: Coarse-grained evaluation utilizes LLMs or MLLMs to compare\\nmodel-generated responses with reference answers. This method involves inputting both\\nthe generated output and the reference into the LLM/MLLM, which evaluates the overall\\nsemantic alignment, coherence, and relevance between the two. The model assesses whether\\nthe generated content captures the core meaning and intent of the reference, providing a holistic\\nscore or qualitative feedback. This approach is computationally efficient and scalable, making\\nit suitable for rapid benchmarking and high-level quality checks in large-scale applications.\\nHowever, its broad focus may overlook fine-grained inaccuracies, such as subtle factual\\nerrors, logical inconsistencies, or nuanced contextual mismatches. Consequently, coarse-\\ngrained evaluation is best used as an initial screening tool or in scenarios where high-level'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 45, 'page_label': '46', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='errors, logical inconsistencies, or nuanced contextual mismatches. Consequently, coarse-\\ngrained evaluation is best used as an initial screening tool or in scenarios where high-level\\nsemantic fidelity is prioritized over detailed precision. For more rigorous evaluation, it is\\noften supplemented by fine-grained metrics that address specific aspects of content quality.\\nIn summary, coarse-grained evaluation offers a pragmatic balance between efficiency and\\neffectiveness, particularly in applications requiring quick assessments or large-scale model\\ncomparisons.\\n– Fine-Grained Evaluation: Fine-grained evaluation, such as RAGChecker [317] and RAGAS\\n[74], offers a nuanced and detailed approach to assessing MRAG systems, surpassing the limita-\\ntions of coarse-grained methods. This approach involves decomposing both model-generated\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 46, 'page_label': '47', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 47\\nresponses and reference answers into granular knowledge points or semantic units, which are\\nindividually evaluated based on criteria such as accuracy, relevance, and alignment with the\\nreference. By analyzing responses at this level of detail, the method enables precise identifica-\\ntion of a model’s strengths and weaknesses, particularly in capturing and reproducing intricate\\ninformation. The fine-grained approach is especially valuable for diagnosing performance\\nissues in handling complex or nuanced content. However, it is computationally intensive,\\nrequiring robust mechanisms for extracting, matching, and evaluating multiple semantic units.\\nCareful design of these mechanisms is essential to ensure evaluation consistency and reliability.\\nDespite its challenges, this method provides a rigorous and comprehensive framework for\\nadvancing the development and refinement of MRAG systems, making it a critical tool in the\\nfield.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 46, 'page_label': '47', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Despite its challenges, this method provides a rigorous and comprehensive framework for\\nadvancing the development and refinement of MRAG systems, making it a critical tool in the\\nfield.\\nThe choice between coarse-grained and fine-grained evaluation depends on the assessment\\nobjectives. Coarse-grained methods are ideal for obtaining quick, high-level insights, whereas\\nfine-grained approaches are better suited for detailed analysis and iterative model refinement.\\nIntegrating both strategies can provide a balanced perspective, combining the efficiency of\\ncoarse-grained evaluation with the precision of fine-grained analysis to comprehensively assess\\nMRAG systems.\\n6 Challenges of MRAG\\nIn this section, we delineate the challenges associated with various modules in a MRAG system.\\nThese challenges span multiple critical components, including document parsing and indexing,\\nsearch planning, retrieval, generation, dataset, and evaluation. Each module presents unique'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 46, 'page_label': '47', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='These challenges span multiple critical components, including document parsing and indexing,\\nsearch planning, retrieval, generation, dataset, and evaluation. Each module presents unique\\ncomplexities that must be addressed to ensure the system’s effectiveness and robustness.\\n6.1 Document Parsing and Indexing\\nDocument parsing and indexing has established the data foundation based on MRAG, which plays\\na crucial role in the entire system. The relevant technologies extensively studied even before the\\nadvent of LLMs, have seen significant advancements in the LLM era. However, they continue to\\nface several challenges that necessitate further exploration and refinement.\\n•Challenges in Data Accuracy and Completeness: As the primary input source, the accuracy\\nand completeness of upstream data are critical. Errors or omissions in the upstream data can\\npropagate and amplify downstream, significantly degrading system performance. For example,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 46, 'page_label': '47', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='and completeness of upstream data are critical. Errors or omissions in the upstream data can\\npropagate and amplify downstream, significantly degrading system performance. For example,\\nwhile MRAG systems have enhanced document information preservation—such as capturing\\nper-page screenshots—they still face challenges in maintaining inter-page relationships. This\\nlimitation is particularly problematic in long documents with associated segments. Preserving\\nthese relationships is essential for ensuring contextually accurate outputs.\\n•Balancing Multimodal and Textual Data: The document parsing module has grown increas-\\ningly complex as modern MRAG systems must handle multimodal data, including images, tables,\\nand text. To address this, contemporary approaches preserve the original multimodal inputs\\nto minimize information loss, while also converting them into textual captions or descriptions.\\nAlthough retaining the original data reduces information loss, relying solely on it has proven'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 46, 'page_label': '47', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='to minimize information loss, while also converting them into textual captions or descriptions.\\nAlthough retaining the original data reduces information loss, relying solely on it has proven\\nsuboptimal. Recent studies highlight the benefits of leveraging textual representations derived\\nfrom multimodal data. For example, Riedler and Langer [312] showed that models generate\\nhigher-quality responses using textual captions from images rather than processing raw images\\ndirectly. Similarly, Ma et al. [259] found that LLMs outperform MLLMs in text generation tasks,\\nrevealing a performance gap between multimodal and text-focused systems. This gap highlights\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 47, 'page_label': '48', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='48 Trovato et al.\\nthe limitations of current multimodal systems in effectively integrating diverse data types, ne-\\ncessitating additional components in document parsing pipelines. These enhancements, while\\nimproving functionality, increase system complexity and expand the volume of data requiring\\nprocessing, storage, and management.\\n6.2 Multimodal Search Planning\\nThe challenges in multimodal search planning can be more effectively understood through a\\nhierarchical framework similar to leveled RAG systems, where queries span a spectrum from simple\\nfactual retrievals to complex, creative tasks. This framework highlights three critical challenges\\nthat must be addressed to advance the field.\\n•Intelligent Adaptive Planning Mechanisms: The primary challenge is developing intelligent\\nadaptive planning mechanisms that can dynamically adjust to the diversity and complexity of\\nqueries. Current systems often rely on predetermined pipelines, which fail to accommodate'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 47, 'page_label': '48', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='adaptive planning mechanisms that can dynamically adjust to the diversity and complexity of\\nqueries. Current systems often rely on predetermined pipelines, which fail to accommodate\\nvariations in query characteristics or computational constraints, leading to inefficient resource\\nallocation and suboptimal performance [125, 474]. While fixed strategies may suffice for homo-\\ngeneous query types, real-world applications handle heterogeneous query patterns that demand\\ndynamic adjustment of retrieval strategies. For example, complex queries involving multi-hop\\nreasoning or creative problem-solving could greatly benefit from a multi-agent collaborative\\napproach [369]. In such a framework, specialized agents could explore parallel reasoning paths,\\npropose complementary retrieval strategies, and collaboratively synthesize findings to construct\\ncomprehensive search plans. This collaborative paradigm not only simulates diverse perspec-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 47, 'page_label': '48', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='propose complementary retrieval strategies, and collaboratively synthesize findings to construct\\ncomprehensive search plans. This collaborative paradigm not only simulates diverse perspec-\\ntives but also facilitates intricate interactions between knowledge sources and reasoning steps.\\nBy evaluating search plans from multiple angles, such systems can balance effectiveness and\\nefficiency, ensuring robust performance across diverse query types.\\n•Query Reformulation and Semantic Alignment: A second major challenge is query reformu-\\nlation, particularly in maintaining semantic alignment between the original multimodal query\\nintent and the reformulated queries [197]. As queries become more sophisticated, accurately\\ncapturing and maintaining their intent grows increasingly complex. This challenge is amplified\\nin multimodal contexts, where queries may integrate text, images, audio, or other data types,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 47, 'page_label': '48', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='capturing and maintaining their intent grows increasingly complex. This challenge is amplified\\nin multimodal contexts, where queries may integrate text, images, audio, or other data types,\\neach requiring precise interpretation. To address this, multi-perspective reformulation strategies\\ncould be employed, leveraging diverse interpretations of the query to generate reformulations\\nthat better align with the original intent. Such strategies might integrate contextual under-\\nstanding, domain-specific knowledge, and cross-modal alignment techniques to ensure semantic\\nconsistency with the user’s intent.\\n•Comprehensive Evaluation Benchmarks: The third critical challenge is the absence of com-\\nprehensive evaluation benchmarks capable of assessing planning mechanisms across diverse\\nquery complexities and scenarios. Existing benchmarks often focus on narrow performance\\naspects, failing to capture the full spectrum of real-world applications. To address this gap, future'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 47, 'page_label': '48', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='query complexities and scenarios. Existing benchmarks often focus on narrow performance\\naspects, failing to capture the full spectrum of real-world applications. To address this gap, future\\nbenchmarks should evaluate systems across multiple dimensions, including adaptability to query\\ndiversity, robustness in handling complex queries, and efficiency in resource utilization. These\\nbenchmarks should incorporate a wide range of query types, from simple factual retrievals to\\nmulti-hop reasoning and creative tasks, ensuring rigorous testing under realistic conditions. Addi-\\ntionally, benchmarks should incorporate metrics for semantic alignment in query reformulation,\\ncomputational efficiency, and scalability.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 48, 'page_label': '49', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 49\\nThese interconnected challenges highlight the need for future research to develop adaptive planning\\nmechanisms capable of addressing both query diversity and complexity. This could involve multi-\\nagent coordination for advanced cases, alongside robust query reformulation and comprehensive\\nevaluation frameworks.\\n6.3 Retrieval\\nMultimodal retrieval has made significant progress but continues to face challenges that can be\\ncategorized into methodological and practical issues. These challenges arise from the inherent\\ncomplexity of integrating and retrieving information across diverse data modalities such as text,\\nimages, audio, and video. Below, we outline the key challenges in this field:\\n•Heterogeneity of Cross-Modal Data : The heterogeneity of data across modalities poses a\\nsignificant challenge in multimodal retrieval and representation learning. Text, being sequential'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 48, 'page_label': '49', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='•Heterogeneity of Cross-Modal Data : The heterogeneity of data across modalities poses a\\nsignificant challenge in multimodal retrieval and representation learning. Text, being sequential\\nand discrete, relies on syntactic and semantic structures best captured by language models, while\\nimages, being spatial and continuous, require convolutional or transformer-based architectures to\\nextract hierarchical visual features. This structural divergence complicates cross-modal alignment\\nand comparison, as each modality demands specialized feature extraction techniques tailored to\\nits unique characteristics. Extracting meaningful and comparable features from each modality is\\nnon-trivial, requiring domain-specific expertise and sophisticated models capable of capturing\\nnuanced data properties. For instance, while transformers excel in processing sequential data like\\ntext, their adaptation to spatial data like images often necessitates architectural modifications,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 48, 'page_label': '49', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='nuanced data properties. For instance, while transformers excel in processing sequential data like\\ntext, their adaptation to spatial data like images often necessitates architectural modifications,\\nsuch as vision transformers (ViTs), to handle pixel arrays. Aligning these features into a unified\\nrepresentation space that preserves cross-modal semantic relationships remains a major challenge.\\nCurrent approaches, including cross-modal transformers and MLLMs, often fail to create a\\ncommon embedding space that adequately captures the semantic richness of each modality while\\nensuring inter-modal consistency.\\n•Cross-modal components (reranker, refiner) : While the dual-tower architecture has made\\nsignificant strides in first-stage retrieval by efficiently encoding and aligning multimodal data (e.g.,\\ntext and images), developing advanced reranking models that enable fine-grained multimodal\\ninteraction remains a challenge. Additionally, refining external multimodal knowledge post-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 48, 'page_label': '49', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='text and images), developing advanced reranking models that enable fine-grained multimodal\\ninteraction remains a challenge. Additionally, refining external multimodal knowledge post-\\nretrieval and reranking remains underexplored, despite its potential to enhance result accuracy\\nand relevance. Addressing these gaps requires innovative methodologies that leverage MLLMs\\nand LLMs to enable sophisticated cross-modal understanding and reasoning.\\n6.4 Generation\\nThe multimodal module in MRAG achieves human-aligned sensory representation through diver-\\nsified modality integration, which significantly enhances user experience and system usability.\\nHowever, achieving these enhancement objectives entails addressing shared challenges across both\\nQA systems and multimodal generation:\\n•Multimodal Input: Multimodal systems face the challenge of integrating diverse data structures\\nand representations across modalities such as text, images, audio, and video. As multimodal'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 48, 'page_label': '49', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='•Multimodal Input: Multimodal systems face the challenge of integrating diverse data structures\\nand representations across modalities such as text, images, audio, and video. As multimodal\\nmodels evolve, they are increasingly required to process arbitrary combinations of modalities (e.g.,\\ntext+image, text+video, image+audio). This necessitates a highly flexible and adaptive framework\\ncapable of dynamically accommodating diverse input configurations. Such frameworks must be\\nmodality-agnostic, enabling seamless integration of any input combination without predefined\\nstructures or extensive retraining. Achieving this flexibility involves designing architectures that\\ngeneralize across modalities, extract relevant features, and fuse them meaningfully, regardless of\\ninput composition.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 49, 'page_label': '50', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='50 Trovato et al.\\n•Multimodal Output:\\n– Coherent and Contextually Relevant Generation : Ensuring consistency across different\\nmodalities in the output presents a significant challenge. For instance, in a text-image pair, the\\nimage must accurately reflect the scene or object described in the text, while the text should\\nprecisely convey the visual content.\\n– Positioning and Integration of Multimodal Elements : In multimodal outputs, such as\\ntext with embedded images or videos, the model must intelligently determine where to\\nintegrate non-textual elements. This requires an understanding of the narrative flow and the\\nidentification of optimal insertion points to enhance coherence and readability. Additionally,\\nthe model should dynamically generate or retrieve relevant multimodal content based on\\ncontext. For instance, when creating a text-image pair, the model may need to generate an\\nimage caption, search for relevant images, and select the most appropriate one. This process'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 49, 'page_label': '50', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='context. For instance, when creating a text-image pair, the model may need to generate an\\nimage caption, search for relevant images, and select the most appropriate one. This process\\nmust be efficient and seamless to ensure the final output is both relevant and high-quality.\\n– Diversity of Outputs : In some applications, generating diverse outputs—such as multiple\\nimages or videos corresponding to a given text description—is essential. However, balancing\\ndiversity with relevance and quality poses a significant challenge. The model must explore a\\nbroad range of possibilities while ensuring each output remains contextually appropriate and\\nadheres to high-quality standards.\\n6.5 Dataset & Evaluation\\nThe advancement of MLLMs has heightened the need for comprehensive evaluation. Despite\\nthe introduction of over a hundred benchmarks by both academic and industrial communities,\\nseveral challenges remain in the current evaluation landscape. First, there is a lack of a universally'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 49, 'page_label': '50', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='the introduction of over a hundred benchmarks by both academic and industrial communities,\\nseveral challenges remain in the current evaluation landscape. First, there is a lack of a universally\\naccepted, standardized capability taxonomy, with existing benchmarks often defining their own\\ndisparate ability dimensions. Second, current benchmarks exhibit significant gaps in critical areas\\nsuch as instruction following, complex multimodal reasoning, multi-turn dialogue, and creativity\\nassessment. Third, task-specific evaluations for MLLMs are insufficient, particularly in commercially\\nrelevant domains like invoice recognition, multimodal knowledge base comprehension, and UI\\nunderstanding and industry. Finally, while existing multimodal benchmarks primarily focus on\\nimage and video modalities, there is a notable deficit in assessing capabilities related to audio\\nand 3D representations. Addressing these challenges is essential for developing more robust and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 49, 'page_label': '50', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='image and video modalities, there is a notable deficit in assessing capabilities related to audio\\nand 3D representations. Addressing these challenges is essential for developing more robust and\\ncomprehensive evaluation methodologies for MLLMs in the future.\\nDespite rapid advancements, current evaluations of MLLMs remain insufficiently comprehensive,\\nprimarily focusing on perception and reasoning abilities through objective questions. This creates a\\nsignificant gap between evaluation methodologies and real-world applications. Moreover, optimiz-\\ning models based on objective assessments often leads developers to prioritize objective question\\ncorpora during instruction tuning, potentially degrading the quality of dialogue experiences. Al-\\nthough subjective multimodal evaluation platforms like WildVision and OpenCompass MultiModal\\nArena have emerged, further research is needed to develop assessment methods that better align'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 49, 'page_label': '50', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='though subjective multimodal evaluation platforms like WildVision and OpenCompass MultiModal\\nArena have emerged, further research is needed to develop assessment methods that better align\\nwith practical usage scenarios. Current evaluation strategies predominantly rely on curated or\\ncrafted questions to assess specific capabilities, yet complex multimodal tasks typically require\\nthe integration of multiple skills. For instance, a chart-related question may involve OCR, spatial\\nrelationship recognition, reasoning, and calculations. The absence of decoupled assessments for\\nthese distinct capabilities represents a major limitation in existing frameworks. Additionally, crucial\\nabilities such as instruction following remain under-evaluated. Multiturn dialogue, the primary\\nmode of human interaction with multimodal models, remains a weakness for most models, and\\ncorresponding evaluations, are still in their infancy. In the realm of complex multimodal reasoning,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 49, 'page_label': '50', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='mode of human interaction with multimodal models, remains a weakness for most models, and\\ncorresponding evaluations, are still in their infancy. In the realm of complex multimodal reasoning,\\ncurrent evaluations predominantly focus on mathematical and examination problems, necessitating\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 50, 'page_label': '51', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 51\\nimprovements in both difficulty and relevance to everyday use cases. Notably, the evaluation of\\nmultimodal creative tasks, a key application area for these models—such as text generation based on\\nimage and textual prompts—remains largely unexplored, highlighting a critical gap in the current\\nevaluation landscape.\\nMLLMs are still in the early stages of development, with limited business applications to date.\\nAs a result, current evaluations primarily focus on assessing foundational capabilities rather than\\nreal-world performance. Moving forward, it is critical to develop evaluation frameworks that\\nmeasure MLLM performance on specific tasks with commercial value, such as large-scale document\\nprocessing, multimodal knowledge base comprehension, anomaly detection, and industrial visual\\ninspection. When designing task-specific evaluations, it is essential to consider not only performance'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 50, 'page_label': '51', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='processing, multimodal knowledge base comprehension, anomaly detection, and industrial visual\\ninspection. When designing task-specific evaluations, it is essential to consider not only performance\\nmetrics but also computational costs and inference speeds, benchmarking them against traditional\\ncomputer vision methods like OCR, object detection, and action recognition to determine practical\\napplicability. Additionally, a key potential of MLLMs lies in their ability to plan and interact with\\nenvironments as agents to solve complex problems. Developing diverse virtual environments\\nfor MLLMs to demonstrate agent-based problem-solving capabilities will likely become a critical\\ncomponent of future evaluations. Current efforts in this domain remain nascent, highlighting a\\npromising area for future research in multimodal AI assessment.\\n7 Future Directions\\nIn this chapter, we propose several suggestions to the future development of multimodal Retrieval-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 50, 'page_label': '51', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='promising area for future research in multimodal AI assessment.\\n7 Future Directions\\nIn this chapter, we propose several suggestions to the future development of multimodal Retrieval-\\nAugmented Generation (MRAG) systems, informed by related research and identified challenges.\\nThese recommendations collectively aim to overcome existing limitations and unlock the full\\npotential of MRAG in complex, real-world scenarios.\\n7.1 Documents Parsing\\nMultimodal document parsing has become a crucial element in MRAG systems, particularly with\\nthe emergence of large language models (LLMs) and multimodal large models (MLLMs). The fusion\\nof text, images, and other data types into a cohesive framework presents both transformative\\nopportunities and notable challenges. This paper provides a detailed analysis of future directions\\nin this evolving field.\\n•Enhancing Data Accuracy and Completeness:\\n– Contextual Relationship Preservation: To improve the accuracy and coherence of multi-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 50, 'page_label': '51', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='in this evolving field.\\n•Enhancing Data Accuracy and Completeness:\\n– Contextual Relationship Preservation: To improve the accuracy and coherence of multi-\\nmodal document parsing, especially for long and complex documents, advanced algorithms are\\nneeded to capture and preserve both inter-page and intra-document relationships. Techniques\\nsuch as graph-based representations and hierarchical document modeling can help maintain\\ncontextual coherence across the document. These methods enable the system to understand\\nstructural and semantic dependencies between sections, tables, figures, and other elements,\\nensuring the preservation of the document’s logical flow. Additionally, cross-referencing mech-\\nanisms are essential for linking related content across pages. These mechanisms dynamically\\nconnect sections, tables, and figures, facilitating seamless retrieval and utilization of contextual\\nrelationships in downstream tasks like information extraction, summarization, or question'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 50, 'page_label': '51', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='connect sections, tables, and figures, facilitating seamless retrieval and utilization of contextual\\nrelationships in downstream tasks like information extraction, summarization, or question\\nanswering. By integrating these approaches, the system can better handle the complexities of\\nlong documents, ensuring accurate maintenance and leveraging of contextual relationships\\nfor enhanced performance in multimodal document understanding tasks. This is particularly\\nrelevant when combining Optical Character Recognition (OCR), LLMs, and MLLMs to process\\nand interpret documents with diverse content types.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 51, 'page_label': '52', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='52 Trovato et al.\\n– Error Detection and Correction: To improve the accuracy and reliability of multimodal\\ndocument parsing, integrating advanced error detection and correction mechanisms is crucial.\\nLeveraging LLMs and MLLMs, systems can validate extracted text against the original docu-\\nment, identifying and correcting inaccuracies or omissions. These models can be enhanced\\nwith consistency-checking algorithms to ensure coherence and accuracy across multimodal\\ndata, including text, images, and tables. For critical documents, a human-in-the-loop (HITL)\\napproach is advisable. This involves human reviewers verifying and refining parsed data,\\nespecially in cases where systems may struggle with complex layouts, ambiguous content,\\nor domain-specific nuances. By combining the strengths of LLMs, MLLMs, and human ex-\\npertise, this hybrid approach ensures high accuracy and reliability, making it suitable for'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 51, 'page_label': '52', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='or domain-specific nuances. By combining the strengths of LLMs, MLLMs, and human ex-\\npertise, this hybrid approach ensures high accuracy and reliability, making it suitable for\\nprecision-demanding applications such as legal, medical, or financial document processing.\\n•Improving Multimodal Data Integration:\\n– Unified Multimodal Representation: Advancing multimodal document parsing requires\\nthe development of unified representation frameworks that seamlessly integrate diverse data\\ntypes, such as text, images, and tables, into a cohesive structure. Such frameworks enable\\nrobust, context-aware analysis by leveraging multimodal transformers—like CLIP, Flamingo,\\nor other state-of-the-art models—to encode disparate modalities into a shared embedding\\nspace. This interoperability enhances downstream tasks, including information extraction,\\nquestion answering, and summarization. A promising approach involves hybrid strategies'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 51, 'page_label': '52', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='space. This interoperability enhances downstream tasks, including information extraction,\\nquestion answering, and summarization. A promising approach involves hybrid strategies\\nthat combine raw multimodal data with textual representations. For instance, raw images\\ncan support visual tasks (e.g., object detection or layout analysis), while textual captions\\nor OCR-derived text can improve text generation tasks (e.g., summarization or translation).\\nThis dual methodology leverages the strengths of each modality, ensuring both accuracy and\\nefficiency in processing complex documents, as demonstrated by recent research. Additionally,\\nintegrating LLMs and MLLMs with Optical Character Recognition (OCR) systems can enhance\\nthe parsing of scanned or image-based documents. By aligning OCR outputs with multimodal\\nembeddings, these systems improve the handling of noisy or unstructured data, enabling more\\naccurate interpretation and contextual understanding.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 51, 'page_label': '52', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='embeddings, these systems improve the handling of noisy or unstructured data, enabling more\\naccurate interpretation and contextual understanding.\\n– Advanced Captioning and Description Generation: To improve multimodal data inte-\\ngration, particularly in document parsing, enhancing automated captioning and description\\ngeneration for non-textual elements like images, tables, and charts is critical. Leveraging state-\\nof-the-art vision-language models (VLMs) and MLLMs can boost the accuracy and contextual\\nrelevance of textual descriptions. These models bridge the gap between visual and textual\\ndata, enabling more comprehensive document understanding. Integrating domain-specific\\nknowledge into captioning models is essential for generating accurate and contextually tai-\\nlored descriptions. This can be achieved by fine-tuning pre-trained models on domain-specific\\ndatasets or incorporating external knowledge bases. Such an approach ensures that descrip-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 51, 'page_label': '52', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='lored descriptions. This can be achieved by fine-tuning pre-trained models on domain-specific\\ndatasets or incorporating external knowledge bases. Such an approach ensures that descrip-\\ntions align with the document’s content, enhancing the utility of multimodal data integration.\\n•Leveraging LLMs and MLLMs for Enhanced Parsing:\\n– LLM/MLLM-Driven Parsing and Indexing: LLMs and MLLMs can be fine-tuned on domain-\\nspecific corpora to improve their ability to parse and interpret complex document structures.\\nLeveraging their advanced multimodal understanding, these models can accurately identify\\nand extract key information—such as legal clauses, scientific hypotheses, or technical speci-\\nfications—even from dense or unstructured text. Fine-tuning enhances their proficiency in\\nrecognizing domain-specific terminology, relationships, and contextual nuances. Furthermore,\\nLLMs and MLLMs can generate metadata, tags, and summaries. By automatically annotat-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 51, 'page_label': '52', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='recognizing domain-specific terminology, relationships, and contextual nuances. Furthermore,\\nLLMs and MLLMs can generate metadata, tags, and summaries. By automatically annotat-\\ning documents with relevant keywords, classifications, or concise summaries, these models\\nstreamline the organization and accessibility of large document repositories. This capability\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 52, 'page_label': '53', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 53\\nis particularly valuable in applications like legal case management, academic research, and\\nenterprise knowledge bases. In multimodal contexts, MLLMs extend these capabilities by\\nintegrating and interpreting data from diverse sources, such as text, images, tables, and dia-\\ngrams. This enables a more comprehensive parsing process, where visual and textual elements\\nare jointly analyzed to extract richer, more accurate information. For example, in scientific\\ndocuments, MLLMs can parse and correlate data from textual descriptions and accompanying\\ncharts, facilitating a deeper understanding of the content.\\n– Bridging the Gap Between LLMs and MLLMs: A promising approach involves hybrid\\narchitectures that combine the strengths of MLLMs and LLMs. MLLMs process raw multimodal\\ninputs (e.g., images, audio, video) to extract meaningful representations, while LLMs generate'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 52, 'page_label': '53', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='architectures that combine the strengths of MLLMs and LLMs. MLLMs process raw multimodal\\ninputs (e.g., images, audio, video) to extract meaningful representations, while LLMs generate\\ncoherent and contextually accurate textual outputs. This division of labor optimizes perfor-\\nmance, as MLLMs excel in multimodal feature extraction and LLMs in linguistic precision. For\\nexample, in document parsing, MLLMs analyze visual layouts, tables, or embedded graphics,\\nwhile LLMs synthesize this information into structured textual formats.\\n7.2 Multimodal Search Planning\\nThe future of multimodal search planning should focus on addressing three key challenges within\\nthe hierarchical framework: intelligent adaptive planning mechanisms, query reformulation and\\nsemantic alignment, and comprehensive evaluation benchmarks. Below are targeted suggestions\\nfor advancing each area.\\n•Intelligent Adaptive Planning Mechanisms:'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 52, 'page_label': '53', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='semantic alignment, and comprehensive evaluation benchmarks. Below are targeted suggestions\\nfor advancing each area.\\n•Intelligent Adaptive Planning Mechanisms:\\n– Multi-Agent Collaborative Systems: To address the challenges of multimodal search and\\ncomplex query resolution, multi-agent collaborative systems can be designed to leverage\\nspecialized agents working in tandem. These systems enhance efficiency, adaptability, and\\nrobustness in handling multi-hop, creative, or cross-modal queries. Key mechanisms include:\\n1) Parallel Reasoning Paths: Specialized agents can simultaneously explore multiple reasoning\\ntrajectories, enabling faster and more comprehensive solutions. This approach is particularly\\neffective for multi-hop queries, where intermediate reasoning steps are critical, or for creative\\ntasks requiring diverse perspectives. By evaluating multiple pathways in parallel, the system\\ncan identify optimal solutions while mitigating the risk of local optima. 2) Complementary Re-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 52, 'page_label': '53', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='tasks requiring diverse perspectives. By evaluating multiple pathways in parallel, the system\\ncan identify optimal solutions while mitigating the risk of local optima. 2) Complementary Re-\\ntrieval Strategies: Agents can employ diverse retrieval methodologies, such as keyword-based,\\nsemantic, or cross-modal retrieval, to address different aspects of a query. For instance, one\\nagent might focus on extracting structured data, while another leverages semantic embeddings\\nor visual-textual alignment for multimodal contexts. The synthesis of these strategies ensures\\nrobust and contextually relevant search outcomes, enhancing the system’s ability to handle\\nheterogeneous data sources. 3) Dynamic Resource Allocation: Agents can monitor computa-\\ntional resources and system constraints in real-time, dynamically adjusting retrieval strategies\\nto optimize performance. For example, under limited computational bandwidth, agents might'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 52, 'page_label': '53', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='tional resources and system constraints in real-time, dynamically adjusting retrieval strategies\\nto optimize performance. For example, under limited computational bandwidth, agents might\\nprioritize lightweight retrieval methods or redistribute tasks to balance load. This adaptive\\nmechanism ensures efficient resource utilization while maintaining high-quality query resolu-\\ntion. 4) Integration with MLLMs and LLMs: The collaborative multi-agent framework can be\\nseamlessly integrated with MLLMs and LLMs to enhance their capabilities. MLLMs can serve\\nas central orchestrators, interpreting multimodal inputs and coordinating agent tasks, while\\nLLMs provide deep contextual understanding and reasoning support. This integration enables\\nthe system to handle complex, multimodal queries with greater precision and adaptability.\\n– Hierarchical Planning Frameworks: To address the challenges of ambiguous or highly\\ncreative queries in multimodal search and planning, integrating human feedback into the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 52, 'page_label': '53', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='– Hierarchical Planning Frameworks: To address the challenges of ambiguous or highly\\ncreative queries in multimodal search and planning, integrating human feedback into the\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 53, 'page_label': '54', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='54 Trovato et al.\\ndecision-making process is essential. Human-in-the-loop (HITL) systems facilitate iterative\\nrefinement by leveraging user expertise to guide and validate intermediate results. These\\ninteractive systems enable users to dynamically adjust search parameters, prioritize modalities,\\nor correct misinterpretations, ensuring more accurate and contextually relevant outcomes. By\\ncombining the strengths of multimodal large language models (MLLMs) with human intuition,\\nHITL systems enhance adaptability, build trust, and improve the robustness of intelligent\\nplanning frameworks. This collaborative approach is particularly valuable in domains requiring\\nnuanced understanding, creativity, or domain-specific knowledge.\\n– Reinforcement Learning for Adaptation: Intelligent adaptive planning mechanisms can\\nbe developed using reinforcement learning (RL) to enable dynamic, context-aware decision-\\nmaking. By modeling the search process as a sequential decision problem, RL agents can'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 53, 'page_label': '54', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='be developed using reinforcement learning (RL) to enable dynamic, context-aware decision-\\nmaking. By modeling the search process as a sequential decision problem, RL agents can\\nbe trained to optimize resource allocation and retrieval accuracy. These agents adapt their\\nstrategies by receiving rewards for minimizing computational overhead, reducing latency, and\\ndelivering precise results tailored to query characteristics, such as modality, complexity, and\\nuser intent.\\n•Query Reformulation and Semantic Alignment:\\n– Multi-Perspective Reformulation: To address the complexity of multimodal search, query\\nreformulation strategies must generate diverse interpretations while preserving the original\\nintent across modalities. This involves: 1) Contextual Understanding: Leveraging contextual\\nembeddings (e.g., from transformer-based models) to capture semantic nuances and contex-\\ntual dependencies, ensuring reformulated queries retain the richness of the original input. 2)'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 53, 'page_label': '54', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='embeddings (e.g., from transformer-based models) to capture semantic nuances and contex-\\ntual dependencies, ensuring reformulated queries retain the richness of the original input. 2)\\nCross-Modal Alignment: Employing advanced techniques like contrastive learning to align rep-\\nresentations across text, images, and audio modalities. By embedding queries and multimodal\\ndata into a shared latent space, this ensures consistent interpretation and retrieval across\\ndiverse data types. 3) Domain-Specific Knowledge Integration: Incorporating domain-specific\\nontologies or knowledge graphs to enhance reformulation accuracy, particularly in specialized\\nfields. This leverages structured domain knowledge to improve the relevance and precision of\\nreformulated queries.\\n– Interactive Query Refinement: A pivotal strategy is interactive query refinement, which\\nallows users to iteratively adjust queries based on intermediate results. Intelligent systems can'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 53, 'page_label': '54', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='– Interactive Query Refinement: A pivotal strategy is interactive query refinement, which\\nallows users to iteratively adjust queries based on intermediate results. Intelligent systems can\\nfacilitate this process by suggesting alternative query formulations, identifying ambiguities,\\nand providing contextual feedback to better align queries with user intent. By integrating user\\nfeedback loops and real-time semantic analysis, these systems dynamically bridge the gap\\nbetween user input and multimodal data, ensuring more precise and contextually relevant\\nsearch outcomes.\\n– Explainable Reformulation: A critical aspect of query reformulation is its explainability.\\nBy offering clear and concise explanations for how queries are transformed, users gain insight\\ninto the system’s reasoning and decision-making processes. For example, when a user submits\\na vague or ambiguous query, the system can generate a reformulated version while detailing'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 53, 'page_label': '54', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='into the system’s reasoning and decision-making processes. For example, when a user submits\\na vague or ambiguous query, the system can generate a reformulated version while detailing\\nthe rationale behind the changes, such as term disambiguation, incorporation of contextual\\ncues, or alignment with multimodal data (e.g., text, images, or audio). This transparency fosters\\nuser trust, enables validation of the system’s interpretation, and enhances user control and\\nsatisfaction. Furthermore, explainable reformulation underscores the importance of semantic\\nalignment, where the system bridges the gap between user intent and the underlying data\\nrepresentation, ensuring the reformulated query accurately reflects the user’s needs.\\n•Comprehensive Evaluation Benchmarks:\\n– Diverse Query Datasets: It is crucial to establish robust benchmarks. These benchmarks\\nshould incorporate diverse query datasets spanning a wide range of query types, from simple'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 53, 'page_label': '54', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='– Diverse Query Datasets: It is crucial to establish robust benchmarks. These benchmarks\\nshould incorporate diverse query datasets spanning a wide range of query types, from simple\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 54, 'page_label': '55', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 55\\nfactual retrievals to complex multi-hop reasoning and creative tasks. The datasets must\\nreflect real-world heterogeneity in query patterns and modalities, capturing the intricacies of\\nuser interactions across text, image, audio, and video inputs. By integrating such diversity,\\nbenchmarks can more accurately assess model performance, generalization capabilities, and\\nadaptability to varied real-world applications.\\n– Multi-Dimensional Metrics: To ensure the effectiveness and reliability of Multimodal Search\\nPlanning systems, robust evaluation frameworks must be established. These frameworks\\nshould employ multi-dimensional metrics to comprehensively assess system performance\\nacross diverse operational scenarios. Key dimensions include: 1) Adaptability: The system’s\\nability to handle a broad spectrum of query types, from simple to highly complex, while'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 54, 'page_label': '55', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='across diverse operational scenarios. Key dimensions include: 1) Adaptability: The system’s\\nability to handle a broad spectrum of query types, from simple to highly complex, while\\nintegrating multiple modalities (e.g., text, images, audio). This metric evaluates the model’s\\nflexibility in addressing varied user needs and its capacity to generalize across domains. 2)\\nRobustness: The system’s resilience under challenging conditions, such as computational con-\\nstraints, noisy or incomplete inputs, and adversarial scenarios. Robustness ensures consistent\\nperformance in real-world applications, where ideal conditions are seldom present. 3) Effi-\\nciency: The optimization of resource utilization (e.g., memory, processing power) and response\\ntime. This metric is critical for scalability and user satisfaction, especially in time-sensitive\\nor resource-constrained environments. 4) Semantic Alignment: The system’s accuracy in'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 54, 'page_label': '55', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='time. This metric is critical for scalability and user satisfaction, especially in time-sensitive\\nor resource-constrained environments. 4) Semantic Alignment: The system’s accuracy in\\npreserving and interpreting the intent of user queries during reformulation or multimodal\\nintegration. This ensures that outputs remain contextually and semantically aligned with the\\nuser’s original request.\\n7.3 Retrieval\\nThe challenges in multimodal retrieval underscore the complexity of integrating and retrieving\\ninformation across diverse data types. To address these issues and advance the field, future research\\nshould prioritize the following directions:\\n•Unified Cross-Modal Representation Learning : The primary objective is to develop robust\\nand unified representation learning frameworks that effectively align and compare data across\\ndiverse modalities, including text, images, audio, and video. A key element of this framework is'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 54, 'page_label': '55', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='and unified representation learning frameworks that effectively align and compare data across\\ndiverse modalities, including text, images, audio, and video. A key element of this framework is\\nthe enhancement of cross-modal attention mechanisms to better model complex interactions\\nbetween modalities. Cross-modal attention layers, inspired by transformer architectures, are\\ncentral to capturing fine-grained relationships. These mechanisms enable one modality to focus\\non relevant features in another, allowing the model to dynamically prioritize the most informative\\naspects of the data. For example, text can guide attention over visual regions, or audio cues can\\nemphasize relevant temporal segments in video data. Techniques such as multi-head cross-modal\\nattention and hierarchical attention further refine this process, ensuring robust and context-aware\\nrepresentations.\\n•Cross-Modal Context: The primary objective is to improve the ability of models to perform'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 54, 'page_label': '55', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='representations.\\n•Cross-Modal Context: The primary objective is to improve the ability of models to perform\\nfine-grained interactions between modalities, particularly in the reranking and refinement stages.\\n– Reranker: Multi-Modal Reranking Models rerank retrieved document list by incorporating\\ndetailed cross-modal interactions, such as text-image, text-audio, or video-text relationships.\\nBy integrating LLMs and MLLMs, reranking models enhance their ability to capture nuanced\\nsemantic alignments between modalities.\\n– Refiner: Leveraging their cross-modal reasoning abilities, MLLMs refine retrieval results\\nthrough knowledge-enhanced refinement, yielding more accurate, contextually relevant, and\\nsemantically rich outputs. This refinement process utilizes MLLMs’ contextual understanding\\nand multimodal alignment to re-rank, filter, or augment retrieved content.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 55, 'page_label': '56', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='56 Trovato et al.\\n7.4 Generation\\nThe future of multimodal generation should focus on overcoming existing challenges on multimodal\\ninput and output. Below are key suggestions for advancing multimodal generation systems.\\n•Flexible and Adaptive Multimodal Input Frameworks: To address the growing complexity\\nof multimodal data, it is crucial to develop modality-agnostic architectures that dynamically\\nadapt to diverse and arbitrary input modality combinations, such as text+image, text+video, or\\nimage+audio. These frameworks should process inputs without relying on predefined structures\\nor extensive retraining.\\n•Coherent and Contextually Relevant Multimodal Output: Achieving coherent and con-\\ntextually relevant outputs in multimodal generation necessitates the development of advanced\\nmodels capable of maintaining consistency across modalities. For example, in text-image gen-\\neration tasks, the generated image must precisely align with the textual description, and the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 55, 'page_label': '56', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='models capable of maintaining consistency across modalities. For example, in text-image gen-\\neration tasks, the generated image must precisely align with the textual description, and the\\ntext should accurately reflect the visual content of the image. This cross-modal consistency is\\nessential for ensuring the reliability and usability of multimodal systems.\\n•Intelligent Positioning and Integration of Multimodal Elements: To seamlessly integrate\\nnon-textual elements (e.g., images, videos, audio) into a narrative, models must be trained to\\nidentify optimal insertion points. This requires a deep understanding of the content’s structure,\\nflow, and contextual nuances to ensure coherence, readability, and enhanced user engagement.\\nAdvanced techniques, such as attention mechanisms, can analyze the narrative’s semantic and\\nsyntactic structure, enabling the model to determine where multimodal elements can comple-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 55, 'page_label': '56', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Advanced techniques, such as attention mechanisms, can analyze the narrative’s semantic and\\nsyntactic structure, enabling the model to determine where multimodal elements can comple-\\nment or enrich the text. Modern multimodal systems must dynamically retrieve or generate\\ncontextually relevant non-textual content. For example, when generating a text-image pair,\\nthe model should use cross-modal alignment techniques to either retrieve an existing image\\nfrom a database or synthesize a new one that aligns with the textual context. This relies on\\nrobust multimodal representation learning, where embeddings from different modalities (text,\\nimage, video) are mapped into a shared latent space, enabling precise cross-modal retrieval or\\ngeneration.\\n•Diversity in Multimodal Outputs: Achieving a balance between diversity, relevance, and\\nquality in multimodal generation requires controlled mechanisms. For instance, in text-to-image'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 55, 'page_label': '56', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='generation.\\n•Diversity in Multimodal Outputs: Achieving a balance between diversity, relevance, and\\nquality in multimodal generation requires controlled mechanisms. For instance, in text-to-image\\ngeneration, models should produce diverse yet faithful representations of textual descriptions.\\nTechniques like conditional sampling can guide models to explore varied latent spaces while\\nadhering to input constraints.\\n7.5 Dataset & Evaluation\\nThe future direction of datasets and evaluation in MRAG should focus on addressing current gaps\\nand challenges while harnessing the unique capabilities of MLLMs. Below are refined suggestions\\nfor advancing datasets and evaluation methodologies in this field.\\n•Comprehensive Benchmark Development : To enhance the evaluation of MLLMs and LLMs\\nin retrieval-augmented generation, it is crucial to develop comprehensive benchmarks that\\naddress key limitations in current assessment frameworks. These benchmarks should focus on'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 55, 'page_label': '56', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='in retrieval-augmented generation, it is crucial to develop comprehensive benchmarks that\\naddress key limitations in current assessment frameworks. These benchmarks should focus on\\nthe following areas: 1) Instruction Following: Create tasks to evaluate the model’s ability to\\ncomprehend and execute complex, multi-step instructions across diverse modalities. This includes\\nassessing precision in adhering to nuanced directives and handling ambiguous or incomplete\\ninputs. 2) Multiturn Dialogue: Develop datasets that simulate real-world conversational dynamics,\\nemphasizing the model’s capacity for context retention, coherence, and adaptability over extended\\ninteractions. Scenarios should include cross-modal references and long-term memory challenges.\\n3) Complex Multimodal Reasoning: Design tasks requiring the integration of multiple modalities\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 56, 'page_label': '57', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 57\\n(e.g., text, images, audio) to solve real-world problems, such as interpreting charts, maps, or\\ncombining visual and textual data for decision-making. 4) Creativity Evaluation: Introduce\\nbenchmarks to assess generative capabilities in creative tasks, such as composing stories, poems,\\nor designing visual artifacts from multimodal inputs. These tasks should measure originality,\\nrelevance, and the ability to synthesize diverse inputs into coherent outputs. 5) Diverse Modalities:\\nExpand evaluation frameworks to include emerging modalities like audio, 3D models, and sensor\\ndata, ensuring robustness and versatility in handling a wide range of input types.\\n•Multimodal Retrieval-Augmented Generation : The development of robust metrics for eval-\\nuating retrieval and generation in multimodal systems requires assessing relevance, precision,\\ndiversity, and cross-modal alignment to ensure semantic consistency and contextual appropri-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 56, 'page_label': '57', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='uating retrieval and generation in multimodal systems requires assessing relevance, precision,\\ndiversity, and cross-modal alignment to ensure semantic consistency and contextual appropri-\\nateness. Metrics should quantify the system’s ability to filter noise and redundancy, delivering\\nconcise and meaningful outputs. For generation quality, coherence, fluency, creativity, and adapt-\\nability are essential, alongside factual accuracy and consistency with retrieved data and external\\nknowledge. Effective multimodal integration is crucial to unify diverse inputs into contextually\\nrich outputs. Comprehensive benchmarks must simulate real-world scenarios, incorporating\\nvaried queries, multimodal sources, and differing complexity levels to evaluate the end-to-end\\nperformance of retrieval-augmented generation (RAG) pipelines.\\n8 Conclusion\\nIn conclusion, this survey comprehensively examines the emerging field of Multimodal Retrieval-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 56, 'page_label': '57', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='performance of retrieval-augmented generation (RAG) pipelines.\\n8 Conclusion\\nIn conclusion, this survey comprehensively examines the emerging field of Multimodal Retrieval-\\nAugmented Generation (MRAG), highlighting its potential to enhance the capabilities of large\\nlanguage models (LLMs) by integrating multimodal data such as text, images, and videos. Unlike\\ntraditional text-based RAG systems, MRAG addresses the challenges of retrieving and generat-\\ning information across different modalities, thereby improving the accuracy and relevance of\\nresponses while reducing hallucinations. The survey systematically analyzes MRAG from four key\\nperspectives: essential components and technologies, datasets, evaluation methods and metrics,\\nand existing limitations. It identifies current challenges, such as effectively integrating multimodal\\nknowledge and ensuring the reliability of generated outputs, while also proposing future research'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 56, 'page_label': '57', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='knowledge and ensuring the reliability of generated outputs, while also proposing future research\\ndirections. By providing a structured overview and forward-looking insights, this survey aims to\\nguide researchers in advancing MRAG, ultimately contributing to the development of more robust\\nand versatile Multimodal Retrieval-Augmented Generation.\\nReferences\\n[1] [n. d.]. jsoup. https://jsoup.org/\\n[2] [n. d.]. pdfminer. https://github.com/pdfminer/pdfminer.six\\n[3] [n. d.]. PyMuPDF. https://github.com/pymupdf/PyMuPDF\\n[4] [n. d.]. realworldQA. https://huggingface.co/datasets/visheratin/realworldqa\\n[5] Ossama Abdel-Hamid, Abdel-rahman Mohamed, Hui Jiang, Li Deng, Gerald Penn, and Dong Yu. 2014. Convolutional\\nneural networks for speech recognition. IEEE/ACM Transactions on audio, speech, and language processing 22, 10\\n(2014), 1533–1545.\\n[6] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 56, 'page_label': '57', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='(2014), 1533–1545.\\n[6] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree,\\nArash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin\\nCai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao\\nCheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao,\\nAmit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett,\\nWenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero\\nKauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li,\\nYunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong\\nLiu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio César Teodoro Mendes,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 56, 'page_label': '57', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio César Teodoro Mendes,\\nArindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid\\nPryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase,\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 57, 'page_label': '58', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='58 Trovato et al.\\nOlli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen,\\nSwadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua\\nWang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu,\\nXiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei\\nYang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang,\\nYi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. 2024. Phi-3 Technical Report: A Highly Capable Language\\nModel Locally on Your Phone. arXiv:2404.14219 [cs.CL] https://arxiv.org/abs/2404.14219\\n[7] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774\\n(2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 57, 'page_label': '58', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774\\n(2023).\\n[8] Emanuele Aiello, Lili Yu, Yixin Nie, Armen Aghajanyan, and Barlas Oguz. 2023. Jointly training large autoregressive\\nmultimodal models. arXiv preprint arXiv:2309.15564 (2023).\\n[9] Akiko Aizawa. 2003. An information-theoretic perspective of tf–idf measures. Information Processing & Management\\n39, 1 (2003), 45–65.\\n[10] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\\nKatherine Millican, Malcolm Reynolds, et al. 2022. Flamingo: a visual language model for few-shot learning. Advances\\nin neural information processing systems 35 (2022), 23716–23736.\\n[11] Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Guimin Hu, Weimin Lyu,\\nLijie Hu, Lu Yu, et al. 2024. Prompt-saw: Leveraging relation-aware graphs for textual prompt compression. arXiv'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 57, 'page_label': '58', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Lijie Hu, Lu Yu, et al. 2024. Prompt-saw: Leveraging relation-aware graphs for textual prompt compression. arXiv\\npreprint arXiv:2404.00489 (2024).\\n[12] Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and R Manmatha. 2021. Docformer: End-to-end\\ntransformer for document understanding. In Proceedings of the IEEE/CVF international conference on computer vision .\\n993–1003.\\n[13] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph,\\nBen Mann, Nova DasSarma, et al. 2021. A general language assistant as a laboratory for alignment. arXiv preprint\\narXiv:2112.00861 (2021).\\n[14] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton,\\nSamir Gadre, Shiori Sagawa, et al. 2023. Openflamingo: An open-source framework for training large autoregressive\\nvision-language models. arXiv preprint arXiv:2308.01390 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 57, 'page_label': '58', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Samir Gadre, Shiori Sagawa, et al. 2023. Openflamingo: An open-source framework for training large autoregressive\\nvision-language models. arXiv preprint arXiv:2308.01390 (2023).\\n[15] Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk Lee. 2019. Character Region Awareness for\\nText Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) .\\n[16] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\\nZhou. 2023. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966\\n1, 2 (2023), 3.\\n[17] Shuai Bai, Shusheng Yang, Jinze Bai, Peng Wang, Xingxuan Zhang, Junyang Lin, Xinggang Wang, Chang Zhou,\\nand Jingren Zhou. 2023. Touchstone: Evaluating vision-language models by language models. arXiv preprint\\narXiv:2308.16890 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 57, 'page_label': '58', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='and Jingren Zhou. 2023. Touchstone: Evaluating vision-language models by language models. arXiv preprint\\narXiv:2308.16890 (2023).\\n[18] Liping Bao, Longhui Wei, Wengang Zhou, Lin Liu, Lingxi Xie, Houqiang Li, and Qi Tian. 2023. Multi-Granularity\\nMatching Transformer for Text-Based Person Search. IEEE Transactions on Multimedia (2023).\\n[19] Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Scott Yih, Sebastian Riedel, and Fabio Petroni. 2022. Autore-\\ngressive search engines: Generating substrings as document identifiers. Advances in Neural Information Processing\\nSystems 35 (2022), 31668–31683.\\n[20] Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann,\\nIbrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda\\nKoppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 57, 'page_label': '58', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos,\\nRishabh Kabra, Matthias Bauer, Matko Bošnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana\\nBalazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and\\nXiaohua Zhai. 2024. PaliGemma: A versatile 3B VLM for transfer. arXiv:2407.07726 [cs.CV] https://arxiv.org/abs/\\n2407.07726\\n[21] Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, and\\nLudwig Schmidt. 2023. Visit-bench: A benchmark for vision-language instruction following inspired by real-world\\nuse. arXiv preprint arXiv:2308.06595 (2023).\\n[22] Chinmoy B Bose and Shyh-Shiaw Kuo. 1994. Connected and degraded text recognition using hidden Markov model.\\nPattern Recognition 27, 10 (1994), 1345–1363.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 57, 'page_label': '58', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[22] Chinmoy B Bose and Shyh-Shiaw Kuo. 1994. Connected and degraded text recognition using hidden Markov model.\\nPattern Recognition 27, 10 (1994), 1345–1363.\\n[23] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\\nPranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural\\ninformation processing systems 33 (2020), 1877–1901.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 58, 'page_label': '59', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 59\\n[24] Davide Caffagni, Federico Cocchi, Nicholas Moratelli, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara.\\n2024. Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs. InProceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition . 1818–1826.\\n[25] Zhiwei Cao, Qian Cao, Yu Lu, Ningxin Peng, Luyang Huang, Shanbo Cheng, and Jinsong Su. 2024. Retaining key\\ninformation under high compression ratios: Query-guided compressor for llms. arXiv preprint arXiv:2406.02376\\n(2024).\\n[26] Bing-Bing Chai, Jozsef Vass, and Xinhua Zhuang. 1999. Significance-linked connected component analysis for wavelet\\nimage coding. IEEE Transactions on Image processing 8, 6 (1999), 774–784.\\n[27] Wei-Cheng Chang, Felix X Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. 2020. Pre-training tasks for\\nembedding-based large-scale retrieval. arXiv preprint arXiv:2002.03932 (2020).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 58, 'page_label': '59', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[27] Wei-Cheng Chang, Felix X Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. 2020. Pre-training tasks for\\nembedding-based large-scale retrieval. arXiv preprint arXiv:2002.03932 (2020).\\n[28] Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk. 2022. Webqa: Multihop\\nand multimodal qa. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 16495–16504.\\n[29] Yunhang Shen Yulei Qin Mengdan Zhang Xu Lin Jinrui Yang Xiawu Zheng Ke Li Xing Sun Yunsheng Wu Rongrong Ji\\nChaoyou Fu, Peixian Chen. 2023. Mme: A comprehensive evaluation benchmark for multimodal large language\\nmodels. arXiv preprint arXiv:2306.13394 (2023).\\n[30] Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu. 2023. X-llm: Bootstrap-\\nping advanced large language models by treating multi-modalities as foreign languages.arXiv preprint arXiv:2305.04160\\n(2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 58, 'page_label': '59', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='ping advanced large language models by treating multi-modalities as foreign languages.arXiv preprint arXiv:2305.04160\\n(2023).\\n[31] Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, and Liqiang Nie. 2024. Lion: Empowering multimodal large\\nlanguage model with dual-level visual knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition . 26540–26550.\\n[32] Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. 2023. Walking down the memory maze:\\nBeyond context limit through interactive reading. arXiv preprint arXiv:2310.05029 (2023).\\n[33] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking large language models in retrieval-augmented\\ngeneration. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 38. 17754–17762.\\n[34] Jieneng Chen, Luoxin Ye, Ju He, Zhaoyang Wang, Daniel Khashabi, and Alan L Yuille. 2024. Efficient large multi-modal'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 58, 'page_label': '59', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[34] Jieneng Chen, Luoxin Ye, Ju He, Zhaoyang Wang, Daniel Khashabi, and Alan L Yuille. 2024. Efficient large multi-modal\\nmodels via visual context compression. Advances in Neural Information Processing Systems 37 (2024), 73986–74007.\\n[35] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas\\nChandra, Yunyang Xiong, and Mohamed Elhoseiny. 2023. Minigpt-v2: large language model as a unified interface for\\nvision-language multi-task learning. arXiv preprint arXiv:2310.09478 (2023).\\n[36] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. 2023. Shikra: Unleashing multimodal\\nllm’s referential dialogue magic. arXiv preprint arXiv:2306.15195 (2023).\\n[37] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2024. Sharegpt4v:\\nImproving large multi-modal models with better captions. In European Conference on Computer Vision . Springer,\\n370–387.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 58, 'page_label': '59', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Improving large multi-modal models with better captions. In European Conference on Computer Vision . Springer,\\n370–387.\\n[38] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao,\\nDahua Lin, et al. 2024. Are We on the Right Way for Evaluating Large Vision-Language Models? arXiv preprint\\narXiv:2403.20330 (2024).\\n[39] Shaoxiang Chen, Zequn Jie, and Lin Ma. 2024. Llava-mole: Sparse mixture of lora experts for mitigating data conflicts\\nin instruction finetuning mllms. arXiv preprint arXiv:2401.16160 (2024).\\n[40] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz,\\nSebastian Goodman, Xiao Wang, Yi Tay, et al. 2023. Pali-x: On scaling up a multilingual vision and language model.\\narXiv preprint arXiv:2305.18565 (2023).\\n[41] Xingyu Chen, Zihan Zhao, Lu Chen, Danyang Zhang, Jiabao Ji, Ao Luo, Yuxuan Xiong, and Kai Yu. 2021. Websrc: A'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 58, 'page_label': '59', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='arXiv preprint arXiv:2305.18565 (2023).\\n[41] Xingyu Chen, Zihan Zhao, Lu Chen, Danyang Zhang, Jiabao Ji, Ao Luo, Yuxuan Xiong, and Kai Yu. 2021. Websrc: A\\ndataset for web-based structural reading comprehension. arXiv preprint arXiv:2101.09465 (2021).\\n[42] Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-Wei Chang. 2023. Can\\npre-trained vision and language models answer visual information-seeking questions? arXiv preprint arXiv:2302.11713\\n(2023).\\n[43] Yiqun Chen, Qi Liu, Yi Zhang, Weiwei Sun, Xinyu Ma, Wei Yang, Daiting Shi, Jiaxin Mao, and Dawei Yin. 2024.\\nTourrank: Utilizing large language models for documents ranking with a tournament-inspired strategy.arXiv preprint\\narXiv:2406.11678 (2024).\\n[44] Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran. 2024. Dress: Instructing large vision-\\nlanguage models to align and interact with humans via natural language feedback. In Proceedings of the IEEE/CVF'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 58, 'page_label': '59', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='language models to align and interact with humans via natural language feedback. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition . 14239–14250.\\n[45] Yiqun Chen, Lingyong Yan, Weiwei Sun, Xinyu Ma, Yi Zhang, Shuaiqiang Wang, Dawei Yin, Yiming Yang, and Jiaxin\\nMao. 2025. Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning. arXiv preprint\\narXiv:2501.15228 (2025).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 59, 'page_label': '60', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='60 Trovato et al.\\n[46] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020.\\nUniter: Universal image-text representation learning. In European conference on computer vision . Springer, 104–120.\\n[47] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu,\\nLewei Lu, et al. 2024. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In\\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition . 24185–24198.\\n[48] Xin Cheng, Xun Wang, Xingxing Zhang, Tao Ge, Si-Qing Chen, Furu Wei, Huishuai Zhang, and Dongyan Zhao. 2024.\\nxrag: Extreme context compression for retrieval-augmented generation with one token.arXiv preprint arXiv:2405.13792\\n(2024).\\n[49] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. 2023. Adapting language models to compress\\ncontexts. arXiv preprint arXiv:2305.14788 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 59, 'page_label': '60', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='(2024).\\n[49] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. 2023. Adapting language models to compress\\ncontexts. arXiv preprint arXiv:2305.14788 (2023).\\n[50] Kyunghyun Cho. 2014. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint\\narXiv:1409.1259 (2014).\\n[51] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and\\nYoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation.\\narXiv preprint arXiv:1406.1078 (2014).\\n[52] Sukmin Cho, Soyeong Jeong, Jeongyeon Seo, and Jong C Park. 2023. Discrete prompt optimization via constrained\\ngeneration for zero-shot re-ranker. arXiv preprint arXiv:2305.13729 (2023).\\n[53] Eunbi Choi, Yongrae Jo, Joel Jang, and Minjoon Seo. 2022. Prompt injection: Parameterization of fixed inputs. arXiv\\npreprint arXiv:2206.11349 (2022).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 59, 'page_label': '60', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[53] Eunbi Choi, Yongrae Jo, Joel Jang, and Minjoon Seo. 2022. Prompt injection: Parameterization of fixed inputs. arXiv\\npreprint arXiv:2206.11349 (2022).\\n[54] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang,\\nXiaolin Wei, et al. 2023. Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices.\\narXiv preprint arXiv:2312.16886 1, 2 (2023), 3.\\n[55] Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang\\nLin, Bo Zhang, et al. 2024. Mobilevlm v2: Faster and stronger baseline for vision language model. arXiv preprint\\narXiv:2402.03766 (2024).\\n[56] Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, and Xia Hu. 2024. Learning to compress\\nprompt in natural language formats. arXiv preprint arXiv:2402.18700 (2024).\\n[57] Sanghyuk Chun, Seong Joon Oh, Rafael Sampaio De Rezende, Yannis Kalantidis, and Diane Larlus. 2021. Probabilistic'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 59, 'page_label': '60', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='prompt in natural language formats. arXiv preprint arXiv:2402.18700 (2024).\\n[57] Sanghyuk Chun, Seong Joon Oh, Rafael Sampaio De Rezende, Yannis Kalantidis, and Diane Larlus. 2021. Probabilistic\\nembeddings for cross-modal retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition. 8415–8424.\\n[58] Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. 2023. Holistic\\nanalysis of hallucination in gpt-4v (ision): Bias and interference challenges. arXiv preprint arXiv:2311.03287 (2023).\\n[59] Zhuyun Dai and Jamie Callan. 2020. Context-aware document term weighting for ad-hoc search. In Proceedings of\\nThe Web Conference 2020 . 1897–1907.\\n[60] Zhuyun Dai and Jamie Callan. 2020. Context-aware term weighting for first stage passage retrieval. In Proceedings of\\nthe 43rd International ACM SIGIR conference on research and development in Information Retrieval . 1533–1536.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 59, 'page_label': '60', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='the 43rd International ACM SIGIR conference on research and development in Information Retrieval . 1533–1536.\\n[61] Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2020. Autoregressive entity retrieval. arXiv\\npreprint arXiv:2010.00904 (2020).\\n[62] Jacob Devlin. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint\\narXiv:1810.04805 (2018).\\n[63] SeungHeon Doh, Minhee Lee, Dasaem Jeong, and Juhan Nam. 2024. Enriching Music Descriptions with A Finetuned-\\nLLM and Metadata for Text-to-Music Retrieval. InICASSP 2024-2024 IEEE International Conference on Acoustics, Speech\\nand Signal Processing (ICASSP) . IEEE, 826–830.\\n[64] Jianfeng Dong, Xirong Li, and Cees GM Snoek. 2018. Predicting visual features from text for image and video caption\\nretrieval. IEEE Transactions on Multimedia 20, 12 (2018), 3377–3388.\\n[65] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 59, 'page_label': '60', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='retrieval. IEEE Transactions on Multimedia 20, 12 (2018), 3377–3388.\\n[65] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu\\nZhou, Haoran Wei, et al . 2023. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint\\narXiv:2309.11499 (2023).\\n[66] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong\\nDuan, Maosong Cao, et al. 2024. Internlm-xcomposer2: Mastering free-form text-image composition and comprehen-\\nsion in vision-language large model. arXiv preprint arXiv:2401.16420 (2024).\\n[67] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong\\nDuan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen,\\nConghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. 2024. InternLM-XComposer2: Mastering'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 59, 'page_label': '60', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. 2024. InternLM-XComposer2: Mastering\\nFree-form Text-Image Composition and Comprehension in Vision-Language Large Model. arXiv:2401.16420 [cs.CV]\\nhttps://arxiv.org/abs/2401.16420\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 60, 'page_label': '61', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 61\\n[68] Alexey Dosovitskiy. 2020. An image is worth 16x16 words: Transformers for image recognition at scale.arXiv preprint\\narXiv:2010.11929 (2020).\\n[69] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson,\\nQuan Vuong, Tianhe Yu, Wenlong Huang, et al. 2023. Palm-e: An embodied multimodal language model. (2023).\\n[70] Andrew Drozdov, Honglei Zhuang, Zhuyun Dai, Zhen Qin, Razieh Rahimi, Xuanhui Wang, Dana Alon, Mohit Iyyer,\\nAndrew McCallum, Donald Metzler, et al . 2023. PaRaDe: Passage ranking using demonstrations with LLMs. In\\nFindings of the Association for Computational Linguistics: EMNLP 2023 . 14242–14252.\\n[71] Yifan Du, Kun Zhou, Yuqi Huo, Yifan Li, Wayne Xin Zhao, Haoyu Lu, Zijia Zhao, Bingning Wang, Weipeng Chen,\\nand Ji-Rong Wen. 2024. Towards Event-oriented Long Video Understanding. arXiv preprint arXiv:2406.14129 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 60, 'page_label': '61', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='and Ji-Rong Wen. 2024. Towards Event-oriented Long Video Understanding. arXiv preprint arXiv:2406.14129 (2024).\\n[72] Martin Engilberge, Louis Chevallier, Patrick Pérez, and Matthieu Cord. 2018. Finding beans in burgers: Deep semantic-\\nvisual embedding with localization. In Proceedings of the IEEE conference on computer vision and pattern recognition .\\n3984–3993.\\n[73] Boris Epshtein, Eyal Ofek, and Yonatan Wexler. 2010. Detecting text in natural scenes with stroke width transform.\\nIn 2010 IEEE computer society conference on computer vision and pattern recognition . IEEE, 2963–2970.\\n[74] Shahul Es, Jithin James, Luis Espinosa Anke, and Steven Schockaert. 2024. Ragas: Automated evaluation of re-\\ntrieval augmented generation. In Proceedings of the 18th Conference of the European Chapter of the Association for\\nComputational Linguistics: System Demonstrations . 150–158.\\n[75] Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. 2017. Vse++: Improving visual-semantic embeddings'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 60, 'page_label': '61', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Computational Linguistics: System Demonstrations . 150–158.\\n[75] Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. 2017. Vse++: Improving visual-semantic embeddings\\nwith hard negatives. arXiv preprint arXiv:1707.05612 (2017).\\n[76] Minghui Fang, Shengpeng Ji, Jialong Zuo, Hai Huang, Yan Xia, Jieming Zhu, Xize Cheng, Xiaoda Yang, Wenrui Liu,\\nGang Wang, et al. 2024. Ace: A generative cross-modal retrieval framework with coarse-to-fine semantic modeling.\\narXiv preprint arXiv:2406.17507 (2024).\\n[77] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. 2024. MMBench-Video:\\nA Long-Form Multi-Shot Benchmark for Holistic Video Understanding. arXiv preprint arXiv:2406.14515 (2024).\\n[78] Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, and Pierre Colombo. 2024.\\nColPali: Efficient Document Retrieval with Vision Language Models. arXiv:2407.01449 [cs.IR] https://arxiv.org/abs/\\n2407.01449'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 60, 'page_label': '61', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='ColPali: Efficient Document Retrieval with Vision Language Models. arXiv:2407.01449 [cs.IR] https://arxiv.org/abs/\\n2407.01449\\n[79] Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, and Pierre Colombo. 2024.\\nColpali: Efficient document retrieval with vision language models. In The Thirteenth International Conference on\\nLearning Representations .\\n[80] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with\\nsimple and efficient sparsity. Journal of Machine Learning Research 23, 120 (2022), 1–39.\\n[81] Hao Feng, Qi Liu, Hao Liu, Jingqun Tang, Wengang Zhou, Houqiang Li, and Can Huang. 2024. Docpedia: Unleashing\\nthe power of large multimodal model in the frequency domain for versatile document understanding. Science China\\nInformation Sciences 67, 12 (2024), 1–14.\\n[82] Paolo Ferragina and Giovanni Manzini. 2000. Opportunistic data structures with applications. In Proceedings 41st'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 60, 'page_label': '61', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Information Sciences 67, 12 (2024), 1–14.\\n[82] Paolo Ferragina and Giovanni Manzini. 2000. Opportunistic data structures with applications. In Proceedings 41st\\nannual symposium on foundations of computer science . IEEE, 390–398.\\n[83] Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and Stéphane Clinchant. 2021. SPLADE v2: Sparse lexical\\nand expansion model for information retrieval. arXiv preprint arXiv:2109.10086 (2021).\\n[84] Thibault Formal, Benjamin Piwowarski, and Stéphane Clinchant. 2021. SPLADE: Sparse lexical and expansion model\\nfor first stage ranking. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in\\nInformation Retrieval . 2288–2292.\\n[85] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang\\nShen, Mengdan Zhang, et al. 2024. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal\\nllms in video analysis. arXiv preprint arXiv:2405.21075 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 60, 'page_label': '61', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Shen, Mengdan Zhang, et al. 2024. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal\\nllms in video analysis. arXiv preprint arXiv:2405.21075 (2024).\\n[86] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu Ma,\\nand Ranjay Krishna. 2025. Blink: Multimodal large language models can see but not perceive. In European Conference\\non Computer Vision . Springer, 148–166.\\n[87] Zheren Fu, Zhendong Mao, Yan Song, and Yongdong Zhang. 2023. Learning semantic relationship among instances\\nfor image-text matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\\n15159–15168.\\n[88] Zheren Fu, Lei Zhang, Hou Xia, and Zhendong Mao. 2024. Linguistic-Aware Patch Slimming Framework for Fine-\\ngrained Cross-Modal Alignment. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\\n26307–26316.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 60, 'page_label': '61', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='grained Cross-Modal Alignment. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\\n26307–26316.\\n[89] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. 2020. Multi-modal transformer for video retrieval.\\nIn Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IV 16 .\\nSpringer, 214–229.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 61, 'page_label': '62', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='62 Trovato et al.\\n[90] Jun Gao, Ziqiang Cao, and Wenjie Li. 2024. SelfCP: Compressing over-limit prompt via the frozen large language\\nmodel itself. Information Processing & Management 61, 6 (2024), 103873.\\n[91] Jun Gao, Ziqiang Cao, and Wenjie Li. 2024. Unifying demonstration selection and compression for in-context learning.\\narXiv preprint arXiv:2405.17062 (2024).\\n[92] Luyu Gao and Jamie Callan. 2021. Condenser: a pre-training architecture for dense retrieval. arXiv preprint\\narXiv:2104.08253 (2021).\\n[93] Luyu Gao and Jamie Callan. 2021. Unsupervised corpus aware language model pre-training for dense passage retrieval.\\narXiv preprint arXiv:2108.05540 (2021).\\n[94] Luyu Gao, Zhuyun Dai, and Jamie Callan. 2021. COIL: Revisit exact lexical match in information retrieval with\\ncontextualized inverted list. arXiv preprint arXiv:2104.07186 (2021).\\n[95] Luyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Benjamin Van Durme, and Jamie Callan. 2021. Complement'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 61, 'page_label': '62', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='contextualized inverted list. arXiv preprint arXiv:2104.07186 (2021).\\n[95] Luyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Benjamin Van Durme, and Jamie Callan. 2021. Complement\\nlexical retrieval model with semantic residual embeddings. In European Conference on Information Retrieval . Springer,\\n146–160.\\n[96] Zhi Gao, Yuntao Du, Xintong Zhang, Xiaojian Ma, Wenjuan Han, Song-Chun Zhu, and Qing Li. 2024. Clova: A\\nclosed-loop visual assistant with tool usage and update. In Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition . 13258–13268.\\n[97] Noa Garcia, Mayu Otani, Chenhui Chu, and Yuta Nakashima. 2020. KnowIT VQA: Answering knowledge-based\\nquestions about videos. In Proceedings of the AAAI conference on artificial intelligence , Vol. 34. 10826–10834.\\n[98] Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. 2013. Optimized product quantization. IEEE transactions on pattern\\nanalysis and machine intelligence 36, 4 (2013), 744–755.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 61, 'page_label': '62', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[98] Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. 2013. Optimized product quantization. IEEE transactions on pattern\\nanalysis and machine intelligence 36, 4 (2013), 744–755.\\n[99] Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. 2023. In-context autoencoder for context\\ncompression in a large language model. arXiv preprint arXiv:2307.06945 (2023).\\n[100] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. 2023. Planting a seed of vision in large language\\nmodel. arXiv preprint arXiv:2307.08041 (2023).\\n[101] Gregor Geigle, Radu Timofte, and Goran Glavaš. 2024. African or European Swallow? Benchmarking Large Vision-\\nLanguage Models for Fine-Grained Object Classification. arXiv preprint arXiv:2406.14496 (2024).\\n[102] Peiyuan Gong, Jiamian Li, and Jiaxin Mao. 2024. Cosearchagent: a lightweight collaborative search agent with large\\nlanguage models. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 61, 'page_label': '62', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='language models. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in\\nInformation Retrieval . 2729–2733.\\n[103] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping\\nLuo, and Kai Chen. 2023. Multimodal-gpt: A vision and language model for dialogue with humans. arXiv preprint\\narXiv:2305.04790 (2023).\\n[104] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the v in vqa matter:\\nElevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition . 6904–6913.\\n[105] Alex Graves and Alex Graves. 2012. Long short-term memory. Supervised sequence labelling with recurrent neural\\nnetworks (2012), 37–45.\\n[106] Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint\\narXiv:2312.00752 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 61, 'page_label': '62', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='networks (2012), 37–45.\\n[106] Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint\\narXiv:2312.00752 (2023).\\n[107] Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu Minzhe, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang,\\nXin Jiang, et al. 2022. Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark. Advances in\\nNeural Information Processing Systems 35 (2022), 26418–26431.\\n[108] Anisha Gunjal, Jihan Yin, and Erhan Bas. 2024. Detecting and preventing hallucinations in large vision language\\nmodels. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 38. 18135–18143.\\n[109] Fang Guo, Wenyu Li, Honglei Zhuang, Yun Luo, Yafu Li, Qi Zhu, Le Yan, and Yue Zhang. 2024. Generating diverse\\ncriteria on-the-fly to improve point-wise LLM rankers. arXiv preprint arXiv:2404.11960 (2024).\\n[110] Weikuo Guo, Huaibo Huang, Xiangwei Kong, and Ran He. 2019. Learning disentangled representation for cross-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 61, 'page_label': '62', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[110] Weikuo Guo, Huaibo Huang, Xiangwei Kong, and Ran He. 2019. Learning disentangled representation for cross-\\nmodal retrieval with deep mutual information estimation. In Proceedings of the 27th ACM International Conference on\\nMultimedia. 1712–1720.\\n[111] Yanming Guo, Yu Liu, Theodoros Georgiou, and Michael S Lew. 2018. A review of semantic segmentation using deep\\nneural networks. International journal of multimedia information retrieval 7 (2018), 87–93.\\n[112] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.\\n2018. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition . 3608–3617.\\n[113] Xiaotian Han, Quanzeng You, Yongfei Liu, Wentao Chen, Huangjie Zheng, Khalil Mrini, Xudong Lin, Yiqi Wang,\\nBohan Zhai, Jianbo Yuan, et al. 2023. InfiMM-Eval: Complex Open-Ended Reasoning Evaluation For Multi-Modal'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 61, 'page_label': '62', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Bohan Zhai, Jianbo Yuan, et al. 2023. InfiMM-Eval: Complex Open-Ended Reasoning Evaluation For Multi-Modal\\nLarge Language Models. arXiv e-prints (2023), arXiv–2311.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 62, 'page_label': '63', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 63\\n[114] Darryl Hannan, Akshay Jain, and Mohit Bansal. 2020. Manymodalqa: Modality disambiguation and qa over diverse\\ninputs. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 7879–7886.\\n[115] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang,\\nYuxiang Zhang, et al. 2024. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual\\nmultimodal scientific problems. arXiv preprint arXiv:2402.14008 (2024).\\n[116] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momentum contrast for unsupervised\\nvisual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition .\\n9729–9738.\\n[117] Zheqi He, Xinya Wu, Pengfei Zhou, Richeng Xuan, Guang Liu, Xi Yang, Qiannan Zhu, and Hua Huang. 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 62, 'page_label': '63', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='9729–9738.\\n[117] Zheqi He, Xinya Wu, Pengfei Zhou, Richeng Xuan, Guang Liu, Xi Yang, Qiannan Zhu, and Hua Huang. 2024.\\nCMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning. arXiv preprint\\narXiv:2401.14011 (2024).\\n[118] Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun-Hsuan Sung, László Lukács, Ruiqi Guo, Sanjiv Kumar, Balint\\nMiklos, and Ray Kurzweil. 2017. Efficient natural language response suggestion for smart reply. arXiv preprint\\narXiv:1705.00652 (2017).\\n[119] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent\\nVanhoucke, Patrick Nguyen, Tara N Sainath, et al. 2012. Deep neural networks for acoustic modeling in speech\\nrecognition: The shared views of four research groups. IEEE Signal processing magazine 29, 6 (2012), 82–97.\\n[120] Sebastian Hofstätter, Omar Khattab, Sophia Althammer, Mete Sertkan, and Allan Hanbury. 2022. Introducing neural'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 62, 'page_label': '63', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[120] Sebastian Hofstätter, Omar Khattab, Sophia Althammer, Mete Sertkan, and Allan Hanbury. 2022. Introducing neural\\nbag of whole-words with colberter: Contextualized late interactions using enhanced reduction. In Proceedings of the\\n31st ACM International Conference on Information & Knowledge Management . 737–747.\\n[121] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao\\nDong, Ming Ding, et al. 2024. Cogagent: A visual language model for gui agents. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition . 14281–14290.\\n[122] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna,\\nChen-Yu Lee, and Tomas Pfister. 2023. Distilling step-by-step! outperforming larger language models with less\\ntraining data and smaller model sizes. arXiv preprint arXiv:2305.02301 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 62, 'page_label': '63', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Chen-Yu Lee, and Tomas Pfister. 2023. Distilling step-by-step! outperforming larger language models with less\\ntraining data and smaller model sizes. arXiv preprint arXiv:2305.02301 (2023).\\n[123] Anwen Hu, Yaya Shi, Haiyang Xu, Jiabo Ye, Qinghao Ye, Ming Yan, Chenliang Li, Qi Qian, Ji Zhang, and Fei Huang.\\n2024. mplug-paperowl: Scientific diagram analysis with the multimodal large language model. In Proceedings of the\\n32nd ACM International Conference on Multimedia . 6929–6938.\\n[124] Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye\\nZhang, et al. 2023. Large multilingual models pivot zero-shot multimodal learning across languages. arXiv preprint\\narXiv:2308.12038 (2023).\\n[125] Wenbo Hu, Jia-Chen Gu, Zi-Yi Dou, Mohsen Fayyaz, Pan Lu, Kai-Wei Chang, and Nanyun Peng. 2024. MRAG-Bench:\\nVision-Centric Evaluation for Retrieval-Augmented Multimodal Models. arXiv preprint arXiv:2410.08182 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 62, 'page_label': '63', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models. arXiv preprint arXiv:2410.08182 (2024).\\n[126] Wenbo Hu, Jia-Chen Gu, Zi-Yi Dou, Mohsen Fayyaz, Pan Lu, Kai-Wei Chang, and Nanyun Peng. 2024. MRAG-Bench:\\nVision-Centric Evaluation for Retrieval-Augmented Multimodal Models. arXiv preprint arXiv:2410.08182 (2024).\\n[127] Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanabhan, Giuseppe\\nOttaviano, and Linjun Yang. 2020. Embedding-based retrieval in facebook search. In Proceedings of the 26th ACM\\nSIGKDD International Conference on Knowledge Discovery & Data Mining . 2553–2561.\\n[128] Minbin Huang, Runhui Huang, Han Shi, Yimeng Chen, Chuanyang Zheng, Xiangguo Sun, Xin Jiang, Zhenguo Li,\\nand Hong Cheng. 2024. Efficient Multi-modal Large Language Models via Visual Token Grouping. arXiv preprint\\narXiv:2411.17773 (2024).\\n[129] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 62, 'page_label': '63', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='arXiv:2411.17773 (2024).\\n[129] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong,\\nJiawei Huang, Jinglin Liu, et al. 2024. Audiogpt: Understanding and generating speech, music, sound, and talking\\nhead. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 38. 23802–23804.\\n[130] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan\\nMohammed, Barun Patra, et al . 2023. Language is not all you need: Aligning perception with language models.\\nAdvances in Neural Information Processing Systems 36 (2023), 72096–72109.\\n[131] Siteng Huang, Biao Gong, Yulin Pan, Jianwen Jiang, Yiliang Lv, Yuyuan Li, and Donglin Wang. 2023. Vop: Text-video\\nco-operative prompt tuning for cross-modal retrieval. In Proceedings of the IEEE/CVF conference on computer vision\\nand pattern recognition . 6565–6574.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 62, 'page_label': '63', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='co-operative prompt tuning for cross-modal retrieval. In Proceedings of the IEEE/CVF conference on computer vision\\nand pattern recognition . 6565–6574.\\n[132] Xijie Huang, Li Lyna Zhang, Kwang-Ting Cheng, Fan Yang, and Mao Yang. 2023. Fewer is more: Boosting LLM\\nreasoning with reinforced context pruning. arXiv preprint arXiv:2312.08901 (2023).\\n[133] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. 2022. Layoutlmv3: Pre-training for document ai with\\nunified text and image masking. In Proceedings of the 30th ACM International Conference on Multimedia . 4083–4091.\\n[134] Yupan Huang, Zaiqiao Meng, Fangyu Liu, Yixuan Su, Nigel Collier, and Yutong Lu. 2023. Sparkles: Unlocking chats\\nacross multiple images for multimodal instruction-following models. arXiv preprint arXiv:2308.16463 (2023).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 63, 'page_label': '64', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='64 Trovato et al.\\n[135] Yan Huang, Wei Wang, and Liang Wang. 2017. Instance-aware image and sentence matching with selective multimodal\\nlstm. In Proceedings of the IEEE conference on computer vision and pattern recognition . 2310–2318.\\n[136] Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, and Jianlong Fu. 2021. Seeing out of the box:\\nEnd-to-end pre-training for vision-language representation learning. In Proceedings of the IEEE/CVF conference on\\ncomputer vision and pattern recognition . 12976–12985.\\n[137] S Humeau. 2019. Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-\\nsentence scoring. arXiv preprint arXiv:1905.01969 (2019).\\n[138] Zaeem Hussain, Mingda Zhang, Xiaozhong Zhang, Keren Ye, Christopher Thomas, Zuha Agha, Nathan Ong, and\\nAdriana Kovashka. 2017. Automatic understanding of image and video advertisements. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition . 1705–1715.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 63, 'page_label': '64', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Adriana Kovashka. 2017. Automatic understanding of image and video advertisements. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition . 1705–1715.\\n[139] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard\\nGrave. 2021. Towards unsupervised dense information retrieval with contrastive learning. arXiv preprint\\narXiv:2112.09118 2, 3 (2021).\\n[140] Aman Jain, Mayank Kothyari, Vishwajeet Kumar, Preethi Jyothi, Ganesh Ramakrishnan, and Soumen Chakrabarti.\\n2021. Select, substitute, search: A new benchmark for knowledge-augmented visual question answering. InProceedings\\nof the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 2491–2498.\\n[141] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. 2017. Tgif-qa: Toward spatio-temporal'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 63, 'page_label': '64', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[141] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. 2017. Tgif-qa: Toward spatio-temporal\\nreasoning in visual question answering. InProceedings of the IEEE conference on computer vision and pattern recognition .\\n2758–2766.\\n[142] Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010. Product quantization for nearest neighbor search. IEEE\\ntransactions on pattern analysis and machine intelligence 33, 1 (2010), 117–128.\\n[143] Hervé Jégou, Matthijs Douze, Cordelia Schmid, and Patrick Pérez. 2010. Aggregating local descriptors into a compact\\nimage representation. In 2010 IEEE computer society conference on computer vision and pattern recognition . IEEE,\\n3304–3311.\\n[144] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and\\nTom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. In\\nInternational conference on machine learning . PMLR, 4904–4916.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 63, 'page_label': '64', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Tom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. In\\nInternational conference on machine learning . PMLR, 4904–4916.\\n[145] Yiren Jian, Chongyang Gao, and Soroush Vosoughi. 2023. Bootstrapping vision-language learning with decoupled\\nlanguage pre-training. Advances in Neural Information Processing Systems 36 (2023), 57–72.\\n[146] Ding Jiang and Mang Ye. 2023. Cross-modal implicit relation reasoning and aligning for text-to-image person retrieval.\\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2787–2797.\\n[147] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Guanglu Song,\\nPeng Gao, et al. 2024. Mmsearch: Benchmarking the potential of large models as multi-modal search engines. arXiv\\npreprint arXiv:2409.12959 (2024).\\n[148] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. Llmlingua: Compressing prompts for'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 63, 'page_label': '64', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:2409.12959 (2024).\\n[148] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. Llmlingua: Compressing prompts for\\naccelerated inference of large language models. arXiv preprint arXiv:2310.05736 (2023).\\n[149] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. Longllmlingua:\\nAccelerating and enhancing llms in long context scenarios via prompt compression. arXiv preprint arXiv:2310.06839\\n(2023).\\n[150] Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen\\nZhuang. 2024. E5-v: Universal embeddings with multimodal large language models. arXiv preprint arXiv:2407.12580\\n(2024).\\n[151] Yutao Jiang, Qiong Wu, Wenhao Lin, Wei Yu, and Yiyi Zhou. 2025. What Kind of Visual Tokens Do We Need?\\nTraining-free Visual Token Pruning for Multi-modal Large Language Models from the Perspective of Graph. arXiv\\npreprint arXiv:2501.02268 (2025).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 63, 'page_label': '64', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Training-free Visual Token Pruning for Multi-modal Large Language Models from the Perspective of Graph. arXiv\\npreprint arXiv:2501.02268 (2025).\\n[152] Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu Chen. 2024. Vlm2vec: Training vision-\\nlanguage models for massive multimodal embedding tasks. arXiv preprint arXiv:2410.05160 (2024).\\n[153] Bowen Jin, Hansi Zeng, Guoyin Wang, Xiusi Chen, Tianxin Wei, Ruirui Li, Zhengyang Wang, Zheng Li, Yang Li,\\nHanqing Lu, et al. 2023. Language models as semantic indexers. arXiv preprint arXiv:2310.07815 (2023).\\n[154] Yang Jin, Zhicheng Sun, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang\\nSong, et al. 2024. Video-lavit: Unified video-language pre-training with decoupled visual-motional tokenization.\\narXiv preprint arXiv:2402.03161 (2024).\\n[155] Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Quzhe Huang, Bin Chen, Chenyi Lei, An Liu, Chengru Song,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 63, 'page_label': '64', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='arXiv preprint arXiv:2402.03161 (2024).\\n[155] Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Quzhe Huang, Bin Chen, Chenyi Lei, An Liu, Chengru Song,\\net al. 2023. Unified language-vision pretraining in llm with dynamic discrete visual tokenization. arXiv preprint\\narXiv:2309.04669 (2023).\\n[156] Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with gpus. IEEE Transactions on\\nBig Data 7, 3 (2019), 535–547.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 64, 'page_label': '65', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 65\\n[157] Jia-Huei Ju, Jheng-Hong Yang, and Chuan-Ju Wang. 2021. Text-to-text multi-view learning for passage re-ranking.\\nIn Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval .\\n1803–1807.\\n[158] Dongwon Jung, Qin Liu, Tenghao Huang, Ben Zhou, and Muhao Chen. 2024. Familiarity-aware evidence compression\\nfor retrieval augmented generation. arXiv preprint arXiv:2409.12468 (2024).\\n[159] Hoyoun Jung and Kyung-Joong Kim. 2024. Discrete prompt compression with reinforcement learning. IEEE Access\\n(2024).\\n[160] Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Ákos Kádár, Adam Trischler, and Yoshua Bengio. 2017.\\nFigureqa: An annotated figure dataset for visual reasoning. arXiv preprint arXiv:1710.07300 (2017).\\n[161] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 64, 'page_label': '65', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[161] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau\\nYih. 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906 (2020).\\n[162] Mehran Kazemi, Nishanth Dikkala, Ankit Anand, Petar Devic, Ishita Dasgupta, Fangyu Liu, Bahare Fatemi, Pranjal\\nAwasthi, Dee Guo, Sreenivas Gollapudi, et al. 2024. ReMI: A Dataset for Reasoning with Multiple Images. arXiv\\npreprint arXiv:2406.09175 (2024).\\n[163] Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage search via contextualized late\\ninteraction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in\\nInformation Retrieval . 39–48.\\n[164] Jing Yu Koh, Daniel Fried, and Russ R Salakhutdinov. 2023. Generating images with multimodal language models.\\nAdvances in Neural Information Processing Systems 36 (2023), 21487–21506.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 64, 'page_label': '65', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[164] Jing Yu Koh, Daniel Fried, and Russ R Salakhutdinov. 2023. Generating images with multimodal language models.\\nAdvances in Neural Information Processing Systems 36 (2023), 21487–21506.\\n[165] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. 2023. Grounding language models to images for multimodal\\ninputs and outputs. In International Conference on Machine Learning . PMLR, 17283–17300.\\n[166] Weize Kong, Swaraj Khadanga, Cheng Li, Shaleen Kumar Gupta, Mingyang Zhang, Wensong Xu, and Michael\\nBendersky. 2022. Multi-aspect dense retrieval. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge\\nDiscovery and Data Mining . 3178–3186.\\n[167] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural\\nnetworks. Advances in neural information processing systems 25 (2012).\\n[168] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. 2024. Lisa: Reasoning segmentation'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 64, 'page_label': '65', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='networks. Advances in neural information processing systems 25 (2012).\\n[168] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. 2024. Lisa: Reasoning segmentation\\nvia large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\\n9579–9589.\\n[169] Yann LeCun, Yoshua Bengio, et al. 1995. Convolutional networks for images, speech, and time series. The handbook\\nof brain theory and neural networks 3361, 10 (1995), 1995.\\n[170] Jaewoo Lee, Joonho Ko, Jinheon Baek, Soyeong Jeong, and Sung Ju Hwang. 2024. Unified Multimodal Interleaved\\nDocument Representation for Retrieval. arXiv:2410.02729 [cs.CL] https://arxiv.org/abs/2410.02729\\n[171] Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi Chen. 2020. Learning dense representations of phrases at scale.\\narXiv preprint arXiv:2012.12624 (2020).\\n[172] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 64, 'page_label': '65', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='arXiv preprint arXiv:2012.12624 (2020).\\n[172] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain\\nquestion answering. arXiv preprint arXiv:1906.00300 (2019).\\n[173] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal,\\nPeter Shaw, Ming-Wei Chang, and Kristina Toutanova. 2023. Pix2struct: Screenshot parsing as pretraining for visual\\nlanguage understanding. In International Conference on Machine Learning . PMLR, 18893–18912.\\n[174] Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He. 2018. Stacked cross attention for image-text\\nmatching. In Proceedings of the European conference on computer vision (ECCV) . 201–216.\\n[175] Sunkyung Lee, Minjin Choi, and Jongwuk Lee. 2023. GLEN: Generative retrieval via lexical index learning. arXiv\\npreprint arXiv:2311.03057 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 64, 'page_label': '65', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[175] Sunkyung Lee, Minjin Choi, and Jongwuk Lee. 2023. GLEN: Generative retrieval via lexical index learning. arXiv\\npreprint arXiv:2311.03057 (2023).\\n[176] Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Minjoon Seo. 2023. Volcano: mitigating multimodal hallucination\\nthrough self-feedback guided revision. arXiv preprint arXiv:2311.07362 (2023).\\n[177] Paul Lerner, Olivier Ferret, Camille Guinaudeau, Hervé Le Borgne, Romaric Besançon, José G Moreno, and Jesús\\nLovón Melgarejo. 2022. ViQuAE, a dataset for knowledge-based visual question answering about named entities. In\\nProceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval .\\n3108–3120.\\n[178] Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. 2024. Seed-bench-2-plus: Benchmarking\\nmultimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 64, 'page_label': '65', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='multimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790 (2024).\\n[179] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. 2024. SEED-Bench:\\nBenchmarking Multimodal Large Language Models. In Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition . 13299–13308.\\n[180] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. 2023. Seed-bench: Benchmarking\\nmultimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125 (2023).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 65, 'page_label': '66', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='66 Trovato et al.\\n[181] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. 2023.\\nMimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425 (2023).\\n[182] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung\\nPoon, and Jianfeng Gao. 2023. Llava-med: Training a large language-and-vision assistant for biomedicine in one day.\\nAdvances in Neural Information Processing Systems 36 (2023), 28541–28564.\\n[183] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang. 2020. Unicoder-vl: A universal encoder for vision\\nand language by cross-modal pre-training. In Proceedings of the AAAI conference on artificial intelligence , Vol. 34.\\n11336–11344.\\n[184] Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. 2022. A survey on retrieval-augmented text generation.\\narXiv preprint arXiv:2202.01110 (2022).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 65, 'page_label': '66', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='11336–11344.\\n[184] Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. 2022. A survey on retrieval-augmented text generation.\\narXiv preprint arXiv:2202.01110 (2022).\\n[185] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pre-training\\nwith frozen image encoders and large language models. In International conference on machine learning . PMLR,\\n19730–19742.\\n[186] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training\\nfor unified vision-language understanding and generation. In International conference on machine learning . PMLR,\\n12888–12900.\\n[187] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. 2023.\\nVideochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355 (2023).\\n[188] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 65, 'page_label': '66', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[188] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. 2024.\\nMvbench: A comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition . 22195–22206.\\n[189] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng\\nKong. 2023. Silkie: Preference distillation for large visual language models. arXiv preprint arXiv:2312.10665 (2023).\\n[190] Lei Li, Yongfeng Zhang, and Li Chen. 2023. Prompt distillation for efficient llm-based recommendation. InProceedings\\nof the 32nd ACM International Conference on Information and Knowledge Management . 1348–1357.\\n[191] Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, and Furu Wei.\\n2023. Trocr: Transformer-based optical character recognition with pre-trained models. In Proceedings of the AAAI'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 65, 'page_label': '66', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='2023. Trocr: Transformer-based optical character recognition with pre-trained models. In Proceedings of the AAAI\\nconference on artificial intelligence , Vol. 37. 13094–13102.\\n[192] Shengzhi Li and Nima Tajbakhsh. 2023. Scigraphqa: A large-scale synthetic multi-turn question-answering dataset\\nfor scientific graphs. arXiv preprint arXiv:2308.03349 (2023).\\n[193] Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yukun Yan, Shuo Wang, and Ge Yu. 2024. Say more with less:\\nUnderstanding prompt learning behaviors through gist compression. arXiv preprint arXiv:2402.16058 (2024).\\n[194] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu\\nWei, et al. 2020. Oscar: Object-semantics aligned pre-training for vision-language tasks. In Computer Vision–ECCV\\n2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX 16 . Springer, 121–137.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 65, 'page_label': '66', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX 16 . Springer, 121–137.\\n[195] Yongqi Li, Hongru Cai, Wenjie Wang, Leigang Qu, Yinwei Wei, Wenjie Li, Liqiang Nie, and Tat-Seng Chua. 2024. Rev-\\nolutionizing Text-to-Image Retrieval as Autoregressive Token-to-Voken Generation.arXiv preprint arXiv:2407.17274\\n(2024).\\n[196] Yucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. 2023. Compressing context to enhance inference efficiency\\nof large language models. arXiv preprint arXiv:2310.06201 (2023).\\n[197] Yangning Li, Yinghui Li, Xinyu Wang, Yong Jiang, Zhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao Zheng, Pengjun\\nXie, Philip S. Yu, Fei Huang, and Jingren Zhou. 2024. Benchmarking Multimodal Retrieval Augmented Generation\\nwith Dynamic VQA Dataset and Self-adaptive Planning Agent. (2024). arXiv:2411.02937 [cs.CL] https://arxiv.org/\\nabs/2411.02937'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 65, 'page_label': '66', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='with Dynamic VQA Dataset and Self-adaptive Planning Agent. (2024). arXiv:2411.02937 [cs.CL] https://arxiv.org/\\nabs/2411.02937\\n[198] Yanwei Li, Chengyao Wang, and Jiaya Jia. 2024. Llama-vid: An image is worth 2 tokens in large language models. In\\nEuropean Conference on Computer Vision . Springer, 323–340.\\n[199] Yongqi Li, Wenjie Wang, Leigang Qu, Liqiang Nie, Wenjie Li, and Tat-Seng Chua. 2024. Generative cross-modal\\nretrieval: Memorizing images in multimodal language models for retrieval and beyond.arXiv preprint arXiv:2402.10805\\n(2024).\\n[200] Yongqi Li, Nan Yang, Liang Wang, Furu Wei, and Wenjie Li. 2023. Multiview identifiers enhanced generative retrieval.\\narXiv preprint arXiv:2305.16675 (2023).\\n[201] Yongqi Li, Nan Yang, Liang Wang, Furu Wei, and Wenjie Li. 2024. Learning to rank in generative retrieval. In\\nProceedings of the AAAI Conference on Artificial Intelligence , Vol. 38. 8716–8723.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 65, 'page_label': '66', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[201] Yongqi Li, Nan Yang, Liang Wang, Furu Wei, and Wenjie Li. 2024. Learning to rank in generative retrieval. In\\nProceedings of the AAAI Conference on Artificial Intelligence , Vol. 38. 8716–8723.\\n[202] Yongqi Li, Zhen Zhang, Wenjie Wang, Liqiang Nie, Wenjie Li, and Tat-Seng Chua. 2024. Distillation Enhanced\\nGenerative Retrieval. arXiv preprint arXiv:2402.10769 (2024).\\n[203] Zongqian Li, Yixuan Su, and Nigel Collier. 2024. 500xCompressor: Generalized Prompt Compression for Large\\nLanguage Models. arXiv preprint arXiv:2408.03094 (2024).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 66, 'page_label': '67', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 67\\n[204] Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li, Van Tu Vu, et al.\\n2024. LEGO: language enhanced multi-modal grounding model. arXiv e-prints (2024), arXiv–2401.\\n[205] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. 2024.\\nMonkey: Image resolution and text label are important things for large multi-modal models. In proceedings of the\\nIEEE/CVF conference on computer vision and pattern recognition . 26763–26773.\\n[206] Minghui Liao, Baoguang Shi, Xiang Bai, Xinggang Wang, and Wenyu Liu. 2016. TextBoxes: A Fast Text Detector with\\na Single Deep Neural Network. arXiv:1611.06779 [cs.CV] https://arxiv.org/abs/1611.06779\\n[207] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Jinfa Huang, Junwu Zhang, Yatian Pang, Munan Ning,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 66, 'page_label': '67', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[207] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Jinfa Huang, Junwu Zhang, Yatian Pang, Munan Ning,\\net al. 2024. Moe-llava: Mixture of experts for large vision-language models. arXiv preprint arXiv:2401.15947 (2024).\\n[208] Jimmy Lin and Xueguang Ma. 2021. A few brief notes on deepimpact, coil, and a conceptual framework for information\\nretrieval techniques. arXiv preprint arXiv:2106.14807 (2021).\\n[209] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. 2024. Vila: On pre-training\\nfor visual language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition .\\n26689–26699.\\n[210] Sheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, and Wei Ping. 2024. Mm-embed:\\nUniversal multimodal retrieval with multimodal llms. arXiv preprint arXiv:2411.02571 (2024).\\n[211] Weizhe Lin, Jinghong Chen, Jingbiao Mei, Alexandru Coca, and Bill Byrne. 2023. Fine-grained late-interaction'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 66, 'page_label': '67', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[211] Weizhe Lin, Jinghong Chen, Jingbiao Mei, Alexandru Coca, and Bill Byrne. 2023. Fine-grained late-interaction\\nmulti-modal retrieval for retrieval augmented visual question answering. Advances in Neural Information Processing\\nSystems 36 (2023), 22820–22840.\\n[212] Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, and\\nHongsheng Li. 2024. Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you\\nwant. arXiv preprint arXiv:2403.20271 (2024).\\n[213] Barys Liskavets, Maxim Ushakov, Shuvendu Roy, Mark Klibanov, Ali Etemad, and Shane Luke. 2024. Prompt com-\\npression with context-aware sentence encoding for fast and improved llm inference. arXiv preprint arXiv:2409.01227\\n(2024).\\n[214] Alexander Liu and Samuel Yang. 2022. Masked autoencoders as the unified learners for pre-trained sentence\\nrepresentation. arXiv preprint arXiv:2208.00231 (2022).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 66, 'page_label': '67', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='(2024).\\n[214] Alexander Liu and Samuel Yang. 2022. Masked autoencoders as the unified learners for pre-trained sentence\\nrepresentation. arXiv preprint arXiv:2208.00231 (2022).\\n[215] Chong Liu, Yuqi Zhang, Hongsong Wang, Weihua Chen, Fan Wang, Yan Huang, Yi-Dong Shen, and Liang Wang.\\n2023. Efficient token-guided image-text retrieval with consistent multimodal contrastive training. IEEE Transactions\\non Image Processing 32 (2023), 3622–3633.\\n[216] Dongyang Liu, Renrui Zhang, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng\\nJin, Kaipeng Zhang, et al. 2024. Sphinx-x: Scaling data and parameters for a family of multi-modal large language\\nmodels. arXiv preprint arXiv:2402.05935 (2024).\\n[217] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong\\nYu. 2023. Mmc: Advancing multimodal chart understanding with large-scale instruction tuning. arXiv preprint\\narXiv:2311.10774 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 66, 'page_label': '67', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Yu. 2023. Mmc: Advancing multimodal chart understanding with large-scale instruction tuning. arXiv preprint\\narXiv:2311.10774 (2023).\\n[218] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual Instruction Tuning. arXiv:2304.08485 [cs.CV]\\nhttps://arxiv.org/abs/2304.08485\\n[219] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. Advances in neural\\ninformation processing systems 36 (2023), 34892–34916.\\n[220] Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, and Yiming Qian. 2023. Tcra-llm: Token compression retrieval\\naugmented large language model for inference cost reduction. arXiv preprint arXiv:2310.15556 (2023).\\n[221] Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. 2024. Visualwebbench:\\nHow far have multimodal llms evolved in web page understanding and grounding? arXiv preprint arXiv:2404.05955\\n(2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 66, 'page_label': '67', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='How far have multimodal llms evolved in web page understanding and grounding? arXiv preprint arXiv:2404.05955\\n(2024).\\n[222] Qi Liu, Bo Wang, Nan Wang, and Jiaxin Mao. 2024. Leveraging passage embeddings for efficient listwise reranking\\nwith large language models. In THE WEB CONFERENCE 2025 .\\n[223] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu,\\net al. 2024. Llava-plus: Learning to use tools for creating multimodal agents. In European Conference on Computer\\nVision. Springer, 126–142.\\n[224] Song Liu, Haoqi Fan, Shengsheng Qian, Yiru Chen, Wenkui Ding, and Zhongyuan Wang. 2021. Hit: Hierarchical\\ntransformer with momentum contrast for video-text retrieval. In Proceedings of the IEEE/CVF international conference\\non computer vision . 11915–11925.\\n[225] Shuo Liu, Kaining Ying, Hao Zhang, Yue Yang, Yuqi Lin, Tianle Zhang, Chuanhao Li, Yu Qiao, Ping Luo, Wenqi'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 66, 'page_label': '67', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='on computer vision . 11915–11925.\\n[225] Shuo Liu, Kaining Ying, Hao Zhang, Yue Yang, Yuqi Lin, Tianle Zhang, Chuanhao Li, Yu Qiao, Ping Luo, Wenqi\\nShao, et al. 2024. Convbench: A multi-turn conversation evaluation benchmark with hierarchical capability for large\\nvision-language models. arXiv preprint arXiv:2403.20194 (2024).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 67, 'page_label': '68', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='68 Trovato et al.\\n[226] Ting Liu, Liangtao Shi, Richang Hong, Yue Hu, Quanjun Yin, and Linfeng Zhang. 2024. Multi-Stage Vision Token\\nDropping: Towards Efficient Multimodal Large Language Model. arXiv preprint arXiv:2411.10803 (2024).\\n[227] Weihao Liu, Fangyu Lei, Tongxu Luo, Jiahe Lei, Shizhu He, Jun Zhao, and Kang Liu. 2023. MMHQA-ICL: Multimodal\\nIn-context Learning for Hybrid Question Answering over Text, Tables and Images. arXiv preprint arXiv:2309.04790\\n(2023).\\n[228] Wenhan Liu, Yutao Zhu, and Zhicheng Dou. 2024. Demorank: Selecting effective demonstrations for large language\\nmodels in ranking task. arXiv preprint arXiv:2406.16332 (2024).\\n[229] Yinhan Liu. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 364\\n(2019).\\n[230] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui\\nHe, Ziwei Liu, et al. 2025. Mmbench: Is your multi-modal model an all-around player?. In European conference on'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 67, 'page_label': '68', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='He, Ziwei Liu, et al. 2025. Mmbench: Is your multi-modal model an all-around player?. In European conference on\\ncomputer vision . Springer, 216–233.\\n[231] Yu Liu, Yanming Guo, Erwin M Bakker, and Michael S Lew. 2017. Learning a recurrent residual fusion network for\\nmultimodal matching. In Proceedings of the IEEE international conference on computer vision . 4107–4116.\\n[232] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. [n. d.].\\nTempcompass: Do video llms really understand videos?, 2024c. URL https://arxiv. org/abs/2403.00476 ([n. d.]).\\n[233] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen\\nJin, and Xiang Bai. 2024. OCRBench: on the hidden mystery of OCR in large multimodal models. Science China\\nInformation Sciences 67, 12 (2024), 220102.\\n[234] Zejun Liu, Fanglin Chen, Jun Xu, Wenjie Pei, and Guangming Lu. 2022. Image-text retrieval with cross-modal semantic'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 67, 'page_label': '68', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Information Sciences 67, 12 (2024), 220102.\\n[234] Zejun Liu, Fanglin Chen, Jun Xu, Wenjie Pei, and Guangming Lu. 2022. Image-text retrieval with cross-modal semantic\\nimportance consistency. IEEE Transactions on Circuits and Systems for Video Technology 33, 5 (2022), 2465–2476.\\n[235] Ziyu Liu, Tao Chu, Yuhang Zang, Xilin Wei, Xiaoyi Dong, Pan Zhang, Zijian Liang, Yuanjun Xiong, Yu Qiao, Dahua\\nLin, et al. 2024. MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset\\nfor LVLMs. arXiv preprint arXiv:2406.11833 (2024).\\n[236] Zhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Ziheng Li, Xizhou Zhu, Lewei Lu, Qifeng Chen, Yu Qiao, Jifeng\\nDai, et al. 2024. Controlllm: Augment language models with tools by searching on graphs. In European Conference on\\nComputer Vision . Springer, 89–105.\\n[237] Zhenghao Liu, Chenyan Xiong, Yuanhuiyi Lv, Zhiyuan Liu, and Ge Yu. 2022. Universal vision-language dense'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 67, 'page_label': '68', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Computer Vision . Springer, 89–105.\\n[237] Zhenghao Liu, Chenyan Xiong, Yuanhuiyi Lv, Zhiyuan Liu, and Ge Yu. 2022. Universal vision-language dense\\nretrieval: Learning a unified representation space for multi-modal retrieval. arXiv preprint arXiv:2209.00179 (2022).\\n[238] Xinwei Long, Jiali Zeng, Fandong Meng, Zhiyuan Ma, Kaiyan Zhang, Bowen Zhou, and Jie Zhou. 2024. Generative\\nmulti-modal knowledge retrieval with large language models. In Proceedings of the AAAI Conference on Artificial\\nIntelligence, Vol. 38. 18733–18741.\\n[239] Siyu Lou, Xuenan Xu, Mengyue Wu, and Kai Yu. 2022. Audio-text retrieval in context. In ICASSP 2022-2022 IEEE\\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 4793–4797.\\n[240] Haoyu Lu, Nanyi Fei, Yuqi Huo, Yizhao Gao, Zhiwu Lu, and Ji-Rong Wen. 2022. Cots: Collaborative two-stream\\nvision-language pre-training model for cross-modal retrieval. In Proceedings of the IEEE/CVF conference on computer'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 67, 'page_label': '68', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='vision-language pre-training model for cross-modal retrieval. In Proceedings of the IEEE/CVF conference on computer\\nVision and pattern recognition . 15692–15701.\\n[241] Junyu Lu, Dixiang Zhang, Songxin Zhang, Zejian Xie, Zhuoyang Song, Cong Lin, Jiaxing Zhang, Bingyi Jing, and\\nPingjian Zhang. 2023. Lyrics: Boosting fine-grained language-vision alignment and comprehension via semantic-aware\\nvisual objects. arXiv preprint arXiv:2312.05278 (2023).\\n[242] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang,\\nMichel Galley, and Jianfeng Gao. 2023. Mathvista: Evaluating mathematical reasoning of foundation models in visual\\ncontexts. arXiv preprint arXiv:2310.02255 (2023).\\n[243] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and\\nAshwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 67, 'page_label': '68', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering.\\nAdvances in Neural Information Processing Systems 35 (2022), 2507–2521.\\n[244] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. 2024. Ovis: Structural\\nembedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797 (2024).\\n[245] Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin. 2024. WildVision:\\nEvaluating Vision-Language Models in the Wild with Human Preferences. arXiv preprint arXiv:2406.11069 (2024).\\n[246] Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2021. Sparse, dense, and attentional representations\\nfor text retrieval. Transactions of the Association for Computational Linguistics 9 (2021), 329–345.\\n[247] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. 2022. Clip4clip: An empirical'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 67, 'page_label': '68', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[247] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. 2022. Clip4clip: An empirical\\nstudy of clip for end to end video clip retrieval and captioning. Neurocomputing 508 (2022), 293–304.\\n[248] Jian Luo, Xuanang Chen, Ben He, and Le Sun. 2024. Prp-graph: Pairwise ranking prompting to llms with graph\\naggregation for effective text re-ranking. InProceedings of the 62nd Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers) . 5766–5776.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 68, 'page_label': '69', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 69\\n[249] Tengchao Lv, Yupan Huang, Jingye Chen, Yuzhong Zhao, Yilin Jia, Lei Cui, Shuming Ma, Yaoyao Chang, Shaohan\\nHuang, Wenhui Wang, et al. 2023. Kosmos-2.5: A multimodal literate model. arXiv preprint arXiv:2309.11419 (2023).\\n[250] Haoyu Ma, Handong Zhao, Zhe Lin, Ajinkya Kale, Zhangyang Wang, Tong Yu, Jiuxiang Gu, Sunav Choudhary, and\\nXiaohui Xie. 2022. Ei-clip: Entity-aware interventional contrastive learning for e-commerce cross-modal retrieval. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 18051–18061.\\n[251] Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Xiang Ji, and Xueqi Cheng. 2021. Prop: Pre-training with\\nrepresentative words prediction for ad-hoc retrieval. In Proceedings of the 14th ACM International Conference on Web\\nSearch and Data Mining . 283–291.\\n[252] Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Yingyan Li, and Xueqi Cheng. 2021. B-PROP: bootstrapped'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 68, 'page_label': '69', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Search and Data Mining . 283–291.\\n[252] Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Yingyan Li, and Xueqi Cheng. 2021. B-PROP: bootstrapped\\npre-training with representative words prediction for ad-hoc retrieval. In Proceedings of the 44th International ACM\\nSIGIR Conference on Research and Development in Information Retrieval . 1513–1522.\\n[253] Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin. 2024. Unifying Multimodal Retrieval via\\nDocument Screenshot Embedding. arXiv:2406.11251 [cs.IR] https://arxiv.org/abs/2406.11251\\n[254] Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin. 2024. Unifying multimodal retrieval via\\ndocument screenshot embedding. arXiv preprint arXiv:2406.11251 (2024).\\n[255] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2024. Fine-tuning llama for multi-stage text retrieval.\\nIn Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval .\\n2421–2425.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 68, 'page_label': '69', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval .\\n2421–2425.\\n[256] Xueguang Ma, Xinyu Zhang, Ronak Pradeep, and Jimmy Lin. 2023. Zero-shot listwise document reranking with a\\nlarge language model. arXiv preprint arXiv:2305.02156 (2023).\\n[257] Yingzi Ma, Yulong Cao, Jiachen Sun, Marco Pavone, and Chaowei Xiao. 2024. Dolphins: Multimodal language model\\nfor driving. In European Conference on Computer Vision . Springer, 403–420.\\n[258] Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong,\\net al. 2024. Mmlongbench-doc: Benchmarking long-context document understanding with visualizations. arXiv\\npreprint arXiv:2407.01523 (2024).\\n[259] Zi-Ao Ma, Tian Lan, Rong-Cheng Tu, Yong Hu, Heyan Huang, and Xian-Ling Mao. 2024. Multi-modal Retrieval\\nAugmented Multi-modal Generation: A Benchmark, Evaluate Metrics and Strong Baselines. arXiv:2411.16365 [cs.CL]'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 68, 'page_label': '69', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Augmented Multi-modal Generation: A Benchmark, Evaluate Metrics and Strong Baselines. arXiv:2411.16365 [cs.CL]\\nhttps://arxiv.org/abs/2411.16365\\n[260] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. 2023. Video-chatgpt: Towards detailed\\nvideo understanding via large vision and language models. arXiv preprint arXiv:2306.05424 (2023).\\n[261] Raman Maini and Himanshu Aggarwal. 2009. Study and comparison of various image edge detection techniques.\\nInternational journal of image processing (IJIP) 3, 1 (2009), 1–11.\\n[262] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. 2023. Egoschema: A diagnostic benchmark for very\\nlong-form video language understanding. Advances in Neural Information Processing Systems 36 (2023), 46212–46244.\\n[263] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019. Ok-vqa: A visual question answering\\nbenchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 68, 'page_label': '69', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern\\nrecognition. 3195–3204.\\n[264] Julieta Martinez, Holger H Hoos, and James J Little. 2014. Stacked quantizers for compositional vector compression.\\narXiv preprint arXiv:1411.2173 (2014).\\n[265] Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, and Shafiq Joty. 2023. Unichart: A universal\\nvision-language pretrained model for chart comprehension and reasoning. arXiv preprint arXiv:2305.14761 (2023).\\n[266] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. 2022. Chartqa: A benchmark for question\\nanswering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244 (2022).\\n[267] Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. 2022. Infographicvqa.\\nIn Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision . 1697–1706.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 68, 'page_label': '69', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision . 1697–1706.\\n[268] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. 2021. Docvqa: A dataset for vqa on document images. In\\nProceedings of the IEEE/CVF winter conference on applications of computer vision . 2200–2209.\\n[269] Lang Mei, Jiaxin Mao, Gang Guo, and Ji-Rong Wen. 2022. Learning Probabilistic Box Embeddings for Effective and\\nEfficient Ranking. In Proceedings of the ACM Web Conference 2022 . 473–482.\\n[270] Lang Mei, Jiaxin Mao, Juan Hu, Naiqiang Tan, Hua Chai, and Ji-Rong Wen. 2023. Improving first-stage retrieval of\\npoint-of-interest search by pre-training models. ACM Transactions on Information Systems 42, 3 (2023), 1–27.\\n[271] Xinhao Mei, Xubo Liu, Jianyuan Sun, Mark D Plumbley, and Wenwu Wang. 2022. On metric learning for audio-text\\ncross-modal retrieval. arXiv preprint arXiv:2203.15537 (2022).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 68, 'page_label': '69', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[271] Xinhao Mei, Xubo Liu, Jianyuan Sun, Mark D Plumbley, and Wenwu Wang. 2022. On metric learning for audio-text\\ncross-modal retrieval. arXiv preprint arXiv:2203.15537 (2022).\\n[272] Thomas Mensink, Jasper Uijlings, Lluis Castrejon, Arushi Goel, Felipe Cadar, Howard Zhou, Fei Sha, André Araujo,\\nand Vittorio Ferrari. 2023. Encyclopedic VQA: Visual questions about detailed properties of fine-grained categories.\\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision . 3113–3124.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 69, 'page_label': '70', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='70 Trovato et al.\\n[273] Antoine Miech, Ivan Laptev, and Josef Sivic. 2018. Learning a text-video embedding from incomplete and heteroge-\\nneous data. arXiv preprint arXiv:1804.02516 (2018).\\n[274] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019.\\nHowto100m: Learning a text-video embedding by watching hundred million narrated video clips. In Proceedings of\\nthe IEEE/CVF international conference on computer vision . 2630–2640.\\n[275] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. 2019. Ocr-vqa: Visual question\\nanswering by reading text in images. In 2019 international conference on document analysis and recognition (ICDAR) .\\nIEEE, 947–952.\\n[276] Niluthpol Chowdhury Mithun, Juncheng Li, Florian Metze, and Amit K Roy-Chowdhury. 2018. Learning joint\\nembedding with multimodal cues for cross-modal video-text retrieval. In Proceedings of the 2018 ACM on international\\nconference on multimedia retrieval . 19–27.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 69, 'page_label': '70', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='embedding with multimodal cues for cross-modal video-text retrieval. In Proceedings of the 2018 ACM on international\\nconference on multimedia retrieval . 19–27.\\n[277] Debjyoti Mondal, Suraj Modi, Subhadarshi Panda, Rituraj Singh, and Godawari Sudhakar Rao. 2024. Kam-cot:\\nKnowledge augmented multimodal chain-of-thoughts reasoning. In Proceedings of the AAAI conference on artificial\\nintelligence, Vol. 38. 18798–18806.\\n[278] Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh,\\nPrakash Murugesan, Peyman Heidari, Yue Liu, et al. 2024. Anymal: An efficient and scalable any-modality augmented\\nlanguage model. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry\\nTrack. 1314–1332.\\n[279] Jesse Mu, Xiang Li, and Noah Goodman. 2023. Learning to compress prompts with gist tokens. Advances in Neural\\nInformation Processing Systems 36 (2023), 19327–19352.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 69, 'page_label': '70', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Track. 1314–1332.\\n[279] Jesse Mu, Xiang Li, and Noah Goodman. 2023. Learning to compress prompts with gist tokens. Advances in Neural\\nInformation Processing Systems 36 (2023), 19327–19352.\\n[280] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and\\nPing Luo. 2023. Embodiedgpt: Vision-language pre-training via embodied chain of thought. Advances in Neural\\nInformation Processing Systems 36 (2023), 25081–25094.\\n[281] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. Ms\\nmarco: A human-generated machine reading comprehension dataset. (2016).\\n[282] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT. arXiv preprint arXiv:1901.04085 (2019).\\n[283] Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020. Document ranking with a pretrained sequence-to-sequence\\nmodel. arXiv preprint arXiv:2003.06713 (2020).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 69, 'page_label': '70', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[283] Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020. Document ranking with a pretrained sequence-to-sequence\\nmodel. arXiv preprint arXiv:2003.06713 (2020).\\n[284] Rodrigo Nogueira, Jimmy Lin, and AI Epistemic. 2019. From doc2query to docTTTTTquery. Online preprint 6, 2\\n(2019).\\n[285] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-stage document ranking with BERT.arXiv\\npreprint arXiv:1910.14424 (2019).\\n[286] Shubham Singh Paliwal, D Vishwanath, Rohit Rahul, Monika Sharma, and Lovekesh Vig. 2019. Tablenet: Deep\\nlearning model for end-to-end table detection and tabular data extraction from scanned document images. In 2019\\nInternational Conference on Document Analysis and Recognition (ICDAR) . IEEE, 128–133.\\n[287] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. 2023. Kosmos-g: Generating\\nimages in context with multimodal large language models. arXiv preprint arXiv:2310.02992 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 69, 'page_label': '70', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='images in context with multimodal large language models. arXiv preprint arXiv:2310.02992 (2023).\\n[288] Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing\\nYang, Chin-Yew Lin, et al . 2024. Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt\\ncompression. arXiv preprint arXiv:2403.12968 (2024).\\n[289] Artemis Panagopoulou, Le Xue, Ning Yu, Junnan Li, Dongxu Li, Shafiq Joty, Ran Xu, Silvio Savarese, Caiming Xiong,\\nand Juan Carlos Niebles. 2023. X-instructblip: A framework for aligning x-modal instruction-aware representations\\nto llms and emergent cross-modal reasoning. arXiv preprint arXiv:2311.18799 (2023).\\n[290] Yanwei Pang, Yuan Yuan, Xuelong Li, and Jing Pan. 2011. Efficient HOG human detection. Signal processing 91, 4\\n(2011), 773–781.\\n[291] Jae Sung Park, Chandra Bhagavatula, Roozbeh Mottaghi, Ali Farhadi, and Yejin Choi. 2020. Visualcomet: Reasoning'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 69, 'page_label': '70', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='(2011), 773–781.\\n[291] Jae Sung Park, Chandra Bhagavatula, Roozbeh Mottaghi, Ali Farhadi, and Yejin Choi. 2020. Visualcomet: Reasoning\\nabout the dynamic context of a still image. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK,\\nAugust 23–28, 2020, Proceedings, Part V 16 . Springer, 508–524.\\n[292] Andrew Parry, Sean MacAvaney, and Debasis Ganguly. 2024. Top-down partitioning for efficient list-wise ranking.\\narXiv preprint arXiv:2405.14589 (2024).\\n[293] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. 2023. Kosmos-2:\\nGrounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824 (2023).\\n[294] Zhiyuan Peng, Xuyang Wu, Qifan Wang, Sravanthi Rajanala, and Yi Fang. 2024. Q-peft: Query-dependent parameter\\nefficient fine-tuning for text reranking with large language models. arXiv preprint arXiv:2404.04522 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 69, 'page_label': '70', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='efficient fine-tuning for text reranking with large language models. arXiv preprint arXiv:2404.04522 (2024).\\n[295] Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong, Jipeng Zhang, Lewei Yao, Jianhua Han, Hang Xu, Lingpeng\\nKong, et al. 2023. Detgpt: Detect what you need via reasoning. arXiv preprint arXiv:2305.14167 (2023).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 70, 'page_label': '71', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 71\\n[296] Ronak Pradeep, Rodrigo Nogueira, and Jimmy Lin. 2021. The expando-mono-duo design pattern for text ranking\\nwith pretrained sequence-to-sequence models. arXiv preprint arXiv:2101.05667 (2021).\\n[297] Xiao Pu, Tianxing He, and Xiaojun Wan. 2024. Style-Compress: An LLM-Based Prompt Compression Framework\\nConsidering Task-Specific Styles. arXiv preprint arXiv:2410.14042 (2024).\\n[298] Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, et al.\\n2024. Cogcom: Train large vision-language models diving into details through chain of manipulations. arXiv preprint\\narXiv:2402.04236 (2024).\\n[299] Jinwei Qi, Yuxin Peng, and Yuxin Yuan. 2018. Cross-media multi-level alignment with relation attention network.\\narXiv preprint arXiv:1804.09539 (2018).\\n[300] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 70, 'page_label': '71', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='arXiv preprint arXiv:1804.09539 (2018).\\n[300] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei,\\nZhe Wei, Miaoxuan Zhang, et al. 2024. We-math: Does your large multimodal model achieve human-like mathematical\\nreasoning? arXiv preprint arXiv:2407.01284 (2024).\\n[301] Jie Qin, Jie Wu, Weifeng Chen, Yuxi Ren, Huixia Li, Hefeng Wu, Xuefeng Xiao, Rui Wang, and Shilei Wen. 2024.\\nDiffusionGPT: LLM-driven text-to-image generation system. arXiv preprint arXiv:2401.10061 (2024).\\n[302] Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald\\nMetzler, et al. 2023. Large language models are effective text rankers with pairwise ranking prompting. arXiv preprint\\narXiv:2306.17563 (2023).\\n[303] Leigang Qu, Haochuan Li, Tan Wang, Wenjie Wang, Yongqi Li, Liqiang Nie, and Tat-Seng Chua. 2024. Unified\\ntext-to-image generation and retrieval. arXiv preprint arXiv:2406.05814 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 70, 'page_label': '71', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[303] Leigang Qu, Haochuan Li, Tan Wang, Wenjie Wang, Yongqi Li, Liqiang Nie, and Tat-Seng Chua. 2024. Unified\\ntext-to-image generation and retrieval. arXiv preprint arXiv:2406.05814 (2024).\\n[304] Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang.\\n2020. RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering.\\narXiv preprint arXiv:2010.08191 (2020).\\n[305] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda\\nAskell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision.\\nIn International conference on machine learning . PMLR, 8748–8763.\\n[306] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are\\nunsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 70, 'page_label': '71', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[306] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are\\nunsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.\\n[307] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M\\nAnwer, Eric Xing, Ming-Hsuan Yang, and Fahad S Khan. 2024. Glamm: Pixel grounding large multimodal model. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 13009–13018.\\n[308] David Rau, Shuai Wang, Hervé Déjean, and Stéphane Clinchant. 2024. Context embeddings for efficient answer\\ngeneration in rag. arXiv preprint arXiv:2407.09252 (2024).\\n[309] Revanth Gangi Reddy, JaeHyeok Doo, Yifei Xu, Md Arafat Sultan, Deevya Swain, Avirup Sil, and Heng Ji. 2024. FIRST:\\nFaster Improved Listwise Reranking with Single Token Decoding. arXiv preprint arXiv:2406.15657 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 70, 'page_label': '71', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Faster Improved Listwise Reranking with Single Token Decoding. arXiv preprint arXiv:2406.15657 (2024).\\n[310] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin. 2024. Pixellm:\\nPixel reasoning with large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition . 26374–26383.\\n[311] TIMODAL RETRIEVAL, KNOWLEDGE-ENHANCED RERANKING, and NOISE-INJECTED TRAINING. [n. d.]. MLLM\\nIS A STRONG RERANKER: ADVANCING MUL. ([n. d.]).\\n[312] Monica Riedler and Stefan Langer. 2024. Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial\\nApplications. arXiv:2410.21943 [cs.CL] https://arxiv.org/abs/2410.21943\\n[313] Jonathan Roberts, Kai Han, Neil Houlsby, and Samuel Albanie. 2024. Scifibench: Benchmarking large multimodal\\nmodels for scientific figure interpretation. arXiv preprint arXiv:2405.08807 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 70, 'page_label': '71', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[313] Jonathan Roberts, Kai Han, Neil Houlsby, and Samuel Albanie. 2024. Scifibench: Benchmarking large multimodal\\nmodels for scientific figure interpretation. arXiv preprint arXiv:2405.08807 (2024).\\n[314] Stephen Robertson. 2004. Understanding inverse document frequency: on theoretical arguments for IDF. Journal of\\ndocumentation 60, 5 (2004), 503–520.\\n[315] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond.Foundations\\nand Trends® in Information Retrieval 3, 4 (2009), 333–389.\\n[316] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. 1995. Okapi at\\nTREC-3. Nist Special Publication Sp 109 (1995), 109.\\n[317] Dongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Cheng Jiayang, Cunxiang Wang,\\nShichao Sun, Huanyu Li, et al. 2024. Ragchecker: A fine-grained framework for diagnosing retrieval-augmented'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 70, 'page_label': '71', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Shichao Sun, Huanyu Li, et al. 2024. Ragchecker: A fine-grained framework for diagnosing retrieval-augmented\\ngeneration. Advances in Neural Information Processing Systems 37 (2024), 21999–22027.\\n[318] Devendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke\\nZettlemoyer. 2022. Improving passage retrieval with zero-shot question generation. arXiv preprint arXiv:2204.07496\\n(2022).\\n[319] Gerard Salton and Christopher Buckley. 1988. Term-weighting approaches in automatic text retrieval. Information\\nprocessing & management 24, 5 (1988), 513–523.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 71, 'page_label': '72', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='72 Trovato et al.\\n[320] Gerard Salton, Anita Wong, and Chung-Shu Yang. 1975. A vector space model for automatic indexing. Commun.\\nACM 18, 11 (1975), 613–620.\\n[321] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. 2022. A-okvqa: A\\nbenchmark for visual question answering using world knowledge. InEuropean conference on computer vision . Springer,\\n146–162.\\n[322] Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar. 2019. Kvqa: Knowledge-aware visual\\nquestion answering. In Proceedings of the AAAI conference on artificial intelligence , Vol. 33. 8876–8884.\\n[323] Shivam Shandilya, Menglin Xia, Supriyo Ghosh, Huiqiang Jiang, Jue Zhang, Qianhui Wu, and Victor Rühle.\\n2024. TACO-RL: Task Aware Prompt Compression Optimization with Reinforcement Learning. arXiv preprint\\narXiv:2409.13035 (2024).\\n[324] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. 2024. Visual'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 71, 'page_label': '72', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='arXiv:2409.13035 (2024).\\n[324] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. 2024. Visual\\ncot: Advancing multi-modal language models with a comprehensive dataset and benchmark for chain-of-thought\\nreasoning. Advances in Neural Information Processing Systems 37 (2024), 8612–8642.\\n[325] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving\\nai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems 36 (2023),\\n38154–38180.\\n[326] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and\\nDouwe Kiela. 2022. Flava: A foundational language and vision alignment model. In Proceedings of the IEEE/CVF\\nconference on computer vision and pattern recognition . 15638–15650.\\n[327] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 71, 'page_label': '72', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='conference on computer vision and pattern recognition . 15638–15650.\\n[327] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.\\n2019. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition. 8317–8326.\\n[328] Hrituraj Singh, Anshul Nasery, Denil Mehta, Aishwarya Agarwal, Jatin Lamba, and Balaji Vasan Srinivasan. 2021.\\nMIMOQA: Multimodal Input Multimodal Output Question Answering. In Proceedings of the 2021 Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language Technologies , Kristina\\nToutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell,\\nTanmoy Chakraborty, and Yichao Zhou (Eds.). Association for Computational Linguistics, Online, 5317–5332. doi:10.\\n18653/v1/2021.naacl-main.418'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 71, 'page_label': '72', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Tanmoy Chakraborty, and Yichao Zhou (Eds.). Association for Computational Linguistics, Online, 5317–5332. doi:10.\\n18653/v1/2021.naacl-main.418\\n[329] Hrituraj Singh, Anshul Nasery, Denil Mehta, Aishwarya Agarwal, Jatin Lamba, and Balaji Vasan Srinivasan. 2021.\\nMimoqa: Multimodal input multimodal output question answering. In Proceedings of the 2021 conference of the north\\namerican chapter of the association for computational linguistics: Human language technologies . 5317–5332.\\n[330] Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana, and Suranga\\nNanayakkara. 2023. Improving the domain adaptation of retrieval augmented generation (RAG) models for open\\ndomain question answering. Transactions of the Association for Computational Linguistics 11 (2023), 1–17.\\n[331] Charlie Snell, Dan Klein, and Ruiqi Zhong. 2022. Learning by distilling context. arXiv preprint arXiv:2209.15189\\n(2022).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 71, 'page_label': '72', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[331] Charlie Snell, Dan Klein, and Ruiqi Zhong. 2022. Learning by distilling context. arXiv preprint arXiv:2209.15189\\n(2022).\\n[332] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo,\\nTian Ye, Yanting Zhang, et al. 2024. Moviechat: From dense token to sparse memory for long video understanding. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 18221–18232.\\n[333] Yale Song and Mohammad Soleymani. 2019. Polysemous visual-semantic embedding for cross-modal retrieval. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 1979–1988.\\n[334] Yiping Song, Rui Yan, Cheng-Te Li, Jian-Yun Nie, Ming Zhang, and Dongyan Zhao. 2018. An Ensemble of Retrieval-\\nBased and Generation-Based Human-Computer Conversation Systems. (2018).\\n[335] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. 2023. Pandagpt: One model to instruction-follow'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 71, 'page_label': '72', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Based and Generation-Based Human-Computer Conversation Systems. (2018).\\n[335] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. 2023. Pandagpt: One model to instruction-follow\\nthem all. arXiv preprint arXiv:2305.16355 (2023).\\n[336] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. 2018. A corpus for reasoning about\\nnatural language grounded in photographs. arXiv preprint arXiv:1811.00491 (2018).\\n[337] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun\\nHuang, and Xinlong Wang. 2024. Generative multimodal models are in-context learners. InProceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition . 14398–14409.\\n[338] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\\nHuang, and Xinlong Wang. 2023. Emu: Generative pretraining in multimodality. arXiv preprint arXiv:2307.05222\\n(2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 71, 'page_label': '72', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Huang, and Xinlong Wang. 2023. Emu: Generative pretraining in multimodality. arXiv preprint arXiv:2307.05222\\n(2023).\\n[339] Weiwei Sun, Zheng Chen, Xinyu Ma, Lingyong Yan, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and\\nZhaochun Ren. 2023. Instruction distillation makes large language models efficient zero-shot rankers. arXiv preprint\\narXiv:2311.01555 (2023).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 72, 'page_label': '73', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 73\\n[340] Weiwei Sun, Lingyong Yan, Zheng Chen, Shuaiqiang Wang, Haichao Zhu, Pengjie Ren, Zhumin Chen, Dawei Yin,\\nMaarten Rijke, and Zhaochun Ren. 2024. Learning to tokenize for generative retrieval.Advances in Neural Information\\nProcessing Systems 36 (2024).\\n[341] Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun\\nRen. 2023. Is ChatGPT good at search? investigating large language models as re-ranking agents. arXiv preprint\\narXiv:2304.09542 (2023).\\n[342] Manan Suri, Puneet Mathur, Franck Dernoncourt, Kanika Goswami, Ryan A. Rossi, and Dinesh Manocha. 2024.\\nVisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal Retrieval-Augmented Generation.\\narXiv:2412.10704 [cs.CL] https://arxiv.org/abs/2412.10704\\n[343] Dídac Surís, Sachit Menon, and Carl Vondrick. 2023. Vipergpt: Visual inference via python execution for reasoning.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 72, 'page_label': '73', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='arXiv:2412.10704 [cs.CL] https://arxiv.org/abs/2412.10704\\n[343] Dídac Surís, Sachit Menon, and Carl Vondrick. 2023. Vipergpt: Visual inference via python execution for reasoning.\\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision . 11888–11898.\\n[344] Adiba Tabassum and Shweta A Dhondse. 2015. Text detection using MSER and stroke width transform. In 2015 Fifth\\nInternational Conference on Communication Systems and Network Technologies . IEEE, 568–571.\\n[345] Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi,\\nand Jonathan Berant. 2021. Multimodalqa: Complex question answering over text, tables and images. arXiv preprint\\narXiv:2104.06039 (2021).\\n[346] Sijun Tan, Xiuyu Li, Shishir Patil, Ziyang Wu, Tianjun Zhang, Kurt Keutzer, Joseph E Gonzalez, and Raluca Ada Popa.\\n2024. Lloco: Learning long contexts offline. arXiv preprint arXiv:2404.07979 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 72, 'page_label': '73', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='2024. Lloco: Learning long contexts offline. arXiv preprint arXiv:2404.07979 (2024).\\n[347] Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito. 2023. SlideVQA: A\\nDataset for Document Visual Question Answering on Multiple Images. arXiv:2301.04883 [cs.CL] https://arxiv.org/\\nabs/2301.04883\\n[348] Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. 2021. Visualmrc: Machine reading comprehension on document\\nimages. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 35. 13878–13888.\\n[349] Raphael Tang, Xinyu Zhang, Xueguang Ma, Jimmy Lin, and Ferhan Ture. 2023. Found in the middle: Permutation\\nself-consistency improves listwise ranking in large language models. arXiv preprint arXiv:2310.07712 (2023).\\n[350] Xu Tang, Yijing Wang, Jingjing Ma, Xiangrong Zhang, Fang Liu, and Licheng Jiao. 2023. Interacting-enhancing\\nfeature transformer for cross-modal remote-sensing image and text retrieval. IEEE Transactions on Geoscience and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 72, 'page_label': '73', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='feature transformer for cross-modal remote-sensing image and text retrieval. IEEE Transactions on Geoscience and\\nRemote Sensing 61 (2023), 1–15.\\n[351] Zineng Tang, Ziyi Yang, Mahmoud Khademi, Yang Liu, Chenguang Zhu, and Mohit Bansal. 2024. Codi-2: In-context\\ninterleaved and interactive any-to-any generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition . 27425–27434.\\n[352] Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta,\\net al. 2022. Transformer memory as a differentiable search index. Advances in Neural Information Processing Systems\\n35 (2022), 21831–21843.\\n[353] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,\\nAndrew M Dai, Anja Hauth, Katie Millican, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv\\npreprint arXiv:2312.11805 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 72, 'page_label': '73', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Andrew M Dai, Anja Hauth, Katie Millican, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv\\npreprint arXiv:2312.11805 (2023).\\n[354] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. Beir: A heterogenous\\nbenchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663 (2021).\\n[355] Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong\\nLu, Jie Zhou, et al . 2024. Mm-interleaved: Interleaved image-text generative modeling via multi-modal feature\\nsynchronizer. arXiv preprint arXiv:2401.10208 (2024).\\n[356] Kaibin Tian, Ruixiang Zhao, Zijie Xin, Bangxiang Lan, and Xirong Li. 2024. Holistic Features are almost Sufficient\\nfor Text-to-Video Retrieval. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\\n17138–17147.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 72, 'page_label': '73', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='for Text-to-Video Retrieval. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\\n17138–17147.\\n[357] Anthony Meng Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, and Steven CH Hoi. 2022. Plug-and-play vqa:\\nZero-shot vqa by conjoining large pretrained models with zero training. arXiv preprint arXiv:2210.08773 (2022).\\n[358] Rubèn Tito, Dimosthenis Karatzas, and Ernest Valveny. 2023. Hierarchical multimodal transformers for multipage\\ndocvqa. Pattern Recognition 144 (2023), 109834.\\n[359] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang,\\nShusheng Yang, Adithya Iyer, Xichen Pan, et al . 2024. Cambrian-1: A fully open, vision-centric exploration of\\nmultimodal llms. arXiv preprint arXiv:2406.16860 (2024).\\n[360] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. 2024. Eyes wide shut? exploring'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 72, 'page_label': '73', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='multimodal llms. arXiv preprint arXiv:2406.16860 (2024).\\n[360] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. 2024. Eyes wide shut? exploring\\nthe visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition. 9568–9578.\\n[361] Atousa Torabi, Niket Tandon, and Leonid Sigal. 2016. Learning language-visual embedding for movie understanding\\nwith natural-language. arXiv preprint arXiv:1609.08124 (2016).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 73, 'page_label': '74', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='74 Trovato et al.\\n[362] A Vaswani. 2017. Attention is all you need. Advances in Neural Information Processing Systems (2017).\\n[363] Thorsten Wagner and Hans-Gerd Lipinski. 2013. IJBlob: an ImageJ library for connected component analysis and\\nshape analysis. Journal of Open Research Software 1, 1 (2013).\\n[364] Ao Wang, Fengyuan Sun, Hui Chen, Zijia Lin, Jungong Han, and Guiguang Ding. 2024. [CLS] Token Tells Everything\\nNeeded for Training-free Efficient MLLMs. arXiv preprint arXiv:2412.05819 (2024).\\n[365] Andong Wang, Bo Wu, Sunli Chen, Zhenfang Chen, Haotian Guan, Wei-Ning Lee, Li Erran Li, and Chuang Gan. 2024.\\nSOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition . 13384–13394.\\n[366] Chenyu Wang, Weixin Luo, Qianyu Chen, Haonan Mai, Jindi Guo, Sixun Dong, Zhengxin Li, Lin Ma, Shenghua Gao,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 73, 'page_label': '74', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='IEEE/CVF Conference on Computer Vision and Pattern Recognition . 13384–13394.\\n[366] Chenyu Wang, Weixin Luo, Qianyu Chen, Haonan Mai, Jindi Guo, Sixun Dong, Zhengxin Li, Lin Ma, Shenghua Gao,\\net al. 2024. Tool-lmm: A large multi-modal model for tool agent learning. arXiv e-prints (2024), arXiv–2401.\\n[367] Dongsheng Wang, Natraj Raman, Mathieu Sibue, Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh\\nNourbakhsh, and Xiaomo Liu. 2023. DocLLM: A layout-aware generative language model for multimodal document\\nunderstanding. arXiv preprint arXiv:2401.00908 (2023).\\n[368] Fei Wang, Xingyu Fu, James Y Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou,\\nKai Zhang, et al. 2024. MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding. arXiv\\npreprint arXiv:2406.09411 (2024).\\n[369] Jinyu Wang, Jingjing Fu, Rui Wang, Lei Song, and Jiang Bian. 2025. PIKE-RAG: sPecIalized KnowledgE and Rationale'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 73, 'page_label': '74', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:2406.09411 (2024).\\n[369] Jinyu Wang, Jingjing Fu, Rui Wang, Lei Song, and Jiang Bian. 2025. PIKE-RAG: sPecIalized KnowledgE and Rationale\\nAugmented Generation. arXiv:2501.11551 [cs.CL] https://arxiv.org/abs/2501.11551\\n[370] Jian Wang, Yonghao He, Cuicui Kang, Shiming Xiang, and Chunhong Pan. 2015. Image-text cross-modal retrieval via\\nmodality-specific feature learning. In Proceedings of the 5th ACM on International Conference on Multimedia Retrieval .\\n347–354.\\n[371] Jiamian Wang, Guohao Sun, Pichao Wang, Dongfang Liu, Sohail Dianat, Majid Rabbani, Raghuveer Rao, and Zhiqiang\\nTao. 2024. Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition . 16551–16560.\\n[372] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. 2024. Measuring multimodal\\nmathematical reasoning with math-vision dataset. arXiv preprint arXiv:2402.14804 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 73, 'page_label': '74', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[372] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. 2024. Measuring multimodal\\nmathematical reasoning with math-vision dataset. arXiv preprint arXiv:2402.14804 (2024).\\n[373] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei.\\n2022. Simlm: Pre-training with representation bottleneck for dense passage retrieval. arXiv preprint arXiv:2207.02578\\n(2022).\\n[374] Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel. 2017. Fvqa: Fact-based visual question\\nanswering. IEEE transactions on pattern analysis and machine intelligence 40, 10 (2017), 2413–2427.\\n[375] Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, and Anthony Dick. 2015. Explicit knowledge-based\\nreasoning for visual question answering. arXiv preprint arXiv:1511.02570 (2015).\\n[376] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 73, 'page_label': '74', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='reasoning for visual question answering. arXiv preprint arXiv:1511.02570 (2015).\\n[376] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao\\nDong, et al. 2024. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035 (2024).\\n[377] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song\\nXiXuan, et al. 2024. Cogvlm: Visual expert for pretrained language models. Advances in Neural Information Processing\\nSystems 37 (2024), 121475–121499.\\n[378] Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhenhang Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu,\\nZhiguo Cao, et al. 2023. The all-seeing project: Towards panoptic visual recognition and understanding of the open\\nworld. arXiv preprint arXiv:2308.01907 (2023).\\n[379] Xinfeng Wang, Jin Cui, Yoshimi Suzuki, and Fumiyo Fukumoto. 2024. Rdrec: Rationale distillation for llm-based'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 73, 'page_label': '74', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='world. arXiv preprint arXiv:2308.01907 (2023).\\n[379] Xinfeng Wang, Jin Cui, Yoshimi Suzuki, and Fumiyo Fukumoto. 2024. Rdrec: Rationale distillation for llm-based\\nrecommendation. arXiv preprint arXiv:2405.10587 (2024).\\n[380] Xiaodan Wang, Lei Li, Zhixu Li, Xuwu Wang, Xiangru Zhu, Chengyu Wang, Jun Huang, and Yanghua Xiao. 2023.\\nAgree: Aligning cross-modal entities for image-text retrieval upon vision-language pre-trained models. InProceedings\\nof the Sixteenth ACM International Conference on Web Search and Data Mining . 456–464.\\n[381] Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas\\nBertasius, Mohit Bansal, et al. 2024. Mementos: A comprehensive benchmark for multimodal large language model\\nreasoning over image sequences. arXiv preprint arXiv:2401.10529 (2024).\\n[382] Xinyu Wang, Bohan Zhuang, and Qi Wu. 2024. Modaverse: Efficiently transforming modalities with llms. InProceedings'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 73, 'page_label': '74', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='reasoning over image sequences. arXiv preprint arXiv:2401.10529 (2024).\\n[382] Xinyu Wang, Bohan Zhuang, and Qi Wu. 2024. Modaverse: Efficiently transforming modalities with llms. InProceedings\\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 26606–26616.\\n[383] Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai\\nZhao, Zheng Liu, et al . 2022. A neural corpus indexer for document retrieval. Advances in Neural Information\\nProcessing Systems 35 (2022), 25600–25614.\\n[384] Yan Wang, Yuting Su, Wenhui Li, Jun Xiao, Xuanya Li, and An-An Liu. 2023. Dual-path rare content enhancement\\nnetwork for image and text matching. IEEE Transactions on Circuits and Systems for Video Technology 33, 10 (2023),\\n6144–6158.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 74, 'page_label': '75', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 75\\n[385] Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. 2023. Learning to filter context\\nfor retrieval-augmented generation. arXiv preprint arXiv:2311.08377 (2023).\\n[386] Zheng Wang, Zhenwei Gao, Mengqun Han, Yang Yang, and Heng Tao Shen. 2024. Estimating the Semantics via\\nSector Embedding for Image-Text Retrieval. IEEE Transactions on Multimedia (2024).\\n[387] Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu,\\nSadhika Malladi, et al . 2024. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. arXiv\\npreprint arXiv:2406.18521 (2024).\\n[388] Zheng Wang, Xing Xu, Jiwei Wei, Ning Xie, Yang Yang, and Heng Tao Shen. 2024. Semantics disentangling for\\ncross-modal retrieval. IEEE Transactions on Image Processing 33 (2024), 2226–2237.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 74, 'page_label': '75', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[388] Zheng Wang, Xing Xu, Jiwei Wei, Ning Xie, Yang Yang, and Heng Tao Shen. 2024. Semantics disentangling for\\ncross-modal retrieval. IEEE Transactions on Image Processing 33 (2024), 2226–2237.\\n[389] Zihan Wang, Yujia Zhou, Yiteng Tu, and Zhicheng Dou. 2023. NOVO: learnable and interpretable document identifiers\\nfor model-based IR. InProceedings of the 32nd ACM International Conference on Information and Knowledge Management .\\n2656–2665.\\n[390] Jônatas Wehrmann and Rodrigo C Barros. 2018. Bidirectional retrieval made simple. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition . 7718–7726.\\n[391] Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. 2024. Uniir:\\nTraining and benchmarking universal multimodal information retrievers. In European Conference on Computer Vision .\\nSpringer, 387–404.\\n[392] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, En Yu, Jianjian Sun, Chunrui Han, and Xiangyu'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 74, 'page_label': '75', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Springer, 387–404.\\n[392] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, En Yu, Jianjian Sun, Chunrui Han, and Xiangyu\\nZhang. 2024. Small language model meets with reinforced vision vocabulary. arXiv preprint arXiv:2401.12503 (2024).\\n[393] Wei Wei, Jiabin Tang, Lianghao Xia, Yangqin Jiang, and Chao Huang. 2024. Promptmm: Multi-modal knowledge\\ndistillation for recommendation with prompt-tuning. In Proceedings of the ACM Web Conference 2024 . 3217–3228.\\n[394] Haoyang Wen, Honglei Zhuang, Hamed Zamani, Alexander Hauptmann, and Michael Bendersky. 2024. Multimodal\\nreranking for knowledge-intensive visual question answering. arXiv preprint arXiv:2407.12277 (2024).\\n[395] Weixi Weng, Jieming Zhu, Xiaojun Meng, Hao Zhang, Rui Zhang, and Chun Yuan. 2024. Learning to Compress\\nContexts for Efficient Knowledge-based Visual Question Answering. arXiv preprint arXiv:2409.07331 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 74, 'page_label': '75', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Contexts for Efficient Knowledge-based Visual Question Answering. arXiv preprint arXiv:2409.07331 (2024).\\n[396] David Wingate, Mohammad Shoeybi, and Taylor Sorensen. 2022. Prompt compression and contrastive conditioning\\nfor controllability and toxicity reduction in language models. arXiv preprint arXiv:2210.03162 (2022).\\n[397] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. 2023. Visual chatgpt:\\nTalking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671 (2023).\\n[398] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong\\nYan, Guangtao Zhai, et al. 2023. Q-bench: A benchmark for general-purpose foundation models on low-level vision.\\narXiv preprint arXiv:2309.14181 (2023).\\n[399] Penghao Wu and Saining Xie. 2024. V?: Guided visual search as a core mechanism in multimodal llms. In Proceedings'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 74, 'page_label': '75', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='arXiv preprint arXiv:2309.14181 (2023).\\n[399] Penghao Wu and Saining Xie. 2024. V?: Guided visual search as a core mechanism in multimodal llms. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 13084–13094.\\n[400] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. 2024. Next-gpt: Any-to-any multimodal llm. In\\nForty-first International Conference on Machine Learning .\\n[401] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. 2024. NExT-GPT: Any-to-Any Multimodal LLM.\\narXiv:2309.05519 [cs.AI] https://arxiv.org/abs/2309.05519\\n[402] Wenhao Wu, Haipeng Luo, Bo Fang, Jingdong Wang, and Wanli Ouyang. 2023. Cap4video: What can auxiliary\\ncaptions do for text-video retrieval?. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition. 10704–10713.\\n[403] Renqiu Xia, Song Mao, Xiangchao Yan, Hongbin Zhou, Bo Zhang, Haoyang Peng, Jiahao Pi, Daocheng Fu, Wenjie'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 74, 'page_label': '75', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Recognition. 10704–10713.\\n[403] Renqiu Xia, Song Mao, Xiangchao Yan, Hongbin Zhou, Bo Zhang, Haoyang Peng, Jiahao Pi, Daocheng Fu, Wenjie\\nWu, Hancheng Ye, et al. 2024. DocGenome: An Open Large-scale Scientific Document Benchmark for Training and\\nTesting Multi-modal Large Language Models. arXiv preprint arXiv:2406.11633 (2024).\\n[404] Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Min Dou, Botian Shi, Junchi\\nYan, et al. 2024. Chartx & chartvlm: A versatile benchmark and foundation model for complicated chart reasoning.\\narXiv preprint arXiv:2402.12185 (2024).\\n[405] Chen-Wei Xie, Jianmin Wu, Yun Zheng, Pan Pan, and Xian-Sheng Hua. 2022. Token embeddings alignment for\\ncross-modal retrieval. In Proceedings of the 30th ACM International Conference on Multimedia . 4555–4563.\\n[406] Yifei Xin, Dongchao Yang, and Yuexian Zou. 2023. Improving text-audio retrieval by text-aware attention pooling and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 74, 'page_label': '75', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[406] Yifei Xin, Dongchao Yang, and Yuexian Zou. 2023. Improving text-audio retrieval by text-aware attention pooling and\\nprior matrix revised loss. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing\\n(ICASSP). IEEE, 1–5.\\n[407] Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang,\\nFeng Wu, et al. 2024. Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy\\nreduction. arXiv preprint arXiv:2410.17247 (2024).\\n[408] Guoxin Xiong, Meng Meng, Tianzhu Zhang, Dongming Zhang, and Yongdong Zhang. 2024. Reference-Aware Adaptive\\nNetwork for Image-Text Matching. IEEE Transactions on Circuits and Systems for Video Technology (2024).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 75, 'page_label': '76', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='76 Trovato et al.\\n[409] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020.\\nApproximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808\\n(2020).\\n[410] Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. Recomp: Improving retrieval-augmented lms with compression and\\nselective augmentation. arXiv preprint arXiv:2310.04408 (2023).\\n[411] Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao,\\nand Ping Luo. 2024. Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence (2024).\\n[412] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. 2020. Layoutlm: Pre-training of text\\nand layout for document image understanding. In Proceedings of the 26th ACM SIGKDD international conference on\\nknowledge discovery & data mining . 1192–1200.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 75, 'page_label': '76', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD international conference on\\nknowledge discovery & data mining . 1192–1200.\\n[413] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang\\nChe, et al. 2020. Layoutlmv2: Multi-modal pre-training for visually-rich document understanding. arXiv preprint\\narXiv:2012.14740 (2020).\\n[414] Zhengzhuo Xu, Sinan Du, Yiyan Qi, Chengjin Xu, Chun Yuan, and Jian Guo. 2023. Chartbench: A benchmark for\\ncomplex visual reasoning in charts. arXiv preprint arXiv:2312.15915 (2023).\\n[415] Ikuya Yamada, Akari Asai, and Hannaneh Hajishirzi. 2021. Efficient passage retrieval with hashing for open-domain\\nquestion answering. arXiv preprint arXiv:2106.00882 (2021).\\n[416] Le Yan, Zhen Qin, Honglei Zhuang, Rolf Jagerman, Xuanhui Wang, Michael Bendersky, and Harrie Oosterhuis. 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 75, 'page_label': '76', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='question answering. arXiv preprint arXiv:2106.00882 (2021).\\n[416] Le Yan, Zhen Qin, Honglei Zhuang, Rolf Jagerman, Xuanhui Wang, Michael Bendersky, and Harrie Oosterhuis. 2024.\\nConsolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing. arXiv preprint\\narXiv:2404.11791 (2024).\\n[417] Siming Yan, Min Bai, Weifeng Chen, Xiong Zhou, Qixing Huang, and Li Erran Li. 2024. Vigor: Improving visual\\ngrounding of large vision language models with fine-grained reward modeling. In European Conference on Computer\\nVision. Springer, 37–53.\\n[418] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and CUI Bin. 2024. Mastering text-to-image\\ndiffusion: Recaptioning, planning, and generating with multimodal llms. In Forty-first International Conference on\\nMachine Learning .\\n[419] Song Yang, Qiang Li, Wenhui Li, Xuanya Li, and An-An Liu. 2022. Dual-level representation enhancement on'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 75, 'page_label': '76', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Machine Learning .\\n[419] Song Yang, Qiang Li, Wenhui Li, Xuanya Li, and An-An Liu. 2022. Dual-level representation enhancement on\\ncharacteristic and context for image-text retrieval. IEEE Transactions on Circuits and Systems for Video Technology 32,\\n11 (2022), 8037–8050.\\n[420] Tianchi Yang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, and Qi Zhang. 2023. Auto\\nsearch indexer for end-to-end document retrieval. arXiv preprint arXiv:2310.12455 (2023).\\n[421] Xiangpeng Yang, Linchao Zhu, Xiaohan Wang, and Yi Yang. 2024. DGL: Dynamic Global-Local Prompt Tuning for\\nText-Video Retrieval. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 38. 6540–6548.\\n[422] Yang Yang, Chubing Zhang, Yi-Chu Xu, Dianhai Yu, De-Chuan Zhan, and Jian Yang. 2021. Rethinking Label-Wise\\nCross-Modal Retrieval from A Semantic Sharing Perspective.. In IJCAI. 3300–3306.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 75, 'page_label': '76', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[422] Yang Yang, Chubing Zhang, Yi-Chu Xu, Dianhai Yu, De-Chuan Zhan, and Jian Yang. 2021. Rethinking Label-Wise\\nCross-Modal Retrieval from A Semantic Sharing Perspective.. In IJCAI. 3300–3306.\\n[423] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael\\nZeng, and Lijuan Wang. 2023. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint\\narXiv:2303.11381 (2023).\\n[424] Zhen Yang, Yingxue Zhang, Fandong Meng, and Jie Zhou. 2023. Teal: Tokenize and embed all for multi-modal large\\nlanguage models. arXiv preprint arXiv:2311.04589 (2023).\\n[425] Linli Yao, Lei Li, Shuhuai Ren, Lean Wang, Yuanxin Liu, Xu Sun, and Lu Hou. 2024. Deco: Decoupling token\\ncompression from semantic abstraction in multimodal large language models. arXiv preprint arXiv:2405.20985 (2024).\\n[426] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 75, 'page_label': '76', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[426] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng\\nTian, et al. 2023. mplug-docowl: Modularized multimodal large language model for document understanding. arXiv\\npreprint arXiv:2307.02499 (2023).\\n[427] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\\nYaya Shi, et al. 2023. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint\\narXiv:2304.14178 (2023).\\n[428] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. 2024.\\nmplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedings of the\\nieee/cvf conference on computer vision and pattern recognition . 13040–13051.\\n[429] Dongyi Yi, Guibo Zhu, Chenglin Ding, Zongshu Li, Dong Yi, and Jinqiao Wang. 2025. MME-Industry: A Cross-Industry'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 75, 'page_label': '76', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='ieee/cvf conference on computer vision and pattern recognition . 13040–13051.\\n[429] Dongyi Yi, Guibo Zhu, Chenglin Ding, Zongshu Li, Dong Yi, and Jinqiao Wang. 2025. MME-Industry: A Cross-Industry\\nMultimodal Evaluation Benchmark. arXiv preprint arXiv:2501.16688 (2025).\\n[430] Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Xiaoshui Huang, Zhiyong Wang, Lu Sheng,\\nLei Bai, et al. 2024. Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark.\\nAdvances in Neural Information Processing Systems 36 (2024).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 76, 'page_label': '77', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 77\\n[431] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo\\nLiu, et al. 2024. Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language models\\ntowards multitask agi. arXiv preprint arXiv:2404.16006 (2024).\\n[432] Chanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Minbyul Jeong, and Jaewoo Kang. 2024. Compact: Compressing\\nretrieved documents actively for question answering. arXiv preprint arXiv:2407.09014 (2024).\\n[433] Soyoung Yoon, Eunbi Choi, Jiyeon Kim, Hyeongu Yun, Yireun Kim, and Seung-won Hwang. 2024. Listt5: Listwise\\nreranking with fusion-in-decoder improves zero-shot retrieval. arXiv preprint arXiv:2402.15838 (2024).\\n[434] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang,\\nBrian Karrer, Shelly Sheynin, et al. 2023. Scaling autoregressive multi-modal models: Pretraining and instruction'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 76, 'page_label': '77', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Brian Karrer, Shelly Sheynin, et al. 2023. Scaling autoregressive multi-modal models: Pretraining and instruction\\ntuning. arXiv preprint arXiv:2309.02591 2, 3 (2023), 3.\\n[435] Qinhan Yu, Zhiyou Xiao, Binghui Li, Zhengren Wang, Chong Chen, and Wentao Zhang. 2025. MRAMG-Bench: A\\nBeyondText Benchmark for Multimodal Retrieval-Augmented Multimodal Generation.arXiv preprint arXiv:2502.04176\\n(2025).\\n[436] Shi Yu, Zhenghao Liu, Chenyan Xiong, Tao Feng, and Zhiyuan Liu. 2021. Few-shot conversational dense retrieval.\\nIn Proceedings of the 44th International ACM SIGIR Conference on research and development in information retrieval .\\n829–838.\\n[437] Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan\\nLiu, et al. 2024. Visrag: Vision-based retrieval-augmented generation on multi-modality documents. arXiv preprint\\narXiv:2410.10594 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 76, 'page_label': '77', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Liu, et al. 2024. Visrag: Vision-based retrieval-augmented generation on multi-modality documents. arXiv preprint\\narXiv:2410.10594 (2024).\\n[438] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng,\\nMaosong Sun, et al. 2024. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional\\nhuman feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 13807–13816.\\n[439] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\\n2023. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490 (2023).\\n[440] Xiaohan Yu, Zhihan Yang, and Chong Chen. 2025. Unveiling the Potential of Multimodal Retrieval Augmented\\nGeneration with Planning. arXiv preprint arXiv:2501.15470 (2025).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 76, 'page_label': '77', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[440] Xiaohan Yu, Zhihan Yang, and Chong Chen. 2025. Unveiling the Potential of Multimodal Retrieval Augmented\\nGeneration with Planning. arXiv preprint arXiv:2501.15470 (2025).\\n[441] Youngjae Yu, Hyungjin Ko, Jongwook Choi, and Gunhee Kim. 2017. End-to-end concept word detection for video\\ncaptioning, retrieval, and question answering. In Proceedings of the IEEE conference on computer vision and pattern\\nrecognition. 3165–3173.\\n[442] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. 2019. Activitynet-qa: A dataset\\nfor understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial\\nIntelligence, Vol. 33. 9127–9134.\\n[443] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. 2024. Osprey:\\nPixel understanding with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition . 28202–28211.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 76, 'page_label': '77', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Pixel understanding with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition . 28202–28211.\\n[444] Zhengqing Yuan, Zhaoxu Li, Weiran Huang, Yanfang Ye, and Lichao Sun. 2023. Tinygpt-v: Efficient multimodal large\\nlanguage model via small backbones. arXiv preprint arXiv:2312.16862 (2023).\\n[445] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming\\nRen, Yuxuan Sun, et al. 2024. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark\\nfor expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 9556–9567.\\n[446] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang,\\nHuan Sun, et al. 2024. Mmmu-pro: A more robust multi-discipline multimodal understanding benchmark. arXiv\\npreprint arXiv:2409.02813 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 76, 'page_label': '77', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Huan Sun, et al. 2024. Mmmu-pro: A more robust multi-discipline multimodal understanding benchmark. arXiv\\npreprint arXiv:2409.02813 (2024).\\n[447] Sukmin Yun, Haokun Lin, Rusiru Thushara, Mohammad Qazim Bhat, Yongxin Wang, Zutao Jiang, Mingkai Deng,\\nJinhong Wang, Tianhua Tao, Junbo Li, et al. 2024. Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation\\nFramework for Multimodal LLMs. arXiv preprint arXiv:2406.20098 (2024).\\n[448] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. From recognition to cognition: Visual commonsense\\nreasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 6720–6731.\\n[449] Hansi Zeng, Chen Luo, Bowen Jin, Sheikh Muhammad Sarwar, Tianxin Wei, and Hamed Zamani. 2024. Scalable and\\neffective generative information retrieval. In Proceedings of the ACM on Web Conference 2024 . 1441–1452.\\n[450] Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, Tao Kong, and Ruihua'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 76, 'page_label': '77', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[450] Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, Tao Kong, and Ruihua\\nSong. 2024. What matters in training a gpt4-style language model with multimodal inputs?. In Proceedings of the\\n2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies (Volume 1: Long Papers) . 7930–7957.\\n[451] Zhixiong Zeng and Wenji Mao. 2022. A comprehensive empirical study of vision-language pre-trained model for\\nsupervised cross-modal retrieval. arXiv preprint arXiv:2201.02772 (2022).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 77, 'page_label': '78', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='78 Trovato et al.\\n[452] ChengXiang Zhai et al. 2008. Statistical language models for information retrieval a critical review. Foundations and\\nTrends® in Information Retrieval 2, 3 (2008), 137–213.\\n[453] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2021. Jointly optimizing query\\nencoder and product quantization to improve retrieval performance. In Proceedings of the 30th ACM International\\nConference on Information & Knowledge Management . 2487–2496.\\n[454] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2021. Optimizing dense retrieval\\nmodel training with hard negatives. In Proceedings of the 44th International ACM SIGIR Conference on Research and\\nDevelopment in Information Retrieval . 1503–1512.\\n[455] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2022. Learning discrete representations'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 77, 'page_label': '78', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Development in Information Retrieval . 1503–1512.\\n[455] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2022. Learning discrete representations\\nvia constrained clustering for effective and efficient dense retrieval. In Proceedings of the Fifteenth ACM International\\nConference on Web Search and Data Mining . 1328–1336.\\n[456] Erhan Zhang, Xingzhu Wang, Peiyuan Gong, Yankai Lin, and Jiaxin Mao. 2024. Usimagent: Large language models for\\nsimulating search users. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development\\nin Information Retrieval . 2687–2692.\\n[457] Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu,\\nShuyue Guo, et al. 2024. Cmmmu: A chinese massive multi-discipline multimodal understanding benchmark. arXiv\\npreprint arXiv:2401.11944 (2024).\\n[458] Hanqi Zhang, Chong Chen, Lang Mei, Qi Liu, and Jiaxin Mao. 2024. Mamba Retriever: Utilizing Mamba for Effective'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 77, 'page_label': '78', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:2401.11944 (2024).\\n[458] Hanqi Zhang, Chong Chen, Lang Mei, Qi Liu, and Jiaxin Mao. 2024. Mamba Retriever: Utilizing Mamba for Effective\\nand Efficient Dense Retrieval. In Proceedings of the 33rd ACM International Conference on Information and Knowledge\\nManagement. 4268–4272.\\n[459] Hang Zhang, Yeyun Gong, Yelong Shen, Jiancheng Lv, Nan Duan, and Weizhu Chen. 2021. Adversarial retriever-ranker\\nfor dense text retrieval. arXiv preprint arXiv:2110.03611 (2021).\\n[460] Hang Zhang, Xin Li, and Lidong Bing. 2023. Video-llama: An instruction-tuned audio-visual language model for\\nvideo understanding. arXiv preprint arXiv:2306.02858 (2023).\\n[461] Han Zhang, Hongwei Shen, Yiming Qiu, Yunjiang Jiang, Songlin Wang, Sulong Xu, Yun Xiao, Bo Long, and Wen-Yun\\nYang. 2021. Joint learning of deep retrieval model and product quantization based embedding index. In Proceedings of\\nthe 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1718–1722.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 77, 'page_label': '78', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1718–1722.\\n[462] Jinxu Zhang, Yongqi Yu, and Yu Zhang. 2024. CREAM: coarse-to-fine retrieval and multi-modal efficient tuning for\\ndocument VQA. In Proceedings of the 32nd ACM International Conference on Multimedia . 925–934.\\n[463] Junyuan Zhang, Qintong Zhang, Bin Wang, Linke Ouyang, Zichen Wen, Ying Li, Ka-Ho Chow, Conghui He, and\\nWentao Zhang. 2024. OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation.\\narXiv:2412.02592 [cs.CV] https://arxiv.org/abs/2412.02592\\n[464] Longhui Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, and Min Zhang. 2023. A two-stage\\nadaptation of large language models for text ranking. arXiv preprint arXiv:2311.16720 (2023).\\n[465] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 77, 'page_label': '78', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[465] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang\\nZhang, Shuangrui Ding, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He,\\nXingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. 2023. InternLM-XComposer: A Vision-Language Large Model\\nfor Advanced Text-image Comprehension and Composition. arXiv:2309.15112 [cs.CV] https://arxiv.org/abs/2309.15112\\n[466] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang,\\nLinke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li,\\nWenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang.\\n2024. InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and\\nOutput. arXiv:2407.03320 [cs.CV] https://arxiv.org/abs/2407.03320'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 77, 'page_label': '78', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='2024. InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and\\nOutput. arXiv:2407.03320 [cs.CV] https://arxiv.org/abs/2407.03320\\n[467] Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, and Zhicheng Dou. 2024. Compressing lengthy context\\nwith ultragist. arXiv preprint arXiv:2405.16635 (2024).\\n[468] Qi Zhang, Zhen Lei, Zhaoxiang Zhang, and Stan Z Li. 2020. Context-aware attention network for image-text retrieval.\\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 3536–3545.\\n[469] Qianchi Zhang, Hainan Zhang, Liang Pang, Hongwei Zheng, and Zhiming Zheng. 2024. AdaComp: Extractive\\nContext Compression with Adaptive Predictor for Retrieval-Augmented Large Language Models. arXiv preprint\\narXiv:2409.01579 (2024).\\n[470] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 77, 'page_label': '78', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='arXiv:2409.01579 (2024).\\n[470] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei\\nChang, Yu Qiao, et al. 2025. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?.\\nIn European Conference on Computer Vision . Springer, 169–186.\\n[471] Shunyu Zhang, Yaobo Liang, Ming Gong, Daxin Jiang, and Nan Duan. 2022. Multi-view document representation\\nlearning for open-domain dense retrieval. arXiv preprint arXiv:2203.08372 (2022).\\n[472] Shilong Zhang, Peize Sun, Shoufa Chen, Minn Xiao, Wenqi Shao, Wenwei Zhang, Yu Liu, Kai Chen, and Ping Luo.\\n[n. d.]. GPT4roi: Instruction tuning large language model on regionof-interest, 2024. In URL https://openreview.\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 78, 'page_label': '79', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='A Survey on Multimodal Retrieval-Augmented Generation 79\\nnet/forum.\\n[473] Tianyu Zhang, Suyuchen Wang, Lu Li, Ge Zhang, Perouz Taslakian, Sai Rajeswar, Jie Fu, Bang Liu, and Yoshua Bengio.\\n2024. VCR: Visual Caption Restoration. arXiv preprint arXiv:2406.06462 (2024).\\n[474] Tao Zhang, Ziqi Zhang, Zongyang Ma, Yuxin Chen, Zhongang Qi, Chunfeng Yuan, Bing Li, Junfu Pu, Yuxuan Zhao,\\nZehua Xie, et al. 2024. m𝑅2AG: Multimodal Retrieval-Reflection-Augmented Generation for Knowledge-Based VQA.\\narXiv preprint arXiv:2411.15041 (2024).\\n[475] Xinyu Zhang, Sebastian Hofstätter, Patrick Lewis, Raphael Tang, and Jimmy Lin. 2023. Rank-without-gpt: Building\\ngpt-independent listwise rerankers on open-source large language models. arXiv preprint arXiv:2312.02969 (2023).\\n[476] Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie\\nLi, and Min Zhang. 2024. GME: Improving Universal Multimodal Retrieval by Multimodal LLMs. arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 78, 'page_label': '79', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Li, and Min Zhang. 2024. GME: Improving Universal Multimodal Retrieval by Multimodal LLMs. arXiv preprint\\narXiv:2412.16855 (2024).\\n[477] Yan Zhang, Zhong Ji, Di Wang, Yanwei Pang, and Xuelong Li. 2024. USER: Unified semantic enhancement with\\nmomentum contrast for image-text retrieval. IEEE Transactions on Image Processing (2024).\\n[478] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. 2023. Llavar: Enhanced\\nvisual instruction tuning for text-rich image understanding. arXiv preprint arXiv:2306.17107 (2023).\\n[479] Yidan Zhang, Ting Zhang, Dong Chen, Yujing Wang, Qi Chen, Xing Xie, Hao Sun, Weiwei Deng, Qi Zhang, Fan Yang,\\net al. 2024. Irgen: Generative modeling for image retrieval. In European Conference on Computer Vision . Springer,\\n21–41.\\n[480] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 78, 'page_label': '79', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='21–41.\\n[480] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang,\\nQingsong Wen, Zhang Zhang, et al. 2024. MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution\\nReal-World Scenarios that are Difficult for Humans? arXiv preprint arXiv:2408.13257 (2024).\\n[481] Zheng Zhang, Chengquan Zhang, Wei Shen, Cong Yao, Wenyu Liu, and Xiang Bai. 2016. Multi-Oriented Text\\nDetection with Fully Convolutional Networks. arXiv:1604.04018 [cs.CV] https://arxiv.org/abs/1604.04018\\n[482] Bingchen Zhao, Yongshuo Zong, Letian Zhang, and Timothy Hospedales. 2024. Benchmarking Multi-Image Un-\\nderstanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning. arXiv\\npreprint arXiv:2406.12742 (2024).\\n[483] Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei, Hongyu Zhou, Jianjian Sun, Yuang Peng, Runpei Dong,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 78, 'page_label': '79', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:2406.12742 (2024).\\n[483] Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei, Hongyu Zhou, Jianjian Sun, Yuang Peng, Runpei Dong,\\nChunrui Han, et al. 2023. Chatspot: Bootstrapping multimodal llms via precise referring instruction tuning. arXiv\\npreprint arXiv:2307.09474 (2023).\\n[484] Shiyu Zhao, Zhenting Wang, Felix Juefei-Xu, Xide Xia, Miao Liu, Xiaofang Wang, Mingfu Liang, Ning Zhang,\\nDimitris N Metaxas, and Licheng Yu. 2024. Accelerating Multimodel Large Language Models by Searching Optimal\\nVision Token Reduction. arXiv preprint arXiv:2412.00556 (2024).\\n[485] Shengwei Zhao, Linhai Xu, Yuying Liu, and Shaoyi Du. 2023. Multi-grained representation learning for cross-modal\\nretrieval. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information\\nRetrieval. 2194–2198.\\n[486] Weichao Zhao, Hao Feng, Qi Liu, Jingqun Tang, Shu Wei, Binghong Wu, Lei Liao, Yongjie Ye, Hao Liu, Wengang'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 78, 'page_label': '79', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Retrieval. 2194–2198.\\n[486] Weichao Zhao, Hao Feng, Qi Liu, Jingqun Tang, Shu Wei, Binghong Wu, Lei Liao, Yongjie Ye, Hao Liu, Wengang\\nZhou, et al. 2024. Tabpedia: Towards comprehensive visual table understanding with concept synergy. arXiv preprint\\narXiv:2406.01326 (2024).\\n[487] Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang. 2023. Bubogpt: Enabling visual\\ngrounding in multi-modal llms. arXiv preprint arXiv:2307.08581 (2023).\\n[488] Zijia Zhao, Haoyu Lu, Yuqi Huo, Yifan Du, Tongtian Yue, Longteng Guo, Bingning Wang, Weipeng Chen, and Jing\\nLiu. 2024. Needle In A Video Haystack: A Scalable Synthetic Framework for Benchmarking Video MLLMs. arXiv\\npreprint arXiv:2406.09367 (2024).\\n[489] Liangli Zhen, Peng Hu, Xu Wang, and Dezhong Peng. 2019. Deep supervised cross-modal retrieval. In Proceedings of\\nthe IEEE/CVF conference on computer vision and pattern recognition . 10394–10403.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 78, 'page_label': '79', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='the IEEE/CVF conference on computer vision and pattern recognition . 10394–10403.\\n[490] Kaizhi Zheng, Xuehai He, and Xin Eric Wang. 2023. Minigpt-5: Interleaved vision-and-language generation via\\ngenerative vokens. arXiv preprint arXiv:2310.02239 (2023).\\n[491] Liu Zheng and Shao Yingxia. 2022. RetroMAE: Pre-training retrieval-oriented transformers via masked auto-encoder.\\narXiv: 2205.12035 (2022).\\n[492] Xiaoyang Zheng, Zilong Wang, Sen Li, Ke Xu, Tao Zhuang, Qingwen Liu, and Xiaoyi Zeng. 2023. Make: Vision-\\nlanguage pre-training based product retrieval in taobao search. In Companion Proceedings of the ACM Web Conference\\n2023. 356–360.\\n[493] Chenyu Zhou, Mengdan Zhang, Peixian Chen, Chaoyou Fu, Yunhang Shen, Xiawu Zheng, Xing Sun, and Rongrong\\nJi. 2024. VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models. arXiv preprint\\narXiv:2406.10228 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 78, 'page_label': '79', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='Ji. 2024. VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models. arXiv preprint\\narXiv:2406.10228 (2024).\\n[494] Dong Zhou, Fang Lei, Lin Li, Yongmei Zhou, and Aimin Yang. 2024. Cross-Modal Interaction via Reinforcement\\nFeedback for Audio-Lyrics Retrieval. IEEE/ACM Transactions on Audio, Speech, and Language Processing (2024).\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 79, 'page_label': '80', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='80 Trovato et al.\\n[495] Junjie Zhou, Zheng Liu, Shitao Xiao, Bo Zhao, and Yongping Xiong. 2024. VISTA: visualized text embedding for\\nuniversal multi-modal retrieval. arXiv preprint arXiv:2406.04292 (2024).\\n[496] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and\\nZheng Liu. 2024. MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding. arXiv preprint\\narXiv:2406.04264 (2024).\\n[497] Kun Zhou, Yeyun Gong, Xiao Liu, Wayne Xin Zhao, Yelong Shen, Anlei Dong, Jingwen Lu, Rangan Majumder, Ji-Rong\\nWen, Nan Duan, et al. 2022. Simans: Simple ambiguous negatives sampling for dense text retrieval. arXiv preprint\\narXiv:2210.11773 (2022).\\n[498] Tianshuo Zhou, Sen Mei, Xinze Li, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu, Yu Gu, and Ge Yu. 2023. MARVEL:\\nunlocking the multi-modal capability of dense retrieval via visual module plugin. arXiv preprint arXiv:2310.14037\\n(2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 79, 'page_label': '80', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='unlocking the multi-modal capability of dense retrieval via visual module plugin. arXiv preprint arXiv:2310.14037\\n(2023).\\n[499] Wangchunshu Zhou, Yuchen Eleanor Jiang, Ryan Cotterell, and Mrinmaya Sachan. 2023. Efficient prompting via\\ndynamic in-context learning. arXiv preprint arXiv:2305.11170 (2023).\\n[500] Yujia Zhou, Zhicheng Dou, and Ji-Rong Wen. 2023. Enhancing generative retrieval with reinforcement learning\\nfrom relevance feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing .\\n12481–12490.\\n[501] Yujia Zhou, Jing Yao, Zhicheng Dou, Ledell Wu, Peitian Zhang, and Ji-Rong Wen. 2022. Ultron: An ultimate retriever\\non corpus with a model-based indexer. arXiv preprint arXiv:2208.09257 (2022).\\n[502] Yu-Jia Zhou, Jing Yao, Zhi-Cheng Dou, Ledell Wu, and Ji-Rong Wen. 2023. DynamicRetriever: a pre-trained model-\\nbased IR system without an explicit index. Machine Intelligence Research 20, 2 (2023), 276–288.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 79, 'page_label': '80', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='based IR system without an explicit index. Machine Intelligence Research 20, 2 (2023), 276–288.\\n[503] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei\\nLi, et al. 2023. Languagebind: Extending video-language pretraining to n-modality by language-based semantic\\nalignment. arXiv preprint arXiv:2310.01852 (2023).\\n[504] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language\\nunderstanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023).\\n[505] Dongsheng Zhu, Xunzhu Tang, Weidong Han, Jinghui Lu, Yukun Zhao, Guoliang Xing, Junfeng Wang, and Dawei\\nYin. 2024. Vislinginstruct: Elevating zero-shot learning in multi-modal language models with autonomous instruction\\noptimization. arXiv preprint arXiv:2402.07398 (2024).\\n[506] Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie Zhao, Hengshuang Zhao, Xiaohua Wang, and Ying Shan. 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 79, 'page_label': '80', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='optimization. arXiv preprint arXiv:2402.07398 (2024).\\n[506] Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie Zhao, Hengshuang Zhao, Xiaohua Wang, and Ying Shan. 2023.\\nVl-gpt: A generative pre-trained transformer for vision and language understanding and generation. arXiv preprint\\narXiv:2312.09251 (2023).\\n[507] Yichen Zhu, Minjie Zhu, Ning Liu, Zhiyuan Xu, and Yaxin Peng. 2024. Llava-phi: Efficient multi-modal assistant\\nwith small language model. In Proceedings of the 1st International Workshop on Efficient Multimedia Computing under\\nLimited. 18–22.\\n[508] Zhengyuan Zhu, Daniel Lee, Hong Zhang, Sai Sree Harsha, Loic Feujio, Akash Maharaj, and Yunyao Li. 2024. MuRAR:\\nA Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering.\\narXiv:2408.08521 [cs.IR] https://arxiv.org/abs/2408.08521\\n[509] Honglei Zhuang, Zhen Qin, Kai Hui, Junru Wu, Le Yan, Xuanhui Wang, and Michael Bendersky. 2023. Beyond yes and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 79, 'page_label': '80', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='arXiv:2408.08521 [cs.IR] https://arxiv.org/abs/2408.08521\\n[509] Honglei Zhuang, Zhen Qin, Kai Hui, Junru Wu, Le Yan, Xuanhui Wang, and Michael Bendersky. 2023. Beyond yes and\\nno: Improving zero-shot llm rankers via scoring fine-grained relevance labels. arXiv preprint arXiv:2310.14122 (2023).\\n[510] Honglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui, Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, and Michael Bendersky.\\n2023. Rankt5: Fine-tuning t5 for text ranking with ranking losses. In Proceedings of the 46th International ACM SIGIR\\nConference on Research and Development in Information Retrieval . 2308–2313.\\n[511] Shengyao Zhuang, Bing Liu, Bevan Koopman, and Guido Zuccon. 2023. Open-source large language models are\\nstrong zero-shot query likelihood models for document ranking. arXiv preprint arXiv:2310.13243 (2023).\\n[512] Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuccon, and Daxin Jiang. 2022. Bridging'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2025-04-15T00:00:41+00:00', 'moddate': '2025-04-15T00:00:41+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '-  Information systems  ->  Multimedia and multimodal retrieval.Language models.-  Computing methodologies  ->  Natural language processing.', 'title': 'A Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Multimodal Retrieval-Augmented Generation.pdf', 'total_pages': 80, 'page': 79, 'page_label': '80', 'source_file': 'Multimodal Retrieval-Augmented Generation.pdf', 'file_type': 'pdf'}, page_content='[512] Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuccon, and Daxin Jiang. 2022. Bridging\\nthe gap between indexing and retrieval for differentiable search index with query generation. arXiv preprint\\narXiv:2206.10128 (2022).\\n[513] Justin Zobel and Alistair Moffat. 2006. Inverted files for text search engines. ACM computing surveys (CSUR) 38, 2\\n(2006), 6–es.\\n[514] Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, and Timothy Hospedales. 2024. Safety fine-tuning at\\n(almost) no cost: A baseline for vision large language models. arXiv preprint arXiv:2402.02207 (2024).\\nReceived 20 February 2007; revised 12 March 2009; accepted 5 June 2009\\n, Vol. 1, No. 1, Article . Publication date: April 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 1\\nNatural Language Processing Advancements By\\nDeep Learning: A Survey\\nAmirsina Torﬁ, Member, IEEE, Rouzbeh A. Shirvani, Yaser Keneshloo, Nader Tavaf,\\nand Edward A. Fox, Fellow, IEEE\\nAbstract—Natural Language Processing (NLP) helps empower\\nintelligent machines by enhancing a better understanding of the\\nhuman language for linguistic-based human-computer communi-\\ncation. Recent developments in computational power and the ad-\\nvent of large amounts of linguistic data have heightened the need\\nand demand for automating semantic analysis using data-driven\\napproaches. The utilization of data-driven strategies is pervasive\\nnow due to the signiﬁcant improvements demonstrated through\\nthe usage of deep learning methods in areas such as Computer\\nVision, Automatic Speech Recognition, and in particular, NLP.\\nThis survey categorizes and addresses the different aspects and\\napplications of NLP that have beneﬁted from deep learning. It'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='Vision, Automatic Speech Recognition, and in particular, NLP.\\nThis survey categorizes and addresses the different aspects and\\napplications of NLP that have beneﬁted from deep learning. It\\ncovers core NLP tasks and applications, and describes how deep\\nlearning methods and models advance these areas. We further\\nanalyze and compare different approaches and state-of-the-art\\nmodels.\\nIndex Terms—Natural Language Processing, Deep Learning,\\nArtiﬁcial Intelligence\\nI. I NTRODUCTION\\nN\\nATURAL Language Processing (NLP) is a sub-discipline\\nof computer science providing a bridge between natural\\nlanguages and computers. It helps empower machines to un-\\nderstand, process, and analyze human language [1]. NLP’s sig-\\nniﬁcance as a tool aiding comprehension of human-generated\\ndata is a logical consequence of the context-dependency\\nof data. Data becomes more meaningful through a deeper\\nunderstanding of its context, which in turn facilitates text\\nanalysis and mining. NLP enables this with the communication'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='of data. Data becomes more meaningful through a deeper\\nunderstanding of its context, which in turn facilitates text\\nanalysis and mining. NLP enables this with the communication\\nstructures and patterns of humans.\\nDevelopment of NLP methods is increasingly reliant on\\ndata-driven approaches which help with building more pow-\\nerful and robust models [2]–[4]. Recent advances in com-\\nputational power, as well as greater availability of big data,\\nenable deep learning, one of the most appealing approaches\\nin the NLP domain [2], [3], [5], especially given that deep\\nlearning has already demonstrated superior performance in\\nadjoining ﬁelds like Computer Vision [6]–[10] and Speech\\nRecognition [11]–[13]. These developments led to a paradigm\\nshift from traditional to novel data-driven approaches aimed\\nat advancing NLP. The reason behind this shift was simple:\\nnew approaches are more promising regarding results, and are\\neasier to engineer.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='at advancing NLP. The reason behind this shift was simple:\\nnew approaches are more promising regarding results, and are\\neasier to engineer.\\nAmirsina Torﬁ, Yaser Keneshloo, and Edward A. Fox were with the\\nDepartment of Computer Science, Virginia Polytechnic Institute and State\\nUniversity, Blacksburg, V A, 24060 USA e-mail: (amirsina.torﬁ@gmail.com,\\nyaserkl@vt.edu, fox@vt.edu). Rouzbeh A. Shirvani is an independent re-\\nsearcher, e-mail: (rouzbeh.asghari@gmail.com). Nader Tavaf was with the\\nUniversity of Minnesota Twin Cities, Minneapolis, MN, 55455 USA e-mail:\\n(tavaf001@umn.edu).\\nAs a sequitur to remarkable progress achieved in adjacent\\ndisciplines utilizing deep learning methods, deep neural net-\\nworks have been applied to various NLP tasks, including part-\\nof-speech tagging [14]–[17], named entity recognition [18],\\n[18]–[21], and semantic role labeling [22]–[25]. Most of the\\nresearch efforts in deep learning associated with NLP appli-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='of-speech tagging [14]–[17], named entity recognition [18],\\n[18]–[21], and semantic role labeling [22]–[25]. Most of the\\nresearch efforts in deep learning associated with NLP appli-\\ncations involve either supervised learning 1 or unsupervised\\nlearning2.\\nThis survey covers the emerging role of deep learning in the\\narea of NLP, across a broad range of categories. The research\\npresented in [26] is primarily focused on architectures, with\\nlittle discussion of applications. More recent works [4], [27]\\nare speciﬁc to certain applications or certain sub-ﬁelds of\\nNLP [21]. Here we build on previous works by describing\\nthe challenges, opportunities, and evaluations of the impact of\\napplying deep learning to NLP problems.\\nThis survey has six sections, including this introduction.\\nSection 2 lays out the theoretical dimensions of NLP and\\nartiﬁcial intelligence, and looks at deep learning as an ap-\\nproach to solving real-world problems. It motivates this study'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='Section 2 lays out the theoretical dimensions of NLP and\\nartiﬁcial intelligence, and looks at deep learning as an ap-\\nproach to solving real-world problems. It motivates this study\\nby addressing the question: Why use deep learning in NLP?\\nThe third section discusses fundamental concepts necessary\\nto understand NLP, covering exemplary issues in representa-\\ntion, frameworks, and machine learning. The fourth section\\nsummarizes benchmark datasets employed in the NLP domain.\\nSection 5 focuses on some of the NLP applications where deep\\nlearning has demonstrated signiﬁcant beneﬁt. Finally, Section\\n6 provides a conclusion, also addressing some open problems\\nand promising areas for improvement.\\nII. B ACKGROUND\\nNLP has long been viewed as one aspect of artiﬁcial\\nintelligence (AI), since understanding and generating natural\\nlanguage are high-level indications of intelligence. Deep learn-\\ning is an effective AI tool, so we next situate deep learning in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='intelligence (AI), since understanding and generating natural\\nlanguage are high-level indications of intelligence. Deep learn-\\ning is an effective AI tool, so we next situate deep learning in\\nthe AI world. After that we explain motivations for applying\\ndeep learning to NLP.\\nA. Artiﬁcial Intelligence and Deep Learning\\nThere have been “islands of success” where big data are\\nprocessed via AI capabilities to produce information to achieve\\ncritical operational goals (e.g., fraud detection). Accordingly,\\n1Learning from training data to predict the type of new unseen test examples\\nby mapping them to known pre-deﬁned labels.\\n2Making sense of data without sticking to speciﬁc tasks and supervisory\\nsignals.\\narXiv:2003.01200v4  [cs.CL]  27 Feb 2021'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 2\\nscientists and consumers anticipate enhancement across a\\nvariety of applications. However, achieving this requires un-\\nderstanding of AI and its mechanisms and means (e.g., algo-\\nrithms). Ted Greenwald, explaining AI to those who are not\\nAI experts, comments: ”Generally AI is anything a computer\\ncan do that formerly was considered a job for a human” [28].\\nAn AI goal is to extend the capabilities of information\\ntechnology (IT) from those to (1) generate, communicate,\\nand store data, to also (2) process data into the knowledge\\nthat decision makers and others need [29]. One reason is\\nthat the available data volume is increasing so rapidly that\\nit is now impossible for people to process all available data.\\nThis leaves two choices: (1) much or even most existing data\\nmust be ignored or (2) AI must be developed to process the\\nvast volumes of available data into the essential pieces of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='This leaves two choices: (1) much or even most existing data\\nmust be ignored or (2) AI must be developed to process the\\nvast volumes of available data into the essential pieces of\\ninformation that decision-makers and others can comprehend.\\nDeep learning is a bridge between the massive amounts of\\ndata and AI.\\n1) Deﬁnitions: Deep learning refers to applying deep neu-\\nral networks to massive amounts of data to learn a procedure\\naimed at handling a task . The task can range from simple\\nclassiﬁcation to complex reasoning. In other words, deep\\nlearning is a set of mechanisms ideally capable of deriving an\\noptimum solution to any problem given a sufﬁciently extensive\\nand relevant input dataset. Loosely speaking, deep learning\\nis detecting and analyzing important structures/features in the\\ndata aimed at formulating a solution to a given problem. Here,\\nAI and deep learning meet. One version of the goal or ambition\\nbehind AI is enabling a machine to outperform what the human'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='data aimed at formulating a solution to a given problem. Here,\\nAI and deep learning meet. One version of the goal or ambition\\nbehind AI is enabling a machine to outperform what the human\\nbrain does. Deep learning is a means to this end.\\n2) Deep Learning Architectures: Numerous deep learning\\narchitectures have been developed in different research areas,\\ne.g., in NLP applications employing recurrent neural networks\\n(RNNs) [30], convolutional neural networks (CNNs) [31], and\\nmore recently, recursive neural networks [32]. We focus our\\ndiscussion on a review of the essential models, explained in\\nrelevant seminal publications.\\nMulti Layer Perceptron: A multilayer perceptron (MLP)\\nhas at least three layers (input, hidden, and output layers). A\\nlayer is simply a collection of neurons operating to transform\\ninformation from the previous layer to the next layer. In the\\nMLP architecture, the neurons in a layer do not communicate\\nwith each other. An MLP employs nonlinear activation func-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='information from the previous layer to the next layer. In the\\nMLP architecture, the neurons in a layer do not communicate\\nwith each other. An MLP employs nonlinear activation func-\\ntions. Every node in a layer connects to all nodes in the next\\nlayer, creating a fully connected network (Fig. 1). MLPs are\\nthe simplest type of Feed-Forward Neural Networks (FNNs).\\nFNNs represent a general category of neural networks in which\\nthe connections between the nodes do not create any cycle, i.e.,\\nin a FNN there is no cycle of information ﬂow.\\nConvolutional Neural Networks: Convolutional neural\\nnetworks (CNNs), whose architecture is inspired by the human\\nvisual cortex, are a subclass of feed-forward neural networks.\\nCNNs are named after the underlying mathematical operation,\\nconvolution, which yields a measure of the interoperability of\\nits input functions. Convolutional neural networks are usually\\nemployed in situations where data is or needs to be represented'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='convolution, which yields a measure of the interoperability of\\nits input functions. Convolutional neural networks are usually\\nemployed in situations where data is or needs to be represented\\nwith a 2D or 3D data map. In the data map representation,\\nthe proximity of data points usually corresponds to their\\nFig. 1. The general architecture of a MLP.\\ninformation correlation.\\nIn convolutional neural networks where the input is an\\nimage, the data map indicates that image pixels are highly cor-\\nrelated to their neighboring pixels. Consequently, the convolu-\\ntional layers have 3 dimensions: width, height, and depth. That\\nassumption possibly explains why the majority of research\\nefforts dedicated to CNNs are conducted in the Computer\\nVision ﬁeld [33].\\nA CNN takes an image represented as an array of numeric\\nvalues. After performing speciﬁc mathematical operations, it\\nrepresents the image in a new output space. This operation is\\nalso called feature extraction, and helps to capture and rep-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='values. After performing speciﬁc mathematical operations, it\\nrepresents the image in a new output space. This operation is\\nalso called feature extraction, and helps to capture and rep-\\nresent key image content. The extracted features can be used\\nfor further analysis, for different tasks. One example is image\\nclassiﬁcation, which aims to categorize images according to\\nsome predeﬁned classes. Other examples include determining\\nwhich objects are present in an image and where they are\\nlocated. See Fig. 2.\\nIn the case of utilizing CNNs for NLP, the inputs are sen-\\ntences or documents represented as matrices. Each row of the\\nmatrix is associated with a language element such as a word\\nor a character. The majority of CNN architectures learn word\\nor sentence representations in their training phase. A variety\\nof CNN architectures were used in various classiﬁcation tasks\\nsuch as Sentiment Analysis and Topic Categorization [31],\\n[34]–[36]. CNNs were employed for Relation Extraction and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='of CNN architectures were used in various classiﬁcation tasks\\nsuch as Sentiment Analysis and Topic Categorization [31],\\n[34]–[36]. CNNs were employed for Relation Extraction and\\nRelation Classiﬁcation as well [37], [38].\\nRecurrent Neural Network: If we line up a sequence of\\nFNNs and feed the output of each FNN as an input to the next\\none, a recurrent neural network (RNN) will be constructed.\\nLike FNNs, layers in an RNN can be categorized into input,\\nhidden, and output layers. In discrete time frames, sequences\\nof input vectors are fed as the input, one vector at a time,\\ne.g., after inputting each batch of vectors, conducting some\\noperations and updating the network weights, the next input\\nbatch will be fed to the network. Thus, as shown in Fig. 3,\\nat each time step we make predictions and use parameters of\\nthe current hidden layer as input to the next time step.\\nHidden layers in recurrent neural networks can carry infor-\\nmation from the past, in other words, memory. This character-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='the current hidden layer as input to the next time step.\\nHidden layers in recurrent neural networks can carry infor-\\nmation from the past, in other words, memory. This character-\\nistic makes them speciﬁcally useful for applications that deal'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 3\\nFig. 2. A typical CNN architecture for object detection. The network provides a feature representation with attention to the speciﬁc region of an image\\n(example shown on the left) that contains the object of interest. Out of the multiple regions represented (see an ordering of the image blocks, giving image\\npixel intensity, on the right) by the network, the one with the highest score will be selected as the main candidate.\\nFig. 3. Recurrent Neural Network (RNN), summarized on the left, expanded\\non the right, for N timesteps, with X indicating input, h hidden layer, and\\nO output\\nwith a sequence of inputs such as language modeling [39], i.e.,\\nrepresenting language in a way that the machine understands.\\nThis concept will be described later in detail.\\nRNNs can carry rich information from the past. Consider\\nthe sentence: “Michael Jackson was a singer; some people\\nconsider him King of Pop.” It’s easy for a human to identify'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='RNNs can carry rich information from the past. Consider\\nthe sentence: “Michael Jackson was a singer; some people\\nconsider him King of Pop.” It’s easy for a human to identify\\nhim as referring to Michael Jackson. The pronoun him happens\\nseven words after Michael Jackson; capturing this dependency\\nis one of the beneﬁts of RNNs, where the hidden layers in an\\nRNN act as memory units. Long Short Term Memory Network\\n(LSTM) [40] is one of the most widely used classes of RNNs.\\nLSTMs try to capture even long time dependencies between\\ninputs from different time steps. Modern Machine Translation\\nand Speech Recognition often rely on LSTMs.\\nFig. 4. Schematic of an Autoencoder\\nAutoencoders: Autoencoders implement unsupervised\\nmethods in deep learning. They are widely used in dimension-\\nality reduction3 or NLP applications which consist of sequence\\n3Dimensionality reduction is an unsupervised learning approach which is\\nthe process of reducing the number of variables that were used to represent'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='3Dimensionality reduction is an unsupervised learning approach which is\\nthe process of reducing the number of variables that were used to represent\\nthe data by identifying the most crucial information.\\nto sequence modeling (see Section III-B [39]. Fig. 4 illustrates\\nthe schematic of an Autoencoder. Since autoencoders are\\nunsupervised, there is no label corresponding to each input.\\nThey aim to learn a code representation for each input. The\\nencoder is like a feed-forward neural network in which the\\ninput gets encoded into a vector (code). The decoder operates\\nsimilarly to the encoder, but in reverse, i.e., constructing\\nan output based on the encoded input. In data compression\\napplications, we want the created output to be as close as\\npossible to the original input. Autoencoders are lossy, meaning\\nthe output is an approximate reconstruction of the input.\\nFig. 5. Generative Adversarial Networks\\nGenerative Adversarial Networks: Goodfellow [41] intro-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='the output is an approximate reconstruction of the input.\\nFig. 5. Generative Adversarial Networks\\nGenerative Adversarial Networks: Goodfellow [41] intro-\\nduced Generative Adversarial Networks (GANs) . As shown in\\nFig. 5, a GAN is a combination of two neural networks, a\\ndiscriminator and a generator. The whole network is trained\\nin an iterative process. First, the generator network generates a\\nfake sample. Then the discriminator network tries to determine\\nwhether this sample (ex.: an input image) is real or fake, i.e.,\\nwhether it came from the real training data (data used for\\nbuilding the model) or not. The goal of the generator is to fool\\nthe discriminator in a way that the discriminator believes the\\nartiﬁcial (i.e., generated) samples synthesized by the generator\\nare real.\\nThis iterative process continues until the generator produces\\nsamples that are indistinguishable by the discriminator. In'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 4\\nother words, the probability of classifying a sample as fake\\nor real becomes like ﬂipping a fair coin for the discriminator.\\nThe goal of the generative model is to capture the distribution\\nof real data while the discriminator tries to identify the fake\\ndata. One of the interesting features of GANs (regarding being\\ngenerative) is: once the training phase is ﬁnished, there is no\\nneed for the discrimination network, so we solely can work\\nwith the generation network. In other words, having access to\\nthe trained generative model is sufﬁcient.\\nDifferent forms of GANs has been introduced, e.g., Sim\\nGAN [8], Wasserstein GAN [42], info GAN [43], and DC\\nGAN [44]. In one of the most elegant GAN implementations\\n[45], entirely artiﬁcial, yet almost perfect, celebrity faces are\\ngenerated; the pictures are not real, but fake photos produced\\nby the network. GAN’s have since received signiﬁcant atten-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='[45], entirely artiﬁcial, yet almost perfect, celebrity faces are\\ngenerated; the pictures are not real, but fake photos produced\\nby the network. GAN’s have since received signiﬁcant atten-\\ntion in various applications and have generated astonishing\\nresult [46]. In the NLP domain, GANs often are used for text\\ngeneration [47], [48].\\nB. Motivation for Deep Learning in NLP\\nDeep learning applications are predicated on the choices\\nof (1) feature representation and (2) deep learning algo-\\nrithm alongside architecture. These are associated with data\\nrepresentation and learning structure, respectively. For data\\nrepresentation, surprisingly, there usually is a disjunction\\nbetween what information is thought to be important for\\nthe task at hand, versus what representation actually yields\\ngood results. For instance, in sentiment analysis, lexicon\\nsemantics, syntactic structure, and context are assumed by\\nsome linguists to be of primary signiﬁcance. Nevertheless,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='good results. For instance, in sentiment analysis, lexicon\\nsemantics, syntactic structure, and context are assumed by\\nsome linguists to be of primary signiﬁcance. Nevertheless,\\nprevious studies based on the bag-of-words (BoW) model\\ndemonstrated acceptable performance [49]. The bag-of-words\\nmodel [50], often viewed as the vector space model, involves\\na representation which accounts only for the words and\\ntheir frequency of occurrence. BoW ignores the order and\\ninteraction of words, and treats each word as a unique feature.\\nBoW disregards syntactic structure, yet provides decent results\\nfor what some would consider syntax-dependent applications.\\nThis observation suggests that simple representations, when\\ncoupled with large amounts of data, may work as well or better\\nthan more complex representations. These ﬁndings corroborate\\nthe argument in favor of the importance of deep learning\\nalgorithms and architectures.\\nOften the progress of NLP is bound to effective language'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='the argument in favor of the importance of deep learning\\nalgorithms and architectures.\\nOften the progress of NLP is bound to effective language\\nmodeling. A goal of statistical language modeling is the prob-\\nabilistic representation of word sequences in language, which\\nis a complicated task due to the curse of dimensionality. The\\nresearch presented in [51] was a breakthrough for language\\nmodeling with neural networks aimed at overcoming the curse\\nof dimensionality by (1) learning a distributed representation\\nof words and (2) providing a probability function for se-\\nquences.\\nA key challenge in NLP research, compared to other do-\\nmains such as Computer Vision, seems to be the complexity\\nof achieving an in-depth representation of language using\\nstatistical models. A primary task in NLP applications is to\\nprovide a representation of texts, such as documents. This in-\\nvolves feature learning, i.e., extracting meaningful information\\nto enable further processing and analysis of the raw data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='provide a representation of texts, such as documents. This in-\\nvolves feature learning, i.e., extracting meaningful information\\nto enable further processing and analysis of the raw data.\\nTraditional methods begin with time-consuming hand-\\ncrafting of features, through careful human analysis of a\\nspeciﬁc application, and are followed by development of\\nalgorithms to extract and utilize instances of those features.\\nOn the other hand, deep supervised feature learning methods\\nare highly data-driven and can be used in more general efforts\\naimed at providing a robust data representation.\\nDue to the vast amounts of unlabeled data, unsupervised\\nfeature learning is considered to be a crucial task in NLP. Un-\\nsupervised feature learning is, in essence, learning the features\\nfrom unlabeled data to provide a low-dimensional representa-\\ntion of a high-dimensional data space. Several approaches such\\nas K-means clustering and principal component analysis have'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='from unlabeled data to provide a low-dimensional representa-\\ntion of a high-dimensional data space. Several approaches such\\nas K-means clustering and principal component analysis have\\nbeen proposed and successfully implemented to this end. With\\nthe advent of deep learning and abundance of unlabeled\\ndata, unsupervised feature learning becomes a crucial task for\\nrepresentation learning, a precursor in NLP applications. Cur-\\nrently, most of the NLP tasks rely on annotated data, while a\\npreponderance of unannotated data further motivates research\\nin leveraging deep data-driven unsupervised methods.\\nGiven the potential superiority of deep learning approaches\\nin NLP applications, it seems crucial to perform a com-\\nprehensive analysis of various deep learning methods and\\narchitectures with particular attention to NLP applications.\\nIII. C ORE CONCEPTS IN NLP\\nA. Feature Representation\\nDistributed representations are a series of compact, low'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='architectures with particular attention to NLP applications.\\nIII. C ORE CONCEPTS IN NLP\\nA. Feature Representation\\nDistributed representations are a series of compact, low\\ndimensional representations of data, each representing some\\ndistinct informative property. For NLP systems, due to issues\\nrelated to the atomic representation of the symbols, it is\\nimperative to learn word representations.\\nAt ﬁrst, let’s concentrate on how the features are rep-\\nresented, and then we focus on different approaches for\\nlearning word representations. The encoded input features can\\nbe characters, words [32], sentences [52], or other linguistic\\nelements. Generally, it is more desirable to provide a compact\\nrepresentation of the words than a sparse one.\\nFig. 6. Considering a given sequence, the skip-thought model generates the\\nsurrounding sequences using the trained encoder. The assumption is that the\\nsurrounding sentences are closely related, contextually.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='surrounding sequences using the trained encoder. The assumption is that the\\nsurrounding sentences are closely related, contextually.\\nHow to select the structure and level of text representa-\\ntion used to be an unresolved question. After proposing the\\nword2vec approach [53], subsequently, doc2vec was proposed\\nin [52] as an unsupervised algorithm and was called Paragraph'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 5\\nVector (PV). The goal behind PV is to learn ﬁxed-length rep-\\nresentations from variable-length text parts such as sentences\\nand documents. One of the main objectives of doc2vec is\\nto overcome the drawbacks of models such as BoW and to\\nprovide promising results for applications such as text classi-\\nﬁcation and sentiment analysis. A more recent approach is the\\nskip-thought model which applies word2vec at the sentence-\\nlevel [54]. By utilizing an encoder-decoder architecture, this\\nmodel generates the surrounding sentences using the given\\nsentence (Fig. 6). Next, let’s investigate different kinds of\\nfeature representation.\\n1) One-Hot Representation: In one-hot encoding, each\\nunique element that needs to be represented has its dimen-\\nsion which results in a very high dimensional, very sparse\\nrepresentation. Assume the words are represented with the\\none-hot encoding method. Regarding representation structure,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='sion which results in a very high dimensional, very sparse\\nrepresentation. Assume the words are represented with the\\none-hot encoding method. Regarding representation structure,\\nthere is no meaningful connection between different words in\\nthe feature space. For example, highly correlated words such\\nas ‘ocean’ and ‘water’ will not be closer to each other (in the\\nrepresentation space) compared to less correlated pairs such as\\n‘ocean’ and ‘ﬁre.’ Nevertheless, some research efforts present\\npromising results using one-hot encoding [2].\\n2) Continuous Bag of Words: Continuous Bag-of-Words\\nmodel (CBOW) has frequently been used in NLP applica-\\ntions. CBOW tries to predict a word given its surrounding\\ncontext, which usually consists of a few nearby words [55].\\nCBOW is neither dependent on the sequential order of words\\nnor necessarily on probabilistic characteristics. So it is not\\ngenerally used for language modeling. This model is typi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='CBOW is neither dependent on the sequential order of words\\nnor necessarily on probabilistic characteristics. So it is not\\ngenerally used for language modeling. This model is typi-\\ncally trained to be utilized as a pre-trained model for more\\nsophisticated tasks. An alternative to CBOW is the weighted\\nCBOW (WCBOW) [56] in which different vectors get different\\nweights reﬂective of relative importance in context. The sim-\\nplest example can be document categorization where features\\nare words and weights are TF-IDF scores [57] of the associated\\nwords.\\n3) Word-Level Embedding: Word embedding is a learned\\nrepresentation for context elements in which, ideally, words\\nwith related semantics become highly correlated in the rep-\\nresentation space. One of the main incentives behind word\\nembedding representations is the high generalization power\\nas opposed to sparse, higher dimensional representations [58].\\nUnlike the traditional bag-of-words model in which different'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='embedding representations is the high generalization power\\nas opposed to sparse, higher dimensional representations [58].\\nUnlike the traditional bag-of-words model in which different\\nwords have entirely different representations regardless of their\\nusage or collocations, learning a distributed representation\\ntakes advantage of word usage in context to provide similar\\nrepresentations for semantically correlated words. There are\\ndifferent approaches to create word embeddings. Several re-\\nsearch efforts, including [53], [55], used random initialization\\nby uniformly sampling random numbers with the objective of\\ntraining an efﬁcient representation of the model on a large\\ndataset. This setup is intuitively acceptable for initialization\\nof the embedding for common features such as part-of-speech\\ntags. However, this may not be the optimum method for rep-\\nresentation of less frequent features such as individual words.\\nFor the latter, pre-trained models, trained in a supervised or'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='tags. However, this may not be the optimum method for rep-\\nresentation of less frequent features such as individual words.\\nFor the latter, pre-trained models, trained in a supervised or\\nunsupervised manner, are usually leveraged for increasing the\\nperformance.\\n4) Character-Level Embedding: The methods mentioned\\nearlier are mostly at higher levels of representation. Lower-\\nlevel representations such as character-level representation\\nrequire special attention as well, due to their simplicity of\\nrepresentation and the potential for correction of unusual\\ncharacter combinations such as misspellings [2]. For generat-\\ning character-level embeddings, CNNs have successfully been\\nutilized [14].\\nCharacter-level embeddings have been used in different\\nNLP applications [59]. One of the main advantages is the\\nability to use small model sizes and represent words with\\nlower-level language elements [14]. Here word embeddings\\nare models utilizing CNNs over the characters. Another mo-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='ability to use small model sizes and represent words with\\nlower-level language elements [14]. Here word embeddings\\nare models utilizing CNNs over the characters. Another mo-\\ntivation for employing character-level embeddings is the out-\\nof-vocabulary word (OOV) issue which is usually encountered\\nwhen, for the given word, there is no equivalent vector in\\nthe word embedding. The character-level approach may sig-\\nniﬁcantly alleviate this problem. Nevertheless, this approach\\nsuffers from a weak correlation between characters and se-\\nmantic and syntactic parts of the language. So, considering\\nthe aforementioned pros and cons of utilizing character-level\\nembeddings, several research efforts tried to propose and im-\\nplement higher-level approaches such as using sub-words [60]\\nto create word embeddings for OOV instances as well as\\ncreating a semantic bridge between the correlated words [61].\\nB. Seq2Seq Framework\\nMost underlying frameworks in NLP applications rely on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='to create word embeddings for OOV instances as well as\\ncreating a semantic bridge between the correlated words [61].\\nB. Seq2Seq Framework\\nMost underlying frameworks in NLP applications rely on\\nsequence-to-sequence (seq2seq) models in which not only the\\ninput but also the output is represented as a sequence. These\\nmodels are common in various applications including machine\\ntranslation4, text summarization 5, speech-to-text, and text-to-\\nspeech applications6.\\nThe most common seq2seq framework is comprised of an\\nencoder and a decoder. The encoder ingests the sequence of\\ninput data and generates a mid-level output which is subse-\\nquently consumed by the decoder to produce the series of ﬁnal\\noutputs. The encoder and decoder are usually implemented via\\na series of Recurrent Neural Networks or LSTM [40] cells.\\nThe encoder takes a sequence of length T, X =\\n{x1,x2,··· ,xT}, where xt ∈ V = {1,··· ,|V|} is the\\nrepresentation of a single input coming from the vocabulary'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='The encoder takes a sequence of length T, X =\\n{x1,x2,··· ,xT}, where xt ∈ V = {1,··· ,|V|} is the\\nrepresentation of a single input coming from the vocabulary\\nV, and then generates the output state ht. Subsequently, the\\ndecoder takes the last state from the encoder, i.e., ht, and\\nstarts generating an output of size L, Y′= {y′\\n1,y′\\n2,··· ,y′\\nL},\\nbased on its current state, st, and the ground-truth output yt.\\nIn different applications, the decoder could take advantage\\nof more information such as a context vector [62] or intra-\\nattention vectors [63] to generate better outputs.\\nOne of the most widely training approaches for seq2seq\\nmodels is called Teacher Forcing [64]. Let us deﬁne y =\\n4The input is a sequence of words from one language (e.g., English) and\\nthe output is the translation to another language (e.g., French).\\n5The input is a complete document (sequence of words) and the output is\\na summary of it (sequence of words).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='the output is the translation to another language (e.g., French).\\n5The input is a complete document (sequence of words) and the output is\\na summary of it (sequence of words).\\n6The input is an audio recording of a speech (sequence of audible elements)\\nand the output is the speech text (sequence of words).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 6\\n{y1,y2,··· ,yL}as the ground-truth output sequence corre-\\nspondent to a given input sequence X. The model training\\nbased on the maximum-likelihood criterion employs the fol-\\nlowing cross-entropy (CE) loss minimization:\\nLCE = −\\nL∑\\nt=1\\nlog pθ(yt|yt−1,st,X) (1)\\nwhere θ is the parameters of the model optimized during the\\ntraining.\\nOnce the model is optimized using the cross-entropy loss,\\nit can generate an entire sequence as follows. Let ˆyt denote\\nthe output generated by the model at time t. Then, the next\\noutput is generated by:\\nˆyt = arg max\\ny\\npθ(y|ˆyt−1,st) (2)\\nIn NLP applications, one can improve the output by using\\nbeam search to ﬁnd a reasonably good output sequence [3].\\nDuring beam search, rather than using argmax for selecting\\nthe best output, we choose the top K outputs at each step,\\ngenerate K different paths for the output sequence, and ﬁnally\\nchoose the one that provides better performance as the ﬁnal'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='the best output, we choose the top K outputs at each step,\\ngenerate K different paths for the output sequence, and ﬁnally\\nchoose the one that provides better performance as the ﬁnal\\noutput. Although, there has been some recent studies [65],\\n[66] on improving the beam search by incorporating a similar\\nmechanism during training of them model, studying this is\\noutside the scope of this paper.\\nGiven a series of the ground-truth output Y and the gener-\\nated model output ˆY, the model performance is evaluated us-\\ning a task-speciﬁc measures such as ROUGE [67], BLEU [68],\\nand METEOR [69]. As an example, ROUGE L, which is an\\nevaluation metric in NLP tasks, uses the largest common sub-\\nstring between ground-truth Y and model output ˆY to evaluate\\nthe generated output.\\nC. Reinforcement Learning in NLP\\nAlthough the seq2seq models explained in Section III-B\\nachieve great successes w.r.t. traditional methods, there are\\nsome issues with how these models are trained. Generally'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='Although the seq2seq models explained in Section III-B\\nachieve great successes w.r.t. traditional methods, there are\\nsome issues with how these models are trained. Generally\\nspeaking, seq2seq models like the ones used in NLP applica-\\ntions face two issues: (1) exposure bias and (2) inconsistency\\nbetween training time and test time measurements [70].\\nMost of the popular seq2seq models are minimizing cross-\\nentropy loss as their optimization objective via Teacher Forc-\\ning (Section III-B). In teacher forcing, during the training of\\nthe model, the decoder utilizes two inputs, the former decoder\\noutput state st−1 and the ground-truth input yt, to determine its\\ncurrent output state st. Moreover, it employs them to create\\nthe next token, i.e., ˆyt. However, at test time, the decoder\\nfully relies on the previously created token from the model\\ndistribution. As the ground-truth data is not available, such\\na step is necessary to predict the next action. Henceforth, in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='fully relies on the previously created token from the model\\ndistribution. As the ground-truth data is not available, such\\na step is necessary to predict the next action. Henceforth, in\\ntraining, the decoder input is coming from the ground truth,\\nwhile, in the test phase, it relies on the previous prediction.\\nThis exposure bias [71] induces error growth through output\\ncreation at the test phase. One approach to remedy this\\nproblem is to remove the ground-truth dependency in training\\nby solely relying on model distribution to minimize the cross-\\nentropy loss. Scheduled sampling [64] is one popular method\\nto handle this setback. During scheduled sampling, we ﬁrst\\npre-train the model using cross-entropy loss and then slowly\\nreplace the ground-truth with samples the model generates.\\nThe second obstacle with seq2seq models is that, when\\ntraining is ﬁnished using the cross-entropy loss, it is typically\\nevaluated using non-differentiable measures such as ROUGE'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='The second obstacle with seq2seq models is that, when\\ntraining is ﬁnished using the cross-entropy loss, it is typically\\nevaluated using non-differentiable measures such as ROUGE\\nor METEOR. This will form an inconsistency between the\\ntraining objective and the test evaluation metric. Recently, it\\nhas been demonstrated that both of these problems can be tack-\\nled by utilizing techniques from reinforcement learning [70].\\nAmong most of the well-known models in reinforcement\\nlearning, policy gradient techniques [72] such as the REIN-\\nFORCE algorithm [73] and actor-critic based models such as\\nvalue-based iteration [74], and Q-learning [75], are among the\\nmost common techniques used in deep learning in NLP.\\nUsing the model predictions (versus the ground-truth) for\\nthe sequence to sequence modeling and generation, at training\\ntime, was initially introduced by Daume et al. [76]. According\\nto their approach, SEARN, the structured prediction can be'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='the sequence to sequence modeling and generation, at training\\ntime, was initially introduced by Daume et al. [76]. According\\nto their approach, SEARN, the structured prediction can be\\ncharacterized as one of the reinforcement learning cases as\\nfollows: The model employs its predictions to produce a\\nsequence of actions (words sequences). Then, at each time\\nstep, a greedy search algorithm is employed to learn the\\noptimal action, and the policy will be trained to predict that\\nparticular action.\\nFig. 7. A simple Actor-Critic framework.\\nIn Actor-Critic training, the actor is usually the same neural\\nnetwork used to generate the output, while the critic is a\\nregression model that estimates how the actor performed on\\nthe input data. The actor later receives the feedback from the\\ncritic and improves its actions. Fig 7 shows this framework.\\nIt is worth noting that action in most of the NLP-related\\napplications is like selecting the next output token while the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='critic and improves its actions. Fig 7 shows this framework.\\nIt is worth noting that action in most of the NLP-related\\napplications is like selecting the next output token while the\\nstate is the decoder output state at each stage of decoding.\\nThese models have mostly been used for robotic [77] and\\nAtari games [78] due to the small action space in these\\napplications. However, when we use them in NLP applications,\\nthey face multiple challenges. The action space in most of the\\nNLP applications could be deﬁned as the number of tokens\\nin the vocabulary (usually between 50K to 150K tokens).\\nComparing this to the action space in a simple Atari game,\\nwhich on average has less than 20 actions [78], shows why\\nthese Actor-Critic models face difﬁculties when applied to\\nNLP applications. A major challenge is the massive action\\nspace in NLP applications, which not only causes difﬁculty'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 7\\nfor the right action selection, but also will make the training\\nprocess very slow. This makes the process of ﬁnding the best\\nActor-Critic model very complicated and model convergence\\nusually requires a lot of tweaks to the models.\\nIV. D ATASETS\\nMany different researchers for different tasks use bench-\\nmark datasets, such as those discussed below. Benchmarking\\nin machine learning refers to the assessment of methods\\nand algorithms, comparing those regarding their capability to\\nlearn speciﬁc patterns. Benchmarking aids validation of a new\\napproach or practice, relative to other existing methods.\\nBenchmark datasets typically take one of three forms.\\n1) The ﬁrst is real-world data, obtained from various real-\\nworld experiments.\\n2) The second is synthetic data, artiﬁcially generated to\\nmimic real-world patterns. Synthetic data is generated\\nfor use instead of real data. Such datasets are of spe-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='world experiments.\\n2) The second is synthetic data, artiﬁcially generated to\\nmimic real-world patterns. Synthetic data is generated\\nfor use instead of real data. Such datasets are of spe-\\ncial interest in applications where the amount of data\\nrequired is much larger than that which is available, or\\nwhere privacy considerations are crucial and strict, such\\nas in the healthcare domain.\\n3) The third type are toy datasets, used for demonstration\\nand visualization purposes. Typically they are artiﬁcially\\ngenerated; often there is no need to represent real-world\\ndata patterns.\\nThe foundation of Deep Learning utilization is the avail-\\nability of data to teach the system about pattern identiﬁcation.\\nThe effectiveness of the model depends on the quality of\\nthe data. Despite the successful implementation of universal\\nlanguage modeling techniques such as BERT [79], however,\\nsuch models can be used solely for pre-training the models.\\nAfterward, the model needs to be trained on the data associated'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='language modeling techniques such as BERT [79], however,\\nsuch models can be used solely for pre-training the models.\\nAfterward, the model needs to be trained on the data associated\\nwith the desired task. Henceforth, based on the everyday\\ndemands in different machine domains such as NLP, creating\\nnew datasets is crucial.\\nOn the other hand, creating new datasets is not usually an\\neasy matter. Informally speaking, the newly created dataset\\nshould be: the right data to train on, sufﬁcient for the eval-\\nuation, and accurate to work on. Answering the questions of\\n“what is the meaning of right and accurate data” is highly\\napplication-based. Basically, the data should have sufﬁcient\\ninformation, which depends on the quality and quantity of the\\ndata.\\nTo create a dataset, the ﬁrst step is always asking “what are\\nwe trying to do and what problem do we need to solve?”\\nand “what kind of data do we need and how much of it\\nis required?” The next step is to create training and testing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='we trying to do and what problem do we need to solve?”\\nand “what kind of data do we need and how much of it\\nis required?” The next step is to create training and testing\\nportions. The training data set is used to train a model to\\nknow how to ﬁnd the connections between the inputs and\\nthe associated outputs. The test data set is used to assess the\\nintelligence of the machine, i.e., how well the trained model\\ncan operate on the unseen test samples. Next, we must conduct\\ndata preparation to make sure the data and its format is simple\\nand understandable for human experts. After that, the issue\\nof data accessibility and ownership may arise. Distribution of\\ndata may need to have speciﬁc authorizations, especially if we\\nare dealing with sensitive or private data.\\nGiven the aforementioned roadmap, creating proper datasets\\nis complicated and of great importance. That’s why few\\ndatasets are frequently chosen by the researchers and develop-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='Given the aforementioned roadmap, creating proper datasets\\nis complicated and of great importance. That’s why few\\ndatasets are frequently chosen by the researchers and develop-\\ners for benchmarking. A summary of widely used benchmark\\ndatasets is provided in Table I.\\nV. D EEP LEARNING FOR NLP TASKS\\nThis section describes NLP applications using deep learn-\\ning. Fig. 8 shows representative NLP tasks (and the categories\\nthey belong to). A fundamental question is: ”How can we\\nevaluate an NLP algorithm, model, or system?” In [80],\\nsome of the most common evaluation metrics have been\\ndescribed. This reference explains the fundamental principles\\nof evaluating NLP systems.\\nA. Basic Tasks\\n1) Part-Of-Speech Tagging: Part-of-Speech tagging is one\\nof the basic tasks in Natural Language Processing. It is the\\nprocess of labeling words with their part of speech categories.\\nPart of speech is leveraged for many crucial tasks such\\nas named entity recognition. One commonly used dataset'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='process of labeling words with their part of speech categories.\\nPart of speech is leveraged for many crucial tasks such\\nas named entity recognition. One commonly used dataset\\nfor Part-of-Speech tagging is the WSJ corpus 7. This dataset\\ncontains over a million tokens and has been utilized widely as\\na benchmark dataset for the performance assessment of POS\\ntagging systems. Traditional methods are still performing very\\nwell for this task [16]. However, neural network based methods\\nhave been proposed for Part-of-Speech tagging [81].\\nFor example, the deep neural network architecture named\\nCharWNN has been developed to join word-level and\\ncharacter-level representations using convolutional neural net-\\nworks for POS tagging [14]. The emphasis in [14] is the\\nimportance of character-level feature extraction as their exper-\\nimental results show the necessity of employing hand-crafted\\nfeatures in the absence of character-level features for achieving'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='importance of character-level feature extraction as their exper-\\nimental results show the necessity of employing hand-crafted\\nfeatures in the absence of character-level features for achieving\\nthe state-of-the-art. In [82], a wide variety of neural network\\nbased models have been proposed for sequence tagging tasks,\\ne.g., LSTM networks, bidirectional LSTM networks, LSTM\\nnetworks with a CRF 8 layer, etc. Sequence tagging itself\\nincludes part of speech tagging, chunking, and named entity\\nrecognition. Likewise, a globally normalized transition-based\\nneural network architecture has been proposed for POS-\\ntagging [83]. State-of-the-art results are summarized in Table\\nII. In [17], authors propose a bidirectional LSTM to perform\\nparts of speech tagging and show that it performs better than\\nconventional machine learning techniques on the same dataset.\\nMore recently, in [84], authors use a pretrained BERT model\\nin combination with one bidirectional LSTM layer and train'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='conventional machine learning techniques on the same dataset.\\nMore recently, in [84], authors use a pretrained BERT model\\nin combination with one bidirectional LSTM layer and train\\nthe latter layer only and outperform the prior state-of-the art\\nPOS architectures.\\n2) Parsing: Parsing is assigning a structure to a recognized\\nstring. There are different types of parsing. Constituency\\nParsing refers in particular to assigning a syntactic structure\\nto a sentence. A greedy parser has been introduced in [92]\\nwhich performs a syntactic and semantic summary of content\\n7Penn Treebank Wall Street Journal (WSJ-PTB).\\n8Conditional Random Field.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 8\\nTABLE I\\nBENCHMARK DATASETS .\\nTask Dataset Link\\nMachine Translation WMT 2014 EN-DE\\nWMT 2014 EN-FR http://www-lium.univ-lemans.fr/ ∼schwenk/cslm joint paper/\\nText Summarization\\nCNN/DM\\nNewsroom\\nDUC\\nGigaword\\nhttps://cs.nyu.edu/ ∼kcho/DMQA/\\nhttps://summari.es/\\nhttps://www-nlpir.nist.gov/projects/duc/data.html\\nhttps://catalog.ldc.upenn.edu/LDC2012T21\\nReading Comprehension\\nQuestion Answering\\nQuestion Generation\\nARC\\nCliCR\\nCNN/DM\\nNewsQA\\nRACE\\nSQuAD\\nStory Cloze Test\\nNarativeQA\\nQuasar\\nSearchQA\\nhttp://data.allenai.org/arc/\\nhttp://aclweb.org/anthology/N18-1140\\nhttps://cs.nyu.edu/ ∼kcho/DMQA/\\nhttps://datasets.maluuba.com/NewsQA\\nhttp://www.qizhexie.com/data/RACE leaderboard\\nhttps://rajpurkar.github.io/SQuAD-explorer/\\nhttp://aclweb.org/anthology/W17-0906.pdf\\nhttps://github.com/deepmind/narrativeqa\\nhttps://github.com/bdhingra/quasar\\nhttps://github.com/nyu-dl/SearchQA\\nSemantic Parsing\\nAMR parsing\\nATIS (SQL Parsing)\\nWikiSQL (SQL Parsing)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='https://github.com/deepmind/narrativeqa\\nhttps://github.com/bdhingra/quasar\\nhttps://github.com/nyu-dl/SearchQA\\nSemantic Parsing\\nAMR parsing\\nATIS (SQL Parsing)\\nWikiSQL (SQL Parsing)\\nhttps://amr.isi.edu/index.html\\nhttps://github.com/jkkummerfeld/text2sql-data/tree/master/data\\nhttps://github.com/salesforce/WikiSQL\\nSentiment Analysis\\nIMDB Reviews\\nSST\\nYelp Reviews\\nSubjectivity Dataset\\nhttp://ai.stanford.edu/ ∼amaas/data/sentiment/\\nhttps://nlp.stanford.edu/sentiment/index.html\\nhttps://www.yelp.com/dataset/challenge\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/\\nText Classiﬁcation\\nAG News\\nDBpedia\\nTREC\\n20 NewsGroup\\nhttp://www.di.unipi.it/ ∼gulli/AG corpus of news articles.html\\nhttps://wiki.dbpedia.org/Datasets\\nhttps://trec.nist.gov/data.html\\nhttp://qwone.com/ ∼jason/20Newsgroups/\\nNatural Language Inference\\nSNLI Corpus\\nMultiNLI\\nSciTail\\nhttps://nlp.stanford.edu/projects/snli/\\nhttps://www.nyu.edu/projects/bowman/multinli/\\nhttp://data.allenai.org/scitail/'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='Natural Language Inference\\nSNLI Corpus\\nMultiNLI\\nSciTail\\nhttps://nlp.stanford.edu/projects/snli/\\nhttps://www.nyu.edu/projects/bowman/multinli/\\nhttp://data.allenai.org/scitail/\\nSemantic Role Labeling Proposition Bank\\nOneNotes\\nhttp://propbank.github.io/\\nhttps://catalog.ldc.upenn.edu/LDC2013T19\\nTABLE II\\nPOS TAGGING STATE -OF-THE -ART MODELS EVALUATED ON THE\\nWSJ-PTB DATASET.\\nModel Accuracy\\nCharacter-aware neural language models [85] 97.53\\nTransfer Learning + GRU [86] 97.55\\nBi-directional LSTM + CNNs + CRF [87] 97.55\\nAdversarial Training + Bi-LSTM [88] 97.59\\nCharacter Composition + Bi-LSTM [89] 97.78\\nString Embedding + LSTM [90] 97.85\\nMeta-BiLSTM [91] 97.96\\nusing vector representations. To enhance the results achieved\\nby [92], the approach proposed in [93] focuses on learning\\nmorphological embeddings. Recently, deep neural network\\nmodels outperformed traditional algorithms. State-of-the-art\\nresults are summarized in Table III.\\nAnother type of parsing is called Dependency Parsing. De-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='models outperformed traditional algorithms. State-of-the-art\\nresults are summarized in Table III.\\nAnother type of parsing is called Dependency Parsing. De-\\npendency structure shows the structural relationships between\\nthe words in a targeted sentence. In dependency parsing,\\nphrasal elements and phrase-structure rules do not contribute\\nto the process. Rather, the syntactic structure of the sentence\\nis expressed only in terms of the words in the sentence and\\nthe associated relations between the words.\\nTABLE III\\nCONSTITUENCY PARSING STATE -OF-THE -ART MODELS EVALUATED ON\\nTHE WSJ-PTB DATASET.\\nModel Accuracy\\nRecurrent neural network grammars (RNNG) [94] 93.6\\nIn-order traversal over syntactic trees + LSTM [95] 94.2\\nModel Combination and Reranking [96] 94.6\\nSelf-Attentive Encoder [97] 95.1\\nNeural networks have shown their superiority regarding\\ngeneralizability and reducing the feature computation cost. In\\n[98], a novel neural network-based approach was proposed for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='Neural networks have shown their superiority regarding\\ngeneralizability and reducing the feature computation cost. In\\n[98], a novel neural network-based approach was proposed for\\na transition-based dependency parser. Neural network based\\nmodels that operate on task-speciﬁc transition systems have\\nalso been utilized for dependency parsing [83]. A regularized\\nparser with bi-afﬁne classiﬁers has been proposed for the pre-\\ndiction of arcs and labels [99]. Bidirectional-LSTMs have been\\nused in dependency parsers for feature representation [100].\\nA new control structure has been introduced for sequence-to-\\nsequence neural networks based on the stack LSTM and has\\nbeen used in transition-based parsing [101]. [102] presents a\\ntransition based multilingual dependency parser which uses\\na bidirectional LSTM to adapt to target languages. In [103],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 9\\nFig. 8. NLP tasks investigated in this study.\\nthe authors provide a comparison on the state of the art deep\\nlearning based parsing methods on a clinical text parsing task.\\nMore recently, in [104], a second-order TreeCRF extension\\nwas added to the biafﬁne [105] parser to demonstrate that\\nstructural learning can further improve parsing performance\\nover the state-of-the-art bi-afﬁne models.\\n3) Semantic Role Labeling: Semantic Role Labeling (SRL)\\nis the process of identiﬁcation and classiﬁcation of text argu-\\nments. It is aimed at the characterization of elements to deter-\\nmine “who” did “what” to “whom” as well as “how,” “where,”\\nand “when.” It identiﬁes the predicate-argument structure of a\\nsentence. The predicate, in essence, refers to “what,” while the\\narguments consist of the associated participants and properties\\nin the text. The goal of SRL is to extract the semantic relations\\nbetween the predicate and the related arguments.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='arguments consist of the associated participants and properties\\nin the text. The goal of SRL is to extract the semantic relations\\nbetween the predicate and the related arguments.\\nMost of the previously-reported research efforts are based\\non explicit representations of semantic roles. Recently, deep\\nlearning approaches have achieved the SRL state-of-the-art\\nwithout taking the explicit syntax representation into consider-\\nation [106]. On the other hand, it is argued that the utilization\\nof syntactic information can be leveraged to improve the per-\\nformance of syntactic-agnostic9 models [107]. A linguistically-\\ninformed self-attention (LISA) model has been proposed to\\nleverage both multi-task learning and self-attention for effec-\\n9Note that being syntactic-agnostic does not imply discarding syntactic\\ninformation. It means they are not explicitly employed.\\ntive utilization of the syntactic information for SRL [108].\\nCurrent state-of-the-art methods employ joint prediction of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='information. It means they are not explicitly employed.\\ntive utilization of the syntactic information for SRL [108].\\nCurrent state-of-the-art methods employ joint prediction of\\npredicates and arguments [109], novel word representation ap-\\nproaches [110], and self-attention models [111]; see Table IV.\\nResearchers in [25] focus on syntax and contextualized word\\nrepresentation to present a unique multilingual SRL model\\nbased on a biafﬁne scorer, argument pruning and bidirectional\\nLSTMs, (see also [112]).\\nTABLE IV\\nSEMANTIC ROLE LABELING CURRENT STATE -OF-THE -ART MODELS\\nEVALUATED ON THE ONTO NOTES DATASET [113]. T HE ACCURACY\\nMETRIC IS F1 SCORE .\\nModel Accuracy ( F1)\\nSelf-Attention + RNN [111] 83.9\\nContextualized Word Representations [110] 84.6\\nArgumented Representations + BiLSTM [109] 85.3\\nB. Text Classiﬁcation\\nThe primary objective of text classiﬁcation is to assign\\npredeﬁned categories to text parts (which could be a word,\\nsentence, or whole document) for preliminary classiﬁcation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='B. Text Classiﬁcation\\nThe primary objective of text classiﬁcation is to assign\\npredeﬁned categories to text parts (which could be a word,\\nsentence, or whole document) for preliminary classiﬁcation\\npurposes and further organization and analysis. A simple ex-\\nample is the categorization of given documents as to political\\nor non-political news articles.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 10\\nThe use of CNNs for sentence classiﬁcation, in which train-\\ning the model on top of pretrained word-vectors through ﬁne-\\ntuning, has resulted in considerable improvements in learning\\ntask-speciﬁc vectors [31]. Later, a Dynamic Convolutional\\nNeural Network (DCNN) architecture – essentially a CNN\\nwith a dynamic k-max pooling method – was applied to\\ncapture the semantic modeling of sentences [114]. In addi-\\ntion to CNNs, RNNs have been used for text classiﬁcation.\\nAn LSTM-RNN architecture has been utilized in [115] for\\nsentence embedding with particular superiority in a deﬁned\\nweb search task. A Hierarchical Attention Network (HAN) has\\nbeen utilized to capture the hierarchical structure of text, with\\na word-level and sentence-level attention mechanism [116].\\nSome models used the combination of both RNNs and\\nCNNs for text classiﬁcation such as [117]. This is a recurrent\\narchitecture in addition to max-pooling with an effective word'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='Some models used the combination of both RNNs and\\nCNNs for text classiﬁcation such as [117]. This is a recurrent\\narchitecture in addition to max-pooling with an effective word\\nrepresentation method, and demonstrates superiority compared\\nto simple window-based neural network approaches. Another\\nuniﬁed architecture is the C-LSTM proposed in [118] for\\nsentence and document modeling in classiﬁcation. Current\\nstate-of-the-art methods are summarized in Table V. A more\\nrecent review of the deep learning based methods for text clas-\\nsiﬁcation is provided in [119]. The latter focuses on different\\narchitectures used for this task, including most recent works\\nin CNN based models, as well as RNN based models, and\\ngraph neural networks. In [120], authors provide a comparison\\nbetween various deep learning methods for text classiﬁcation,\\nconcluding that GRUs and LSTMs can actually perform better\\nthan CNN-based models.\\nTABLE V\\nTHE CLASSIFICATION ACCURACY OF STATE -OF-THE -ART METHODS ,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='concluding that GRUs and LSTMs can actually perform better\\nthan CNN-based models.\\nTABLE V\\nTHE CLASSIFICATION ACCURACY OF STATE -OF-THE -ART METHODS ,\\nEVALUATED ON THE AG NEWS CORPUS DATASET [2].\\nModel Accuracy\\nCNN [121] 91.33\\nDeep Pyramid CNN [122] 93.13\\nCNN [123] 93.43\\nUniversal Language Model Fine-tuning (ULMFiT) [124] 94.99\\nC. Information Extraction\\nInformation extraction identiﬁes structured information\\nfrom “unstructured” data such as social media posts and\\nonline news. Deep learning has been utilized for information\\nextraction regarding subtasks such as Named Entity Recogni-\\ntion, Relation Extraction , Coreference Resolution, and Event\\nExtraction.\\n1) Named Entity Recognition: Named Entity Recogni-\\ntion (NER) aims to locate and categorize named entities in\\ncontext into pre-deﬁned categories such as the names of people\\nand places. The application of deep neural networks in NER\\nhas been investigated by the employment of CNN [125] and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='context into pre-deﬁned categories such as the names of people\\nand places. The application of deep neural networks in NER\\nhas been investigated by the employment of CNN [125] and\\nRNN architectures [126], as well as hybrid bidirectional LSTM\\nand CNN architectures [19]. NeuroNER [127], a named-entity\\nrecognition tool, operates based on artiﬁcial neural networks.\\nState-of-the-art models are reported in Table VI. [21] provides\\nan extensive discussion on recent deep learning methods for\\nnamed entity recognition. The latter concludes that the work\\npresented in [128] outperforms other recent models (with an\\nF-score of 93.5 on the CoNLL03 dataset).\\nTABLE VI\\nSTATE OF THE ART MODELS REGARDING NAME ENTITY RECOGNITION .\\nEVALUATION IS PERFORMED ON THE CONLL-2003 S HARED TASK\\nDATASET [129]. T HE EVALUATION METRIC IS F1 SCORE .\\nModel Accuracy\\nSemi-supervised Sequence Modeling [130] 92.61\\nGoogle BERT [131] 92.8\\nContextual String Embeddings [90] 93.09'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='DATASET [129]. T HE EVALUATION METRIC IS F1 SCORE .\\nModel Accuracy\\nSemi-supervised Sequence Modeling [130] 92.61\\nGoogle BERT [131] 92.8\\nContextual String Embeddings [90] 93.09\\n2) Relation Extraction: Relation Extraction aims to ﬁnd\\nthe semantic relationships between entity pairs. The recursive\\nneural network (RNN) model has been proposed for semantic\\nrelationship classiﬁcation by learning compositional vector\\nrepresentations [132]. For relation classiﬁcation, CNN archi-\\ntectures have been employed as well, by extracting lexical\\nand sentence level features [37]. More recently, in [133],\\nbidirectional tree-structured LSTMs were shown to perform\\nwell for relation extraction. [134] provides a more recent\\nreview on relation extraction.\\n3) Coreference Resolution: Coreference resolution includes\\nidentiﬁcation of the mentions in a context that refer to the\\nsame entity. For instance, the mentions “car,” “Camry,” and\\n“it” could all refer to the same entity. For the ﬁrst time in [135],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='identiﬁcation of the mentions in a context that refer to the\\nsame entity. For instance, the mentions “car,” “Camry,” and\\n“it” could all refer to the same entity. For the ﬁrst time in [135],\\nReinforcement Learning (RL) was applied to coreference\\nresolution. Current widely used methods leverage an attention\\nmechanism [136]. More recently, in [137], authors adopt a\\nreinforcement learning policy gradient approach to coreference\\nresolution and provide state-of-the art performance on the\\nEnglish OntoNotes v5.0 benchmark task. [138] reformulates\\ncoreference resolution as a span prediction task as in question\\nanswering and provide superior performance on the CoNLL-\\n2012 benchmark task.\\n4) Event Extraction: A speciﬁc type of extracted infor-\\nmation from text is an event. Such extraction may involve\\nrecognizing trigger words related to an event and assign-\\ning labels to entity mentions that represent event triggers.\\nConvolutional neural networks have been utilized for event'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='recognizing trigger words related to an event and assign-\\ning labels to entity mentions that represent event triggers.\\nConvolutional neural networks have been utilized for event\\ndetection; they handle problems with feature-based approaches\\nincluding exhaustive feature engineering and error propagation\\nphenomena for feature generation [139]. In 2018, Nguyen\\nand Grishman applied graph-CNN (GCCN) where the con-\\nvolutional operations are applied to syntactically dependent\\nwords as well as consecutive words [140]; their adding entity\\ninformation reﬂected the state-of-the-art using CNN models.\\n[141] uses a novel inverse reinforcement learning approach\\nbased on generative adversarial networks (imitation learning)\\nto tackle joint entity and event extraction. More recently, in\\n[142], authors proposed a model for document-level event\\nextraction using a combined dependency-based GCN (for\\nlocal context) and a hypergraph (as an aggregator for global\\ncontext).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 11\\nD. Sentiment analysis\\nThe primary goal in sentiment analysis is the extraction\\nof subjective information from text by contextual mining.\\nSentiment analysis is considered high-level reasoning based on\\nsource data. Sentiment analysis is sometimes called opinion\\nmining, as its primary goal is to analyze human opinion,\\nsentiments, and even emotions regarding products, problems,\\nand varied subjects. Seminal works on sentiment analysis or\\nopinion mining include [143], [144]. Since 2000, much atten-\\ntion has been given to sentiment analysis, due to its relation\\nto a wide variety of applications [145], its associations with\\nnew research challenges, and the availability of abundant data.\\n[146] provides a more recent review of the sentiment analysis\\nmethods relying on deep learning and gives an insightful\\ndiscussion on the drawbacks as well as merits of deep learning\\nmethods for sentiment analysis.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='methods relying on deep learning and gives an insightful\\ndiscussion on the drawbacks as well as merits of deep learning\\nmethods for sentiment analysis.\\nA critical aspect of research in sentiment analysis is content\\ngranularity. Considering this criterion, sentiment analysis is\\ngenerally divided into three categories/levels: document level,\\nsentence level, and aspect level.\\n1) Document-level Sentiment Analysis: At the document\\nlevel, the task is to determine whether the whole document\\nreﬂects a positive or negative sentiment about exactly one\\nentity. This differs from opinion mining regarding multiple\\nentries. The Gated Recurrent Neural Network architecture\\nhas been utilized successfully for effectively encoding the\\nsentences’ relations in the semantic structure of the docu-\\nment [147]. Domain adaptation has been investigated as well,\\nto deploy the trained model on unseen new sources [148].\\nMore recently, in [149] authors provide an LSTM-based model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='ment [147]. Domain adaptation has been investigated as well,\\nto deploy the trained model on unseen new sources [148].\\nMore recently, in [149] authors provide an LSTM-based model\\nfor document-level sentiment analysis that captures semantic\\nrelations between sentences. In [150], authors use a CNN-\\nbidirectional LSTM model to process long texts.\\n2) Sentence-level Sentiment Analysis: At the sentence-\\nlevel, sentiment analysis determines the positivity, negativity,\\nor neutrality regarding an opinion expressed in a sentence. One\\ngeneral assumption for sentence-level sentiment classiﬁcation\\nis the existence of only one opinion from a single opinion\\nholder in an expressed sentence. Recursive autoencoders have\\nbeen employed for sentence-level sentiment label prediction\\nby learning the vector space representations for phrases [151].\\nLong Short-Term Memory (LSTM) recurrent models have\\nalso been utilized for tweet sentiment prediction [152]. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='by learning the vector space representations for phrases [151].\\nLong Short-Term Memory (LSTM) recurrent models have\\nalso been utilized for tweet sentiment prediction [152]. The\\nSentiment Treebank and Recursive Neural Tensor Networks\\n[153] have shown promise for predicting ﬁne-grained sen-\\ntiment labels. [154] provides a cloud-based hybrid machine\\nlearning model for sentence level sentiment analysis. More\\nrecently in [155], propose A Lexicalized Domain Ontology\\nand a Regularized Neural Attention model (ALDONAr) for\\nsentence-level aspect-based sentiment analysis that uses a\\nCNN classiﬁcation module with BERT word embeddings and\\nachieves state-of-the art results.\\n3) Aspect-level Sentiment Analysis: Document-level and\\nsentence-level sentiment analysis usually focus on the senti-\\nment itself, not the target of the sentiment, e.g., a product.\\nAspect-level sentiment analysis directly targets an opinion,\\nwith the assumption of the existence of the sentiment and its'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='ment itself, not the target of the sentiment, e.g., a product.\\nAspect-level sentiment analysis directly targets an opinion,\\nwith the assumption of the existence of the sentiment and its\\ntarget. A document or sentence may not have a generally posi-\\ntive or negative sentiment, but may have multiple subparts with\\ndifferent targets, each with a positive or negative sentiment.\\nThis can make aspect-level analysis even more challenging\\nthan other types of sentiment categorization.\\nAspect-level sentiment analysis usually involves Aspect\\nSentiment Classiﬁcation and Aspect Extraction . The former\\ndetermines opinions on different aspects (positive, neutral,\\nor negative) while the latter identiﬁes the target aspect for\\nevaluation in context. As an example consider the following\\nsentence: “This car is old. It must be repaired and sold!” .\\n“This car” is what is subject to evaluation and must be\\nextracted ﬁrst. Here, the opinion about this aspect is negative.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='sentence: “This car is old. It must be repaired and sold!” .\\n“This car” is what is subject to evaluation and must be\\nextracted ﬁrst. Here, the opinion about this aspect is negative.\\nFor aspect-level sentiment classiﬁcation, attention-based\\nLSTMs are proposed to connect the aspect and sentence\\ncontent for sentiment classiﬁcation [156]. For aspect extrac-\\ntion, deep learning has successfully been proposed in opinion\\nmining [157]. State-of-the-art methods rely on converting\\naspect-based sentiment analysis to sentence-pair classiﬁcation\\ntasks [79], post-training approaches [158] on the popular\\nlanguage model BERT [131], and employment of pre-trained\\nembeddings [159]. [160] provides a recent comparative review\\non aspect-based sentiment analysis. Also recently, [161] pro-\\nposed a dual-attention model which tries to extract the implicit\\nrelation between the aspect and opinion terms. In [162] authors\\npropose a novel Aspect-Guided Deep Transition model for\\naspect-based sentiment analysis.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='relation between the aspect and opinion terms. In [162] authors\\npropose a novel Aspect-Guided Deep Transition model for\\naspect-based sentiment analysis.\\nE. Machine Translation\\nMachine Translation (MT) is one of the areas of NLP\\nthat has been profoundly affected by the advances in deep\\nlearning. The ﬁrst subsection below explains methods used in\\nthe pre-deep learning period, as explained in reference NLP\\ntextbooks such as “Speech and Language Processing” [163].\\nThe remainder of this section is dedicated to delving into\\nrecent innovations in MT which are based on neural networks,\\nstarted by [164]. [165], [166] provide reviews on various deep\\nlearning architectures used for MT.\\n1) Traditional Machine Translation: One of the ﬁrst\\ndemonstrations of machine translation happened in 1954 [167]\\nin which the authors tried to translate from Russian to English.\\nThis translation system was based on six simple rules, but\\nhad a very limited vocabulary. It was not until the 1990s that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='in which the authors tried to translate from Russian to English.\\nThis translation system was based on six simple rules, but\\nhad a very limited vocabulary. It was not until the 1990s that\\nsuccessful statistical implementations of machine translation\\nemerged as more bilingual corpora became available [163].\\nIn [68] the BLEU score was introduced as a new evaluation\\nmetric, allowing more rapid improvement than when the only\\napproach involved using human labor for evaluation.\\n2) Neural Machine Translation: It was after the success\\nof the neural network in image classiﬁcation tasks that re-\\nsearchers started to use neural networks in machine translation\\n(NMT). Around 2013, research groups started to achieve\\nbreakthrough results in NMT. Unlike traditional statistical\\nmachine translation, NMT is based on an end-to-end neural\\nnetwork [168]. This implies that there is no need for extensive\\npreprocessing and word alignments. Instead, the focus shifted\\ntoward network structure.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 12\\nFig. 11 shows an example of an end-to-end recurrent neural\\nnetwork for machine translation. A sequence of input tokens\\nis fed into the network. Once it reaches an end-of-sentence\\n(EOS) token, it starts generating the output sequence. The\\noutput sequence is generated in the same recurrent manner as\\nthe input sequence until it reaches an end-of-sentence token.\\nOne major advantage of this approach is that there is no need\\nto specify the length of the sequence; the network takes it\\ninto account automatically. In other words, the end-of-sentence\\ntoken determines the length of the sequence. Networks implic-\\nitly learn that longer input sentences usually lead to longer\\noutput sentences with varying length, and that ordering can\\nchange. For instance, the second example in Fig. 9 shows that\\nadjectives generally come before nouns in English but after\\nnouns in Spanish. There is no need to explicitly specify this'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='change. For instance, the second example in Fig. 9 shows that\\nadjectives generally come before nouns in English but after\\nnouns in Spanish. There is no need to explicitly specify this\\nsince the network can capture such properties. Moreover, the\\namount of memory that is used by NMT is just a fraction\\nof the memory that is used in traditional statistical machine\\ntranslation [169].\\nFig. 9. Alignment in Machine Translation\\n[164] was one of the early works that incorporated recurrent\\nneural networks for machine translation. They were able to\\nachieve a perplexity (a measure where lower values indicate\\nbetter models) that was 43% less than the state-of-the-art\\nalignment based translation models. Their recurrent continuous\\ntranslation model (RCTM) is able to capture word ordering,\\nsyntax, and meaning of the source sentence explicitly. It maps\\na source sentence into a probability distribution over sentences\\nin the target language. RCTM estimates the probability P(f|e)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='syntax, and meaning of the source sentence explicitly. It maps\\na source sentence into a probability distribution over sentences\\nin the target language. RCTM estimates the probability P(f|e)\\nof translating a sentence e = e1 + ... + ek in the source\\nlanguage to target language sentence f = f1 +...+fm. RCTM\\nestimates P(f|e) by considering source sentence e as well as\\nthe preceding words in the target language f1:i−1:\\nP(f|e) =\\nm∏\\ni=1\\nP(fi|f1:i−1,e) (3)\\nThe representation generated by RCTM acts on n-grams in\\nthe lower layers, and acts more on the whole sentence as one\\nmoves to the upper layers. This hierarchical representation is\\nperformed by applying different layers of convolution. First a\\ncontinuous representation of each word is generated; i.e., if\\nthe sentence is e= e1...ek, the representation of the word ei\\nwill be v(ei) ∈Rq×1. This will result in sentence matrix Ee ∈\\nRq×k in which Ee\\n:,i = v(ei). This matrix representation of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='the sentence is e= e1...ek, the representation of the word ei\\nwill be v(ei) ∈Rq×1. This will result in sentence matrix Ee ∈\\nRq×k in which Ee\\n:,i = v(ei). This matrix representation of the\\nsentence will be fed into a series of convolution layers in order\\nto generate the ﬁnal representation e for the recurrent neural\\nnetwork. The approach is illustrated in Fig. 10. Equations for\\nthe pipeline are as follows.\\ns= S.csm(e) (4)\\nh1 = σ(I.v(f1) +s) (5)\\nhi+1 = σ(R.hi + I.v(fi+1) +s) (6)\\noi+1 = O.hi (7)\\nIn order to take into account the sentence length, the authors\\nintroduced RCTM II which estimates the length of the target\\nsentence. RCTM II was able to achieve better perplexity on\\nWMT datasets (see top portion of Table I) than other existing\\nmachine translation systems.\\nFig. 10. Recurrent Continuous Translation Models (RCTM) [164].\\nIn another line of work, [170] presented an end-to-end\\nsequence learning approach without heavy assumptions on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='Fig. 10. Recurrent Continuous Translation Models (RCTM) [164].\\nIn another line of work, [170] presented an end-to-end\\nsequence learning approach without heavy assumptions on\\nthe structure of the sequence. Their approach consists of two\\nLSTMs, one for mapping the input to a vector of ﬁxed di-\\nmension and another LSTM for decoding the output sequence\\nfrom the vector. Their model was able to handle long sentences\\nas well as sentence representations that are sensitive to word\\norder. As shown in Fig. 11, the model reads ”ABC” as an\\ninput sequence and produces ”WXYZ” as output sequence.\\nThe < EOS >token indicates the end of prediction. The\\nnetwork was trained by maximizing the log probability of the\\ntranslation ( η) given the input sequence ( ζ). In other words,\\nthe objective function is:\\n1/|D|\\n∑\\n(η,ζ)∈D\\nlogP(η|ζ) (8)\\nD is the training set and |D| is its size. One of the\\nnovelties of their approach was reversing word order of the\\nsource sentence. This helps the LSTM to learn long term'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='1/|D|\\n∑\\n(η,ζ)∈D\\nlogP(η|ζ) (8)\\nD is the training set and |D| is its size. One of the\\nnovelties of their approach was reversing word order of the\\nsource sentence. This helps the LSTM to learn long term\\ndependencies.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 13\\nFig. 11. Sequence to sequence learning with LSTM.\\nHaving a ﬁxed-length vector in the decoder phase is one\\nof the bottlenecks of the encoder-decoder approach. [168]\\nargues that a network will have a hard time compressing\\nall the information from the input sentence into a ﬁxed-size\\nvector. They address this by allowing the network to search\\nsegments of the source sentence that are useful for predicting\\nthe translation. Instead of representing the input sentence as a\\nﬁxed-size vector, in [168] the input sentence is encoded to a\\nsequence of vectors and a subset of them is chosen by using\\na method called attention mechanism as shown in Fig. 12.\\nIn their approach P(yi|y1,...,y i−1,X) =g(yi−1,si,ci), in\\nwhich si = f(si−1,yi−1,ci). While previously cwas the same\\nfor all time steps, here ctakes a different value, ci, at each time\\nstep. This accounts for the attention mechasim (context vector)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='which si = f(si−1,yi−1,ci). While previously cwas the same\\nfor all time steps, here ctakes a different value, ci, at each time\\nstep. This accounts for the attention mechasim (context vector)\\naround that speciﬁc time step. ci is computed according to the\\nfollowing:\\nci = ∑Tx\\nj=1 αijhj, αij = exp(eij)∑Tx\\nk=1 exp(eik) , eij = a(si−1,hj).\\nHere ais the alignment model that is represented by a feed\\nforward neural network. Also hj = [\\n→\\nhT\\nj ,\\n←\\nhT\\nj ], which is a way to\\ninclude information both about preceding and following words\\nin hj. The model was able to outperform the simple encoder-\\ndecoder approach regardless of input sentence length.\\nImproved machine translation models continue to emerge,\\ndriven in part by the growth in people’s interest and need\\nto understand other languages Most of them are variants of\\nthe end-to-end decoder-encoder approach. For example, [171]\\ntries to deal with the problem of rare words. Their LSTM\\nnetwork consists of encoder and decoder layers using residual'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='the end-to-end decoder-encoder approach. For example, [171]\\ntries to deal with the problem of rare words. Their LSTM\\nnetwork consists of encoder and decoder layers using residual\\nlayers along with the attention mechanism. Their system\\nwas able to decrease training time, speed up inference, and\\nhandle translation of rare words. Comparisons between some\\nof the state-of-the-art neural machine translation models are\\nsummarized in Table VII.\\nTABLE VII\\nTHE MACHINE TRANSLATION STATE -OF-THE -ART MODELS EVALUATED\\nON THE English-German dataset of ACL 2014 Ninth Workshop on Statistical\\nMachine TRranslation. THE EVALUATION METRIC IS BLEU SCORE .\\nModel Accuracy\\nConvolutional Seq-to-Seq [172] 25.2\\nAttention Is All You Need [173] 28.4\\nWeighted Transformer [174] 28.9\\nSelf Attention [175] 29.2\\nDeepL Translation Machine 10 33.3\\nBack-translation [176] 35.0\\nMore recently, [177] provides an interesting single-model\\nimplementation of massively multilingual NMT. In [178],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='DeepL Translation Machine 10 33.3\\nBack-translation [176] 35.0\\nMore recently, [177] provides an interesting single-model\\nimplementation of massively multilingual NMT. In [178],\\nauthors use BERT to extract contextual embeddings and com-\\nFig. 12. Attention Mechasim for Neural Machine Translation [168].\\nbine BERT with an attention-based NMT model and provide\\nstate-of-the-art results on various benchmark datasets. [179]\\nproposes mBART which is a seq-to-seq denoising autoen-\\ncoder and reports that using a pretrained, locked (i.e. no\\nmodiﬁcations) mBART improves performance in terms of\\nthe BLEU point. [180] proposes an interesting adversarial\\nframework for robustifying NMT against noisy inputs and\\nreports performance gains over the Transformer model. [181]\\nis also an insightful recent work where the authors sample\\ncontext words from the predicted sequence as well as the\\nground truth to try to reconcile the training and inference'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='is also an insightful recent work where the authors sample\\ncontext words from the predicted sequence as well as the\\nground truth to try to reconcile the training and inference\\nprocesses. Finally, [182] is a successful recent effort to prevent\\nthe forgetting that often accompanies in translating pre-trained\\nlanguage models to other NMT task. [182] achieves that aim\\nprimarily by using a dynamically gated model and asymptotic\\ndistillation.\\nF . Question Answering\\nQuestion answering (QA) is a ﬁne-grained version of Infor-\\nmation Retrieval (IR). In IR a desired set of information has to\\nbe retrieved from a set of documents. The desired information\\ncould be a speciﬁc document, text, image, etc. On the other\\nhand, in QA speciﬁc answers are sought, typically ones that\\ncan be inferred from available documents. Other areas of NLP\\nsuch as reading comprehension and dialogue systems intersect\\nwith question answering.\\nResearch in computerized question answering has pro-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='can be inferred from available documents. Other areas of NLP\\nsuch as reading comprehension and dialogue systems intersect\\nwith question answering.\\nResearch in computerized question answering has pro-\\nceeded since the 1960s. In this section, we present a general\\noverview of question answering system history, and focus on\\nthe breakthroughs in the ﬁeld. Like all other ﬁelds in NLP,\\nquestion answering was also impacted by the advancement of\\ndeep learning [183], so we provide an overview of QA in deep\\nlearning contexts. We brieﬂy visit visual question answering\\nas well.\\n1) Rule-based Question Answering: Baseball [184] is one\\nof the early works (1961) on QA where an effort was made to\\nanswer questions related to baseball games by using a game\\ndatabase. The baseball system consists of (1) question read-in,\\n(2) dictionary lookup for words in the question, (3) syntactic\\n(POS) analysis of the words in question, (4) content analysis'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 14\\nfor extracting the input question, and (5) estimating relevance\\nregarding answering the input question.\\nIBM’s [185] statistical question answering system consisted\\nof four major components:\\n1) Question/Answer Type Classiﬁcation\\n2) Query Expansion/Information Retrieval\\n3) Name Entity Making\\n4) Answer Selection\\nSome QA systems fail when semantically equivalent re-\\nlationships are phrased differently. [186] addressed this by\\nproposing fuzzy relation matching based on mutual informa-\\ntion and expectation maximization.\\n2) Question answering in the era of deep learning: Smart-\\nphones (Siri, Ok Google, Alexa, etc.) and virtual personal\\nassistants are common examples of QA systems with which\\nmany interact on a daily basis. While earlier such systems\\nemployed rule-based methods, today their core algorithm is\\nbased on deep learning. Table VIII presents some questions\\nand answers provided by Siri on an iPhone.\\nTABLE VIII'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='employed rule-based methods, today their core algorithm is\\nbased on deep learning. Table VIII presents some questions\\nand answers provided by Siri on an iPhone.\\nTABLE VIII\\nTYPICAL QUESTION ANSWERING PERFORMANCE BASED ON DEEP\\nLEARNING .\\nQuestion Answer\\nWho invented polio vaccine? The answer I found is Jonas Salk\\nWho wrote Harry Potter? J.K.Rowling wrote Harry Potter in\\n1997\\nWhen was Einstein born? Albert Einstein was born March\\n14, 1879\\n[188] was one of the ﬁrst machine learning based papers\\nthat reported results on QA for a reading comprehension\\ntest. The system tries to pick a sentence in the database that\\nhas an answer to a question, and a feature vector represents\\neach question-sentence pair. The main contribution of [188]\\nis proposing a feature vector representation framework which\\nis aimed to provide information for learning the model. There\\nare ﬁve classiﬁers (location, date, etc.), one for each type of\\nquestion. They were able to achieve accuracy competitive with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='is aimed to provide information for learning the model. There\\nare ﬁve classiﬁers (location, date, etc.), one for each type of\\nquestion. They were able to achieve accuracy competitive with\\nprevious approaches.\\nAs illustrated in Fig. 13, [187] uses convolutional neural\\nnetworks in order to encode Question-Answer sentence pairs\\nin the form of ﬁxed length vectors regardless of the length\\nof the input sentence. Instead of using distance measures like\\ncosine correlation, they incorporate a non-linear tensor layer to\\nmatch the relevance between question and answer. Equation 9\\ncalculates the matching degree between question q and its\\ncorresponding answer a.\\ns(q,a) =uTf(vT\\nqM[1:r]va + V\\n[vq\\nva\\n]\\n+ b) (9)\\nf is the standard element-wise non-linearity function,\\nM[1:r]∈Rns×ns×r\\nis a tensor, V ∈Rr×2ns , b ∈Rr, u ∈Rr.\\nThe model tries to capture the interaction between question\\nand answer. Inspired by ﬁndings in neuroscience, [81] incorpo-\\nrated episodic memory 11 in their Dynamic Memory Network'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='The model tries to capture the interaction between question\\nand answer. Inspired by ﬁndings in neuroscience, [81] incorpo-\\nrated episodic memory 11 in their Dynamic Memory Network\\n11A kind of long-term memory that includes conscious recall of previous\\nactivities together with their meaning.\\n(DMN). By processing input sequences and questions, DMN\\nforms episodic memories to answer relevant questions. As\\nillustrated in Fig. 14, their system is trained based on raw\\nInput-Question-Answer triplets.\\nDMN consists of four modules that communicate with each\\nother as shown in Fig. 15. The input module encodes raw\\ninput text into a distributed vector representation; likewise\\nthe question module encodes a question into its distributed\\nvector representation. The episodic memory module uses the\\nattention mechanism in order to focus on a speciﬁc part of\\nthe input module. Through an iterative process, this module\\nproduces a memory vector representation that considers the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='attention mechanism in order to focus on a speciﬁc part of\\nthe input module. Through an iterative process, this module\\nproduces a memory vector representation that considers the\\nquestion as well as previous memory. The answer module\\nuses the ﬁnal memory vector to generate an answer. The model\\nimproved upon state-of-the-art results on tasks such as the ones\\nshown in Fig. 14. DMN is one of the architectures that could\\npotentially be used for a variety of NLP applications such as\\nclassiﬁcation, question answering, and sequence modeling.\\n[189] introduced a Dynamic Coattention Network (DCN)\\nin order to address local maxima corresponding to incorrect\\nanswers; it is considered to be one of the best approaches to\\nquestion answering.\\n3) Visual Question Answering: Given an input image, Vi-\\nsual Question Answering (VQA) tries to answer a natural\\nlanguage question about the image [190]. VQN addresses mul-\\ntiple problems such as object detection, image segmentation,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='sual Question Answering (VQA) tries to answer a natural\\nlanguage question about the image [190]. VQN addresses mul-\\ntiple problems such as object detection, image segmentation,\\nsentiment analysis, etc. [190] introduced the task of VQA\\nby providing a dataset containing over 250K images, 760K\\nquestions, and around 10M answers. [191] proposed a neural-\\nbased approach to answer the questions regarding the input\\nimages. As illustrated in Fig. 16, Neural-Image-QA is a deep\\nnetwork consisting of CNN and LSTM. Since the questions\\ncan have multiple answers, the problem is decomposed into\\npredicting a set of answer words aq,x = {a1,a2,...,a N(q,x)}\\nfrom a ﬁnite vocabulary set ν where N(q,x) represents the\\ncount of answer words regarding a given question.\\nDo humans and computers look at the same regions to\\nanswer questions about an image? [193] tries to answer this\\nquestion by conducting large-scale studies on human attention\\nin VQA. Their ﬁndings show that VQAs do not seem to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='answer questions about an image? [193] tries to answer this\\nquestion by conducting large-scale studies on human attention\\nin VQA. Their ﬁndings show that VQAs do not seem to\\nbe looking at the same regions as humans. Finally, [192]\\nincorporates a spatial memory network for VQA. Fig. 17\\nshows the inference process of their model. As illustrated in\\nthe ﬁgure, the speciﬁc attention mechanism in their system can\\nhighlight areas of interest in the input image. [194] introduces\\nBLOCK, a bilinear fusion model based on superdiagonal\\ntensor decomposition for the VQA task, with state-of-the-\\nart performance and the code made public on github. To\\nimprove the generalization of existing models to test data of\\ndifferent distribution, [195] introduces a self-critical training\\nobjective to help ﬁnd visual regions of prominent visual/textual\\ncorrelation with a focus on recognizing inﬂuential objects and\\ndetecting and devaluing incorrect dominant answers.\\nG. Document Summarization'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='correlation with a focus on recognizing inﬂuential objects and\\ndetecting and devaluing incorrect dominant answers.\\nG. Document Summarization\\nDocument summarization refers to a set of problems involv-\\ning generation of summary sentences given one or multiple\\ndocuments as input.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 15\\nFig. 13. Fixed length vector sentence representation for input Questions and Answers [187].\\nFig. 14. Example of Dynamic Memory Network (DMN) input-question-\\nanswer triplet\\nFig. 15. Interaction between four modules of Dynamic Memory Network [78].\\nGenerally, text summarization ﬁts into two categories:\\n1) Extractive Summarization, where the goal is to iden-\\ntify the most salient sentences in the document and\\nreturn them as the summary.\\n2) Abstractive Summarization, where the goal is to gen-\\nerate summary sentences from scratch; they may contain\\nnovel words that do not appear in the original document.\\nEach of these methods has its own advantages and disad-\\nvantages. Extractive summarization is prone to generate long\\nand sometimes overlapping summary sentences; however, the\\nresult reﬂects the author’s mode of expression. Abstractive\\nFig. 16. Neural Image Question Answering [191].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='and sometimes overlapping summary sentences; however, the\\nresult reﬂects the author’s mode of expression. Abstractive\\nFig. 16. Neural Image Question Answering [191].\\nFig. 17. Spatial Memory Network for VQA. Bright Areas are regions the\\nmodel is attending [192].\\nmethods generate a shorter summary but they are hard to train.\\nThere is a vast amount of research on the topic of text\\nsummarization using extractive and abstractive methods. As\\none of the earliest works on using neural networks for ex-\\ntractive summarization, [196] proposed a framework that\\nused a ranking technique to extract the most salient sentences\\nin the input. This model was improved by [197] which\\nused a document-level encoder to represent sentences, and\\na classiﬁer to rank these sentences. On the other hand, in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 16\\nabstractive summarization, it was [198] which, for the ﬁrst\\ntime, used attention over a sequence-to-sequence (seq2seq)\\nmodel for the problem of headline generation. However, since\\nsimple attention models perform worse than extractive models,\\ntherefore more effective attention models such as graph-based\\nattention [199] and transformers [173] have been proposed for\\nthis task. To further improve abstractive text summarization\\nmodels, [200] proposed the ﬁrst pointer-generator model and\\napplied it to the DeepMind QA dataset [201]. As a result\\nof this work, the CNN/Daily Mail dataset emerged which is\\nnow one of the widely used datasets for the summarization\\ntask. A copy mechanism was also adopted by [202] for\\nsimilar tasks. But their analysis reveals a key problem with\\nattention-based encoder-decoder models: they often generate\\nunusual summaries consisting of repeated phrases. Recently,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='similar tasks. But their analysis reveals a key problem with\\nattention-based encoder-decoder models: they often generate\\nunusual summaries consisting of repeated phrases. Recently,\\n[62] reached state-of-the-art results on the abstractive text\\nsummarization using a similar framework. They alleviated the\\nunnatural summaries by avoiding generating unknown tokens\\nand replacing these words with tokens from the input article.\\nLater, researchers moved their focus to methods that use\\nsentence-embedding to ﬁrst select the most salient sentence\\nin the document and then change them to make them more\\nabstractive [203], [204]. In these models, salient sentences\\nare extracted ﬁrst and then a paraphrasing model is used to\\nmake them abstractive. The extraction employs a sentence\\nclassiﬁer or ranker while the abstractor tries to remove the\\nextra information in a sentence and present it as a shorter\\nsummary. Fast-RL [203] is the ﬁrst framework in this family of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='classiﬁer or ranker while the abstractor tries to remove the\\nextra information in a sentence and present it as a shorter\\nsummary. Fast-RL [203] is the ﬁrst framework in this family of\\nworks. In Fast-RL, the extractor is pre-trained to select salient\\nsentences and the abstractor is pre-trained using a pointer-\\ngenerator model to generate paraphrases. Finally, to merge\\nthese two non-differentiable components, they propose using\\nActor-Critic Q-learning methods in which the actor receives\\na single document and generates the output while the critic\\nevaluates the output based on comparison with the ground-\\ntruth summary.\\nThough the standard way to evaluate the performance of\\nsummarization models is with ROUGE [67] and BLEU [68],\\nthere are major problems with such measures. For instance, the\\nROUGE measure focuses on the number of shared n-grams\\nbetween two sentences. Such a method incorrectly assigns\\na low score to an abstractive summary that uses different'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='ROUGE measure focuses on the number of shared n-grams\\nbetween two sentences. Such a method incorrectly assigns\\na low score to an abstractive summary that uses different\\nwords yet provides an excellent paraphrase that humans would\\nrate highly. Clearly, better automated evaluation methods are\\nneeded in such cases.\\nThere are additional problems with current summarization\\nmodels. Shi et al. [205] provides a comprehensive survey on\\ntext summarization.\\n[206] provides a recent survey on summarization methods.\\n[207] provides an advanced composite deep learning model,\\nbased on LSTMs and Restricted Boltzmann Machine, for\\nmulti-doc opinion summarization. A very inﬂuential recent\\nwork, [208], introduces H IBERT ( HIerachical Bidirectional\\nEncoder Representations from Transformers) as a pre-trained\\ninitialization for document summarization and report state-of-\\nthe-art performance.\\nH. Dialogue Systems\\nDialogue Systems are quickly becoming a principal in-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='initialization for document summarization and report state-of-\\nthe-art performance.\\nH. Dialogue Systems\\nDialogue Systems are quickly becoming a principal in-\\nstrument in human-computer interaction, due in part to their\\npromising potential and commercial value [209]. One appli-\\ncation is automated customer service, supporting both online\\nand bricks-and-mortar businesses. Customers expect an ever-\\nincreasing level of speed, accuracy, and respect while dealing\\nwith companies and their services. Due to the high cost of\\nknowledgeable human resources, companies frequently turn\\nto intelligent conversational machines. Note that the phrases\\nconversational machines and dialogue machines are often used\\ninterchangeably.\\nDialogue systems are usually task-based or non-task-\\nbased (Fig. 18). Though there might be Automatic Speech\\nRecognition (ASR) and Language-to-Speech (L2S) compo-\\nnents in a dialogue system, the discussion of this section is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='based (Fig. 18). Though there might be Automatic Speech\\nRecognition (ASR) and Language-to-Speech (L2S) compo-\\nnents in a dialogue system, the discussion of this section is\\nsolely about the linguistic components of dialogue systems;\\nconcepts associated with speech technology are ignored.\\nDespite useful statistical models employed in the backend\\nof dialogue systems (especially in language understanding\\nmodules), most deployed dialogue systems rely on expensive\\nhand-crafted and manual features for operation. Furthermore,\\nthe generalizability of these manually engineered systems to\\nother domains and functionalities is problematic. Hence, recent\\nattention has focused on deep learning for the enhancement of\\nperformance, generalizability, and robustness. Deep learning\\nfacilitates the creation of end-to-end task-oriented dialogue\\nsystems, which enriches the framework to generalize conver-\\nsations beyond annotated task-speciﬁc dialogue resources.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='facilitates the creation of end-to-end task-oriented dialogue\\nsystems, which enriches the framework to generalize conver-\\nsations beyond annotated task-speciﬁc dialogue resources.\\n1) Task-based Systems: The structure of a task-based dia-\\nlogue system usually consists of the following elements:\\n• Natural Language Understanding (NLU) : This compo-\\nnent deals with understanding and interpreting user’s\\nspoken context by assigning a constituent structure to the\\nspoken utterance (e.g., a sentence) and captures its syn-\\ntactic representation and semantic interpretation, to allow\\nthe back-end operation/task. NLU is usually leveraged\\nregardless of the dialogue context.\\n• Dialogue Manager (DM) : The generated representation\\nby NLU would be handled by the dialogue manager,\\nwhich investigates the context and returns a reasonable\\nsemantic-related response.\\n• Natural Language Generation (NLG) : The natural lan-\\nguage generation (NLG) component produces an utter-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='which investigates the context and returns a reasonable\\nsemantic-related response.\\n• Natural Language Generation (NLG) : The natural lan-\\nguage generation (NLG) component produces an utter-\\nance based on the response provided by the DM compo-\\nnent.\\nThe general pipeline is as follows: NLU module (i.e.,\\nsemantic decoder) transforms the output of the speech recog-\\nnition module to some dialogue elements. Then the DM\\nprocesses these dialogue elements and provides a suitable\\nresponse which is fed to the NLG for response generation.\\nThe main pipeline in NLU is to classify the user query domain\\nand user intent, and ﬁll a set of slots to create a semantic\\nframe. It is usually customary to perform the intent prediction\\nand the slot ﬁlling simultaneously [210]. Most of the task-\\noriented dialogue systems employ slot-ﬁlling approaches to\\nclassify user intent in the speciﬁc domain of the conversation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 17\\nFig. 18. The framework of a dialogue system. A dialogue system can be task oriented or used for natural language generation based on the user input which\\nis also known as a chat bot.\\nFor this aim, having predeﬁned tasks is required; this depends\\non manually crafted states with different associated slots.\\nHenceforth, a designed dialogue system would be of limited\\nor no use for other tasks.\\nRecent task-oriented dialogue systems have been designed\\nbased on deep reinforcement learning, which provided promis-\\ning results regarding performance [211], domain adapta-\\ntion [212], and dialogue generation [213]. This was due to\\na shift towards end-to-end trainable frameworks to design\\nand deploy task-oriented dialogue systems. Instead of the\\ntraditionally utilized pipeline, an end-to-end framework in-\\ncorporates and uses a single module that deals with external\\ndatabases. Despite the tractability of end-to-end dialogue'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='traditionally utilized pipeline, an end-to-end framework in-\\ncorporates and uses a single module that deals with external\\ndatabases. Despite the tractability of end-to-end dialogue\\nsystems (i.e., easy to train and simple to engineer), due to\\ntheir need for interoperability with external databases via\\nqueries, they are not well-suited for task-oriented settings.\\nSome approaches to this challenge include converting the user\\ninput into internal representations [214], combining supervised\\nand reinforced learning [215], and extending the memory\\nnetwork approach [216] for question-answering to a dialog\\nsystem [217].\\n2) Non-task-based Systems: As opposed to task-based dia-\\nlogue systems, the goal behind designing and deploying non-\\ntask-based dialogue systems is to empower a machine with\\nthe ability to have a natural conversation with humans [218].\\nTypically, chatbots are of one of the following types: retrieval-\\nbased methods and generative methods. Retrieval-based mod-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='the ability to have a natural conversation with humans [218].\\nTypically, chatbots are of one of the following types: retrieval-\\nbased methods and generative methods. Retrieval-based mod-\\nels have access to information resources and can provide more\\nconcise, ﬂuent, and accurate responses. However, they are\\nlimited regarding the variety of responses they can provide\\ndue to their dependency on backend data resources. Generative\\nmodels, on the other hand, have the advantage of being able\\nto produce suitable responses when such responses are not in\\nthe corpus. However, as opposed to retrieval-based models,\\nthey are more prone to grammatical and conceptual mistakes\\narising from their generative models.\\nRetrieval-based methods select an appropriate response\\nfrom the candidate responses. Therefore, the key element is the\\nquery-response operation. In general, this problem has been\\nformulated as a search problem and uses IR techniques for task'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='from the candidate responses. Therefore, the key element is the\\nquery-response operation. In general, this problem has been\\nformulated as a search problem and uses IR techniques for task\\ncompletion [219]. Retrieval-based methods usually employ\\neither Single-turn Response Matching or Multi-turn Response\\nMatching. In the ﬁrst type, the current query (message) is\\nsolely used to select a suitable response [220]. The latter\\ntype takes the current message and previous utterances as the\\nsystem input and retrieves a response based on the instant and\\ntemporal information. The model tries to choose a response\\nwhich considers the whole context to guarantee conversation\\nconsistency. An LSTM-based model has been proposed [221]\\nfor context and response vectors creation. In [222], various\\nfeatures and multiple data inputs have been incorporated to\\nbe ingested using a deep learning framework. Current base\\nmodels regarding retrieval-based chatbots rely on multi-turn'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='features and multiple data inputs have been incorporated to\\nbe ingested using a deep learning framework. Current base\\nmodels regarding retrieval-based chatbots rely on multi-turn\\nresponse selection augmented by an attention mechanism and\\nsequence matching [223].\\nGenerative models don’t assume the availability of pre-\\ndeﬁned responses. New responses are produced from scratch\\nand are based on the trained model. Generative models are\\ntypically based on sequence to sequence models and map an\\ninput query to a target element as the response. In general,\\ndesigning and implementing a dialogue agent to be able to\\nconverse at the human level is very challenging. The typical\\napproach usually consists of learning and imitating human\\nconversation. For this goal, the machine is generally trained on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 18\\nlarge corpora of conversations. However, this does not directly\\nremedy the issue of encountering out-of-corpus conversation .\\nThe question is: How can an agent be taught to generate\\nproper responses to conversations that it never has seen? It\\nmust handle content that is not exactly available in the data\\ncorpus that the machine has been trained on, due to the lack\\nof content matching between the query and the corresponding\\nresponse, resulting from the wide range of plausible queries\\nthat humans can provide.\\nTo tackle the aforementioned general problem, some fun-\\ndamental questions must be answered: (1) What are the core\\ncharacteristics of a natural conversation? (2) How can these\\ncharacteristics be measured? (3) How can we incorporate this\\nknowledge in a machine, i.e., the dialogue system? Effective\\nintegration of these three elements determines the intelligence\\nof a machine. A qualitative criterion is to observe if the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='knowledge in a machine, i.e., the dialogue system? Effective\\nintegration of these three elements determines the intelligence\\nof a machine. A qualitative criterion is to observe if the\\ngenerated utterances can be distinguished from natural human\\ndialogues. For quantitative evaluation, adversarial evaluation\\nwas initially used for quality assessment of sentence gener-\\nation [224] and employed for quality evaluation of dialogue\\nsystems [225]. Recent advancements in sequence to sequence\\nmodeling encouraged many research efforts regarding natural\\nlanguage generation [226]. Furthermore, deep reinforcement\\nlearning yields promising performance in natural language\\ngeneration [213].\\n3) Final note on dialogue systems: Despite remarkable\\nadvancements in AI and much attention dedicated to dia-\\nlogue systems, in reality, successful commercial tools, such\\nas Apple’s Siri and Amazon’s Alexa, still heavily rely on\\nhandcrafted features. It still is very challenging to design and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='logue systems, in reality, successful commercial tools, such\\nas Apple’s Siri and Amazon’s Alexa, still heavily rely on\\nhandcrafted features. It still is very challenging to design and\\ntrain data-driven dialogue machines given the complexity of\\nthe natural language, the difﬁculties in framework design, and\\nthe complex nature of available data sources.\\nVI. C ONCLUSION\\nIn this article, we presented a comprehensive survey of\\nthe most distinguished works in Natural Language Processing\\nusing deep learning. We provided a categorized context for\\nintroducing different NLP core concepts, aspects, and applica-\\ntions, and emphasized the most signiﬁcant conducted research\\nefforts in each associated category. Deep learning and NLP are\\ntwo of the most rapidly developing research topics nowadays.\\nDue to this rapid progress, it is hoped that soon, new effective\\nmodels will supersede the current state-of-the-art approaches.\\nThis may cause some of the references provided in the survey'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='Due to this rapid progress, it is hoped that soon, new effective\\nmodels will supersede the current state-of-the-art approaches.\\nThis may cause some of the references provided in the survey\\nto become dated, but those are likely to be cited by new\\npublications that describe improved methods\\nNeverthless, one of the essential characteristics of this\\nsurvey is its educational aspect, which provides a precise\\nunderstanding of the critical elements of this ﬁeld and explains\\nthe most notable research works. Hopefully, this survey will\\nguide students and researchers with essential resources, both\\nto learn what is necessary to know, and to advance further the\\nintegration of NLP with deep learning.\\nREFERENCES\\n[1] C. D. Manning, C. D. Manning, and H. Sch ¨utze, Foundations of\\nstatistical natural language processing . MIT Press, 1999.\\n[2] X. Zhang, J. Zhao, and Y . LeCun, “Character-level convolutional\\nnetworks for text classiﬁcation,” in Advances in neural information'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='statistical natural language processing . MIT Press, 1999.\\n[2] X. Zhang, J. Zhao, and Y . LeCun, “Character-level convolutional\\nnetworks for text classiﬁcation,” in Advances in neural information\\nprocessing systems, pp. 649–657, 2015.\\n[3] K. Cho, B. Van Merri ¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares,\\nH. Schwenk, and Y . Bengio, “Learning phrase representations us-\\ning RNN encoder-decoder for statistical machine translation,” arXiv\\npreprint arXiv:1406.1078, 2014.\\n[4] S. Wu, K. Roberts, S. Datta, J. Du, Z. Ji, Y . Si, S. Soni, Q. Wang,\\nQ. Wei, Y . Xiang, B. Zhao, and H. Xu, “Deep learning in clinical\\nnatural language processing: a methodical review,” Journal of the\\nAmerican Medical Informatics Association , vol. 27, pp. 457–470, mar\\n2020.\\n[5] R. Collobert and J. Weston, “A uniﬁed architecture for natural lan-\\nguage processing: Deep neural networks with multitask learning,” in\\nProceedings of the 25th international conference on Machine learning ,\\npp. 160–167, ACM, 2008.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='guage processing: Deep neural networks with multitask learning,” in\\nProceedings of the 25th international conference on Machine learning ,\\npp. 160–167, ACM, 2008.\\n[6] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and\\nL. Fei-Fei, “Large-scale video classiﬁcation with convolutional neural\\nnetworks,” in Proceedings of the IEEE conference on Computer Vision\\nand Pattern Recognition, pp. 1725–1732, 2014.\\n[7] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, “Learning and transferring\\nmid-level image representations using convolutional neural networks,”\\nin Proceedings of the IEEE conference on Computer Vision and Pattern\\nRecognition, pp. 1717–1724, 2014.\\n[8] A. Shrivastava, T. Pﬁster, O. Tuzel, J. Susskind, W. Wang, and R. Webb,\\n“Learning from simulated and unsupervised images through adversarial\\ntraining,” in Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pp. 2107–2116, 2017.\\n[9] A. V oulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='training,” in Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pp. 2107–2116, 2017.\\n[9] A. V oulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis,\\n“Deep Learning for Computer Vision: A Brief Review,”Computational\\nIntelligence and Neuroscience , Feb 2018.\\n[10] N. O’Mahony, S. Campbell, A. Carvalho, S. Harapanahalli, G. V .\\nHernandez, L. Krpalkova, D. Riordan, and J. Walsh, “Deep learning vs.\\ntraditional computer vision,” in Advances in Computer Vision (K. Arai\\nand S. Kapoor, eds.), (Cham), pp. 128–144, Springer International\\nPublishing, 2020.\\n[11] A. Graves and N. Jaitly, “Towards end-to-end speech recognition with\\nrecurrent neural networks,” in International Conference on Machine\\nLearning, pp. 1764–1772, 2014.\\n[12] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg,\\nC. Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen, et al. , “Deep\\nspeech 2: End-to-end speech recognition in English and Mandarin,” in\\nICML, pp. 173–182, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='C. Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen, et al. , “Deep\\nspeech 2: End-to-end speech recognition in English and Mandarin,” in\\nICML, pp. 173–182, 2016.\\n[13] U. Kamath, J. Liu, and J. Whitaker, Deep learning for NLP and speech\\nrecognition, vol. 84. Springer, 2019.\\n[14] C. D. Santos and B. Zadrozny, “Learning character-level representa-\\ntions for part-of-speech tagging,” in Proceedings of the 31st Interna-\\ntional Conference on Machine Learning (ICML-14) , pp. 1818–1826,\\n2014.\\n[15] B. Plank, A. Søgaard, and Y . Goldberg, “Multilingual part-of-speech\\ntagging with bidirectional long short-term memory models and auxil-\\niary loss,” arXiv preprint arXiv:1604.05529 , 2016.\\n[16] C. D. Manning, “Part-of-speech tagging from 97% to 100%: is it\\ntime for some linguistics?,” in International Conference on Intelligent\\nText Processing and Computational Linguistics, pp. 171–189, Springer,\\n2011.\\n[17] R. D. Deshmukh and A. Kiwelekar, “Deep learning techniques for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='Text Processing and Computational Linguistics, pp. 171–189, Springer,\\n2011.\\n[17] R. D. Deshmukh and A. Kiwelekar, “Deep learning techniques for\\npart of speech tagging by natural language processing,” in 2020\\n2nd International Conference on Innovative Mechanisms for Industry\\nApplications (ICIMIA), pp. 76–81, IEEE, 2020.\\n[18] G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and\\nC. Dyer, “Neural architectures for named entity recognition,” arXiv\\npreprint arXiv:1603.01360, 2016.\\n[19] J. P. Chiu and E. Nichols, “Named entity recognition with bidirectional\\nLSTM-CNNs,” arXiv preprint arXiv:1511.08308 , 2015.\\n[20] V . Yadav and S. Bethard, “A survey on recent advances in\\nnamed entity recognition from deep learning models,” arXiv preprint\\narXiv:1910.11470, 2019.\\n[21] J. Li, A. Sun, J. Han, and C. Li, “A survey on deep learning for\\nnamed entity recognition,” IEEE Transactions on Knowledge and Data\\nEngineering, 2020.\\n[22] J. Zhou and W. Xu, “End-to-end learning of semantic role labeling'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='named entity recognition,” IEEE Transactions on Knowledge and Data\\nEngineering, 2020.\\n[22] J. Zhou and W. Xu, “End-to-end learning of semantic role labeling\\nusing recurrent neural networks,” in Proceedings of the 53rd Annual\\nMeeting of the Association for Computational Linguistics and the\\n7th International Joint Conference on Natural Language Processing\\n(Volume 1: Long Papers), vol. 1, pp. 1127–1137, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 19\\n[23] D. Marcheggiani, A. Frolov, and I. Titov, “A simple and accurate\\nsyntax-agnostic neural model for dependency-based semantic role\\nlabeling,” arXiv preprint arXiv:1701.02593 , 2017.\\n[24] L. He, K. Lee, M. Lewis, and L. Zettlemoyer, “Deep semantic role\\nlabeling: What works and what’s next,” in Proceedings of the 55th\\nAnnual Meeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers), vol. 1, pp. 473–483, 2017.\\n[25] S. He, Z. Li, and H. Zhao, “Syntax-aware multilingual semantic role\\nlabeling,” arXiv preprint arXiv:1909.00310 , 2019.\\n[26] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent trends in\\ndeep learning based natural language processing,” IEEE Computational\\nIntelligence Magazine, vol. 13, no. 3, pp. 55–75, 2018.\\n[27] Y . Kang, Z. Cai, C.-W. Tan, Q. Huang, and H. Liu, “Natural language\\nprocessing (NLP) in management research: A literature review,” Jour-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='Intelligence Magazine, vol. 13, no. 3, pp. 55–75, 2018.\\n[27] Y . Kang, Z. Cai, C.-W. Tan, Q. Huang, and H. Liu, “Natural language\\nprocessing (NLP) in management research: A literature review,” Jour-\\nnal of Management Analytics , vol. 7, pp. 139–172, apr 2020.\\n[28] T. Greenwald, “What exactly is artiﬁcial in-\\ntelligence, anyway?.” https://www.wsj.com/articles/\\nwhat-exactly-is-artiﬁcial-intelligence-anyway-1525053960, April\\n2018. Wall Street Journal Online Article.\\n[29] U. Sivarajah, M. M. Kamal, Z. Irani, and V . Weerakkody, “Critical\\nanalysis of big data challenges and analytical methods,” Journal of\\nBusiness Research, vol. 70, pp. 263–286, 2017.\\n[30] Z. C. Lipton, J. Berkowitz, and C. Elkan, “A critical review of\\nrecurrent neural networks for sequence learning,” arXiv preprint\\narXiv:1506.00019, 2015.\\n[31] Y . Kim, “Convolutional neural networks for sentence classiﬁcation,”\\narXiv preprint arXiv:1408.5882 , 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='arXiv:1506.00019, 2015.\\n[31] Y . Kim, “Convolutional neural networks for sentence classiﬁcation,”\\narXiv preprint arXiv:1408.5882 , 2014.\\n[32] R. Socher, C. C. Lin, C. Manning, and A. Y . Ng, “Parsing natural scenes\\nand natural language with recursive neural networks,” in Proceedings\\nof the 28th international conference on machine learning (ICML-11) ,\\npp. 129–136, 2011.\\n[33] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\\nwith deep convolutional neural networks,” in Advances in neural\\ninformation processing systems , pp. 1097–1105, 2012.\\n[34] C. dos Santos and M. Gatti, “Deep convolutional neural networks for\\nsentiment analysis of short texts,” in Proceedings of COLING 2014, the\\n25th International Conference on Computational Linguistics: Technical\\nPapers, pp. 69–78, 2014.\\n[35] R. Johnson and T. Zhang, “Effective use of word order for text\\ncategorization with convolutional neural networks,” arXiv preprint\\narXiv:1412.1058, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='Papers, pp. 69–78, 2014.\\n[35] R. Johnson and T. Zhang, “Effective use of word order for text\\ncategorization with convolutional neural networks,” arXiv preprint\\narXiv:1412.1058, 2014.\\n[36] R. Johnson and T. Zhang, “Semi-supervised convolutional neural\\nnetworks for text categorization via region embedding,” in Advances\\nin neural information processing systems , pp. 919–927, 2015.\\n[37] D. Zeng, K. Liu, S. Lai, G. Zhou, and J. Zhao, “Relation classiﬁcation\\nvia convolutional deep neural network,” in Proceedings of COLING\\n2014, the 25th International Conference on Computational Linguistics:\\nTechnical Papers, pp. 2335–2344, 2014.\\n[38] T. H. Nguyen and R. Grishman, “Relation extraction: Perspective from\\nconvolutional neural networks,” in Proceedings of the 1st Workshop on\\nVector Space Modeling for Natural Language Processing , pp. 39–48,\\n2015.\\n[39] T. Mikolov, M. Karaﬁ ´at, L. Burget, J. ˇCernock`y, and S. Khudanpur,\\n“Recurrent neural network based language model,” in Eleventh Annual'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='2015.\\n[39] T. Mikolov, M. Karaﬁ ´at, L. Burget, J. ˇCernock`y, and S. Khudanpur,\\n“Recurrent neural network based language model,” in Eleventh Annual\\nConference of the International Speech Communication Association ,\\n2010.\\n[40] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\\n[41] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\\nS. Ozair, A. Courville, and Y . Bengio, “Generative adversarial nets,”\\nin Advances in neural information processing systems , pp. 2672–2680,\\n2014.\\n[42] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,” arXiv\\npreprint arXiv:1701.07875, 2017.\\n[43] X. Chen, Y . Duan, R. Houthooft, J. Schulman, I. Sutskever, and\\nP. Abbeel, “Infogan: Interpretable representation learning by informa-\\ntion maximizing generative adversarial nets,” in Advances in neural\\ninformation processing systems , pp. 2172–2180, 2016.\\n[44] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='tion maximizing generative adversarial nets,” in Advances in neural\\ninformation processing systems , pp. 2172–2180, 2016.\\n[44] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation\\nlearning with deep convolutional generative adversarial networks,”\\narXiv preprint arXiv:1511.06434 , 2015.\\n[45] T. Karras, T. Aila, S. Laine, and J. Lehtinen, “Progressive growing\\nof GANs for improved quality, stability, and variation,” arXiv preprint\\narXiv:1710.10196, 2017.\\n[46] N. Tavaf, A. Torﬁ, K. Ugurbil, and P.-F. Van de Moortele,\\n“GRAPPA-GANs for Parallel MRI Reconstruction,” arXiv preprint\\narXiv:2101.03135, Jan 2021.\\n[47] L. Yu, W. Zhang, J. Wang, and Y . Yu, “Seqgan: Sequence generative\\nadversarial nets with policy gradient,” in Thirty-First AAAI Conference\\non Artiﬁcial Intelligence , 2017.\\n[48] J. Li, W. Monroe, T. Shi, S. Jean, A. Ritter, and D. Jurafsky,\\n“Adversarial learning for neural dialogue generation,” arXiv preprint\\narXiv:1701.06547, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='on Artiﬁcial Intelligence , 2017.\\n[48] J. Li, W. Monroe, T. Shi, S. Jean, A. Ritter, and D. Jurafsky,\\n“Adversarial learning for neural dialogue generation,” arXiv preprint\\narXiv:1701.06547, 2017.\\n[49] B. Pang, L. Lee, and S. Vaithyanathan, “Thumbs up?: sentiment classi-\\nﬁcation using machine learning techniques,” inProceedings of the ACL-\\n02 conference on Empirical methods in natural language processing-\\nVolume 10 , pp. 79–86, Association for Computational Linguistics,\\n2002.\\n[50] Z. S. Harris, “Distributional structure,” Word, vol. 10, no. 2-3, pp. 146–\\n162, 1954.\\n[51] Y . Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural proba-\\nbilistic language model,” Journal of machine learning research, vol. 3,\\nno. Feb., pp. 1137–1155, 2003.\\n[52] Q. Le and T. Mikolov, “Distributed representations of sentences\\nand documents,” in International Conference on Machine Learning ,\\npp. 1188–1196, 2014.\\n[53] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='and documents,” in International Conference on Machine Learning ,\\npp. 1188–1196, 2014.\\n[53] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,\\n“Distributed representations of words and phrases and their compo-\\nsitionality,” in Advances in neural information processing systems ,\\npp. 3111–3119, 2013.\\n[54] R. Kiros, Y . Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Tor-\\nralba, and S. Fidler, “Skip-thought vectors,” in Advances in neural\\ninformation processing systems , pp. 3294–3302, 2015.\\n[55] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of\\nword representations in vector space,” arXiv preprint arXiv:1301.3781,\\n2013.\\n[56] G. Lebanon et al. , Riemannian geometry and statistical machine\\nlearning. LAP LAMBERT Academic Publishing, 2015.\\n[57] J. Leskovec, A. Rajaraman, and J. D. Ullman, Mining of massive\\ndatasets. Cambridge University Press, 2014.\\n[58] Y . Goldberg, “Neural network methods for natural language process-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='[57] J. Leskovec, A. Rajaraman, and J. D. Ullman, Mining of massive\\ndatasets. Cambridge University Press, 2014.\\n[58] Y . Goldberg, “Neural network methods for natural language process-\\ning,” Synthesis Lectures on Human Language Technologies , vol. 10,\\nno. 1, pp. 1–309, 2017.\\n[59] J. Wehrmann, W. Becker, H. E. Cagnini, and R. C. Barros, “A character-\\nbased convolutional neural network for language-agnostic Twitter sen-\\ntiment analysis,” in Neural Networks (IJCNN), 2017 International Joint\\nConference on, pp. 2384–2391, IEEE, 2017.\\n[60] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, “Enriching word\\nvectors with subword information,” arXiv preprint arXiv:1607.04606 ,\\n2016.\\n[61] J. Botha and P. Blunsom, “Compositional morphology for word rep-\\nresentations and language modelling,” in International Conference on\\nMachine Learning, pp. 1899–1907, 2014.\\n[62] A. See, P. J. Liu, and C. D. Manning, “Get to the point: Summarization'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='resentations and language modelling,” in International Conference on\\nMachine Learning, pp. 1899–1907, 2014.\\n[62] A. See, P. J. Liu, and C. D. Manning, “Get to the point: Summarization\\nwith pointer-generator networks,” in ACL, vol. 1, pp. 1073–1083, 2017.\\n[63] R. Paulus, C. Xiong, and R. Socher, “A deep reinforced model for\\nabstractive summarization,” arXiv preprint arXiv:1705.04304 , 2017.\\n[64] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, “Scheduled sampling\\nfor sequence prediction with recurrent neural networks,” in Advances\\nin Neural Information Processing Systems , pp. 1171–1179, 2015.\\n[65] K. Goyal, G. Neubig, C. Dyer, and T. Berg-Kirkpatrick, “A continuous\\nrelaxation of beam search for end-to-end training of neural sequence\\nmodels,” in Thirty-Second AAAI Conference on Artiﬁcial Intelligence ,\\n2018.\\n[66] W. Kool, H. Van Hoof, and M. Welling, “Stochastic beams and where\\nto ﬁnd them: The gumbel-top-k trick for sampling sequences with-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='2018.\\n[66] W. Kool, H. Van Hoof, and M. Welling, “Stochastic beams and where\\nto ﬁnd them: The gumbel-top-k trick for sampling sequences with-\\nout replacement,” in International Conference on Machine Learning ,\\npp. 3499–3508, 2019.\\n[67] C.-Y . Lin, “Rouge: A package for automatic evaluation of summaries,”\\nin Text summarization branches out , pp. 74–81, 2004.\\n[68] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: a method\\nfor automatic evaluation of machine translation,” in Proceedings of\\nthe 40th annual meeting on Association for Computational Linguistics ,\\npp. 311–318, Association for Computational Linguistics, 2002.\\n[69] S. Banerjee and A. Lavie, “METEOR: An automatic metric for\\nMT evaluation with improved correlation with human judgments,” in\\nProceedings of the ACL workshop on intrinsic and extrinsic evaluation\\nmeasures for machine translation and/or summarization , pp. 65–72,\\n2005.\\n[70] Y . Keneshloo, T. Shi, C. K. Reddy, and N. Ramakrishnan, “Deep rein-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='measures for machine translation and/or summarization , pp. 65–72,\\n2005.\\n[70] Y . Keneshloo, T. Shi, C. K. Reddy, and N. Ramakrishnan, “Deep rein-\\nforcement learning for sequence to sequence models,” arXiv preprint\\narXiv:1805.09461, 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 20\\n[71] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba, “Sequence\\nlevel training with recurrent neural networks,” arXiv preprint\\narXiv:1511.06732, 2015.\\n[72] W. Zaremba and I. Sutskever, “Reinforcement learning neural Turing\\nmachines-revised,” arXiv preprint arXiv:1505.00521 , 2015.\\n[73] R. J. Williams, “Simple statistical gradient-following algorithms for\\nconnectionist reinforcement learning,” in Reinforcement Learning ,\\npp. 5–32, Springer, 1992.\\n[74] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\\nMIT Press, 2018.\\n[75] C. J. Watkins and P. Dayan, “Q-learning,” Machine Learning, vol. 8,\\nno. 3-4, pp. 279–292, 1992.\\n[76] H. Daum ´e, J. Langford, and D. Marcu, “Search-based structured\\nprediction,” Machine learning, vol. 75, no. 3, pp. 297–325, 2009.\\n[77] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training of\\ndeep visuomotor policies,” The Journal of Machine Learning Research,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='[77] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training of\\ndeep visuomotor policies,” The Journal of Machine Learning Research,\\nvol. 17, no. 1, pp. 1334–1373, 2016.\\n[78] V . Mnih, N. Heess, A. Graves, et al. , “Recurrent models of visual\\nattention,” in Advances in neural information processing systems ,\\npp. 2204–2212, 2014.\\n[79] C. Sun, L. Huang, and X. Qiu, “Utilizing BERT for aspect-based\\nsentiment analysis via constructing auxiliary sentence,” arXiv preprint\\narXiv:1903.09588, 2019.\\n[80] P. Resnik and J. Lin, “Evaluation of NLP systems,” The handbook\\nof computational linguistics and natural language processing , vol. 57,\\npp. 271–295, 2010.\\n[81] A. Kumar, O. Irsoy, P. Ondruska, M. Iyyer, J. Bradbury, I. Gulrajani,\\nV . Zhong, R. Paulus, and R. Socher, “Ask me anything: Dynamic\\nmemory networks for natural language processing,” in International\\nConference on Machine Learning , pp. 1378–1387, 2016.\\n[82] Z. Huang, W. Xu, and K. Yu, “Bidirectional LSTM-CRF models for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='memory networks for natural language processing,” in International\\nConference on Machine Learning , pp. 1378–1387, 2016.\\n[82] Z. Huang, W. Xu, and K. Yu, “Bidirectional LSTM-CRF models for\\nsequence tagging,” arXiv preprint arXiv:1508.01991 , 2015.\\n[83] D. Andor, C. Alberti, D. Weiss, A. Severyn, A. Presta, K. Ganchev,\\nS. Petrov, and M. Collins, “Globally normalized transition-based neural\\nnetworks,” arXiv preprint arXiv:1603.06042 , 2016.\\n[84] X. Xue and J. Zhang, “Part-of-speech tagging of building codes\\nempowered by deep learning and transformational rules,” Advanced\\nEngineering Informatics, vol. 47, p. 101235, 2021.\\n[85] L. Liu, J. Shang, X. Ren, F. F. Xu, H. Gui, J. Peng, and J. Han,\\n“Empower sequence labeling with task-aware neural language model,”\\nin Thirty-Second AAAI Conference on Artiﬁcial Intelligence , 2018.\\n[86] Z. Yang, R. Salakhutdinov, and W. W. Cohen, “Transfer learning for\\nsequence tagging with hierarchical recurrent networks,” arXiv preprint\\narXiv:1703.06345, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='[86] Z. Yang, R. Salakhutdinov, and W. W. Cohen, “Transfer learning for\\nsequence tagging with hierarchical recurrent networks,” arXiv preprint\\narXiv:1703.06345, 2017.\\n[87] X. Ma and E. Hovy, “End-to-end sequence labeling via bi-directional\\nLSTM-CNNs-CRF,” arXiv preprint arXiv:1603.01354 , 2016.\\n[88] M. Yasunaga, J. Kasai, and D. Radev, “Robust multilingual\\npart-of-speech tagging via adversarial training,” arXiv preprint\\narXiv:1711.04903, 2017.\\n[89] W. Ling, T. Lu ´ıs, L. Marujo, R. F. Astudillo, S. Amir, C. Dyer, A. W.\\nBlack, and I. Trancoso, “Finding function in form: Compositional\\ncharacter models for open vocabulary word representation,” arXiv\\npreprint arXiv:1508.02096, 2015.\\n[90] A. Akbik, D. Blythe, and R. V ollgraf, “Contextual string embeddings\\nfor sequence labeling,” in Proceedings of the 27th International Con-\\nference on Computational Linguistics , pp. 1638–1649, 2018.\\n[91] B. Bohnet, R. McDonald, G. Simoes, D. Andor, E. Pitler, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='for sequence labeling,” in Proceedings of the 27th International Con-\\nference on Computational Linguistics , pp. 1638–1649, 2018.\\n[91] B. Bohnet, R. McDonald, G. Simoes, D. Andor, E. Pitler, and\\nJ. Maynez, “Morphosyntactic tagging with a Meta-BiLSTM model over\\ncontext sensitive token encodings,” arXiv preprint arXiv:1805.08237 ,\\n2018.\\n[92] J. Legrand and R. Collobert, “Joint RNN-based greedy parsing and\\nword composition,” arXiv preprint arXiv:1412.7028 , 2014.\\n[93] J. Legrand and R. Collobert, “Deep neural networks for syntactic\\nparsing of morphologically rich languages,” in Proceedings of the\\n54th Annual Meeting of the Association for Computational Linguistics\\n(Volume 2: Short Papers), pp. 573–578, 2016.\\n[94] A. Kuncoro, M. Ballesteros, L. Kong, C. Dyer, G. Neubig, and N. A.\\nSmith, “What do recurrent neural network grammars learn about\\nsyntax?,” arXiv preprint arXiv:1611.05774 , 2016.\\n[95] J. Liu and Y . Zhang, “In-order transition-based constituent parsing,”'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='Smith, “What do recurrent neural network grammars learn about\\nsyntax?,” arXiv preprint arXiv:1611.05774 , 2016.\\n[95] J. Liu and Y . Zhang, “In-order transition-based constituent parsing,”\\narXiv preprint arXiv:1707.05000 , 2017.\\n[96] D. Fried, M. Stern, and D. Klein, “Improving neural parsing by\\ndisentangling model combination and reranking effects,” arXiv preprint\\narXiv:1707.03058, 2017.\\n[97] N. Kitaev and D. Klein, “Constituency parsing with a self-attentive\\nencoder,” arXiv preprint arXiv:1805.01052 , 2018.\\n[98] D. Chen and C. Manning, “A fast and accurate dependency parser using\\nneural networks,” in Proceedings of the 2014 conference on empirical\\nmethods in natural language processing (EMNLP), pp. 740–750, 2014.\\n[99] T. Dozat and C. D. Manning, “Deep biafﬁne attention for neural\\ndependency parsing,” arXiv preprint arXiv:1611.01734 , 2016.\\n[100] E. Kiperwasser and Y . Goldberg, “Simple and accurate dependency\\nparsing using bidirectional LSTM feature representations,” arXiv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='dependency parsing,” arXiv preprint arXiv:1611.01734 , 2016.\\n[100] E. Kiperwasser and Y . Goldberg, “Simple and accurate dependency\\nparsing using bidirectional LSTM feature representations,” arXiv\\npreprint arXiv:1603.04351, 2016.\\n[101] C. Dyer, M. Ballesteros, W. Ling, A. Matthews, and N. A. Smith,\\n“Transition-based dependency parsing with stack long short-term mem-\\nory,” arXiv preprint arXiv:1505.08075 , 2015.\\n[102] S. Jaf and C. Calder, “Deep learning for natural language parsing,”\\nIEEE Access, vol. 7, pp. 131363–131373, 2019.\\n[103] Y . Zhang, F. Tiryaki, M. Jiang, and H. Xu, “Parsing clinical text\\nusing the state-of-the-art deep learning based parsers: a systematic\\ncomparison,” BMC medical informatics and decision making , vol. 19,\\nno. 3, p. 77, 2019.\\n[104] Y . Zhang, Z. Li, and M. Zhang, “Efﬁcient second-order treecrf for\\nneural dependency parsing,” arXiv preprint arXiv:2005.00975 , 2020.\\n[105] T. Dozat and C. D. Manning, “Deep biafﬁne attention for neural'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='neural dependency parsing,” arXiv preprint arXiv:2005.00975 , 2020.\\n[105] T. Dozat and C. D. Manning, “Deep biafﬁne attention for neural\\ndependency parsing,” 2017.\\n[106] Z. Tan, M. Wang, J. Xie, Y . Chen, and X. Shi, “Deep semantic role\\nlabeling with self-attention,” arXiv preprint arXiv:1712.01586 , 2017.\\n[107] D. Marcheggiani and I. Titov, “Encoding sentences with graph\\nconvolutional networks for semantic role labeling,” arXiv preprint\\narXiv:1703.04826, 2017.\\n[108] E. Strubell, P. Verga, D. Andor, D. Weiss, and A. McCallum,\\n“Linguistically-informed self-attention for semantic role labeling,”\\narXiv preprint arXiv:1804.08199 , 2018.\\n[109] L. He, K. Lee, O. Levy, and L. Zettlemoyer, “Jointly predicting\\npredicates and arguments in neural semantic role labeling,” arXiv\\npreprint arXiv:1805.04787, 2018.\\n[110] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\\nand L. Zettlemoyer, “Deep contextualized word representations,” arXiv\\npreprint arXiv:1802.05365, 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='[110] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\\nand L. Zettlemoyer, “Deep contextualized word representations,” arXiv\\npreprint arXiv:1802.05365, 2018.\\n[111] Z. Tan, M. Wang, J. Xie, Y . Chen, and X. Shi, “Deep semantic role\\nlabeling with self-attention,” in Thirty-Second AAAI Conference on\\nArtiﬁcial Intelligence, 2018.\\n[112] Z. Li, S. He, H. Zhao, Y . Zhang, Z. Zhang, X. Zhou, and X. Zhou,\\n“Dependency or span, end-to-end uniform semantic role labeling,” in\\nProceedings of the AAAI Conference on Artiﬁcial Intelligence , vol. 33,\\npp. 6730–6737, 2019.\\n[113] S. Pradhan, A. Moschitti, N. Xue, H. T. Ng, A. Bj ¨orkelund,\\nO. Uryupina, Y . Zhang, and Z. Zhong, “Towards robust linguistic anal-\\nysis using OntoNotes,” in Proceedings of the Seventeenth Conference\\non Computational Natural Language Learning , pp. 143–152, 2013.\\n[114] N. Kalchbrenner, E. Grefenstette, and P. Blunsom, “A convo-\\nlutional neural network for modelling sentences,” arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='on Computational Natural Language Learning , pp. 143–152, 2013.\\n[114] N. Kalchbrenner, E. Grefenstette, and P. Blunsom, “A convo-\\nlutional neural network for modelling sentences,” arXiv preprint\\narXiv:1404.2188, 2014.\\n[115] H. Palangi, L. Deng, Y . Shen, J. Gao, X. He, J. Chen, X. Song,\\nand R. Ward, “Deep sentence embedding using long short-term\\nmemory networks: Analysis and application to information retrieval,”\\nIEEE/ACM Transactions on Audio, Speech and Language Processing\\n(TASLP), vol. 24, no. 4, pp. 694–707, 2016.\\n[116] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, “Hierar-\\nchical attention networks for document classiﬁcation,” in Proceedings\\nof the 2016 Conference of the North American Chapter of the Associ-\\nation for Computational Linguistics: Human Language Technologies ,\\npp. 1480–1489, 2016.\\n[117] S. Lai, L. Xu, K. Liu, and J. Zhao, “Recurrent convolutional neural\\nnetworks for text classiﬁcation.,” in AAAI, vol. 333, pp. 2267–2273,\\n2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='pp. 1480–1489, 2016.\\n[117] S. Lai, L. Xu, K. Liu, and J. Zhao, “Recurrent convolutional neural\\nnetworks for text classiﬁcation.,” in AAAI, vol. 333, pp. 2267–2273,\\n2015.\\n[118] C. Zhou, C. Sun, Z. Liu, and F. Lau, “A C-LSTM neural network for\\ntext classiﬁcation,” arXiv preprint arXiv:1511.08630 , 2015.\\n[119] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu,\\nand J. Gao, “Deep learning based text classiﬁcation: A comprehensive\\nreview,”arXiv preprint arXiv:2004.03705 , 2020.\\n[120] M. Zulqarnain, R. Ghazali, Y . M. M. Hassim, and M. Rehan, “A\\ncomparative review on deep learning models for text classiﬁcation,”\\nIndones. J. Electr. Eng. Comput. Sci, vol. 19, no. 1, pp. 325–335, 2020.\\n[121] A. Conneau, H. Schwenk, L. Barrault, and Y . LeCun, “Very deep\\nconvolutional networks for text classiﬁcation,” in Proceedings of the\\n15th Conference of the European Chapter of the Association for\\nComputational Linguistics: Volume 1, Long Papers , vol. 1, pp. 1107–\\n1116, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 21\\n[122] R. Johnson and T. Zhang, “Deep pyramid convolutional neural net-\\nworks for text categorization,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics (Volume 1:\\nLong Papers), vol. 1, pp. 562–570, 2017.\\n[123] R. Johnson and T. Zhang, “Supervised and semi-supervised text\\ncategorization using LSTM for region embeddings,” arXiv preprint\\narXiv:1602.02373, 2016.\\n[124] J. Howard and S. Ruder, “Universal language model ﬁne-tuning for\\ntext classiﬁcation,” in Proceedings of the 56th Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers) ,\\nvol. 1, pp. 328–339, 2018.\\n[125] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and\\nP. Kuksa, “Natural language processing (almost) from scratch,”Journal\\nof Machine Learning Research, vol. 12, no. Aug., pp. 2493–2537, 2011.\\n[126] G. Mesnil, X. He, L. Deng, and Y . Bengio, “Investigation of recurrent-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='of Machine Learning Research, vol. 12, no. Aug., pp. 2493–2537, 2011.\\n[126] G. Mesnil, X. He, L. Deng, and Y . Bengio, “Investigation of recurrent-\\nneural-network architectures and learning methods for spoken language\\nunderstanding.,” in Interspeech, pp. 3771–3775, 2013.\\n[127] F. Dernoncourt, J. Y . Lee, and P. Szolovits, “NeuroNER: an easy-to-\\nuse program for named-entity recognition based on neural networks,”\\nConference on Empirical Methods on Natural Language Processing\\n(EMNLP), 2017.\\n[128] A. Baevski, S. Edunov, Y . Liu, L. Zettlemoyer, and M. Auli, “Cloze-\\ndriven pretraining of self-attention networks,” 2019.\\n[129] E. F. Tjong Kim Sang and F. De Meulder, “Introduction to the CoNLL-\\n2003 shared task: Language-independent named entity recognition,” in\\nProceedings of the seventh conference on Natural language learning\\nat HLT-NAACL 2003-Volume 4, pp. 142–147, Association for Compu-\\ntational Linguistics, 2003.\\n[130] K. Clark, M.-T. Luong, C. D. Manning, and Q. V . Le, “Semi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='at HLT-NAACL 2003-Volume 4, pp. 142–147, Association for Compu-\\ntational Linguistics, 2003.\\n[130] K. Clark, M.-T. Luong, C. D. Manning, and Q. V . Le, “Semi-\\nsupervised sequence modeling with cross-view training,”arXiv preprint\\narXiv:1809.08370, 2018.\\n[131] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\\ntraining of deep bidirectional transformers for language understanding,”\\narXiv preprint arXiv:1810.04805 , 2018.\\n[132] R. Socher, B. Huval, C. D. Manning, and A. Y . Ng, “Semantic compo-\\nsitionality through recursive matrix-vector spaces,” in Proceedings of\\nthe 2012 joint conference on empirical methods in natural language\\nprocessing and computational natural language learning , pp. 1201–\\n1211, Association for Computational Linguistics, 2012.\\n[133] Z. Geng, G. Chen, Y . Han, G. Lu, and F. Li, “Semantic relation\\nextraction using sequential and tree-structured lstm with attention,”\\nInformation Sciences, vol. 509, pp. 183–192, 2020.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='[133] Z. Geng, G. Chen, Y . Han, G. Lu, and F. Li, “Semantic relation\\nextraction using sequential and tree-structured lstm with attention,”\\nInformation Sciences, vol. 509, pp. 183–192, 2020.\\n[134] X. Han, T. Gao, Y . Lin, H. Peng, Y . Yang, C. Xiao, Z. Liu, P. Li,\\nM. Sun, and J. Zhou, “More data, more relations, more context and\\nmore openness: A review and outlook for relation extraction,” arXiv\\npreprint arXiv:2004.03186, 2020.\\n[135] K. Clark and C. D. Manning, “Deep reinforcement learn-\\ning for mention-ranking coreference models,” arXiv preprint\\narXiv:1609.08667, 2016.\\n[136] K. Lee, L. He, and L. Zettlemoyer, “Higher-order coreference resolu-\\ntion with coarse-to-ﬁne inference,” arXiv preprint arXiv:1804.05392 ,\\n2018.\\n[137] H. Fei, X. Li, D. Li, and P. Li, “End-to-end deep reinforcement learning\\nbased coreference resolution,” in Proceedings of the 57th Annual\\nMeeting of the Association for Computational Linguistics , pp. 660–\\n665, 2019.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='based coreference resolution,” in Proceedings of the 57th Annual\\nMeeting of the Association for Computational Linguistics , pp. 660–\\n665, 2019.\\n[138] W. Wu, F. Wang, A. Yuan, F. Wu, and J. Li, “Corefqa: Coreference\\nresolution as query-based span prediction,” in Proceedings of the 58th\\nAnnual Meeting of the Association for Computational Linguistics ,\\npp. 6953–6963, 2020.\\n[139] Y . Chen, L. Xu, K. Liu, D. Zeng, and J. Zhao, “Event extraction via\\ndynamic multi-pooling convolutional neural networks,” in Proceedings\\nof the 53rd Annual Meeting of the Association for Computational\\nLinguistics and the 7th International Joint Conference on Natural\\nLanguage Processing (Volume 1: Long Papers) , vol. 1, pp. 167–176,\\n2015.\\n[140] T. H. Nguyen and R. Grishman, “Graph convolutional networks with\\nargument-aware pooling for event detection,” in Thirty-Second AAAI\\nConference on Artiﬁcial Intelligence , 2018.\\n[141] T. Zhang, H. Ji, and A. Sil, “Joint entity and event extraction with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='argument-aware pooling for event detection,” in Thirty-Second AAAI\\nConference on Artiﬁcial Intelligence , 2018.\\n[141] T. Zhang, H. Ji, and A. Sil, “Joint entity and event extraction with\\ngenerative adversarial imitation learning,” Data Intelligence , vol. 1,\\nno. 2, pp. 99–120, 2019.\\n[142] W. Zhao, J. Zhang, J. Yang, T. He, H. Ma, and Z. Li, “A novel\\njoint biomedical event extraction framework via two-level modeling\\nof documents,” Information Sciences, vol. 550, pp. 27–40, 2021.\\n[143] T. Nasukawa and J. Yi, “Sentiment analysis: Capturing favorability\\nusing natural language processing,” in Proceedings of the 2nd Interna-\\ntional Conference on Knowledge Capture , pp. 70–77, ACM, 2003.\\n[144] K. Dave, S. Lawrence, and D. M. Pennock, “Mining the peanut gallery:\\nOpinion extraction and semantic classiﬁcation of product reviews,” in\\nProceedings of the 12th international conference on World Wide Web ,\\npp. 519–528, ACM, 2003.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='Opinion extraction and semantic classiﬁcation of product reviews,” in\\nProceedings of the 12th international conference on World Wide Web ,\\npp. 519–528, ACM, 2003.\\n[145] A. R. Pathak, B. Agarwal, M. Pandey, and S. Rautaray, “Application of\\ndeep learning approaches for sentiment analysis,” in Deep Learning-\\nBased Approaches for Sentiment Analysis , pp. 1–31, Springer, 2020.\\n[146] A. Yadav and D. K. Vishwakarma, “Sentiment analysis using deep\\nlearning architectures: a review,”Artiﬁcial Intelligence Review, vol. 53,\\nno. 6, pp. 4335–4385, 2020.\\n[147] D. Tang, B. Qin, and T. Liu, “Document modeling with gated recurrent\\nneural network for sentiment classiﬁcation,” in Proceedings of the\\n2015 conference on empirical methods in natural language processing ,\\npp. 1422–1432, 2015.\\n[148] X. Glorot, A. Bordes, and Y . Bengio, “Domain adaptation for large-\\nscale sentiment classiﬁcation: A deep learning approach,” in Proceed-\\nings of the 28th international conference on machine learning (ICML-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='scale sentiment classiﬁcation: A deep learning approach,” in Proceed-\\nings of the 28th international conference on machine learning (ICML-\\n11), pp. 513–520, 2011.\\n[149] G. Rao, W. Huang, Z. Feng, and Q. Cong, “Lstm with sentence\\nrepresentations for document-level sentiment classiﬁcation,” Neuro-\\ncomputing, vol. 308, pp. 49–57, 2018.\\n[150] M. Rhanoui, M. Mikram, S. Yousﬁ, and S. Barzali, “A cnn-bilstm\\nmodel for document-level sentiment analysis,” Machine Learning and\\nKnowledge Extraction, vol. 1, no. 3, pp. 832–847, 2019.\\n[151] R. Socher, J. Pennington, E. H. Huang, A. Y . Ng, and C. D. Manning,\\n“Semi-supervised recursive autoencoders for predicting sentiment dis-\\ntributions,” in Proceedings of the conference on empirical methods in\\nnatural language processing , pp. 151–161, Association for Computa-\\ntional Linguistics, 2011.\\n[152] X. Wang, Y . Liu, S. Chengjie, B. Wang, and X. Wang, “Predicting\\npolarities of tweets by composing word embeddings with long short-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='tional Linguistics, 2011.\\n[152] X. Wang, Y . Liu, S. Chengjie, B. Wang, and X. Wang, “Predicting\\npolarities of tweets by composing word embeddings with long short-\\nterm memory,” in Proceedings of the 53rd Annual Meeting of the\\nAssociation for Computational Linguistics and the 7th International\\nJoint Conference on Natural Language Processing (Volume 1: Long\\nPapers), vol. 1, pp. 1343–1353, 2015.\\n[153] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng,\\nand C. Potts, “Recursive deep models for semantic compositionality\\nover a sentiment treebank,” in Proceedings of the 2013 conference\\non empirical methods in natural language processing , pp. 1631–1642,\\n2013.\\n[154] R. Arulmurugan, K. Sabarmathi, and H. Anandakumar, “Classiﬁcation\\nof sentence level sentiment analysis using cloud machine learning\\ntechniques,” Cluster Computing, vol. 22, no. 1, pp. 1199–1209, 2019.\\n[155] D. Me ˇskel˙e and F. Frasincar, “Aldonar: A hybrid solution for sentence-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='techniques,” Cluster Computing, vol. 22, no. 1, pp. 1199–1209, 2019.\\n[155] D. Me ˇskel˙e and F. Frasincar, “Aldonar: A hybrid solution for sentence-\\nlevel aspect-based sentiment analysis using a lexicalized domain ontol-\\nogy and a regularized neural attention model,” Information Processing\\n& Management, vol. 57, no. 3, p. 102211, 2020.\\n[156] Y . Wang, M. Huang, L. Zhao,et al., “Attention-based LSTM for aspect-\\nlevel sentiment classiﬁcation,” in Proceedings of the 2016 Conference\\non Empirical Methods in Natural Language Processing , pp. 606–615,\\n2016.\\n[157] Y . Ma, H. Peng, T. Khan, E. Cambria, and A. Hussain, “Sentic lstm: a\\nhybrid network for targeted aspect-based sentiment analysis,”Cognitive\\nComputation, vol. 10, no. 4, pp. 639–650, 2018.\\n[158] H. Xu, B. Liu, L. Shu, and P. S. Yu, “BERT post-training for review\\nreading comprehension and aspect-based sentiment analysis,” arXiv\\npreprint arXiv:1904.02232, 2019.\\n[159] H. Xu, B. Liu, L. Shu, and P. S. Yu, “Double embeddings and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='reading comprehension and aspect-based sentiment analysis,” arXiv\\npreprint arXiv:1904.02232, 2019.\\n[159] H. Xu, B. Liu, L. Shu, and P. S. Yu, “Double embeddings and\\nCNN-based sequence labeling for aspect extraction,” arXiv preprint\\narXiv:1805.04601, 2018.\\n[160] H. H. Do, P. Prasad, A. Maag, and A. Alsadoon, “Deep learning for\\naspect-based sentiment analysis: a comparative review,”Expert Systems\\nwith Applications, vol. 118, pp. 272–299, 2019.\\n[161] S. Rida-E-Fatima, A. Javed, A. Banjar, A. Irtaza, H. Dawood, H. Da-\\nwood, and A. Alamri, “A multi-layer dual attention deep learning model\\nwith reﬁned word embeddings for aspect-based sentiment analysis,”\\nIEEE Access, vol. 7, pp. 114795–114807, 2019.\\n[162] Y . Liang, F. Meng, J. Zhang, J. Xu, Y . Chen, and J. Zhou, “A\\nnovel aspect-guided deep transition model for aspect based sentiment\\nanalysis,” arXiv preprint arXiv:1909.00324 , 2019.\\n[163] D. Jurafsky and J. H. Martin, Speech and Language Processing .\\nPrentice Hall, 2008.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 22\\n[164] N. Kalchbrenner and P. Blunsom, “Recurrent continuous translation\\nmodels,” in Proceedings of the 2013 Conference on Empirical Methods\\nin Natural Language Processing , pp. 1700–1709, 2013.\\n[165] S. P. Singh, A. Kumar, H. Darbari, L. Singh, A. Rastogi, and S. Jain,\\n“Machine translation using deep learning: An overview,” in 2017\\ninternational conference on computer, communications and electronics\\n(comptelix), pp. 162–167, IEEE, 2017.\\n[166] S. Yang, Y . Wang, and X. Chu, “A survey of deep learning techniques\\nfor neural machine translation,” arXiv preprint arXiv:2002.07526 ,\\n2020.\\n[167] L. E. Dostert, “The Georgetown-IBM experiment,” 1955). Machine\\ntranslation of languages. John Wiley & Sons, New York , pp. 124–135,\\n1955.\\n[168] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine translation by\\njointly learning to align and translate,” arXiv preprint arXiv:1409.0473,\\n2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='1955.\\n[168] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine translation by\\njointly learning to align and translate,” arXiv preprint arXiv:1409.0473,\\n2014.\\n[169] K. Cho, B. Van Merri ¨enboer, D. Bahdanau, and Y . Bengio, “On the\\nproperties of neural machine translation: Encoder-decoder approaches,”\\narXiv preprint arXiv:1409.1259 , 2014.\\n[170] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to sequence learning\\nwith neural networks,” in Advances in neural information processing\\nsystems, pp. 3104–3112, 2014.\\n[171] Y . Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi, W. Macherey,\\nM. Krikun, Y . Cao, Q. Gao, K. Macherey, et al. , “Google’s neural\\nmachine translation system: Bridging the gap between human and\\nmachine translation,” arXiv preprint arXiv:1609.08144 , 2016.\\n[172] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y . N. Dauphin,\\n“Convolutional sequence to sequence learning,” arXiv preprint\\narXiv:1705.03122, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='[172] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y . N. Dauphin,\\n“Convolutional sequence to sequence learning,” arXiv preprint\\narXiv:1705.03122, 2017.\\n[173] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in\\nAdvances in Neural Information Processing Systems , pp. 5998–6008,\\n2017.\\n[174] K. Ahmed, N. S. Keskar, and R. Socher, “Weighted transformer\\nnetwork for machine translation,” arXiv preprint arXiv:1711.02132 ,\\n2017.\\n[175] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative\\nposition representations,” arXiv preprint arXiv:1803.02155 , 2018.\\n[176] S. Edunov, M. Ott, M. Auli, and D. Grangier, “Understanding back-\\ntranslation at scale,” arXiv preprint arXiv:1808.09381 , 2018.\\n[177] R. Aharoni, M. Johnson, and O. Firat, “Massively multilingual neural\\nmachine translation,” 2019.\\n[178] J. Zhu, Y . Xia, L. Wu, D. He, T. Qin, W. Zhou, H. Li, and T.-Y . Liu,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='[177] R. Aharoni, M. Johnson, and O. Firat, “Massively multilingual neural\\nmachine translation,” 2019.\\n[178] J. Zhu, Y . Xia, L. Wu, D. He, T. Qin, W. Zhou, H. Li, and T.-Y . Liu,\\n“Incorporating bert into neural machine translation,” 2020.\\n[179] Y . Liu, J. Gu, N. Goyal, X. Li, S. Edunov, M. Ghazvininejad,\\nM. Lewis, and L. Zettlemoyer, “Multilingual denoising pre-training\\nfor neural machine translation,” Transactions of the Association for\\nComputational Linguistics, vol. 8, pp. 726–742, 2020.\\n[180] Y . Cheng, L. Jiang, and W. Macherey, “Robust neural machine transla-\\ntion with doubly adversarial inputs,” arXiv preprint arXiv:1906.02443,\\n2019.\\n[181] W. Zhang, Y . Feng, F. Meng, D. You, and Q. Liu, “Bridging the gap\\nbetween training and inference for neural machine translation,” 2019.\\n[182] J. Yang, M. Wang, H. Zhou, C. Zhao, W. Zhang, Y . Yu, and L. Li,\\n“Towards making the most of bert in neural machine translation,” in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='[182] J. Yang, M. Wang, H. Zhou, C. Zhao, W. Zhang, Y . Yu, and L. Li,\\n“Towards making the most of bert in neural machine translation,” in\\nProceedings of the AAAI Conference on Artiﬁcial Intelligence , vol. 34,\\npp. 9378–9385, 2020.\\n[183] A. Bordes, S. Chopra, and J. Weston, “Question answering with\\nsubgraph embeddings,” arXiv preprint arXiv:1406.3676 , 2014.\\n[184] B. F. Green Jr, A. K. Wolf, C. Chomsky, and K. Laughery, “Baseball:\\nan automatic question-answerer,” in Papers presented at the May 9-11,\\n1961, Western Joint IRE-AIEE-ACM Computer Conference , pp. 219–\\n224, ACM, 1961.\\n[185] A. Ittycheriah, M. Franz, W.-J. Zhu, A. Ratnaparkhi, and R. J. Mam-\\nmone, “IBM’s statistical question answering system.,” in TREC, 2000.\\n[186] H. Cui, R. Sun, K. Li, M.-Y . Kan, and T.-S. Chua, “Question answering\\npassage retrieval using dependency relations,” in Proceedings of the\\n28th annual international ACM SIGIR conference on Research and\\ndevelopment in information retrieval , pp. 400–407, ACM, 2005.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='passage retrieval using dependency relations,” in Proceedings of the\\n28th annual international ACM SIGIR conference on Research and\\ndevelopment in information retrieval , pp. 400–407, ACM, 2005.\\n[187] X. Qiu and X. Huang, “Convolutional neural tensor network architec-\\nture for community-based question answering.,” in IJCAI, pp. 1305–\\n1311, 2015.\\n[188] H. T. Ng, L. H. Teo, and J. L. P. Kwan, “A machine learning\\napproach to answering questions for reading comprehension tests,”\\nin Proceedings of the 2000 Joint SIGDAT conference on Empirical\\nmethods in natural language processing and very large corpora: held\\nin conjunction with the 38th Annual Meeting of the Association for\\nComputational Linguistics-Volume 13 , pp. 124–132, Association for\\nComputational Linguistics, 2000.\\n[189] C. Xiong, V . Zhong, and R. Socher, “Dynamic coattention networks\\nfor question answering,” arXiv preprint arXiv:1611.01604 , 2016.\\n[190] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zit-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='for question answering,” arXiv preprint arXiv:1611.01604 , 2016.\\n[190] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zit-\\nnick, and D. Parikh, “VQA: Visual question answering,” inProceedings\\nof the IEEE international conference on computer vision , pp. 2425–\\n2433, 2015.\\n[191] M. Malinowski, M. Rohrbach, and M. Fritz, “Ask your neurons:\\nA neural-based approach to answering questions about images,” in\\nProceedings of the IEEE international conference on computer vision ,\\npp. 1–9, 2015.\\n[192] H. Xu and K. Saenko, “Ask, attend and answer: Exploring question-\\nguided spatial attention for visual question answering,” in European\\nConference on Computer Vision , pp. 451–466, Springer, 2016.\\n[193] A. Das, H. Agrawal, L. Zitnick, D. Parikh, and D. Batra, “Human\\nattention in visual question answering: Do humans and deep networks\\nlook at the same regions?,”Computer Vision and Image Understanding,\\nvol. 163, pp. 90–100, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='attention in visual question answering: Do humans and deep networks\\nlook at the same regions?,”Computer Vision and Image Understanding,\\nvol. 163, pp. 90–100, 2017.\\n[194] H. Ben-Younes, R. Cadene, N. Thome, and M. Cord, “Block: Bilinear\\nsuperdiagonal fusion for visual question answering and visual relation-\\nship detection,” in Proceedings of the AAAI Conference on Artiﬁcial\\nIntelligence, vol. 33, pp. 8102–8109, 2019.\\n[195] J. Wu and R. J. Mooney, “Self-critical reasoning for robust visual\\nquestion answering,” 2019.\\n[196] R. Nallapati, F. Zhai, and B. Zhou, “SummaRuNNer: A recurrent\\nneural network based sequence model for extractive summarization of\\ndocuments.,” in AAAI, pp. 3075–3081, 2017.\\n[197] S. Narayan, S. B. Cohen, and M. Lapata, “Ranking sentences for ex-\\ntractive summarization with reinforcement learning,” in NAACL:HLT,\\nvol. 1, pp. 1747–1759, 2018.\\n[198] A. M. Rush, S. Chopra, and J. Weston, “A neural attention model for\\nabstractive sentence summarization,” in EMNLP, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='vol. 1, pp. 1747–1759, 2018.\\n[198] A. M. Rush, S. Chopra, and J. Weston, “A neural attention model for\\nabstractive sentence summarization,” in EMNLP, 2015.\\n[199] J. Tan, X. Wan, and J. Xiao, “Abstractive document summarization with\\na graph-based attentional neural model,” inACL, vol. 1, pp. 1171–1181,\\n2017.\\n[200] R. Nallapati, B. Zhou, C. dos Santos, C. Gulcehre, and B. Xiang,\\n“Abstractive text summarization using sequence-to-sequence RNNs\\nand beyond,” in Proceedings of The 20th SIGNLL Conference on\\nComputational Natural Language Learning , pp. 280–290, 2016.\\n[201] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay,\\nM. Suleyman, and P. Blunsom, “Teaching machines to read and\\ncomprehend,” in NIPS, pp. 1693–1701, 2015.\\n[202] J. Gu, Z. Lu, H. Li, and V . O. Li, “Incorporating copying mechanism in\\nsequence-to-sequence learning,” in ACL, vol. 1, pp. 1631–1640, 2016.\\n[203] Y .-C. Chen and M. Bansal, “Fast abstractive summarization with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='sequence-to-sequence learning,” in ACL, vol. 1, pp. 1631–1640, 2016.\\n[203] Y .-C. Chen and M. Bansal, “Fast abstractive summarization with\\nreinforce-selected sentence rewriting,” in ACL, 2018.\\n[204] Q. Zhou, N. Yang, F. Wei, S. Huang, M. Zhou, and T. Zhao, “Neu-\\nral document summarization by jointly learning to score and select\\nsentences,” in ACL, pp. 654–663, ACL, 2018.\\n[205] T. Shi, Y . Keneshloo, N. Ramakrishnan, and C. K. Reddy, “Neural\\nabstractive text summarization with sequence-to-sequence models,”\\narXiv preprint arXiv:1812.02303 , 2018.\\n[206] C. Ma, W. E. Zhang, M. Guo, H. Wang, and Q. Z. Sheng, “Multi-\\ndocument summarization via deep learning techniques: A survey,”\\narXiv preprint arXiv:2011.04843 , 2020.\\n[207] A. Abdi, S. Hasan, S. M. Shamsuddin, N. Idris, and J. Piran, “A hybrid\\ndeep learning architecture for opinion-oriented multi-document sum-\\nmarization based on multi-feature fusion,” Knowledge-Based Systems,\\nvol. 213, p. 106658, 2021.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='deep learning architecture for opinion-oriented multi-document sum-\\nmarization based on multi-feature fusion,” Knowledge-Based Systems,\\nvol. 213, p. 106658, 2021.\\n[208] X. Zhang, F. Wei, and M. Zhou, “Hibert: Document level pre-training\\nof hierarchical bidirectional transformers for document summariza-\\ntion,” arXiv preprint arXiv:1905.06566 , 2019.\\n[209] E. Merdivan, D. Singh, S. Hanke, and A. Holzinger, “Dialogue sys-\\ntems for intelligent human computer interactions,” Electronic Notes in\\nTheoretical Computer Science , vol. 343, pp. 57–71, 2019.\\n[210] D. Hakkani-T ¨ur, G. T ¨ur, A. Celikyilmaz, Y .-N. Chen, J. Gao, L. Deng,\\nand Y .-Y . Wang, “Multi-domain joint semantic frame parsing using\\nbi-directional RNN-LSTM,” in Interspeech, pp. 715–719, 2016.\\n[211] C. Toxtli, J. Cranshaw, et al. , “Understanding chatbot-mediated task\\nmanagement,” in Proceedings of the 2018 CHI Conference on Human\\nFactors in Computing Systems , p. 58, ACM, 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='[211] C. Toxtli, J. Cranshaw, et al. , “Understanding chatbot-mediated task\\nmanagement,” in Proceedings of the 2018 CHI Conference on Human\\nFactors in Computing Systems , p. 58, ACM, 2018.\\n[212] V . Ilievski, C. Musat, A. Hossmann, and M. Baeriswyl, “Goal-oriented\\nchatbot dialog management bootstrapping with transfer learning,”arXiv\\npreprint arXiv:1802.00500, 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 22, 'page_label': '23', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 23\\n[213] J. Li, W. Monroe, A. Ritter, D. Jurafsky, M. Galley, and J. Gao, “Deep\\nreinforcement learning for dialogue generation,” in Proceedings of the\\nConference on Empirical Methods in Natural Language Processing ,\\npp. 1192–1202, 2016.\\n[214] T.-H. Wen, D. Vandyke, N. Mrksic, M. Gasic, L. M. Rojas-Barahona,\\nP.-H. Su, S. Ultes, and S. Young, “A network-based end-to-end train-\\nable task-oriented dialogue system,” arXiv preprint arXiv:1604.04562,\\n2016.\\n[215] J. D. Williams and G. Zweig, “End-to-end LSTM-based dialog control\\noptimized with supervised and reinforcement learning,” arXiv preprint\\narXiv:1606.01269, 2016.\\n[216] S. Sukhbaatar, J. Weston, R. Fergus, et al. , “End-to-end memory\\nnetworks,” in Advances in neural information processing systems ,\\npp. 2440–2448, 2015.\\n[217] A. Bordes, Y .-L. Boureau, and J. Weston, “Learning end-to-end goal-\\noriented dialog,” arXiv preprint arXiv:1605.07683 , 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 22, 'page_label': '23', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='pp. 2440–2448, 2015.\\n[217] A. Bordes, Y .-L. Boureau, and J. Weston, “Learning end-to-end goal-\\noriented dialog,” arXiv preprint arXiv:1605.07683 , 2016.\\n[218] A. Ritter, C. Cherry, and W. B. Dolan, “Data-driven response generation\\nin social media,” in Proceedings of the conference on empirical\\nmethods in natural language processing , pp. 583–593, Association for\\nComputational Linguistics, 2011.\\n[219] Z. Ji, Z. Lu, and H. Li, “An information retrieval approach to short\\ntext conversation,” arXiv preprint arXiv:1408.6988 , 2014.\\n[220] B. Hu, Z. Lu, H. Li, and Q. Chen, “Convolutional neural network\\narchitectures for matching natural language sentences,” in Advances in\\nneural information processing systems , pp. 2042–2050, 2014.\\n[221] R. Lowe, N. Pow, I. Serban, and J. Pineau, “The Ubuntu dialogue\\ncorpus: A large dataset for research in unstructured multi-turn dialogue\\nsystems,” arXiv preprint arXiv:1506.08909 , 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 22, 'page_label': '23', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='[221] R. Lowe, N. Pow, I. Serban, and J. Pineau, “The Ubuntu dialogue\\ncorpus: A large dataset for research in unstructured multi-turn dialogue\\nsystems,” arXiv preprint arXiv:1506.08909 , 2015.\\n[222] R. Yan, Y . Song, and H. Wu, “Learning to respond with deep neural\\nnetworks for retrieval-based human-computer conversation system,”\\nin Proceedings of the 39th International ACM SIGIR conference on\\nResearch and Development in Information Retrieval , pp. 55–64, ACM,\\n2016.\\n[223] X. Zhou, L. Li, D. Dong, Y . Liu, Y . Chen, W. X. Zhao, D. Yu, and\\nH. Wu, “Multi-turn response selection for chatbots with deep attention\\nmatching network,” in Proceedings of the 56th Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers) ,\\nvol. 1, pp. 1118–1127, 2018.\\n[224] S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Jozefowicz, and\\nS. Bengio, “Generating sentences from a continuous space,” arXiv\\npreprint arXiv:1511.06349, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-03-02T01:16:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-03-02T01:16:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf', 'total_pages': 23, 'page': 22, 'page_label': '23', 'source_file': 'NLP Advancements by Deep Learning.pdf', 'file_type': 'pdf'}, page_content='vol. 1, pp. 1118–1127, 2018.\\n[224] S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Jozefowicz, and\\nS. Bengio, “Generating sentences from a continuous space,” arXiv\\npreprint arXiv:1511.06349, 2015.\\n[225] A. Kannan and O. Vinyals, “Adversarial evaluation of dialogue mod-\\nels,” arXiv preprint arXiv:1701.08198 , 2017.\\n[226] O. Vinyals and Q. Le, “A neural conversational model,” arXiv preprint\\narXiv:1506.05869, 2015.')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6d085993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List , Dict,Any,Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b3c79c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)), '(Request ID: 71340c8c-ff59-4627-865f-b4773edc0b42)')' thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x194a0d67770>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "\n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "\n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "\n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "\n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "    def get_embedding_dimension(self) -> int:\n",
    "        \"\"\"Get the Dimension of the Embedding model\"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model Not Loaded\")\n",
    "        return self.model.get_sentence_embedding_dimension()\n",
    "    \n",
    "\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "18074304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. collection:pdf_documents\n",
      "Existing docuement in collection:1630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x194832cae40>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a chromaDB vector store\"\"\"\n",
    "\n",
    "    def __init__(self, collection_name:str = \"pdf_documents\",persist_directory:str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the chromaDB collection\n",
    "            persist_directory: Dorectory to persist the vector store\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory  = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize chromaDB client and collectins\"\"\"\n",
    "\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory,exist_ok=True)\n",
    "            self.client=chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            self.collection=self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\":\"PDF document embedding for RAG\"}\n",
    "            )\n",
    "\n",
    "            print(f\"Vector store initialized. collection:{self.collection_name}\")\n",
    "            print(f\"Existing docuement in collection:{self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store:{e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def add_documents(self,documents:List[Any],embeddings:np.ndarray):\n",
    "        \"\"\"\n",
    "        Add document and their embeddings to the vector store\n",
    "\n",
    "        Args:\n",
    "            documents: List of Langchain dosuments\n",
    "            embeddings:corresponding embeddings for the documents \n",
    "        \"\"\"\n",
    "\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of Documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store....\")\n",
    "\n",
    "        #prepare data for chromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i,(doc,embedding) in enumerate (zip(documents, embeddings)):\n",
    "            #GEnerate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            #prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            #document content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            #Embeddings\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        #Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "\n",
    "            print(f\"successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore = VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "63ed78b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 815 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 26/26 [00:53<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (815, 384)\n",
      "Adding 815 documents to vector store....\n",
      "successfully added 815 documents to vector store\n",
      "Total documents in collection: 2445\n"
     ]
    }
   ],
   "source": [
    "### Convert the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the Embeddings\n",
    "\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##store int he vector dtaabase\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365aa13b",
   "metadata": {},
   "source": [
    "RAG Retrival Pipeline from vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "40083893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c3bd9ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x194a0b22ba0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5984a6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is NLP'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_3b80b95e_647',\n",
       "  'content': 'of data. Data becomes more meaningful through a deeper\\nunderstanding of its context, which in turn facilitates text\\nanalysis and mining. NLP enables this with the communication\\nstructures and patterns of humans.\\nDevelopment of NLP methods is increasingly reliant on\\ndata-driven approaches which help with building more pow-\\nerful and robust models [2]–[4]. Recent advances in com-\\nputational power, as well as greater availability of big data,\\nenable deep learning, one of the most appealing approaches\\nin the NLP domain [2], [3], [5], especially given that deep\\nlearning has already demonstrated superior performance in\\nadjoining ﬁelds like Computer Vision [6]–[10] and Speech\\nRecognition [11]–[13]. These developments led to a paradigm\\nshift from traditional to novel data-driven approaches aimed\\nat advancing NLP. The reason behind this shift was simple:\\nnew approaches are more promising regarding results, and are\\neasier to engineer.',\n",
       "  'metadata': {'source_file': 'NLP Advancements by Deep Learning.pdf',\n",
       "   'title': '',\n",
       "   'page': 0,\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'content_length': 938,\n",
       "   'total_pages': 23,\n",
       "   'moddate': '2021-03-02T01:16:19+00:00',\n",
       "   'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf',\n",
       "   'trapped': '/False',\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'creationdate': '2021-03-02T01:16:19+00:00',\n",
       "   'keywords': '',\n",
       "   'page_label': '1',\n",
       "   'doc_index': 647,\n",
       "   'author': '',\n",
       "   'subject': ''},\n",
       "  'similarity_score': 0.087918221950531,\n",
       "  'distance': 0.912081778049469,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_bfff9054_647',\n",
       "  'content': 'of data. Data becomes more meaningful through a deeper\\nunderstanding of its context, which in turn facilitates text\\nanalysis and mining. NLP enables this with the communication\\nstructures and patterns of humans.\\nDevelopment of NLP methods is increasingly reliant on\\ndata-driven approaches which help with building more pow-\\nerful and robust models [2]–[4]. Recent advances in com-\\nputational power, as well as greater availability of big data,\\nenable deep learning, one of the most appealing approaches\\nin the NLP domain [2], [3], [5], especially given that deep\\nlearning has already demonstrated superior performance in\\nadjoining ﬁelds like Computer Vision [6]–[10] and Speech\\nRecognition [11]–[13]. These developments led to a paradigm\\nshift from traditional to novel data-driven approaches aimed\\nat advancing NLP. The reason behind this shift was simple:\\nnew approaches are more promising regarding results, and are\\neasier to engineer.',\n",
       "  'metadata': {'source_file': 'NLP Advancements by Deep Learning.pdf',\n",
       "   'total_pages': 23,\n",
       "   'title': '',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'page': 0,\n",
       "   'subject': '',\n",
       "   'content_length': 938,\n",
       "   'creationdate': '2021-03-02T01:16:19+00:00',\n",
       "   'author': '',\n",
       "   'doc_index': 647,\n",
       "   'keywords': '',\n",
       "   'file_type': 'pdf',\n",
       "   'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf',\n",
       "   'page_label': '1',\n",
       "   'trapped': '/False',\n",
       "   'moddate': '2021-03-02T01:16:19+00:00',\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'creator': 'LaTeX with hyperref'},\n",
       "  'similarity_score': 0.087918221950531,\n",
       "  'distance': 0.912081778049469,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_6998edc2_647',\n",
       "  'content': 'of data. Data becomes more meaningful through a deeper\\nunderstanding of its context, which in turn facilitates text\\nanalysis and mining. NLP enables this with the communication\\nstructures and patterns of humans.\\nDevelopment of NLP methods is increasingly reliant on\\ndata-driven approaches which help with building more pow-\\nerful and robust models [2]–[4]. Recent advances in com-\\nputational power, as well as greater availability of big data,\\nenable deep learning, one of the most appealing approaches\\nin the NLP domain [2], [3], [5], especially given that deep\\nlearning has already demonstrated superior performance in\\nadjoining ﬁelds like Computer Vision [6]–[10] and Speech\\nRecognition [11]–[13]. These developments led to a paradigm\\nshift from traditional to novel data-driven approaches aimed\\nat advancing NLP. The reason behind this shift was simple:\\nnew approaches are more promising regarding results, and are\\neasier to engineer.',\n",
       "  'metadata': {'file_type': 'pdf',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'doc_index': 647,\n",
       "   'page_label': '1',\n",
       "   'moddate': '2021-03-02T01:16:19+00:00',\n",
       "   'source_file': 'NLP Advancements by Deep Learning.pdf',\n",
       "   'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'author': '',\n",
       "   'title': '',\n",
       "   'content_length': 938,\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'subject': '',\n",
       "   'trapped': '/False',\n",
       "   'total_pages': 23,\n",
       "   'creationdate': '2021-03-02T01:16:19+00:00',\n",
       "   'keywords': '',\n",
       "   'page': 0},\n",
       "  'similarity_score': 0.087918221950531,\n",
       "  'distance': 0.912081778049469,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_7982d906_650',\n",
       "  'content': 'Section 2 lays out the theoretical dimensions of NLP and\\nartiﬁcial intelligence, and looks at deep learning as an ap-\\nproach to solving real-world problems. It motivates this study\\nby addressing the question: Why use deep learning in NLP?\\nThe third section discusses fundamental concepts necessary\\nto understand NLP, covering exemplary issues in representa-\\ntion, frameworks, and machine learning. The fourth section\\nsummarizes benchmark datasets employed in the NLP domain.\\nSection 5 focuses on some of the NLP applications where deep\\nlearning has demonstrated signiﬁcant beneﬁt. Finally, Section\\n6 provides a conclusion, also addressing some open problems\\nand promising areas for improvement.\\nII. B ACKGROUND\\nNLP has long been viewed as one aspect of artiﬁcial\\nintelligence (AI), since understanding and generating natural\\nlanguage are high-level indications of intelligence. Deep learn-\\ning is an effective AI tool, so we next situate deep learning in',\n",
       "  'metadata': {'title': '',\n",
       "   'creationdate': '2021-03-02T01:16:19+00:00',\n",
       "   'keywords': '',\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'author': '',\n",
       "   'content_length': 954,\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'doc_index': 650,\n",
       "   'file_type': 'pdf',\n",
       "   'total_pages': 23,\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'source_file': 'NLP Advancements by Deep Learning.pdf',\n",
       "   'page': 0,\n",
       "   'subject': '',\n",
       "   'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf',\n",
       "   'page_label': '1',\n",
       "   'trapped': '/False',\n",
       "   'moddate': '2021-03-02T01:16:19+00:00'},\n",
       "  'similarity_score': 0.07082951068878174,\n",
       "  'distance': 0.9291704893112183,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_7dc35a1f_650',\n",
       "  'content': 'Section 2 lays out the theoretical dimensions of NLP and\\nartiﬁcial intelligence, and looks at deep learning as an ap-\\nproach to solving real-world problems. It motivates this study\\nby addressing the question: Why use deep learning in NLP?\\nThe third section discusses fundamental concepts necessary\\nto understand NLP, covering exemplary issues in representa-\\ntion, frameworks, and machine learning. The fourth section\\nsummarizes benchmark datasets employed in the NLP domain.\\nSection 5 focuses on some of the NLP applications where deep\\nlearning has demonstrated signiﬁcant beneﬁt. Finally, Section\\n6 provides a conclusion, also addressing some open problems\\nand promising areas for improvement.\\nII. B ACKGROUND\\nNLP has long been viewed as one aspect of artiﬁcial\\nintelligence (AI), since understanding and generating natural\\nlanguage are high-level indications of intelligence. Deep learn-\\ning is an effective AI tool, so we next situate deep learning in',\n",
       "  'metadata': {'subject': '',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'doc_index': 650,\n",
       "   'file_type': 'pdf',\n",
       "   'page_label': '1',\n",
       "   'author': '',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'title': '',\n",
       "   'trapped': '/False',\n",
       "   'content_length': 954,\n",
       "   'keywords': '',\n",
       "   'total_pages': 23,\n",
       "   'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf',\n",
       "   'source_file': 'NLP Advancements by Deep Learning.pdf',\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'moddate': '2021-03-02T01:16:19+00:00',\n",
       "   'creationdate': '2021-03-02T01:16:19+00:00',\n",
       "   'page': 0},\n",
       "  'similarity_score': 0.07082951068878174,\n",
       "  'distance': 0.9291704893112183,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is NLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3686d063",
   "metadata": {},
   "source": [
    "RAG Pipeline- VectorDB To LLM Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aec47e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyBKZ39T9km8CoyE0MaMrnsS7bJVWwYP-04\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "02bc338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "487b4d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiLLM:\n",
    "    def __init__(self, model_name: str = \"Gemini 2.5 Pro\", api_key: str = None):\n",
    "        \"\"\"\n",
    "        Initialize Gemini LLM\n",
    "        \n",
    "        Args:\n",
    "            model_name: Gemini model name (e.g., \"gemini-1\", \"gemini-1.5\", etc.)\n",
    "            api_key: Google Cloud API key or set GOOGLE_API_KEY environment variable\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key or os.environ.get(\"GOOGLE_API_KEY\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Google API key is required. Set GOOGLE_API_KEY environment variable or pass api_key parameter.\")\n",
    "        \n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            model=self.model_name,\n",
    "            api_key=self.api_key,\n",
    "            temperature=0.1,\n",
    "            max_output_tokens=1024\n",
    "        )\n",
    "        \n",
    "        print(f\"Initialized Gemini LLM with model: {self.model_name}\")\n",
    "\n",
    "    def generate_response(self, query: str, context: str, max_length: int = 500) -> str:\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=\"\"\"You are a helpful AI assistant. Use the following context to answer the question accurately and concisely.\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            Answer: Provide a clear and informative answer based on the context above. If the context doesn't contain enough information to answer the question, say so.\"\"\"\n",
    "                    )\n",
    "        \n",
    "        formatted_prompt = prompt_template.format(context=context, question=query)\n",
    "        \n",
    "        try:\n",
    "            messages = [HumanMessage(content=formatted_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "        \n",
    "    def generate_response_simple(self, query: str, context: str) -> str:\n",
    "        simple_prompt = f\"\"\"Based on this context: {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            messages = [HumanMessage(content=simple_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3c62e249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Gemini LLM with model: Gemini 2.5 Pro\n",
      "Gemini LLM initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# from google.cloud.aiplatform.gapic import PredictionServiceClient\n",
    "\n",
    "# Initialize Gemini LLM (you'll need to set GOOGLE_API_KEY environment variable)\n",
    "try:\n",
    "    gemini_llm = GeminiLLM(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "    print(\"Gemini LLM initialized successfully!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    print(\"Please set your GOOGLE_API_KEY environment variable to use the LLM.\")\n",
    "    gemini_llm = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cfcb3f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'what is Deep Learning'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 58.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_c6bc9693_653',\n",
       "  'content': 'This leaves two choices: (1) much or even most existing data\\nmust be ignored or (2) AI must be developed to process the\\nvast volumes of available data into the essential pieces of\\ninformation that decision-makers and others can comprehend.\\nDeep learning is a bridge between the massive amounts of\\ndata and AI.\\n1) Deﬁnitions: Deep learning refers to applying deep neu-\\nral networks to massive amounts of data to learn a procedure\\naimed at handling a task . The task can range from simple\\nclassiﬁcation to complex reasoning. In other words, deep\\nlearning is a set of mechanisms ideally capable of deriving an\\noptimum solution to any problem given a sufﬁciently extensive\\nand relevant input dataset. Loosely speaking, deep learning\\nis detecting and analyzing important structures/features in the\\ndata aimed at formulating a solution to a given problem. Here,\\nAI and deep learning meet. One version of the goal or ambition\\nbehind AI is enabling a machine to outperform what the human',\n",
       "  'metadata': {'subject': '',\n",
       "   'source_file': 'NLP Advancements by Deep Learning.pdf',\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'page': 1,\n",
       "   'keywords': '',\n",
       "   'author': '',\n",
       "   'creationdate': '2021-03-02T01:16:19+00:00',\n",
       "   'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'trapped': '/False',\n",
       "   'content_length': 979,\n",
       "   'file_type': 'pdf',\n",
       "   'doc_index': 653,\n",
       "   'page_label': '2',\n",
       "   'title': '',\n",
       "   'total_pages': 23,\n",
       "   'moddate': '2021-03-02T01:16:19+00:00'},\n",
       "  'similarity_score': 0.3916710615158081,\n",
       "  'distance': 0.6083289384841919,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_907185d3_653',\n",
       "  'content': 'This leaves two choices: (1) much or even most existing data\\nmust be ignored or (2) AI must be developed to process the\\nvast volumes of available data into the essential pieces of\\ninformation that decision-makers and others can comprehend.\\nDeep learning is a bridge between the massive amounts of\\ndata and AI.\\n1) Deﬁnitions: Deep learning refers to applying deep neu-\\nral networks to massive amounts of data to learn a procedure\\naimed at handling a task . The task can range from simple\\nclassiﬁcation to complex reasoning. In other words, deep\\nlearning is a set of mechanisms ideally capable of deriving an\\noptimum solution to any problem given a sufﬁciently extensive\\nand relevant input dataset. Loosely speaking, deep learning\\nis detecting and analyzing important structures/features in the\\ndata aimed at formulating a solution to a given problem. Here,\\nAI and deep learning meet. One version of the goal or ambition\\nbehind AI is enabling a machine to outperform what the human',\n",
       "  'metadata': {'trapped': '/False',\n",
       "   'page_label': '2',\n",
       "   'content_length': 979,\n",
       "   'creationdate': '2021-03-02T01:16:19+00:00',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'source_file': 'NLP Advancements by Deep Learning.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'subject': '',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'moddate': '2021-03-02T01:16:19+00:00',\n",
       "   'doc_index': 653,\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'keywords': '',\n",
       "   'page': 1,\n",
       "   'total_pages': 23,\n",
       "   'author': '',\n",
       "   'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf',\n",
       "   'title': ''},\n",
       "  'similarity_score': 0.3916710615158081,\n",
       "  'distance': 0.6083289384841919,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_348a3b0d_653',\n",
       "  'content': 'This leaves two choices: (1) much or even most existing data\\nmust be ignored or (2) AI must be developed to process the\\nvast volumes of available data into the essential pieces of\\ninformation that decision-makers and others can comprehend.\\nDeep learning is a bridge between the massive amounts of\\ndata and AI.\\n1) Deﬁnitions: Deep learning refers to applying deep neu-\\nral networks to massive amounts of data to learn a procedure\\naimed at handling a task . The task can range from simple\\nclassiﬁcation to complex reasoning. In other words, deep\\nlearning is a set of mechanisms ideally capable of deriving an\\noptimum solution to any problem given a sufﬁciently extensive\\nand relevant input dataset. Loosely speaking, deep learning\\nis detecting and analyzing important structures/features in the\\ndata aimed at formulating a solution to a given problem. Here,\\nAI and deep learning meet. One version of the goal or ambition\\nbehind AI is enabling a machine to outperform what the human',\n",
       "  'metadata': {'title': '',\n",
       "   'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf',\n",
       "   'content_length': 979,\n",
       "   'trapped': '/False',\n",
       "   'author': '',\n",
       "   'page': 1,\n",
       "   'keywords': '',\n",
       "   'total_pages': 23,\n",
       "   'file_type': 'pdf',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'page_label': '2',\n",
       "   'doc_index': 653,\n",
       "   'moddate': '2021-03-02T01:16:19+00:00',\n",
       "   'subject': '',\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'creationdate': '2021-03-02T01:16:19+00:00',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'source_file': 'NLP Advancements by Deep Learning.pdf'},\n",
       "  'similarity_score': 0.3916710615158081,\n",
       "  'distance': 0.6083289384841919,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_39970960_651',\n",
       "  'content': 'intelligence (AI), since understanding and generating natural\\nlanguage are high-level indications of intelligence. Deep learn-\\ning is an effective AI tool, so we next situate deep learning in\\nthe AI world. After that we explain motivations for applying\\ndeep learning to NLP.\\nA. Artiﬁcial Intelligence and Deep Learning\\nThere have been “islands of success” where big data are\\nprocessed via AI capabilities to produce information to achieve\\ncritical operational goals (e.g., fraud detection). Accordingly,\\n1Learning from training data to predict the type of new unseen test examples\\nby mapping them to known pre-deﬁned labels.\\n2Making sense of data without sticking to speciﬁc tasks and supervisory\\nsignals.\\narXiv:2003.01200v4  [cs.CL]  27 Feb 2021',\n",
       "  'metadata': {'keywords': '',\n",
       "   'page': 0,\n",
       "   'title': '',\n",
       "   'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf',\n",
       "   'moddate': '2021-03-02T01:16:19+00:00',\n",
       "   'trapped': '/False',\n",
       "   'file_type': 'pdf',\n",
       "   'page_label': '1',\n",
       "   'content_length': 746,\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'subject': '',\n",
       "   'source_file': 'NLP Advancements by Deep Learning.pdf',\n",
       "   'doc_index': 651,\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'author': '',\n",
       "   'creationdate': '2021-03-02T01:16:19+00:00',\n",
       "   'total_pages': 23},\n",
       "  'similarity_score': 0.14470791816711426,\n",
       "  'distance': 0.8552920818328857,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_39a8999e_651',\n",
       "  'content': 'intelligence (AI), since understanding and generating natural\\nlanguage are high-level indications of intelligence. Deep learn-\\ning is an effective AI tool, so we next situate deep learning in\\nthe AI world. After that we explain motivations for applying\\ndeep learning to NLP.\\nA. Artiﬁcial Intelligence and Deep Learning\\nThere have been “islands of success” where big data are\\nprocessed via AI capabilities to produce information to achieve\\ncritical operational goals (e.g., fraud detection). Accordingly,\\n1Learning from training data to predict the type of new unseen test examples\\nby mapping them to known pre-deﬁned labels.\\n2Making sense of data without sticking to speciﬁc tasks and supervisory\\nsignals.\\narXiv:2003.01200v4  [cs.CL]  27 Feb 2021',\n",
       "  'metadata': {'source_file': 'NLP Advancements by Deep Learning.pdf',\n",
       "   'subject': '',\n",
       "   'total_pages': 23,\n",
       "   'page': 0,\n",
       "   'page_label': '1',\n",
       "   'content_length': 746,\n",
       "   'author': '',\n",
       "   'trapped': '/False',\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'file_type': 'pdf',\n",
       "   'moddate': '2021-03-02T01:16:19+00:00',\n",
       "   'title': '',\n",
       "   'creationdate': '2021-03-02T01:16:19+00:00',\n",
       "   'doc_index': 651,\n",
       "   'source': '..\\\\data\\\\pdf\\\\NLP Advancements by Deep Learning.pdf',\n",
       "   'keywords': '',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'creator': 'LaTeX with hyperref'},\n",
       "  'similarity_score': 0.14470791816711426,\n",
       "  'distance': 0.8552920818328857,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get the context from the retriever and pass it to the LLM\n",
    "\n",
    "rag_retriever.retrieve(\"what is Deep Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e4d75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Gemini LLM (set GOOGLE_API_KEY in environment)\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"Gemini 2.5 Pro\",  # or whichever Gemini model you want\n",
    "    api_key=google_api_key,\n",
    "    temperature=0.1,\n",
    "    max_output_tokens=1024\n",
    ")\n",
    "\n",
    "def rag_simple(query, retriever, llm, top_k=3):\n",
    "    # Retrieve the context\n",
    "    results = retriever.retrieve(query, top_k=top_k)\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    \n",
    "    # Generate the answer using Gemini LLM\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "        \n",
    "    # Gemini LLM expects a list of messages (HumanMessage instances)\n",
    "    from langchain_core.messages import HumanMessage\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d474bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is NLP?'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[93]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m answer=\u001b[43mrag_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is NLP?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mrag_retriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[92]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mrag_simple\u001b[39m\u001b[34m(query, retriever, llm, top_k)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Gemini LLM expects a list of messages (HumanMessage instances)\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m].text\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\OneDrive\\Documents\\RAG Project\\RAG-Project\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:824\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    809\u001b[39m inheritable_metadata = {\n\u001b[32m    810\u001b[39m     **(metadata \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    811\u001b[39m     **\u001b[38;5;28mself\u001b[39m._get_ls_params(stop=stop, **kwargs),\n\u001b[32m    812\u001b[39m }\n\u001b[32m    814\u001b[39m callback_manager = CallbackManager.configure(\n\u001b[32m    815\u001b[39m     callbacks,\n\u001b[32m    816\u001b[39m     \u001b[38;5;28mself\u001b[39m.callbacks,\n\u001b[32m   (...)\u001b[39m\u001b[32m    821\u001b[39m     \u001b[38;5;28mself\u001b[39m.metadata,\n\u001b[32m    822\u001b[39m )\n\u001b[32m    823\u001b[39m messages_to_trace = [\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[43m_format_for_tracing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_list\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m message_list \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[32m    825\u001b[39m ]\n\u001b[32m    826\u001b[39m run_managers = callback_manager.on_chat_model_start(\n\u001b[32m    827\u001b[39m     \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m    828\u001b[39m     messages_to_trace,\n\u001b[32m   (...)\u001b[39m\u001b[32m    833\u001b[39m     batch_size=\u001b[38;5;28mlen\u001b[39m(messages),\n\u001b[32m    834\u001b[39m )\n\u001b[32m    835\u001b[39m results = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\OneDrive\\Documents\\RAG Project\\RAG-Project\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:125\u001b[39m, in \u001b[36m_format_for_tracing\u001b[39m\u001b[34m(messages)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages:\n\u001b[32m    124\u001b[39m     message_to_trace = message\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mmessage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    126\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(message.content):\n\u001b[32m    127\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(block, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    128\u001b[39m                 \u001b[38;5;66;03m# Update image content blocks to OpenAI # Chat Completions format.\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'tuple' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"What is NLP?\",rag_retriever,llm)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG-Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
